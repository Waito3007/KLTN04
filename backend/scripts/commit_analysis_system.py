# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import warnings
warnings.filterwarnings('ignore')

class CommitAnalysisSystem:
    def __init__(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
        self.vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            ngram_range=(1, 1)
        )
        self.model = RandomForestClassifier(
            n_estimators=30,
            max_depth=8,
            n_jobs=1,
            class_weight='balanced'
        )
        self.client = None

    def init_dask_client(self):
        """Kh·ªüi t·∫°o Dask client"""
        self.client = Client(n_workers=2, threads_per_worker=1, memory_limit='2GB')

    @staticmethod
    def lightweight_heuristic(msg):
        """H√†m heuristic tƒ©nh ƒë·ªÉ x·ª≠ l√Ω song song"""
        if not isinstance(msg, str) or not msg.strip():
            return 0
        msg = msg.lower()[:150]
        return int(any(kw in msg for kw in ['fix', 'bug', 'error', 'fail']))

    def process_large_file(self, input_path, output_dir):
        """X·ª≠ l√Ω file l·ªõn v·ªõi Dask """
        try:
            if self.client:
                self.client.close()
            self.init_dask_client()

            # ƒê·ªçc file v·ªõi Dask
            ddf = dd.read_csv(
                str(input_path),
                blocksize="20MB",
                dtype={'message': 'string'},
                usecols=['commit', 'message'],
                na_values=['', 'NA', 'N/A', 'nan']
            )
            
            # S·ª≠a l·ªói: Thay .notna() b·∫±ng .notnull() cho Dask
            ddf = ddf[ddf['message'].notnull()]
            
            # G√°n nh√£n
            ddf['is_critical'] = ddf['message'].map(
                self.lightweight_heuristic,
                meta=('is_critical', 'int8')
            )
            
           # L∆∞u k·∫øt qu·∫£ (ƒë√£ s·ª≠a ph·∫ßn compute)
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # S·ª≠a l·ªói: G·ªçi compute() tr·ª±c ti·∫øp tr√™n to_csv()
            ddf.to_csv(
                str(output_dir / "part_*.csv"),
                index=False
            )
            
            return True
        except Exception as e:
            print(f"üö® L·ªói x·ª≠ l√Ω file: {str(e)}")
            return False
        finally:
            if self.client:
                self.client.close()

    def clean_data(self, df):
        """L√†m s·∫°ch d·ªØ li·ªáu"""
        if 'message' not in df.columns:
            raise ValueError("Thi·∫øu c·ªôt 'message' trong d·ªØ li·ªáu")
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df):
        """G√°n nh√£n t·ª± ƒë·ªông"""
        df = self.clean_data(df)
        df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
        return df

    def train_model(self, df):
        """Hu·∫•n luy·ªán m√¥ h√¨nh"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical'].values
        self.model.fit(X, y)

    def evaluate(self, test_df):
        """ƒê√°nh gi√° m√¥ h√¨nh"""
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        print(classification_report(y_test, self.model.predict(X_test)))

    def save_model(self, path):
        """L∆∞u m√¥ h√¨nh"""
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer
        }, str(path))

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch commit...")
    system = CommitAnalysisSystem()
    
    input_path = Path("D:/Project/KLTN04/data/oneline.csv")
    output_dir = Path("D:/Project/KLTN04/data/processed")
    
    if system.process_large_file(input_path, output_dir):
        print("‚úÖ ƒê√£ x·ª≠ l√Ω file th√†nh c√¥ng")
        
        # N·∫°p v√† x·ª≠ l√Ω d·ªØ li·ªáu
        df = pd.concat([pd.read_csv(f) for f in output_dir.glob("part_*.csv")])
        df = system.auto_label(df)
        
        # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
        system.train_model(df)
        test_df = df.sample(frac=0.2, random_state=42)
        system.evaluate(test_df)
        
        # L∆∞u m√¥ h√¨nh
        model_path = "backend/models/commit_classifier.joblib"
        system.save_model(model_path)
        print(f"üíæ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {model_path}")

if __name__ == "__main__":
    main()