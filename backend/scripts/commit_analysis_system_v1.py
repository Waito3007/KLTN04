# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import logging
from typing import Optional, Union
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CommitAnalysisSystem:
    """H·ªá th·ªëng ph√¢n t√≠ch commit t·ª± ƒë·ªông v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn"""
    
    VERSION = "1.0.0"
    
    def __init__(self, model_params: Optional[dict] = None, 
                 vectorizer_params: Optional[dict] = None):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng ph√¢n t√≠ch commit
        
        Args:
            model_params: Tham s·ªë cho RandomForestClassifier
            vectorizer_params: Tham s·ªë cho TfidfVectorizer
        """
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        default_vectorizer_params = {
            'max_features': 1000,
            'stop_words': 'english',
            'ngram_range': (1, 2),  # Th√™m bigram
            'min_df': 5,
            'max_df': 0.8
        }
        
        default_model_params = {
            'n_estimators': 100,
            'max_depth': 15,
            'class_weight': 'balanced',
            'random_state': 42
        }
        
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or default_vectorizer_params))
        self.model = RandomForestClassifier(**(model_params or default_model_params))
        self.client = None
        self._is_trained = False

    def init_dask_client(self, **kwargs):
        """Kh·ªüi t·∫°o Dask client v·ªõi c·∫•u h√¨nh t√πy ch·ªçn"""
        default_config = {
            'n_workers': 2,
            'threads_per_worker': 1,
            'memory_limit': '2GB',
            'silence_logs': logging.ERROR
        }
        config = {**default_config, **kwargs}
        
        try:
            self.client = Client(**config)
            logger.info(f"Dask client initialized with config: {config}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Dask client: {str(e)}")
            return False

    @staticmethod
    def lightweight_heuristic(msg: str) -> int:
        """Ph√¢n lo·∫°i commit s·ª≠ d·ª•ng heuristic ƒë∆°n gi·∫£n
        
        Args:
            msg: N·ªôi dung commit message
            
        Returns:
            1 n·∫øu l√† commit quan tr·ªçng (bugfix), 0 n·∫øu kh√¥ng
        """
        if not isinstance(msg, str) or not msg.strip():
            return 0
            
        msg = msg.lower()[:200]  # Gi·ªõi h·∫°n ƒë·ªô d√†i x·ª≠ l√Ω
        keywords = {
            'fix', 'bug', 'error', 'fail', 'patch', 
            'resolve', 'crash', 'defect', 'issue'
        }
        return int(any(kw in msg for kw in keywords))

    def process_large_file(self, input_path: Union[str, Path], output_dir: Union[str, Path]) -> bool:
        """X·ª≠ l√Ω file d·ªØ li·ªáu l·ªõn b·∫±ng Dask"""
        try:
            input_path = Path(input_path)
            output_dir = Path(output_dir)

            if not input_path.exists():
                logger.error(f"Input file not found: {input_path}")
                return False

            logger.info(f"Starting processing large file: {input_path}")
            start_time = datetime.now()

            # Kh·ªüi t·∫°o Dask client
            if not self.init_dask_client():
                return False

            try:
                # ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu
                ddf = dd.read_csv(
                    str(input_path),
                    blocksize="10MB",  # Gi·∫£m k√≠ch th∆∞·ªõc block ƒë·ªÉ an to√†n
                    dtype={'message': 'string'},
                    usecols=['commit', 'message'],
                    na_values=['', 'NA', 'N/A', 'nan']
                )

                # L·ªçc v√† g√°n nh√£n
                ddf = ddf[ddf['message'].notnull()]
                ddf['is_critical'] = ddf['message'].map(
                    self.lightweight_heuristic,
                    meta=('is_critical', 'int8')
                )

                # L∆∞u k·∫øt qu·∫£
                output_dir.mkdir(exist_ok=True, parents=True)
                output_path = str(output_dir / f"processed_{input_path.stem}.csv")

                # S·ª≠ d·ª•ng dask.dataframe.to_csv v·ªõi single_file=True
                ddf.to_csv(
                    output_path,
                    index=False,
                    single_file=True
                )

                logger.info(f"Processing completed in {datetime.now() - start_time}")
                logger.info(f"Results saved to: {output_path}")
                return True

            except Exception as e:
                logger.exception(f"Error during processing: {str(e)}")
                return False

        except Exception as e:
            logger.exception(f"System error: {str(e)}")
            return False

        finally:
            if self.client:
                self.client.close()
                self.client = None

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """L√†m s·∫°ch d·ªØ li·ªáu ƒë·∫ßu v√†o
        
        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu commit
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        if 'message' not in df.columns:
            raise ValueError("Input data must contain 'message' column")
            
        df = df.copy()
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df: pd.DataFrame) -> pd.DataFrame:
        """T·ª± ƒë·ªông g√°n nh√£n cho d·ªØ li·ªáu commit
        
        Args:
            df: DataFrame ch·ª©a c√°c commit message
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
        """
        try:
            df = self.clean_data(df)
            df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
            logger.info(f"Label distribution:\n{df['is_critical'].value_counts()}")
            return df
        except Exception as e:
            logger.error(f"Auto-labeling failed: {str(e)}")
            raise

    def train_model(self, df: pd.DataFrame) -> bool:
        """Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i commit
        
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
            
        Returns:
            True n·∫øu hu·∫•n luy·ªán th√†nh c√¥ng
        """
        try:
            logger.info("Starting model training...")
            
            X = self.vectorizer.fit_transform(df['message'])
            y = df['is_critical'].values
            
            self.model.fit(X, y)
            self._is_trained = True
            
            logger.info("Model training completed successfully")
            return True
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return False

    def evaluate(self, test_df: pd.DataFrame) -> None:
        """ƒê√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh
        
        Args:
            test_df: DataFrame ch·ª©a d·ªØ li·ªáu test
        """
        if not self._is_trained:
            logger.warning("Model has not been trained yet")
            return
            
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        y_pred = self.model.predict(X_test)
        
        report = classification_report(
            y_test, 
            y_pred, 
            target_names=['normal', 'critical']
        )
        logger.info(f"\nModel evaluation:\n{report}")

    def save_model(self, path: Union[str, Path]) -> bool:
        """L∆∞u m√¥ h√¨nh v√† vectorizer
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n l∆∞u model
            
        Returns:
            True n·∫øu l∆∞u th√†nh c√¥ng
        """
        try:
            path = Path(path)
            path.parent.mkdir(exist_ok=True, parents=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'version': self.VERSION,
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, str(path))
            logger.info(f"Model saved to {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            return False

    @classmethod
    def load_model(cls, path: Union[str, Path]):
        """T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model
            
        Returns:
            Instance c·ªßa CommitAnalysisSystem v·ªõi model ƒë√£ t·∫£i
        """
        try:
            path = Path(path)
            model_data = joblib.load(str(path))
            
            system = cls()
            system.model = model_data['model']
            system.vectorizer = model_data['vectorizer']
            system._is_trained = True
            
            logger.info(f"Loaded model (v{model_data.get('version', 'unknown')} "
                       f"created at {model_data.get('timestamp', 'unknown')}")
            return system
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

def main():
    """Entry point cho ·ª©ng d·ª•ng"""
    try:
        logger.info("üöÄ Starting commit analysis system")
        
        # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
        input_path = Path("D:/Project/KLTN04/data/oneline.csv")
        output_dir = Path("D:/Project/KLTN04/data/processed")
        model_path = Path("backend/models/commit_classifier_v1.joblib")
        
        # Kh·ªüi t·∫°o h·ªá th·ªëng
        system = CommitAnalysisSystem()
        
        # X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn
        if system.process_large_file(input_path, output_dir):
            # T·ªïng h·ª£p k·∫øt qu·∫£
            df = pd.concat([
                pd.read_csv(f) 
                for f in output_dir.glob("processed_*.csv")
            ])
            
            # G√°n nh√£n v√† hu·∫•n luy·ªán
            labeled_data = system.auto_label(df)
            system.train_model(labeled_data)
            
            # ƒê√°nh gi√° tr√™n t·∫≠p test
            test_df = labeled_data.sample(frac=0.2, random_state=42)
            system.evaluate(test_df)
            
            # L∆∞u model
            if system.save_model(model_path):
                logger.info(f"‚úÖ Pipeline completed successfully. Model saved to {model_path}")
        
    except Exception as e:
        logger.exception("‚ùå Critical error in main pipeline")
    finally:
        logger.info("üèÅ System shutdown")

if __name__ == "__main__":
    main()