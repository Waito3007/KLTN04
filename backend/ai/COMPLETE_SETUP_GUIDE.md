# ğŸš€ HÆ¯á»šNG DáºªN HOÃ€N CHá»ˆNH: MULTIMODAL FUSION MODEL

## HÆ°á»›ng dáº«n tá»« A Ä‘áº¿n Z Ä‘á»ƒ setup, táº¡o dataset vÃ  train model

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
2. [CÃ i Ä‘áº·t mÃ´i trÆ°á»ng](#cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
3. [Táº¡o vÃ  chuáº©n bá»‹ dataset](#táº¡o-vÃ -chuáº©n-bá»‹-dataset)
4. [Kiá»ƒm tra há»‡ thá»‘ng](#kiá»ƒm-tra-há»‡-thá»‘ng)
5. [Training model](#training-model)
6. [ÄÃ¡nh giÃ¡ model](#Ä‘Ã¡nh-giÃ¡-model)
7. [Deployment](#deployment)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ–¥ï¸ YÃŠU Cáº¦U Há»† THá»NG

### Pháº§n cá»©ng

- **RAM**: Tá»‘i thiá»ƒu 8GB, khuyáº¿n nghá»‹ 16GB+
- **GPU**: CUDA-compatible (optional, khuyáº¿n nghá»‹ cho training nhanh)
- **Storage**: Ãt nháº¥t 10GB trá»‘ng
- **CPU**: Multi-core processor

### Pháº§n má»m

- **Python**: 3.8+ (khuyáº¿n nghá»‹ 3.9-3.11)
- **Git**: Latest version
- **CUDA Toolkit**: 11.8+ (náº¿u cÃ³ GPU)

---

## âš™ï¸ CÃ€I Äáº¶T MÃ”I TRÆ¯á»œNG

### BÆ°á»›c 1: Clone Repository

```bash
# Clone project
git clone <your-repository-url>
cd KLTN04

# Chuyá»ƒn Ä‘áº¿n thÆ° má»¥c AI
cd backend/ai
```

### BÆ°á»›c 2: Táº¡o Virtual Environment

```bash
# Táº¡o virtual environment
python -m venv venv

# Activate environment (Windows)
venv\Scripts\activate

# Activate environment (Linux/Mac)
source venv/bin/activate
```

### BÆ°á»›c 3: CÃ i Ä‘áº·t Dependencies

```bash
# CÃ i Ä‘áº·t requirements cÆ¡ báº£n
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers
pip install scikit-learn
pip install pandas numpy
pip install nltk textblob
pip install jupyter notebook
pip install matplotlib seaborn
pip install tqdm
pip install kaggle

# Hoáº·c tá»« file requirements (náº¿u cÃ³)
pip install -r requirements.txt
```

### BÆ°á»›c 4: Setup NLTK

```bash
# Cháº¡y script setup NLTK
python setup_nltk.py
```

Hoáº·c manual setup:

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')
```

### BÆ°á»›c 5: Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
# Test imports
python test_imports.py
```

---

## ğŸ“Š Táº O VÃ€ CHUáº¨N Bá»Š DATASET

### Option 1: Sá»­ dá»¥ng Dataset cÃ³ sáºµn (Khuyáº¿n nghá»‹)

```bash
# Kiá»ƒm tra dataset cÃ³ sáºµn
python check_data_format.py
```

Náº¿u file `training_data/improved_100k_multimodal_training.json` Ä‘Ã£ cÃ³:

- âœ… Dataset Ä‘Ã£ sáºµn sÃ ng (100K samples)
- âœ… CÃ³ thá»ƒ bá» qua bÆ°á»›c táº¡o dataset

### Option 2: Táº¡o Dataset tá»« GitHub

#### BÆ°á»›c 2.1: Setup Kaggle (náº¿u cáº§n)

```bash
# Setup Kaggle credentials
python setup_kaggle.py

# Download dataset tá»« Kaggle
python download_kaggle_dataset.py
```

#### BÆ°á»›c 2.2: Download GitHub Commits

```bash
# Download commits tá»« GitHub
python download_github_commits.py
```

#### BÆ°á»›c 2.3: Process Dataset

```bash
# Xá»­ lÃ½ dataset 100K
python process_large_dataset_100k_v2.py
```

### Option 3: Táº¡o Simple Dataset Ä‘á»ƒ test

```bash
# Táº¡o dataset nhá» Ä‘á»ƒ test
python simple_dataset_creator.py
```

### BÆ°á»›c 2.4: Kiá»ƒm tra Dataset

```bash
# Kiá»ƒm tra format vÃ  quality cá»§a dataset
python check_data_format.py
```

**Expected Output:**

```
Data type: <class 'dict'>
Data keys: ['train_data', 'val_data']
Train samples: 80000, Validation samples: 20000
âœ… Dataset format is correct
```

---

## ğŸ” KIá»‚M TRA Há»† THá»NG

### BÆ°á»›c 3.1: Test Components

```bash
# Test individual components
python test_minimal_enhanced_processor.py
python test_multimodal_structure.py
```

### BÆ°á»›c 3.2: Comprehensive System Test

```bash
# Test toÃ n bá»™ há»‡ thá»‘ng
python evaluate_multimodal_model.py
```

**Expected Output:**

```
ğŸ¯ MULTIMODAL MODEL EVALUATION SUMMARY
âœ… All tests passed! Model is ready for training.
```

### BÆ°á»›c 3.3: Quick Training Test

```bash
# Test training trÃªn dataset nhá»
python quick_training_test.py
```

**Expected Output:**

```
ğŸ¯ QUICK TRAINING TEST RESULTS
âœ… Training Success: True
ğŸ“Š Training Accuracy: 85%+
ğŸ“Š Validation Accuracy: 85%+
```

---

## ğŸš‚ TRAINING MODEL

### BÆ°á»›c 4.1: Chá»n Training Script

**For Full Training (100K dataset):**

```bash
python train_enhanced_100k_fixed.py
```

**For Quick Test (Small dataset):**

```bash
python quick_training_test.py
```

### BÆ°á»›c 4.2: Monitor Training

Training sáº½ hiá»ƒn thá»‹:

```
2025-06-10 16:29:26,729 - INFO - Epoch 1/50, Batch 0/2500, Loss: 4.5201, LR: 1.00e-03
2025-06-10 16:29:34,515 - INFO - Epoch 1/50, Batch 100/2500, Loss: 1.7963, LR: 1.00e-03
...
```

### BÆ°á»›c 4.3: Training Parameters

```python
# CÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong script:
epochs = 50                    # Sá»‘ epochs
batch_size = 32               # Batch size
learning_rate = 1e-3          # Learning rate
patience = 10                 # Early stopping patience
```

### BÆ°á»›c 4.4: Model Checkpoints

Models Ä‘Æ°á»£c lÆ°u táº¡i:

```
trained_models/enhanced_multimodal_fusion_100k/
â”œâ”€â”€ best_enhanced_model.pth           # Best model
â”œâ”€â”€ enhanced_training_history.json    # Training history
â””â”€â”€ model_config.json                # Model configuration
```

---

## ğŸ“ˆ ÄÃNH GIÃ MODEL

### BÆ°á»›c 5.1: Evaluate Model Performance

```bash
# ÄÃ¡nh giÃ¡ model Ä‘Ã£ train
python evaluate_multimodal_fusion.py
```

### BÆ°á»›c 5.2: Generate Final Report

```bash
# Táº¡o bÃ¡o cÃ¡o cuá»‘i cÃ¹ng
python generate_final_report.py
```

### BÆ°á»›c 5.3: Test Inference

```bash
# Test inference vá»›i model Ä‘Ã£ train
python multimodal_commit_inference.py
```

---

## ğŸš€ DEPLOYMENT

### BÆ°á»›c 6.1: Model Integration

```bash
# Test integration vá»›i backend
python test_multimodal_integration.py
```

### BÆ°á»›c 6.2: API Integration

Integrate model vÃ o backend API:

```python
# Trong backend/services/
from ai.multimodal_fusion.inference import MultimodalInference

# Load trained model
inference = MultimodalInference(
    model_path="ai/trained_models/enhanced_multimodal_fusion_100k/best_enhanced_model.pth"
)

# Predict commit analysis
result = inference.predict(commit_message, metadata)
```

### BÆ°á»›c 6.3: Production Setup

Xem [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) Ä‘á»ƒ setup production.

---

## ğŸ› ï¸ TROUBLESHOOTING

### Lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch kháº¯c phá»¥c

#### 1. ImportError: No module named 'torch'

```bash
# CÃ i láº¡i PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### 2. CUDA out of memory

```bash
# Giáº£m batch size trong training script
batch_size = 16  # hoáº·c 8
```

#### 3. NLTK data not found

```bash
# Download NLTK data
python -c "import nltk; nltk.download('all')"
```

#### 4. UnicodeEncodeError

```bash
# Set environment variables
set PYTHONIOENCODING=utf-8
set PYTHONUTF8=1
```

#### 5. Dataset not found

```bash
# Kiá»ƒm tra Ä‘Æ°á»ng dáº«n dataset
ls training_data/
python check_data_format.py
```

#### 6. Model config error

```bash
# Kiá»ƒm tra model config
python test_multimodal_structure.py
```

---

## ğŸ“š SCRIPTS REFERENCE

### Core Scripts

| Script                         | Purpose                    | When to use          |
| ------------------------------ | -------------------------- | -------------------- |
| `setup_nltk.py`                | Setup NLTK dependencies    | First time setup     |
| `test_imports.py`              | Test all imports           | After installation   |
| `check_data_format.py`         | Check dataset format       | Before training      |
| `evaluate_multimodal_model.py` | Comprehensive evaluation   | System validation    |
| `quick_training_test.py`       | Quick training test        | Before full training |
| `train_enhanced_100k_fixed.py` | Full model training        | Main training        |
| `generate_final_report.py`     | Generate evaluation report | After training       |

### Data Processing Scripts

| Script                             | Purpose              | Dataset Size |
| ---------------------------------- | -------------------- | ------------ |
| `simple_dataset_creator.py`        | Create test dataset  | ~1K samples  |
| `download_github_commits.py`       | Download from GitHub | Variable     |
| `process_large_dataset_100k_v2.py` | Process 100K dataset | 100K samples |

### Testing Scripts

| Script                               | Purpose              | Test Level   |
| ------------------------------------ | -------------------- | ------------ |
| `test_minimal_enhanced_processor.py` | Test text processor  | Component    |
| `test_multimodal_structure.py`       | Test model structure | Architecture |
| `test_multimodal_integration.py`     | Test integration     | System       |
| `final_system_validation.py`         | Full system test     | Complete     |

---

## â±ï¸ TIMELINE ESTIMATE

### Quick Setup (Testing)

- **Setup Environment**: 30 minutes
- **Test Dataset**: 10 minutes
- **Quick Training**: 15 minutes
- **Total**: ~1 hour

### Full Setup (Production)

- **Setup Environment**: 30 minutes
- **Download/Process 100K Dataset**: 2-4 hours
- **Full Training**: 4-8 hours (depending on hardware)
- **Evaluation**: 30 minutes
- **Total**: 7-13 hours

---

## ğŸ¯ SUCCESS CRITERIA

### âœ… Successful Setup Indicators

1. **Environment Setup**:

   ```
   python test_imports.py
   # Output: âœ… All imports successful
   ```

2. **Dataset Ready**:

   ```
   python check_data_format.py
   # Output: âœ… Dataset format is correct
   ```

3. **System Validation**:

   ```
   python evaluate_multimodal_model.py
   # Output: ğŸ‰ All tests passed! Model is ready for training.
   ```

4. **Training Success**:

   ```
   python quick_training_test.py
   # Output: âœ… Training Success: True, Accuracy: 85%+
   ```

5. **Final Evaluation**:
   ```
   python generate_final_report.py
   # Output: ğŸ‰ CONGRATULATIONS! Your multimodal model is production-ready!
   ```

---

## ğŸ“ SUPPORT

### Náº¿u gáº·p váº¥n Ä‘á»:

1. **Check logs**: Xem file `.log` trong thÆ° má»¥c AI
2. **Run diagnostics**: `python test_core_system.py`
3. **Check system status**: `python evaluate_multimodal_model.py`
4. **Review error messages**: Sá»­ dá»¥ng troubleshooting guide

### Common Commands

```bash
# Quick health check
python test_imports.py && python test_multimodal_structure.py

# Full system validation
python evaluate_multimodal_model.py

# Training from scratch
python check_data_format.py && python quick_training_test.py

# Production training
python train_enhanced_100k_fixed.py
```

---

## ğŸ‰ CONCLUSION

Sau khi hoÃ n thÃ nh hÆ°á»›ng dáº«n nÃ y, báº¡n sáº½ cÃ³:

- âœ… **Working environment** vá»›i táº¥t cáº£ dependencies
- âœ… **100K multimodal dataset** Ä‘Ã£ Ä‘Æ°á»£c processed
- âœ… **Trained model** vá»›i performance 85%+
- âœ… **Production-ready system** Ä‘á»ƒ analyze commits
- âœ… **Integration** vá»›i backend API

**Next Steps:**

1. Integrate model vÃ o production environment
2. Monitor performance trÃªn real data
3. Fine-tune hyperparameters náº¿u cáº§n
4. Scale up Ä‘á»ƒ handle larger datasets

**Happy Training! ğŸš€**
