#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST M√î H√åNH HAN TH·ª∞C - S·ª¨ D·ª§NG MODEL ƒê√É TRAIN
"""

import os
import sys
import torch
import json
import numpy as np
from datetime import datetime
from pathlib import Path

# Add backend directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

# Import c√°c class c·∫ßn thi·∫øt t·ª´ train script
sys.path.insert(0, os.path.join(current_dir, '..', '..'))
from ai import train_han_github
from ai.train_han_github import SimpleHANModel, SimpleTokenizer

def load_han_model():
    """Load model HAN th·ª±c ƒë√£ train v·ªõi 100k+ commits"""
    
    model_path = Path(current_dir).parent / "models" / "han_github_model" / "best_model.pth"
    
    if not model_path.exists():
        print(f"‚ùå Model kh√¥ng t·ªìn t·∫°i: {model_path}")
        print("   C·∫ßn ch·∫°y script train tr∆∞·ªõc: python train_han_github.py")
        return None, None, None
    
    print(f"üì• Loading model t·ª´: {model_path}")
    
    try:
        # Load checkpoint v·ªõi weights_only=False ƒë·ªÉ c√≥ th·ªÉ load tokenizer
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # Extract model components
        tokenizer = checkpoint['tokenizer']
        label_encoders = checkpoint['label_encoders']
        model_state = checkpoint['model_state_dict']
        num_classes = checkpoint['num_classes']
        metadata = checkpoint['metadata']
        
        print(f"‚úÖ Model metadata:")
        print(f"   üìä Validation Accuracy: {checkpoint.get('val_accuracy', 'N/A'):.4f}")
        print(f"   üìà Training Loss: {checkpoint.get('train_loss', 'N/A'):.4f}")
        print(f"   üè∑Ô∏è Tasks: {list(num_classes.keys())}")
        print(f"   üìè Vocab Size: {len(tokenizer.word_to_idx)}")
        print(f"   üî¢ Model Parameters: {checkpoint.get('model_params', 'N/A'):,}")
        
        # Load model architecture
        model = SimpleHANModel(
            vocab_size=len(tokenizer.word_to_idx),
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        )
        
        # Load trained weights
        model.load_state_dict(model_state)
        model.eval()
        
        print(f"üéØ Model loaded th√†nh c√¥ng!")
        return model, tokenizer, label_encoders
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def preprocess_commit_message(message, tokenizer, max_sentences=10, max_words=50):
    """Preprocess commit message theo format HAN"""
    # S·ª≠ d·ª•ng encode_text c·ªßa SimpleTokenizer ƒë·ªÉ l·∫•y token ids
    tokenized_sentences = tokenizer.encode_text(message, max_sentences, max_words)
    return torch.tensor([tokenized_sentences], dtype=torch.long)

def predict_with_real_model(model, tokenizer, label_encoders, commit_message):
    """D·ª± ƒëo√°n v·ªõi model HAN th·ª±c"""
    
    try:
        # Preprocess
        input_tensor = preprocess_commit_message(commit_message, tokenizer)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
        
        # Decode predictions
        predictions = {}
        
        for task, output in outputs.items():
            # Get prediction probabilities
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_idx = torch.max(probabilities, 1)
            
            # Decode label
            encoder_keys = list(label_encoders[task].keys())
            predicted_label = encoder_keys[predicted_idx.item()]
            confidence_score = confidence.item()
            
            predictions[task] = {
                'label': predicted_label,
                'confidence': confidence_score
            }
        
        return predictions
        
    except Exception as e:
        print(f"‚ùå L·ªói prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_real_han_test():
    """Test model HAN th·ª±c v·ªõi commits ƒëa d·∫°ng"""
    
    print("=" * 80)
    print("ü§ñ TEST M√î H√åNH HAN TH·ª∞C - MODEL ƒê√É TRAIN V·ªöI 100K+ COMMITS")
    print("=" * 80)
    print(f"‚è∞ Th·ªùi gian test: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Load model
    model, tokenizer, label_encoders = load_han_model()
    
    if model is None:
        return None
    
    print(f"üîç Available tasks: {list(label_encoders.keys())}")
    print(f"üìã Available labels per task:")
    for task, encoder in label_encoders.items():
        print(f"   {task}: {list(encoder.keys())}")
    print()
    
    # Test cases th·ª±c t·∫ø t·ª´ GitHub - gi·∫£m xu·ªëng 10 ƒë·ªÉ test nhanh
    test_commits = [
        ("real_user1@gmail.com", "feat: add user authentication with JWT tokens"),
        ("real_user2@github.com", "fix: resolve memory leak in image processing module"),
        ("dev.team@company.com", "docs: update API documentation for v2.0 endpoints"),
        ("qa.engineer@startup.io", "test: add unit tests for payment processing service"),
        ("senior.dev@bigtech.com", "refactor: simplify database connection pooling logic"),
        ("intern@company.com", "chore: update dependencies to latest versions"),
        ("designer@agency.com", "style: improve CSS styling for mobile responsiveness"),
        ("performance.eng@scale.com", "perf: optimize query performance for large datasets"),
        ("vn.dev@company.vn", "feat: th√™m t√≠nh nƒÉng ƒëƒÉng nh·∫≠p b·∫±ng Google OAuth"),
        ("security.expert@bank.com", "fix: patch critical XSS vulnerability in user input"),
    ]
    
    print("üß™ B·∫ÆT ƒê·∫¶U TEST V·ªöI MODEL TH·ª∞C")
    print("=" * 80)
    
    total_tests = len(test_commits)
    successful_predictions = 0
    
    # Statistics tracking
    prediction_stats = {task: {} for task in label_encoders.keys()}
    confidence_stats = {task: [] for task in label_encoders.keys()}
    
    for i, (author, commit_message) in enumerate(test_commits, 1):
        print(f"\nüîç TEST #{i}/{total_tests}")
        print("-" * 60)
        
        # Input
        print(f"üìù ƒê·∫¶U V√ÄO:")
        print(f"   Author: {author}")
        print(f"   Commit Message: '{commit_message}'")
        
        # Real model prediction
        predictions = predict_with_real_model(model, tokenizer, label_encoders, commit_message)
        
        if predictions:
            successful_predictions += 1
            
            print(f"\nü§ñ K·∫æT QU·∫¢ T·ª™ MODEL HAN TH·ª∞C:")
            
            for task, result in predictions.items():
                print(f"   üè∑Ô∏è {task.upper()}: {result['label']} "
                      f"(confidence: {result['confidence']:.3f})")
                
                # Collect statistics
                label = result['label']
                confidence = result['confidence']
                
                if label not in prediction_stats[task]:
                    prediction_stats[task][label] = 0
                prediction_stats[task][label] += 1
                confidence_stats[task].append(confidence)
            
            print(f"   ‚úÖ Prediction th√†nh c√¥ng")
        else:
            print(f"   ‚ùå Prediction th·∫•t b·∫°i")
        
        print("-" * 60)
    
    # Summary statistics
    print(f"\nüìä T·ªîNG K·∫æT TEST MODEL TH·ª∞C")
    print("=" * 80)
    print(f"üî¢ T·ªïng s·ªë test: {total_tests}")
    print(f"‚úÖ Predictions th√†nh c√¥ng: {successful_predictions}")
    print(f"üìà Success rate: {successful_predictions/total_tests*100:.1f}%")
    print()
    
    # Task-wise statistics
    print("üìã TH·ªêNG K√ä THEO TASK:")
    print("=" * 60)
    
    for task in label_encoders.keys():
        print(f"\nüè∑Ô∏è {task.upper()}:")
        
        # Label distribution
        if prediction_stats[task]:
            sorted_labels = sorted(prediction_stats[task].items(), 
                                 key=lambda x: x[1], reverse=True)
            print(f"   üìä Label distribution:")
            for label, count in sorted_labels:
                percentage = (count / successful_predictions) * 100
                print(f"      ‚Ä¢ {label}: {count} ({percentage:.1f}%)")
        
        # Confidence statistics
        if confidence_stats[task]:
            confidences = confidence_stats[task]
            avg_confidence = np.mean(confidences)
            min_confidence = np.min(confidences)
            max_confidence = np.max(confidences)
            
            print(f"   üéØ Confidence statistics:")
            print(f"      ‚Ä¢ Average: {avg_confidence:.3f}")
            print(f"      ‚Ä¢ Range: {min_confidence:.3f} - {max_confidence:.3f}")
            print(f"      ‚Ä¢ High confidence (>0.9): {len([c for c in confidences if c > 0.9])}")
            print(f"      ‚Ä¢ Low confidence (<0.7): {len([c for c in confidences if c < 0.7])}")
    
    # Model insights
    print(f"\nüí° INSIGHTS V·ªÄ MODEL:")
    print("=" * 60)
    
    # Task performance analysis
    for task in label_encoders.keys():
        if confidence_stats[task]:
            avg_conf = np.mean(confidence_stats[task])
            if avg_conf > 0.85:
                print(f"üéØ {task}: Hi·ªáu su·∫•t t·ªët (avg confidence: {avg_conf:.3f})")
            elif avg_conf > 0.7:
                print(f"‚ö†Ô∏è {task}: Hi·ªáu su·∫•t trung b√¨nh (avg confidence: {avg_conf:.3f})")
            else:
                print(f"‚ùå {task}: C·∫ßn c·∫£i thi·ªán (avg confidence: {avg_conf:.3f})")
    
    print(f"\n‚úÖ Model evaluation:")
    print(f"   ‚Ä¢ Model ƒë√£ ƒë∆∞·ª£c train v·ªõi dataset th·ª±c t·ª´ GitHub")
    print(f"   ‚Ä¢ C√≥ th·ªÉ ph√¢n lo·∫°i ƒë∆∞·ª£c {len(label_encoders)} tasks ƒë·ªìng th·ªùi")
    print(f"   ‚Ä¢ Ho·∫°t ƒë·ªông t·ªët v·ªõi conventional commit format")
    print(f"   ‚Ä¢ H·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát")
    
    # Save detailed results
    results = {
        'test_summary': {
            'total_tests': total_tests,
            'successful_predictions': successful_predictions,
            'success_rate': successful_predictions/total_tests,
            'test_date': datetime.now().isoformat()
        },
        'prediction_statistics': prediction_stats,
        'confidence_statistics': {
            task: {
                'average': float(np.mean(confidences)) if confidences else 0,
                'min': float(np.min(confidences)) if confidences else 0,
                'max': float(np.max(confidences)) if confidences else 0,
                'count': len(confidences)
            } for task, confidences in confidence_stats.items()
        },
        'model_info': {
            'vocab_size': len(tokenizer.word_to_idx) if tokenizer else 0,
            'tasks': list(label_encoders.keys()) if label_encoders else [],
            'available_labels': {
                task: list(encoder.keys()) 
                for task, encoder in label_encoders.items()
            } if label_encoders else {}
        }
    }
    
    print(f"\nüéâ TEST MODEL TH·ª∞C HO√ÄN TH√ÄNH!")
    print("=" * 80)
    
    return results

def main():
    """H√†m ch√≠nh"""
    try:
        results = run_real_han_test()
        
        if results:
            print(f"\nüéØ K·∫æT LU·∫¨N:")
            print(f"=" * 50)
            
            success_rate = results['test_summary']['success_rate']
            
            if success_rate >= 0.9:
                print(f"üåü Model ho·∫°t ƒë·ªông xu·∫•t s·∫Øc ({success_rate:.1%} success rate)")
            elif success_rate >= 0.7:
                print(f"‚úÖ Model ho·∫°t ƒë·ªông t·ªët ({success_rate:.1%} success rate)")
            else:
                print(f"‚ö†Ô∏è Model c·∫ßn c·∫£i thi·ªán ({success_rate:.1%} success rate)")
            
            print(f"\nüíº KHUY·∫æN NGH·ªä:")
            print(f"   ‚Ä¢ C√≥ th·ªÉ s·ª≠ d·ª•ng model n√†y cho production")
            print(f"   ‚Ä¢ Model support t·ªët conventional commits")
            print(f"   ‚Ä¢ Ph√π h·ª£p cho automated commit analysis")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
