"""
Download vÃ  xá»­ lÃ½ dataset GitHub Commit Messages tá»« Kaggle
Dataset: mrisdal/github-commit-messages
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import re
from collections import Counter
import zipfile

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"âŒ Lá»—i setup Kaggle API: {e}")
        print("Vui lÃ²ng cháº¡y: python quick_kaggle_setup.py")
        return None

def download_dataset(api, dataset_name="dhruvildave/github-commit-messages-dataset", force_download=False):
    """Download dataset tá»« Kaggle"""
    try:
        # Táº¡o thÆ° má»¥c download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Kiá»ƒm tra Ä‘Ã£ download chÆ°a
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"âœ… Dataset Ä‘Ã£ tá»“n táº¡i trong {download_dir}")
            return download_dir, csv_files[0]
        
        print(f"ğŸ“¥ Äang download dataset: {dataset_name}")
        print(f"ğŸ“ VÃ o thÆ° má»¥c: {download_dir}")
        
        # Download dataset
        api.dataset_download_files(
            dataset_name, 
            path=str(download_dir), 
            unzip=True
        )
        
        # TÃ¬m file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file CSV trong dataset")
            return None, None
        
        csv_file = csv_files[0]
        print(f"âœ… Download thÃ nh cÃ´ng: {csv_file}")
        print(f"ğŸ“Š KÃ­ch thÆ°á»›c file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return download_dir, csv_file
        
    except Exception as e:
        print(f"âŒ Lá»—i download dataset: {e}")
        return None, None

def analyze_commit_data(csv_file, sample_size=None):
    """PhÃ¢n tÃ­ch dá»¯ liá»‡u commit"""
    try:
        print(f"\nğŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U: {csv_file.name}")
        print("="*60)
        
        # Äá»c dá»¯ liá»‡u vá»›i chunk Ä‘á»ƒ tiáº¿t kiá»‡m memory
        print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u...")
        
        # Äá»c má»™t pháº§n nhá» trÆ°á»›c Ä‘á»ƒ xem cáº¥u trÃºc
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"ğŸ“‹ Columns: {list(sample_df.columns)}")
        print(f"ğŸ“ Sample shape: {sample_df.shape}")
          # Äá»c toÃ n bá»™ hoáº·c sample má»™t cÃ¡ch hiá»‡u quáº£
        if sample_size:
            print(f"ğŸ“Š Sampling {sample_size:,} records...")
            # Sá»­ dá»¥ng chunk reading Ä‘á»ƒ memory-efficient sampling
            chunk_size = 10000
            sampled_chunks = []
            total_read = 0
            
            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
                total_read += len(chunk)
                
                # Random sample tá»« chunk nÃ y
                chunk_sample_size = min(sample_size // 10, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
                
                # Dá»«ng khi Ä‘Ã£ Ä‘á»§ data
                total_sampled = sum(len(c) for c in sampled_chunks)
                if total_sampled >= sample_size:
                    break
                
                if total_read % 50000 == 0:
                    print(f"  ÄÃ£ Ä‘á»c: {total_read:,} records...")
            
            # Combine chunks
            df = pd.concat(sampled_chunks, ignore_index=True)
            if len(df) > sample_size:
                df = df.sample(n=sample_size).reset_index(drop=True)
            
            print(f"ğŸ“Š ÄÃ£ sample {len(df):,} commits tá»« {total_read:,} total")
        else:
            print("ğŸ“– Äá»c toÃ n bá»™ dataset (cÃ³ thá»ƒ máº¥t thá»i gian)...")
            df = pd.read_csv(csv_file)
            print(f"ğŸ“Š ÄÃ£ Ä‘á»c {len(df):,} commits")
        
        # Hiá»ƒn thá»‹ thÃ´ng tin cÆ¡ báº£n
        print(f"\nğŸ“ˆ THá»NG KÃŠ CÆ  Báº¢N:")
        print(f"  â€¢ Tá»•ng sá»‘ commits: {len(df):,}")
        print(f"  â€¢ Columns: {df.shape[1]}")
        print(f"  â€¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # Kiá»ƒm tra columns quan trá»ng
        important_cols = ['message', 'commit', 'subject', 'body', 'author', 'repo']
        available_cols = [col for col in important_cols if col in df.columns]
        print(f"  â€¢ Available important columns: {available_cols}")
        
        # TÃ¬m column chá»©a commit message
        message_col = None
        for col in ['message', 'subject', 'commit']:
            if col in df.columns:
                message_col = col
                break
        
        if not message_col:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y column chá»©a commit message")
            return None
        
        print(f"  â€¢ Sá»­ dá»¥ng column: '{message_col}' lÃ m commit message")
        
        # LÃ m sáº¡ch dá»¯ liá»‡u
        print(f"\nğŸ§¹ LÃ€M Sáº CH Dá»® LIá»†U:")
        original_count = len(df)
        
        # Loáº¡i bá» messages rá»—ng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  â€¢ Sau khi loáº¡i bá» empty: {len(df):,} (-{original_count - len(df)})")
        
        # Loáº¡i bá» messages quÃ¡ ngáº¯n hoáº·c quÃ¡ dÃ i
        df = df[df[message_col].str.len().between(5, 500)]
        print(f"  â€¢ Sau khi lá»c Ä‘á»™ dÃ i (5-500 chars): {len(df):,}")
        
        # Loáº¡i bá» duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  â€¢ Sau khi loáº¡i bá» duplicates: {len(df):,}")
        
        # PhÃ¢n tÃ­ch ná»™i dung
        print(f"\nğŸ“ PHÃ‚N TÃCH Ná»˜I DUNG:")
        messages = df[message_col].astype(str)
        
        # Thá»‘ng kÃª Ä‘á»™ dÃ i
        lengths = messages.str.len()
        print(f"  â€¢ Äá»™ dÃ i trung bÃ¬nh: {lengths.mean():.1f} chars")
        print(f"  â€¢ Äá»™ dÃ i median: {lengths.median():.1f} chars")
        print(f"  â€¢ Min/Max: {lengths.min()}/{lengths.max()} chars")
          # Top words - sample Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i memory
        sample_size_for_words = min(5000, len(messages))
        print(f"  â€¢ Analyzing words from {sample_size_for_words} samples...")
        
        all_words = []
        sample_messages = messages.sample(n=sample_size_for_words) if len(messages) > sample_size_for_words else messages
        
        for msg in sample_messages:
            words = re.findall(r'\b[a-zA-Z]+\b', str(msg).lower())
            all_words.extend(words)
        
        word_counts = Counter(all_words).most_common(20)
        print(f"\nğŸ”¤ TOP 20 WORDS (from {sample_size_for_words} samples):")
        for word, count in word_counts:
            print(f"    {word}: {count}")
        
        return df, message_col
        
    except Exception as e:
        print(f"âŒ Lá»—i phÃ¢n tÃ­ch dá»¯ liá»‡u: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def classify_commits(df, message_col):
    """PhÃ¢n loáº¡i commits theo cÃ¡c tiÃªu chÃ­"""
    print(f"\nğŸ·ï¸  PHÃ‚N LOáº I COMMITS:")
    print("="*60)
    
    messages = df[message_col].astype(str).str.lower()
    classifications = []
    
    # Process in batches Ä‘á»ƒ trÃ¡nh memory issues
    batch_size = 1000
    total_batches = (len(messages) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(messages))
        batch_messages = messages[start_idx:end_idx]
        
        batch_classifications = []
        for message in batch_messages:
            labels = {
                'commit_type': classify_commit_type(message),
                'purpose': classify_purpose(message),
                'sentiment': classify_sentiment(message),
                'tech_tag': classify_tech_tag(message)
            }
            batch_classifications.append(labels)
        
        classifications.extend(batch_classifications)
        
        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:
            print(f"  ÄÃ£ xá»­ lÃ½: {end_idx:,}/{len(messages):,} ({(end_idx/len(messages)*100):.1f}%)")
    
    # Thá»‘ng kÃª phÃ¢n loáº¡i
    print(f"\nğŸ“Š THá»NG KÃŠ PHÃ‚N LOáº I:")
    
    for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
        values = [c[category] for c in classifications]
        counter = Counter(values)
        print(f"\n{category.upper()}:")
        for value, count in counter.most_common(10):
            percentage = (count / len(values)) * 100
            print(f"    {value}: {count:,} ({percentage:.1f}%)")
    
    return classifications

def classify_commit_type(message):
    """PhÃ¢n loáº¡i commit type"""
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch)\b'],
        'docs': [r'\b(doc|documentation|readme|comment)\b'],
        'style': [r'\b(style|format|lint|clean|prettier)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup)\b'],
        'test': [r'\b(test|spec|testing|coverage)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge)\b'],
        'perf': [r'\b(perf|performance|optimize|speed)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message):
    """PhÃ¢n loáº¡i má»¥c Ä‘Ã­ch"""
    if re.search(r'\b(add|new|implement|create|build)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message):
    """PhÃ¢n loáº¡i sentiment"""
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'bad']
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message):
    """PhÃ¢n loáº¡i technology tag"""
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi)\b'],
        'java': [r'\b(java|maven|gradle|spring)\b'],
        'css': [r'\b(css|scss|sass|style|styling)\b'],
        'html': [r'\b(html|template|markup)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service)\b'],
        'docker': [r'\b(docker|container|dockerfile)\b'],
        'git': [r'\b(git|merge|branch|commit|pull)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration)\b'],
        'security': [r'\b(security|auth|token|password|encrypt)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def save_processed_data(df, message_col, classifications, output_dir):
    """LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½"""
    try:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Táº¡o training data format
        training_data = []
        
        for idx, (_, row) in enumerate(df.iterrows()):
            if idx >= len(classifications):
                break
                
            labels = classifications[idx]
            
            training_data.append({
                'text': row[message_col],
                'labels': labels,
                'metadata': {
                    'source': 'github-commit-messages',
                    'original_index': idx
                }
            })
        
        # Táº¡o metadata
        metadata = {
            'total_samples': len(training_data),
            'created_at': datetime.now().isoformat(),
            'source_dataset': 'mrisdal/github-commit-messages',
            'message_column': message_col,
            'statistics': {}
        }
        
        # Thá»‘ng kÃª cho metadata
        for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            values = [item['labels'][category] for item in training_data]
            metadata['statistics'][category] = dict(Counter(values))
        
        # Save training data
        output_file = output_dir / 'github_commits_training_data.json'
        final_data = {
            'metadata': metadata,
            'data': training_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ÄÃƒ LÃ€U Dá»® LIá»†U:")
        print(f"  ğŸ“ File: {output_file}")
        print(f"  ğŸ“Š Samples: {len(training_data):,}")
        print(f"  ğŸ“ Size: {output_file.stat().st_size / (1024*1024):.1f} MB")
        
        # LÆ°u sample Ä‘á»ƒ preview
        sample_file = output_dir / 'sample_preview.json'
        sample_data = {
            'metadata': metadata,
            'data': training_data[:100]  # 100 samples Ä‘áº§u
        }
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ” Sample: {sample_file}")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ Lá»—i lÆ°u dá»¯ liá»‡u: {e}")
        return None

def main():
    """Main function"""
    print("ğŸš€ GITHUB COMMIT MESSAGES DOWNLOADER")
    print("="*60)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        return
    
    # Download dataset
    download_dir, csv_file = download_dataset(api)
    if not csv_file:
        return
      # Há»i user vá» sample size
    print(f"\nğŸ“Š TÃ™Y CHá»ŒN PROCESSING:")
    print("1. Sample 1K commits (test nhanh)")
    print("2. Sample 5K commits (demo)")
    print("3. Sample 10K commits (khuyÃªn dÃ¹ng)")
    print("4. Sample 50K commits (training tá»‘t)")
    print("5. Sample 100K commits (dataset lá»›n)")
    print("6. Xá»­ lÃ½ toÃ n bá»™ dataset (cáº£nh bÃ¡o: cÃ³ thá»ƒ ráº¥t lÃ¢u)")
    
    choice = input("Chá»n option (1-6): ").strip()
    
    sample_sizes = {
        '1': 1000,
        '2': 5000,
        '3': 10000,
        '4': 50000,
        '5': 100000,
        '6': None
    }
    
    sample_size = sample_sizes.get(choice, 10000)
    
    if sample_size is None:
        print("âš ï¸  Cáº¢NH BÃO: Báº¡n Ä‘Ã£ chá»n xá»­ lÃ½ toÃ n bá»™ dataset!")
        print("   Äiá»u nÃ y cÃ³ thá»ƒ máº¥t ráº¥t nhiá»u thá»i gian vÃ  bá»™ nhá»›.")
        confirm = input("Báº¡n cÃ³ cháº¯c cháº¯n khÃ´ng? (yes/no): ").lower()
        if confirm != 'yes':
            sample_size = 10000
            print("ğŸ”„ Chuyá»ƒn vá» sample 10K commits")
    
    # Analyze data
    df, message_col = analyze_commit_data(csv_file, sample_size)
    if df is None:
        return
    
    # Classify commits
    classifications = classify_commits(df, message_col)
    
    # Save processed data
    output_dir = Path(__file__).parent / "training_data"
    output_file = save_processed_data(df, message_col, classifications, output_dir)
    
    if output_file:
        print(f"\nğŸ‰ HOÃ€N THÃ€NH!")
        print(f"ğŸ“‹ BÃ¢y giá» báº¡n cÃ³ thá»ƒ:")
        print(f"  â€¢ Train HAN: python train_han_with_kaggle.py")
        print(f"  â€¢ Train XGBoost: python train_xgboost.py")
        print(f"  ğŸ“ Data file: {output_file}")

if __name__ == "__main__":
    main()
