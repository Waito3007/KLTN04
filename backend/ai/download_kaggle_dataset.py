"""
Script Ä‘á»ƒ táº£i dataset commit tá»« Kaggle vÃ  chuáº©n bá»‹ dá»¯ liá»‡u cho mÃ´ hÃ¬nh HAN
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import zipfile
import shutil
from typing import List, Dict, Any, Tuple
import logging

# Thiáº¿t láº­p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class KaggleDatasetDownloader:
    def __init__(self, base_dir: str = None):
        """
        Khá»Ÿi táº¡o class Ä‘á»ƒ táº£i dataset tá»« Kaggle
        
        Args:
            base_dir: ThÆ° má»¥c gá»‘c Ä‘á»ƒ lÆ°u dá»¯ liá»‡u
        """
        self.base_dir = base_dir or os.path.dirname(__file__)
        self.data_dir = os.path.join(self.base_dir, 'kaggle_data')
        self.processed_dir = os.path.join(self.base_dir, 'training_data')
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def check_kaggle_config(self) -> bool:
        """Kiá»ƒm tra cáº¥u hÃ¬nh Kaggle API"""
        try:
            import kaggle
            logger.info("âœ… Kaggle API Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh")
            return True
        except ImportError:
            logger.error("âŒ Kaggle package chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. Cháº¡y: pip install kaggle")
            return False
        except OSError as e:
            logger.error(f"âŒ Lá»—i cáº¥u hÃ¬nh Kaggle API: {e}")
            logger.info("Vui lÃ²ng:")
            logger.info("1. Táº¡o API token táº¡i: https://www.kaggle.com/settings")
            logger.info("2. Äáº·t file kaggle.json vÃ o ~/.kaggle/ (Linux/Mac) hoáº·c C:\\Users\\<username>\\.kaggle\\ (Windows)")
            logger.info("3. Cáº¥p quyá»n 600 cho file: chmod 600 ~/.kaggle/kaggle.json")
            return False
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """
        Táº£i dataset tá»« Kaggle
        
        Args:
            dataset_name: TÃªn dataset trÃªn Kaggle (format: username/dataset-name)
            force_download: CÃ³ táº£i láº¡i náº¿u Ä‘Ã£ tá»“n táº¡i hay khÃ´ng
            
        Returns:
            bool: True náº¿u thÃ nh cÃ´ng
        """
        if not self.check_kaggle_config():
            return False
            
        try:
            import kaggle
            
            dataset_path = os.path.join(self.data_dir, dataset_name.split('/')[-1])
            
            if os.path.exists(dataset_path) and not force_download:
                logger.info(f"Dataset {dataset_name} Ä‘Ã£ tá»“n táº¡i, bá» qua táº£i xuá»‘ng")
                return True
                
            logger.info(f"ğŸ”„ Äang táº£i dataset: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=self.data_dir, 
                unzip=True
            )
            
            logger.info(f"âœ… Táº£i thÃ nh cÃ´ng dataset: {dataset_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº£i dataset {dataset_name}: {e}")
            return False
    
    def list_popular_commit_datasets(self) -> List[str]:
        """Liá»‡t kÃª cÃ¡c dataset commit phá»• biáº¿n trÃªn Kaggle"""
        return [
            "shashankbansal6/git-commits-message-dataset",
            "madhav28/git-commit-messages",
            "aashita/git-commit-messages",
            "jainaru/commit-classification-dataset",
            "shubhamjain0594/commit-message-generation",
            "saurabhshahane/conventional-commit-messages",
            "devanshunigam/commits",
            "ashydv/commits-dataset"
        ]
    
    def process_commit_dataset(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ dá»¯ liá»‡u commit tá»« cÃ¡c file CSV
        
        Args:
            csv_files: Danh sÃ¡ch cÃ¡c file CSV
            
        Returns:
            Dict chá»©a dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        """
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"ğŸ”„ Äang xá»­ lÃ½ file: {csv_file}")
            
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"ğŸ“Š Sá»‘ lÆ°á»£ng records: {len(df)}")
                logger.info(f"ğŸ“‹ CÃ¡c cá»™t: {list(df.columns)}")
                
                # Chuáº©n hÃ³a tÃªn cá»™t
                df.columns = df.columns.str.lower().str.strip()
                
                # TÃ¬m cá»™t chá»©a commit message
                message_cols = [col for col in df.columns if 
                              any(keyword in col for keyword in ['message', 'commit', 'msg', 'text', 'description'])]
                
                if not message_cols:
                    logger.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t commit message trong {csv_file}")
                    continue
                
                message_col = message_cols[0]
                logger.info(f"ğŸ“ Sá»­ dá»¥ng cá»™t '{message_col}' lÃ m commit message")
                
                # Xá»­ lÃ½ dá»¯ liá»‡u
                for _, row in df.iterrows():
                    commit_msg = str(row.get(message_col, '')).strip()
                    
                    if not commit_msg or commit_msg == 'nan' or len(commit_msg) < 5:
                        continue
                    
                    # TrÃ­ch xuáº¥t thÃ´ng tin khÃ¡c náº¿u cÃ³
                    author = str(row.get('author', row.get('committer', 'unknown'))).strip()
                    repo = str(row.get('repo', row.get('repository', row.get('project', 'unknown')))).strip()
                    
                    # PhÃ¢n loáº¡i commit dá»±a trÃªn message
                    commit_type = self.classify_commit_type(commit_msg)
                    purpose = self.classify_commit_purpose(commit_msg)
                    sentiment = self.classify_sentiment(commit_msg)
                    tech_tags = self.extract_tech_tags(commit_msg)
                    
                    data_point = {
                        'commit_message': commit_msg,
                        'commit_type': commit_type,
                        'purpose': purpose,
                        'sentiment': sentiment,
                        'tech_tag': tech_tags[0] if tech_tags else 'general',
                        'author': author if author != 'nan' else 'unknown',
                        'source_repo': repo if repo != 'nan' else 'unknown'
                    }
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi xá»­ lÃ½ file {csv_file}: {e}")
                continue
        
        logger.info(f"âœ… Tá»•ng cá»™ng xá»­ lÃ½ Ä‘Æ°á»£c {len(all_data)} commit messages")
        return {'data': all_data, 'total_count': len(all_data)}
    
    def classify_commit_type(self, message: str) -> str:
        """PhÃ¢n loáº¡i loáº¡i commit dá»±a trÃªn message"""
        message_lower = message.lower()
        
        # Conventional commit patterns
        if message_lower.startswith(('feat:', 'feature:')):return 'feat'
        elif message_lower.startswith(('fix:', 'bugfix:')):return 'fix'
        elif message_lower.startswith(('docs:', 'doc:')):return 'docs'
        elif message_lower.startswith(('style:', 'format:')):return 'style'
        elif message_lower.startswith(('refactor:', 'refact:')):return 'refactor'
        elif message_lower.startswith(('test:', 'tests:')):return 'test'
        elif message_lower.startswith(('chore:', 'build:', 'ci:')):return 'chore'
        
        # Keyword-based classification
        elif any(word in message_lower for word in ['add', 'implement', 'create', 'new']):
            return 'feat'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            return 'fix'
        elif any(word in message_lower for word in ['update', 'modify', 'change']):
            return 'feat'
        elif any(word in message_lower for word in ['remove', 'delete', 'clean']):
            return 'chore'
        elif any(word in message_lower for word in ['test', 'spec']):
            return 'test'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
            return 'docs'
        else:
            return 'other'
    
    def classify_commit_purpose(self, message: str) -> str:
        """PhÃ¢n loáº¡i má»¥c Ä‘Ã­ch commit"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['feature', 'feat', 'add', 'implement', 'new']):
            return 'Feature Implementation'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'patch']):
            return 'Bug Fix'
        elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
            return 'Refactoring'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
            return 'Documentation Update'
        elif any(word in message_lower for word in ['test', 'spec', 'testing']):
            return 'Test Update'
        elif any(word in message_lower for word in ['security', 'secure', 'vulnerability']):
            return 'Security Patch'
        elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
            return 'Code Style/Formatting'
        elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy', 'pipeline']):
            return 'Build/CI/CD Script Update'
        else:
            return 'Other'
    
    def classify_sentiment(self, message: str) -> str:
        """PhÃ¢n loáº¡i cáº£m xÃºc trong commit message"""
        message_lower = message.lower()
        
        positive_words = ['improve', 'enhance', 'optimize', 'upgrade', 'better', 'good', 'great', 'awesome']
        negative_words = ['bug', 'error', 'issue', 'problem', 'fail', 'broken', 'wrong']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        if any(word in message_lower for word in urgent_words):
            return 'urgent'
        elif any(word in message_lower for word in positive_words):
            return 'positive'
        elif any(word in message_lower for word in negative_words):
            return 'negative'
        else:
            return 'neutral'
    
    def extract_tech_tags(self, message: str) -> List[str]:
        """TrÃ­ch xuáº¥t cÃ¡c tag cÃ´ng nghá»‡ tá»« commit message"""
        message_lower = message.lower()
        tech_tags = []
        
        tech_keywords = {
            'javascript': ['js', 'javascript', 'node', 'npm', 'yarn'],
            'python': ['python', 'py', 'pip', 'django', 'flask'],
            'java': ['java', 'maven', 'gradle', 'spring'],
            'react': ['react', 'jsx', 'component'],
            'vue': ['vue', 'vuex', 'nuxt'],
            'angular': ['angular', 'ng', 'typescript'],
            'css': ['css', 'sass', 'scss', 'less', 'style'],
            'html': ['html', 'dom', 'markup'],
            'database': ['sql', 'mysql', 'postgres', 'mongodb', 'database', 'db'],
            'api': ['api', 'rest', 'graphql', 'endpoint'],
            'docker': ['docker', 'container', 'dockerfile'],
            'git': ['git', 'merge', 'branch', 'commit'],
            'testing': ['test', 'spec', 'jest', 'mocha', 'junit'],
            'security': ['security', 'auth', 'oauth', 'jwt', 'ssl'],
            'performance': ['performance', 'optimize', 'cache', 'speed'],
            'ui': ['ui', 'ux', 'interface', 'design', 'layout']
        }
        
        for category, keywords in tech_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                tech_tags.append(category)
        
        return tech_tags if tech_tags else ['general']
    
    def save_processed_data(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ theo Ä‘á»‹nh dáº¡ng cho HAN model
        
        Args:
            data: Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
            filename: TÃªn file Ä‘á»ƒ lÆ°u
            
        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kaggle_training_data_{timestamp}.json'
        
        filepath = os.path.join(self.processed_dir, filename)
        
        # Chuáº©n bá»‹ dá»¯ liá»‡u theo format HAN
        han_format_data = []
        
        for item in data['data']:
            han_item = {
                'text': item['commit_message'],
                'labels': {
                    'commit_type': item['commit_type'],
                    'purpose': item['purpose'],
                    'sentiment': item['sentiment'],
                    'tech_tag': item['tech_tag'],
                    'author': item['author'],
                    'source_repo': item['source_repo']
                }
            }
            han_format_data.append(han_item)
        
        # Thá»‘ng kÃª dá»¯ liá»‡u
        stats = self.generate_statistics(han_format_data)
        
        # LÆ°u file
        output_data = {
            'metadata': {
                'total_samples': len(han_format_data),
                'created_at': datetime.now().isoformat(),
                'source': 'kaggle_datasets',
                'statistics': stats
            },
            'data': han_format_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ÄÃ£ lÆ°u {len(han_format_data)} samples vÃ o {filepath}")
        return filepath
    
    def generate_statistics(self, data: List[Dict]) -> Dict[str, Any]:
        """Táº¡o thá»‘ng kÃª cho dá»¯ liá»‡u"""
        stats = {}
        
        # Äáº¿m theo tá»«ng label category
        for label_type in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            label_counts = {}
            for item in data:
                label = item['labels'][label_type]
                label_counts[label] = label_counts.get(label, 0) + 1
            stats[label_type] = label_counts
        
        # Thá»‘ng kÃª Ä‘á»™ dÃ i text
        text_lengths = [len(item['text'].split()) for item in data]
        stats['text_length'] = {
            'min': min(text_lengths),
            'max': max(text_lengths),
            'mean': np.mean(text_lengths),
            'median': np.median(text_lengths)
        }
        
        return stats
    
    def download_and_process_datasets(self, dataset_names: List[str] = None) -> List[str]:
        """
        Táº£i vÃ  xá»­ lÃ½ nhiá»u dataset cÃ¹ng lÃºc
        
        Args:
            dataset_names: Danh sÃ¡ch tÃªn dataset, náº¿u None sáº½ dÃ¹ng danh sÃ¡ch máº·c Ä‘á»‹nh
            
        Returns:
            List[str]: Danh sÃ¡ch Ä‘Æ°á»ng dáº«n file Ä‘Ã£ xá»­ lÃ½
        """
        if not dataset_names:
            dataset_names = self.list_popular_commit_datasets()
        
        processed_files = []
        
        logger.info(f"ğŸ¯ Báº¯t Ä‘áº§u táº£i vÃ  xá»­ lÃ½ {len(dataset_names)} datasets")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            logger.info(f"\nğŸ“¦ [{i}/{len(dataset_names)}] Xá»­ lÃ½ dataset: {dataset_name}")
            
            # Táº£i dataset
            if not self.download_dataset(dataset_name):
                logger.warning(f"âš ï¸ Bá» qua dataset {dataset_name} do lá»—i táº£i xuá»‘ng")
                continue
            
            # TÃ¬m file CSV trong thÆ° má»¥c dataset
            dataset_dir = os.path.join(self.data_dir)
            csv_files = []
            
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(root, file))
            
            if not csv_files:
                logger.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file CSV trong dataset {dataset_name}")
                continue
            
            # Xá»­ lÃ½ dá»¯ liá»‡u
            try:
                processed_data = self.process_commit_dataset(csv_files)
                
                if processed_data['total_count'] > 0:
                    # LÆ°u dá»¯ liá»‡u vá»›i tÃªn dataset
                    dataset_short_name = dataset_name.split('/')[-1].replace('-', '_')
                    filename = f'kaggle_{dataset_short_name}_{datetime.now().strftime("%Y%m%d")}.json'
                    
                    saved_file = self.save_processed_data(processed_data, filename)
                    processed_files.append(saved_file)
                else:
                    logger.warning(f"âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ tá»« dataset {dataset_name}")
                    
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi xá»­ lÃ½ dataset {dataset_name}: {e}")
                continue
        
        logger.info(f"\nğŸ‰ HoÃ n thÃ nh! ÄÃ£ xá»­ lÃ½ {len(processed_files)} datasets thÃ nh cÃ´ng")
        return processed_files
    
    def merge_datasets(self, json_files: List[str], output_filename: str = None) -> str:
        """
        Gá»™p nhiá»u file JSON thÃ nh má»™t file duy nháº¥t
        
        Args:
            json_files: Danh sÃ¡ch Ä‘Æ°á»ng dáº«n file JSON
            output_filename: TÃªn file output
            
        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ gá»™p
        """
        if not output_filename:
            output_filename = f'merged_kaggle_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        output_path = os.path.join(self.processed_dir, output_filename)
        
        all_data = []
        total_stats = {}
        
        logger.info(f"ğŸ”„ Gá»™p {len(json_files)} files")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    all_data.extend(file_data['data'])
                    
                    # Gá»™p thá»‘ng kÃª
                    if 'statistics' in file_data.get('metadata', {}):
                        file_stats = file_data['metadata']['statistics']
                        for key, value in file_stats.items():
                            if key not in total_stats:
                                total_stats[key] = {}
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if subkey in total_stats[key]:
                                        total_stats[key][subkey] += subvalue
                                    else:
                                        total_stats[key][subkey] = subvalue
                        
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi Ä‘á»c file {json_file}: {e}")
                continue
        
        # LÆ°u file gá»™p
        merged_data = {
            'metadata': {
                'total_samples': len(all_data),
                'created_at': datetime.now().isoformat(),
                'source': 'merged_kaggle_datasets',
                'source_files': json_files,
                'statistics': total_stats
            },
            'data': all_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ÄÃ£ gá»™p {len(all_data)} samples vÃ o {output_path}")
        return output_path


def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y script"""
    print("=" * 80)
    print("ğŸš€ KAGGLE DATASET DOWNLOADER VÃ€ PROCESSOR CHO HAN MODEL")
    print("=" * 80)
    
    # Khá»Ÿi táº¡o downloader
    downloader = KaggleDatasetDownloader()
    
    # Hiá»ƒn thá»‹ menu
    print("\nğŸ“‹ CÃ¡c tÃ¹y chá»n:")
    print("1. Táº£i vÃ  xá»­ lÃ½ táº¥t cáº£ datasets phá»• biáº¿n")
    print("2. Táº£i vÃ  xá»­ lÃ½ dataset cá»¥ thá»ƒ")
    print("3. Chá»‰ xá»­ lÃ½ dá»¯ liá»‡u cÃ³ sáºµn")
    print("4. Hiá»ƒn thá»‹ danh sÃ¡ch datasets phá»• biáº¿n")
    
    choice = input("\nğŸ”¸ Chá»n tÃ¹y chá»n (1-4): ").strip()
    
    if choice == '1':
        # Táº£i táº¥t cáº£ datasets phá»• biáº¿n
        logger.info("ğŸ“¦ Táº£i táº¥t cáº£ datasets phá»• biáº¿n...")
        processed_files = downloader.download_and_process_datasets()
        
        if processed_files:
            # Gá»™p táº¥t cáº£ files
            if len(processed_files) > 1:
                merged_file = downloader.merge_datasets(processed_files)
                logger.info(f"ğŸ¯ File dá»¯ liá»‡u cuá»‘i cÃ¹ng: {merged_file}")
            else:
                logger.info(f"ğŸ¯ File dá»¯ liá»‡u: {processed_files[0]}")
        
    elif choice == '2':
        # Táº£i dataset cá»¥ thá»ƒ
        dataset_name = input("ğŸ”¸ Nháº­p tÃªn dataset (format: username/dataset-name): ").strip()
        if dataset_name:
            processed_files = downloader.download_and_process_datasets([dataset_name])
            if processed_files:
                logger.info(f"ğŸ¯ File dá»¯ liá»‡u: {processed_files[0]}")
        
    elif choice == '3':
        # Xá»­ lÃ½ dá»¯ liá»‡u cÃ³ sáºµn
        csv_files = []
        for root, dirs, files in os.walk(downloader.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
        
        if csv_files:
            logger.info(f"ğŸ” TÃ¬m tháº¥y {len(csv_files)} file CSV")
            processed_data = downloader.process_commit_dataset(csv_files)
            if processed_data['total_count'] > 0:
                saved_file = downloader.save_processed_data(processed_data)
                logger.info(f"ğŸ¯ File dá»¯ liá»‡u: {saved_file}")
        else:
            logger.warning("âŒ KhÃ´ng tÃ¬m tháº¥y file CSV nÃ o")
        
    elif choice == '4':
        # Hiá»ƒn thá»‹ danh sÃ¡ch
        datasets = downloader.list_popular_commit_datasets()
        print("\nğŸ“‹ Danh sÃ¡ch datasets commit phá»• biáº¿n:")
        for i, dataset in enumerate(datasets, 1):
            print(f"  {i}. {dataset}")
    
    else:
        logger.error("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")
    
    print("\n" + "=" * 80)
    print("âœ… HOÃ€N THÃ€NH!")
    print("=" * 80)


if __name__ == "__main__":
    main()
