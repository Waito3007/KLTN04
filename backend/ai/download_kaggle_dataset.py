"""
Script ƒë·ªÉ t·∫£i dataset commit t·ª´ Kaggle v√† chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh HAN
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import zipfile
import shutil
from typing import List, Dict, Any, Tuple
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class KaggleDatasetDownloader:
    def __init__(self, base_dir: str = None):
        """
        Kh·ªüi t·∫°o class ƒë·ªÉ t·∫£i dataset t·ª´ Kaggle
        
        Args:
            base_dir: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u d·ªØ li·ªáu
        """
        self.base_dir = base_dir or os.path.dirname(__file__)
        self.data_dir = os.path.join(self.base_dir, 'kaggle_data')
        self.processed_dir = os.path.join(self.base_dir, 'training_data')
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def check_kaggle_config(self) -> bool:
        """Ki·ªÉm tra c·∫•u h√¨nh Kaggle API"""
        try:
            import kaggle
            logger.info("‚úÖ Kaggle API ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh")
            return True
        except ImportError:
            logger.error("‚ùå Kaggle package ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install kaggle")
            return False
        except OSError as e:
            logger.error(f"‚ùå L·ªói c·∫•u h√¨nh Kaggle API: {e}")
            logger.info("Vui l√≤ng:")
            logger.info("1. T·∫°o API token t·∫°i: https://www.kaggle.com/settings")
            logger.info("2. ƒê·∫∑t file kaggle.json v√†o ~/.kaggle/ (Linux/Mac) ho·∫∑c C:\\Users\\<username>\\.kaggle\\ (Windows)")
            logger.info("3. C·∫•p quy·ªÅn 600 cho file: chmod 600 ~/.kaggle/kaggle.json")
            return False
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """
        T·∫£i dataset t·ª´ Kaggle
        
        Args:
            dataset_name: T√™n dataset tr√™n Kaggle (format: username/dataset-name)
            force_download: C√≥ t·∫£i l·∫°i n·∫øu ƒë√£ t·ªìn t·∫°i hay kh√¥ng
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        if not self.check_kaggle_config():
            return False
            
        try:
            import kaggle
            
            dataset_path = os.path.join(self.data_dir, dataset_name.split('/')[-1])
            
            if os.path.exists(dataset_path) and not force_download:
                logger.info(f"Dataset {dataset_name} ƒë√£ t·ªìn t·∫°i, b·ªè qua t·∫£i xu·ªëng")
                return True
                
            logger.info(f"üîÑ ƒêang t·∫£i dataset: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=self.data_dir, 
                unzip=True
            )
            
            logger.info(f"‚úÖ T·∫£i th√†nh c√¥ng dataset: {dataset_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫£i dataset {dataset_name}: {e}")
            return False
    
    def list_popular_commit_datasets(self) -> List[str]:
        """Li·ªát k√™ c√°c dataset commit ph·ªï bi·∫øn tr√™n Kaggle"""
        return [
            "shashankbansal6/git-commits-message-dataset",
            "madhav28/git-commit-messages",
            "aashita/git-commit-messages",
            "jainaru/commit-classification-dataset",
            "shubhamjain0594/commit-message-generation",
            "saurabhshahane/conventional-commit-messages",
            "devanshunigam/commits",
            "ashydv/commits-dataset"
        ]
    
    def process_commit_dataset(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu commit t·ª´ c√°c file CSV
        
        Args:
            csv_files: Danh s√°ch c√°c file CSV
            
        Returns:
            Dict ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        """
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"üîÑ ƒêang x·ª≠ l√Ω file: {csv_file}")
            
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"üìä S·ªë l∆∞·ª£ng records: {len(df)}")
                logger.info(f"üìã C√°c c·ªôt: {list(df.columns)}")
                
                # Chu·∫©n h√≥a t√™n c·ªôt
                df.columns = df.columns.str.lower().str.strip()
                
                # T√¨m c·ªôt ch·ª©a commit message
                message_cols = [col for col in df.columns if 
                              any(keyword in col for keyword in ['message', 'commit', 'msg', 'text', 'description'])]
                
                if not message_cols:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt commit message trong {csv_file}")
                    continue
                
                message_col = message_cols[0]
                logger.info(f"üìù S·ª≠ d·ª•ng c·ªôt '{message_col}' l√†m commit message")
                
                # X·ª≠ l√Ω d·ªØ li·ªáu
                for _, row in df.iterrows():
                    commit_msg = str(row.get(message_col, '')).strip()
                    
                    if not commit_msg or commit_msg == 'nan' or len(commit_msg) < 5:
                        continue
                    
                    # Tr√≠ch xu·∫•t th√¥ng tin kh√°c n·∫øu c√≥
                    author = str(row.get('author', row.get('committer', 'unknown'))).strip()
                    repo = str(row.get('repo', row.get('repository', row.get('project', 'unknown')))).strip()
                    
                    # Ph√¢n lo·∫°i commit d·ª±a tr√™n message
                    commit_type = self.classify_commit_type(commit_msg)
                    purpose = self.classify_commit_purpose(commit_msg)
                    sentiment = self.classify_sentiment(commit_msg)
                    tech_tags = self.extract_tech_tags(commit_msg)
                    
                    data_point = {
                        'commit_message': commit_msg,
                        'commit_type': commit_type,
                        'purpose': purpose,
                        'sentiment': sentiment,
                        'tech_tag': tech_tags[0] if tech_tags else 'general',
                        'author': author if author != 'nan' else 'unknown',
                        'source_repo': repo if repo != 'nan' else 'unknown'
                    }
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω file {csv_file}: {e}")
                continue
        
        logger.info(f"‚úÖ T·ªïng c·ªông x·ª≠ l√Ω ƒë∆∞·ª£c {len(all_data)} commit messages")
        return {'data': all_data, 'total_count': len(all_data)}
    
    def classify_commit_type(self, message: str) -> str:
        """Ph√¢n lo·∫°i lo·∫°i commit d·ª±a tr√™n message"""
        message_lower = message.lower()
        
        # Conventional commit patterns
        if message_lower.startswith(('feat:', 'feature:')):return 'feat'
        elif message_lower.startswith(('fix:', 'bugfix:')):return 'fix'
        elif message_lower.startswith(('docs:', 'doc:')):return 'docs'
        elif message_lower.startswith(('style:', 'format:')):return 'style'
        elif message_lower.startswith(('refactor:', 'refact:')):return 'refactor'
        elif message_lower.startswith(('test:', 'tests:')):return 'test'
        elif message_lower.startswith(('chore:', 'build:', 'ci:')):return 'chore'
        
        # Keyword-based classification
        elif any(word in message_lower for word in ['add', 'implement', 'create', 'new']):
            return 'feat'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            return 'fix'
        elif any(word in message_lower for word in ['update', 'modify', 'change']):
            return 'feat'
        elif any(word in message_lower for word in ['remove', 'delete', 'clean']):
            return 'chore'
        elif any(word in message_lower for word in ['test', 'spec']):
            return 'test'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
            return 'docs'
        else:
            return 'other'
    
    def classify_commit_purpose(self, message: str) -> str:
        """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch commit"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['feature', 'feat', 'add', 'implement', 'new']):
            return 'Feature Implementation'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'patch']):
            return 'Bug Fix'
        elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
            return 'Refactoring'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
            return 'Documentation Update'
        elif any(word in message_lower for word in ['test', 'spec', 'testing']):
            return 'Test Update'
        elif any(word in message_lower for word in ['security', 'secure', 'vulnerability']):
            return 'Security Patch'
        elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
            return 'Code Style/Formatting'
        elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy', 'pipeline']):
            return 'Build/CI/CD Script Update'
        else:
            return 'Other'
    
    def classify_sentiment(self, message: str) -> str:
        """Ph√¢n lo·∫°i c·∫£m x√∫c trong commit message"""
        message_lower = message.lower()
        
        positive_words = ['improve', 'enhance', 'optimize', 'upgrade', 'better', 'good', 'great', 'awesome']
        negative_words = ['bug', 'error', 'issue', 'problem', 'fail', 'broken', 'wrong']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        if any(word in message_lower for word in urgent_words):
            return 'urgent'
        elif any(word in message_lower for word in positive_words):
            return 'positive'
        elif any(word in message_lower for word in negative_words):
            return 'negative'
        else:
            return 'neutral'
    
    def extract_tech_tags(self, message: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c tag c√¥ng ngh·ªá t·ª´ commit message"""
        message_lower = message.lower()
        tech_tags = []
        
        tech_keywords = {
            'javascript': ['js', 'javascript', 'node', 'npm', 'yarn'],
            'python': ['python', 'py', 'pip', 'django', 'flask'],
            'java': ['java', 'maven', 'gradle', 'spring'],
            'react': ['react', 'jsx', 'component'],
            'vue': ['vue', 'vuex', 'nuxt'],
            'angular': ['angular', 'ng', 'typescript'],
            'css': ['css', 'sass', 'scss', 'less', 'style'],
            'html': ['html', 'dom', 'markup'],
            'database': ['sql', 'mysql', 'postgres', 'mongodb', 'database', 'db'],
            'api': ['api', 'rest', 'graphql', 'endpoint'],
            'docker': ['docker', 'container', 'dockerfile'],
            'git': ['git', 'merge', 'branch', 'commit'],
            'testing': ['test', 'spec', 'jest', 'mocha', 'junit'],
            'security': ['security', 'auth', 'oauth', 'jwt', 'ssl'],
            'performance': ['performance', 'optimize', 'cache', 'speed'],
            'ui': ['ui', 'ux', 'interface', 'design', 'layout']
        }
        
        for category, keywords in tech_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                tech_tags.append(category)
        
        return tech_tags if tech_tags else ['general']
    
    def save_processed_data(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω theo ƒë·ªãnh d·∫°ng cho HAN model
        
        Args:
            data: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            filename: T√™n file ƒë·ªÉ l∆∞u
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kaggle_training_data_{timestamp}.json'
        
        filepath = os.path.join(self.processed_dir, filename)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu theo format HAN
        han_format_data = []
        
        for item in data['data']:
            han_item = {
                'text': item['commit_message'],
                'labels': {
                    'commit_type': item['commit_type'],
                    'purpose': item['purpose'],
                    'sentiment': item['sentiment'],
                    'tech_tag': item['tech_tag'],
                    'author': item['author'],
                    'source_repo': item['source_repo']
                }
            }
            han_format_data.append(han_item)
        
        # Th·ªëng k√™ d·ªØ li·ªáu
        stats = self.generate_statistics(han_format_data)
        
        # L∆∞u file
        output_data = {
            'metadata': {
                'total_samples': len(han_format_data),
                'created_at': datetime.now().isoformat(),
                'source': 'kaggle_datasets',
                'statistics': stats
            },
            'data': han_format_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(han_format_data)} samples v√†o {filepath}")
        return filepath
    
    def generate_statistics(self, data: List[Dict]) -> Dict[str, Any]:
        """T·∫°o th·ªëng k√™ cho d·ªØ li·ªáu"""
        stats = {}
        
        # ƒê·∫øm theo t·ª´ng label category
        for label_type in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            label_counts = {}
            for item in data:
                label = item['labels'][label_type]
                label_counts[label] = label_counts.get(label, 0) + 1
            stats[label_type] = label_counts
        
        # Th·ªëng k√™ ƒë·ªô d√†i text
        text_lengths = [len(item['text'].split()) for item in data]
        stats['text_length'] = {
            'min': min(text_lengths),
            'max': max(text_lengths),
            'mean': np.mean(text_lengths),
            'median': np.median(text_lengths)
        }
        
        return stats
    
    def download_and_process_datasets(self, dataset_names: List[str] = None) -> List[str]:
        """
        T·∫£i v√† x·ª≠ l√Ω nhi·ªÅu dataset c√πng l√∫c
        
        Args:
            dataset_names: Danh s√°ch t√™n dataset, n·∫øu None s·∫Ω d√πng danh s√°ch m·∫∑c ƒë·ªãnh
            
        Returns:
            List[str]: Danh s√°ch ƒë∆∞·ªùng d·∫´n file ƒë√£ x·ª≠ l√Ω
        """
        if not dataset_names:
            dataset_names = self.list_popular_commit_datasets()
        
        processed_files = []
        
        logger.info(f"üéØ B·∫Øt ƒë·∫ßu t·∫£i v√† x·ª≠ l√Ω {len(dataset_names)} datasets")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            logger.info(f"\nüì¶ [{i}/{len(dataset_names)}] X·ª≠ l√Ω dataset: {dataset_name}")
            
            # T·∫£i dataset
            if not self.download_dataset(dataset_name):
                logger.warning(f"‚ö†Ô∏è B·ªè qua dataset {dataset_name} do l·ªói t·∫£i xu·ªëng")
                continue
            
            # T√¨m file CSV trong th∆∞ m·ª•c dataset
            dataset_dir = os.path.join(self.data_dir)
            csv_files = []
            
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(root, file))
            
            if not csv_files:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file CSV trong dataset {dataset_name}")
                continue
            
            # X·ª≠ l√Ω d·ªØ li·ªáu
            try:
                processed_data = self.process_commit_dataset(csv_files)
                
                if processed_data['total_count'] > 0:
                    # L∆∞u d·ªØ li·ªáu v·ªõi t√™n dataset
                    dataset_short_name = dataset_name.split('/')[-1].replace('-', '_')
                    filename = f'kaggle_{dataset_short_name}_{datetime.now().strftime("%Y%m%d")}.json'
                    
                    saved_file = self.save_processed_data(processed_data, filename)
                    processed_files.append(saved_file)
                else:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá t·ª´ dataset {dataset_name}")
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω dataset {dataset_name}: {e}")
                continue
        
        logger.info(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {len(processed_files)} datasets th√†nh c√¥ng")
        return processed_files
    
    def merge_datasets(self, json_files: List[str], output_filename: str = None) -> str:
        """
        G·ªôp nhi·ªÅu file JSON th√†nh m·ªôt file duy nh·∫•t
        
        Args:
            json_files: Danh s√°ch ƒë∆∞·ªùng d·∫´n file JSON
            output_filename: T√™n file output
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ g·ªôp
        """
        if not output_filename:
            output_filename = f'merged_kaggle_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        output_path = os.path.join(self.processed_dir, output_filename)
        
        all_data = []
        total_stats = {}
        
        logger.info(f"üîÑ G·ªôp {len(json_files)} files")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    all_data.extend(file_data['data'])
                    
                    # G·ªôp th·ªëng k√™
                    if 'statistics' in file_data.get('metadata', {}):
                        file_stats = file_data['metadata']['statistics']
                        for key, value in file_stats.items():
                            if key not in total_stats:
                                total_stats[key] = {}
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if subkey in total_stats[key]:
                                        total_stats[key][subkey] += subvalue
                                    else:
                                        total_stats[key][subkey] = subvalue
                        
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ƒë·ªçc file {json_file}: {e}")
                continue
        
        # L∆∞u file g·ªôp
        merged_data = {
            'metadata': {
                'total_samples': len(all_data),
                'created_at': datetime.now().isoformat(),
                'source': 'merged_kaggle_datasets',
                'source_files': json_files,
                'statistics': total_stats
            },
            'data': all_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ g·ªôp {len(all_data)} samples v√†o {output_path}")
        return output_path


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y script"""
    print("=" * 80)
    print("üöÄ KAGGLE DATASET DOWNLOADER V√Ä PROCESSOR CHO HAN MODEL")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o downloader
    downloader = KaggleDatasetDownloader()
    
    # Hi·ªÉn th·ªã menu
    print("\nüìã C√°c t√πy ch·ªçn:")
    print("1. T·∫£i v√† x·ª≠ l√Ω t·∫•t c·∫£ datasets ph·ªï bi·∫øn")
    print("2. T·∫£i v√† x·ª≠ l√Ω dataset c·ª• th·ªÉ")
    print("3. Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn")
    print("4. Hi·ªÉn th·ªã danh s√°ch datasets ph·ªï bi·∫øn")
    
    choice = input("\nüî∏ Ch·ªçn t√πy ch·ªçn (1-4): ").strip()
    
    if choice == '1':
        # T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn
        logger.info("üì¶ T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn...")
        processed_files = downloader.download_and_process_datasets()
        
        if processed_files:
            # G·ªôp t·∫•t c·∫£ files
            if len(processed_files) > 1:
                merged_file = downloader.merge_datasets(processed_files)
                logger.info(f"üéØ File d·ªØ li·ªáu cu·ªëi c√πng: {merged_file}")
            else:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '2':
        # T·∫£i dataset c·ª• th·ªÉ
        dataset_name = input("üî∏ Nh·∫≠p t√™n dataset (format: username/dataset-name): ").strip()
        if dataset_name:
            processed_files = downloader.download_and_process_datasets([dataset_name])
            if processed_files:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '3':
        # X·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn
        csv_files = []
        for root, dirs, files in os.walk(downloader.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
        
        if csv_files:
            logger.info(f"üîç T√¨m th·∫•y {len(csv_files)} file CSV")
            processed_data = downloader.process_commit_dataset(csv_files)
            if processed_data['total_count'] > 0:
                saved_file = downloader.save_processed_data(processed_data)
                logger.info(f"üéØ File d·ªØ li·ªáu: {saved_file}")
        else:
            logger.warning("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o")
        
    elif choice == '4':
        # Hi·ªÉn th·ªã danh s√°ch
        datasets = downloader.list_popular_commit_datasets()
        print("\nüìã Danh s√°ch datasets commit ph·ªï bi·∫øn:")
        for i, dataset in enumerate(datasets, 1):
            print(f"  {i}. {dataset}")
    
    else:
        logger.error("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
    
    print("\n" + "=" * 80)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("=" * 80)


if __name__ == "__main__":
    main()
