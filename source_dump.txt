# ==================================================
# Path: C:\SAN\KLTN\KLTN04
# Detected tech: javascript, python, react, rust, typescript
# ==================================================

## DIRECTORY STRUCTURE
```
KLTN04/
├── .git/
├── .venv/
├── backend/
│   ├── .venv/
│   ├── __pycache__/
│   ├── ai/
│   │   ├── __pycache__/
│   │   ├── baseline/
│   │   │   ├── bilstm_baseline.py
│   │   │   └── tfidf_logreg_baseline.py
│   │   ├── collected_data/
│   │   │   ├── .gitkeep
│   │   │   └── commit_messages_raw.json
│   │   ├── data_preparation/
│   │   │   ├── balance_training_data.py
│   │   │   ├── build_training_data.py
│   │   │   └── clean_training_data.py
│   │   ├── data_preprocessing/
│   │   │   ├── __pycache__/
│   │   │   ├── embedding_loader.py
│   │   │   └── text_processor.py
│   │   ├── evaluation/
│   │   │   ├── __pycache__/
│   │   │   ├── attention_visualizer.py
│   │   │   └── metrics_calculator.py
│   │   ├── modelAi/
│   │   │   ├── __pycache__/
│   │   │   ├── ner/
│   │   │   │   ├── cfg
│   │   │   │   ├── model
│   │   │   │   └── moves
│   │   │   ├── textcat/
│   │   │   │   ├── cfg
│   │   │   │   └── model
│   │   │   ├── textcat_multilabel/
│   │   │   │   ├── cfg
│   │   │   │   └── model
│   │   │   ├── vocab/
│   │   │   │   ├── key2row
│   │   │   │   ├── lookups.bin
│   │   │   │   ├── strings.json
│   │   │   │   ├── vectors
│   │   │   │   └── vectors.cfg
│   │   │   ├── config.cfg
│   │   │   ├── fetch_github_commits.py
│   │   │   ├── generate_data.py
│   │   │   ├── meta.json
│   │   │   └── tokenizer
│   │   ├── models/
│   │   │   ├── __pycache__/
│   │   │   ├── han_multitask.pth
│   │   │   ├── han_multitask_best.pth
│   │   │   ├── han_multitask_final.pth
│   │   │   ├── hierarchical_attention.py
│   │   │   └── shared_layers.py
│   │   ├── scriptPhantich/
│   │   │   └── analyze_data_balance.py
│   │   ├── scripts/
│   │   │   ├── extract_code_comments.py
│   │   │   ├── extract_code_diffs.py
│   │   │   └── extract_readme.py
│   │   ├── taotukhoa/
│   │   │   ├── subintent_keywords.json
│   │   │   └── taotukhoa.py
│   │   ├── testmodel/
│   │   │   ├── test_han.py
│   │   │   └── test_model.py
│   │   ├── training/
│   │   │   ├── __pycache__/
│   │   │   ├── loss_functions.py
│   │   │   └── multitask_trainer.py
│   │   ├── training_data/
│   │   │   └── han_training_samples.json
│   │   ├── training_logs/
│   │   │   ├── training_log_20250531_173345.txt
│   │   │   ├── training_log_20250531_173411.txt
│   │   │   ├── training_log_20250531_173727.txt
│   │   │   ├── training_log_20250531_174058.txt
│   │   │   ├── training_log_20250531_174334.txt
│   │   │   ├── training_log_20250531_174738.txt
│   │   │   └── training_log_20250531_174946.txt
│   │   ├── __init__.py
│   │   ├── analyze_data_balance.py
│   │   ├── predict.py
│   │   ├── train_han_multitask_fixed.py
│   │   └── train_spacy.py
│   ├── api/
│   │   ├── __pycache__/
│   │   ├── routes/
│   │   │   ├── __pycache__/
│   │   │   ├── __init__.py
│   │   │   ├── ai_suggestions.py
│   │   │   ├── auth.py
│   │   │   ├── commit_routes.py
│   │   │   ├── github.py
│   │   │   ├── gitlab.py
│   │   │   ├── repo.py
│   │   │   └── users.py
│   │   ├── __init__.py
│   │   └── deps.py
│   ├── core/
│   │   ├── __pycache__/
│   │   ├── config.py
│   │   ├── lifespan.py
│   │   ├── logger.py
│   │   ├── oauth.py
│   │   └── security.py
│   ├── db/
│   ├── migrations/
│   │   ├── __pycache__/
│   │   ├── versions/
│   │   │   └── __pycache__/
│   │   ├── README
│   │   ├── env.py
│   │   └── script.py.mako
│   ├── models/
│   │   ├── commit_classifier.joblib
│   │   ├── commit_classifier_v1.joblib
│   │   ├── commit_model.py
│   │   ├── han_multitask.pth
│   │   └── task_model.py
│   ├── notebooks/
│   │   └── Model_Training.ipynb
│   ├── schemas/
│   │   ├── __pycache__/
│   │   └── commit.py
│   ├── scripts/
│   │   ├── commit_analysis_system.py
│   │   └── commit_analysis_system_v1.py
│   ├── services/
│   │   ├── __pycache__/
│   │   ├── __init__.py
│   │   ├── ai_model.py
│   │   ├── ai_service.py
│   │   ├── branch_service.py
│   │   ├── commit_service.py
│   │   ├── github_service.py
│   │   ├── gitlab_service.py
│   │   ├── issue_service.py
│   │   ├── model_loader.py
│   │   ├── repo_service.py
│   │   ├── report_generator.py
│   │   └── user_service.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── formatter.py
│   │   └── scheduler.py
│   ├── __init__.py
│   ├── alembic.ini
│   ├── main.py
│   └── requirements.txt
├── frontend/
│   ├── node_modules/
│   ├── public/
│   │   └── vite.svg
│   ├── src/
│   │   ├── api/
│   │   │   └── github.js
│   │   ├── assets/
│   │   │   └── react.svg
│   │   ├── components/
│   │   │   ├── Branchs/
│   │   │   │   └── BranchSelector.jsx
│   │   │   ├── Dashboard/
│   │   │   │   ├── AIInsightWidget.jsx
│   │   │   │   ├── OverviewCard.jsx
│   │   │   │   ├── RepoListFilter.jsx
│   │   │   │   └── TaskBoard.jsx
│   │   │   ├── commits/
│   │   │   │   ├── AnalyzeGitHubCommits.jsx
│   │   │   │   ├── CommitAnalysisBadge.jsx
│   │   │   │   ├── CommitAnalysisModal.jsx
│   │   │   │   ├── CommitList.jsx
│   │   │   │   └── CommitTable.jsx
│   │   │   └── repo/
│   │   │       └── RepoList.jsx
│   │   ├── features/
│   │   │   └── github/
│   │   │       └── GithubRepoFetcher.jsx
│   │   ├── pages/
│   │   │   ├── AuthSuccess.jsx
│   │   │   ├── Dashboard.jsx
│   │   │   ├── Login.jsx
│   │   │   └── RepoDetails.jsx
│   │   ├── utils/
│   │   │   └── types.js
│   │   ├── App.css
│   │   ├── App.jsx
│   │   ├── config.js
│   │   ├── index.css
│   │   └── main.jsx
│   ├── .gitignore
│   ├── README.md
│   ├── eslint.config.js
│   ├── index.html
│   ├── package-lock.json
│   ├── package.json
│   └── vite.config.js
├── .env
├── .gitignore
├── Backend.txt
├── Frontend.txt
├── README.md
├── poetry.lock
├── pyproject.toml
└── source_dump.txt
```

## FILE CONTENTS

### backend\main.py
```py
# backend/main.py
from fastapi import FastAPI
from core.lifespan import lifespan
from core.config import setup_middlewares
from core.logger import setup_logger
from core.config import setup_middlewares, setup_routers
from services.ai_service import router as ai_router
from api.routes.commit_routes import router as commit_router

from api.routes.auth import auth_router
from api.routes.github import github_router

setup_logger()  # Bật logger trước khi chạy app


app = FastAPI(lifespan=lifespan)

setup_routers(app)

setup_middlewares(app)

# Include routers trực tiếp
app.include_router(auth_router, prefix="/api")
app.include_router(github_router, prefix="/api")
app.include_router(ai_router, prefix="/ai")
app.include_router(commit_router)

@app.get("/")
def root():
    return {"message": "TaskFlowAI backend is running "}

```

### backend\__init__.py
```py

```

### backend\ai\analyze_data_balance.py
```py

```

### backend\ai\predict.py
```py
# Gọi mô hình để phân loại message
```

### backend\ai\train_han_multitask_fixed.py
```py
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from torch.utils.data import DataLoader, Dataset
from ai.models.hierarchical_attention import HierarchicalAttentionNetwork
from ai.data_preprocessing.text_processor import TextProcessor
from ai.data_preprocessing.embedding_loader import EmbeddingLoader
from ai.training.multitask_trainer import MultiTaskTrainer
from ai.training.loss_functions import UncertaintyWeightingLoss
from ai.evaluation.metrics_calculator import calc_metrics
import numpy as np
import json
import glob
import time
from datetime import datetime

# Kiểm tra và thiết lập device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

def save_training_log(epoch, train_loss, val_loss, task_metrics, log_file):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n=== Epoch {epoch} ({timestamp}) ===\n")
        f.write(f"Train Loss: {train_loss:.4f}\n")
        f.write(f"Val Loss: {val_loss:.4f}\n")
        for task, metrics in task_metrics.items():
            f.write(f"{task}: {metrics}\n")
        f.write("-" * 50 + "\n")

class CommitDataset(Dataset):
    def __init__(self, samples, processor, embed_loader, author_map=None, repo_map=None):
        self.samples = samples
        self.processor = processor
        self.embed_loader = embed_loader
        self.author_map = author_map or {}
        self.repo_map = repo_map or {}
        self.embedding_cache = {}
        
        # Chuẩn hóa ánh xạ nhãn cho từng task
        self.purpose_map = {
            'Feature Implementation': 0,
            'Bug Fix': 1,
            'Refactoring': 2,
            'Documentation Update': 3,
            'Test Update': 4,
            'Security Patch': 5,
            'Code Style/Formatting': 6,
            'Build/CI/CD Script Update': 7,
            'Other': 8
        }
        self.sentiment_map = {'positive': 0, 'neutral': 1, 'negative': 2}
        self.tech_vocab = [
            'python', 'fastapi', 'react', 'javascript', 'typescript', 'docker', 'sqlalchemy', 'pytorch', 'spacy',
            'css', 'html', 'postgresql', 'mysql', 'mongodb', 'redis', 'vue', 'angular', 'flask', 'django',
            'node', 'express', 'graphql', 'rest', 'api', 'gitlab', 'github', 'ci', 'cd', 'kubernetes', 'helm',
            'pytest', 'unittest', 'junit', 'cicd', 'github actions', 'travis', 'jenkins', 'circleci', 'webpack',
            'babel', 'vite', 'npm', 'yarn', 'pip', 'poetry', 'black', 'flake8', 'isort', 'prettier', 'eslint',
            'jwt', 'oauth', 'sso', 'celery', 'rabbitmq', 'kafka', 'grpc', 'protobuf', 'swagger', 'openapi',
            'sentry', 'prometheus', 'grafana', 'nginx', 'apache', 'linux', 'ubuntu', 'windows', 'macos',
            'aws', 'azure', 'gcp', 'firebase', 'heroku', 'netlify', 'vercel', 'tailwind', 'bootstrap', 'material ui'
        ]
        
        # Add mapping for commit types
        self.commit_type_map = {
            'feat': 0,
            'fix': 1,
            'docs': 2,
            'refactor': 3,
            'style': 4,
            'test': 5,
            'chore': 6,
            'uncategorized': 7
        }

        # Track user-specific commit statistics
        self.user_commit_stats = {}
        for sample in self.samples:
            user = sample.get('source_info', {}).get('author_name', 'Unknown')
            commit_type = sample.get('commit_type', 'uncategorized')
            if user not in self.user_commit_stats:
                self.user_commit_stats[user] = {ctype: 0 for ctype in self.commit_type_map.keys()}
            if commit_type in self.commit_type_map:
                self.user_commit_stats[user][commit_type] += 1

        # Pre-compute embeddings for all words in the dataset
        print("Pre-computing word embeddings...")
        self._precompute_embeddings()
        
    def _precompute_embeddings(self):
        # Collect all unique words
        unique_words = set()
        for sample in self.samples:
            doc = self.processor.process_document(sample['raw_text'])
            for sent in doc:
                unique_words.update(str(word) for word in sent)
        
        # Pre-compute embeddings
        for word in unique_words:
            if word not in self.embedding_cache:
                self.embedding_cache[word] = self.embed_loader.get_word_embedding(word)
        print(f"Cached embeddings for {len(self.embedding_cache)} unique words")

    def _detect_commit_type(self, raw_text):
        # Detect commit type from raw text based on conventional commit format
        if ':' not in raw_text:
            return 'uncategorized'
        prefix = raw_text.split(':')[0].lower().strip()
        if prefix in self.commit_type_map:
            return prefix
        if 'feat' in prefix or 'feature' in prefix:
            return 'feat'
        if 'fix' in prefix or 'bug' in prefix:
            return 'fix'
        if 'doc' in prefix:
            return 'docs'
        if 'refactor' in prefix:
            return 'refactor'
        if 'style' in prefix or 'format' in prefix:
            return 'style'
        if 'test' in prefix:
            return 'test'
        if 'chore' in prefix or 'build' in prefix or 'ci' in prefix:
            return 'chore'
        return 'uncategorized'

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        doc = self.processor.process_document(sample['raw_text'])
        embed_doc = np.zeros((self.processor.max_sent_len, self.processor.max_word_len, 768))
        
        # Use cached embeddings
        for i, sent in enumerate(doc):
            for j, word in enumerate(sent):
                word_str = str(word)
                embed_doc[i, j] = self.embedding_cache[word_str]
        
        # Convert numpy array to torch tensor, keep on CPU
        embed_doc = torch.tensor(embed_doc, dtype=torch.float32)
        
        # Detect commit type from raw text
        raw_text = sample.get('raw_text', '')
        commit_type = self._detect_commit_type(raw_text)
        commit_type_idx = self.commit_type_map.get(commit_type, 7)  # 7 for uncategorized

        # Get other labels
        purpose = self.purpose_map.get(sample.get('purpose', 'Other'), 8)
        suspicious = int(sample.get('suspicious', 0))
        tech_tags = sample.get('tech_tags', [])
        if isinstance(tech_tags, list) and tech_tags and tech_tags[0] in self.tech_vocab:
            tech_idx = self.tech_vocab.index(tech_tags[0])
        else:
            tech_idx = 0
        sentiment = self.sentiment_map.get(sample.get('sentiment', 'neutral'), 1)
        
        # Workload labels
        source_info = sample.get('source_info', {})
        author_idx = self.author_map.get(source_info.get('author_name', 'Unknown'), 0)
        repo_idx = self.repo_map.get(source_info.get('repo_id', 'Unknown'), 0)
        
        labels_tensor_dict = {
            'commit_type': torch.tensor(commit_type_idx),
            'purpose': torch.tensor(purpose),
            'suspicious': torch.tensor(suspicious),
            'tech_tag': torch.tensor(tech_idx),
            'sentiment': torch.tensor(sentiment),
            'author': torch.tensor(author_idx),
            'source_repo': torch.tensor(repo_idx)
        }
        
        return {
            'input': embed_doc,
            'labels': labels_tensor_dict
        }

def load_commit_data(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        samples = json.load(f)

    print(f"Loaded {len(samples)} samples from {json_path}")
    
    # Build author and repo mappings
    authors = set()
    repos = set()
    for item in samples:
        source_info = item.get('source_info', {})
        authors.add(source_info.get('author_name', 'Unknown'))
        repos.add(source_info.get('repo_id', 'Unknown'))
    
    author_map = {author: idx for idx, author in enumerate(sorted(authors))}
    repo_map = {repo: idx for idx, repo in enumerate(sorted(repos))}
    
    print(f"Found {len(authors)} unique authors and {len(repos)} unique repositories")
    return samples, {'author_map': author_map, 'repo_map': repo_map}

def main():
    # 1. Load training data
    base_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(base_dir, "collected_data", "commit_messages_raw.json")
    if not os.path.exists(json_path):
        json_path = os.path.join(base_dir, "../collected_data/commit_messages_raw.json")
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"Không tìm thấy file dữ liệu huấn luyện: {json_path}")
        
    print("Loading data...")
    samples, mappings = load_commit_data(json_path)
    processor = TextProcessor()
    embed_loader = EmbeddingLoader(embedding_type='codebert')
    embed_loader.load()

    # Split data
    print("Splitting data...")
    train_size = int(0.8 * len(samples))
    train_samples = samples[:train_size]
    val_samples = samples[train_size:]
    print(f"Train samples: {len(train_samples)}, Validation samples: {len(val_samples)}")

    # Create datasets
    train_dataset = CommitDataset(
        train_samples, 
        processor, 
        embed_loader,
        author_map=mappings['author_map'],
        repo_map=mappings['repo_map']
    )
    val_dataset = CommitDataset(
        val_samples,
        processor,
        embed_loader,
        author_map=mappings['author_map'],
        repo_map=mappings['repo_map']
    )

    # Optimize batch size based on GPU memory
    batch_size = 64 if device.type == 'cuda' else 8
    num_workers = 6 if device.type == 'cuda' else 0

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True if device.type == 'cuda' else False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True if device.type == 'cuda' else False
    )

    # Initialize model
    print("Initializing model...")
    num_classes_dict = {
        'purpose': 9,
        'tech_tag': len(train_dataset.tech_vocab),
        'suspicious': 2,
        'sentiment': 3,
        'author': len(mappings['author_map']),
        'source_repo': len(mappings['repo_map']),
        'commit_type': 8
    }
    
    model = HierarchicalAttentionNetwork(
        embed_dim=768, 
        hidden_dim=128,
        num_classes_dict=num_classes_dict
    ).to(device)
    
    # Optimizer and losses
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # GPU optimizations
    if device.type == 'cuda':
        print("Enabling CUDA optimizations...")
        torch.backends.cudnn.benchmark = True
        if torch.cuda.device_count() > 1:
            print(f"Using {torch.cuda.device_count()} GPUs!")
            model = torch.nn.DataParallel(model)
    
    loss_fns = {
        'purpose': torch.nn.CrossEntropyLoss(),
        'suspicious': torch.nn.CrossEntropyLoss(),
        'tech_tag': torch.nn.CrossEntropyLoss(),
        'sentiment': torch.nn.CrossEntropyLoss(),
        'author': torch.nn.CrossEntropyLoss(),
        'source_repo': torch.nn.CrossEntropyLoss(),
        'commit_type': torch.nn.CrossEntropyLoss()
    }
    
    multitask_loss = UncertaintyWeightingLoss(num_tasks=len(loss_fns))
    trainer = MultiTaskTrainer(model, optimizer, loss_fns, device=device)

    # Setup logging
    log_dir = os.path.join(base_dir, "training_logs")
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    
    # Save initial config to log
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Training Configuration\n{'='*20}\n")
        f.write(f"Device: {device}\n")
        f.write(f"Batch size: {batch_size}\n")
        f.write(f"Number of workers: {num_workers}\n")
        f.write(f"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}\n")
        f.write(f"Model classes: {num_classes_dict}\n\n")

    # Training loop
    num_epochs = 10
    best_val_loss = float('inf')
    val_loss = float('inf')
    print(f"Starting training on {device}...")
    
    try:
        for epoch in range(num_epochs):
            # Training
            train_loss = trainer.train_epoch(train_loader)

            # Validation
            val_loss, preds, true_labels = trainer.validate(val_loader)

            # Calculate metrics
            task_metrics = {}
            for task in preds:
                metrics = calc_metrics(true_labels[task].numpy(), preds[task].argmax(-1).numpy())
                task_metrics[task] = metrics

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                    'author_map': mappings['author_map'],
                    'repo_map': mappings['repo_map']
                }, os.path.join(base_dir, "models", "han_multitask_best.pth"))

            # Memory cleanup for GPU
            if device.type == 'cuda':
                torch.cuda.empty_cache()

    except KeyboardInterrupt:
        print("\nTraining interrupted by user. Saving current model state...")
    except Exception as e:
        print(f"\nError occurred during training: {str(e)}")
    finally:
        # Save final model
        torch.save({
            'model_state_dict': model.state_dict(),
            'author_map': mappings['author_map'],
            'repo_map': mappings['repo_map'],
            'final_val_loss': val_loss
        }, os.path.join(base_dir, "models", "han_multitask_final.pth"))

        print("Training finished!")
        print(f"Best validation loss: {best_val_loss:.4f}")

if __name__ == "__main__":
    main()

```

### backend\ai\train_spacy.py
```py
import spacy
from spacy.training.example import Example
import random
import json

# Tải mô hình đa ngôn ngữ
nlp = spacy.load("xx_ent_wiki_sm")

# Thêm textcat_multilabel vào pipeline nếu chưa có
if "textcat_multilabel" not in nlp.pipe_names:
    textcat = nlp.add_pipe("textcat_multilabel", last=True)
else:
    textcat = nlp.get_pipe("textcat_multilabel")

# Thêm các nhãn
labels = ["feat", "fix", "docs", "style", "refactor", "chore", "test", "uncategorized",
          "auth", "search", "cart", "order", "profile", "product", "api", "ui", "notification", "dashboard"]

for label in labels:
    textcat.add_label(label)

# Đọc dữ liệu từ file JSON
with open('train_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

train_data = []
for entry in data:
    text = entry["text"]
    cats = entry["cats"]
    example = Example.from_dict(nlp.make_doc(text), {"cats": cats})
    train_data.append(example)

optimizer = nlp.begin_training()
for epoch in range(10):
    random.shuffle(train_data)
    losses = {}
    for example in train_data:
        nlp.update([example], losses=losses)
    print(f"Epoch {epoch} - Losses: {losses}")

output_dir = "modelAi"
nlp.to_disk(output_dir)
print(f"Model saved to {output_dir}")

```

### backend\ai\__init__.py
```py

```

### backend\ai\baseline\bilstm_baseline.py
```py
# BiLSTM baseline cho so sánh
import torch
import torch.nn as nn

class BiLSTMBaseline(nn.Module):
    def __init__(self, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim*2, num_classes)
    def forward(self, x):
        # x: (batch, seq_len, embed_dim)
        h, _ = self.lstm(x)
        out = h[:, -1, :]
        return self.fc(out)

def train_bilstm(model, dataloader, optimizer, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in dataloader:
        x = batch['input'].to(device)
        y = batch['label'].to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = loss_fn(out, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def predict_bilstm(model, dataloader, device):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            x = batch['input'].to(device)
            y = batch['label'].to(device)
            out = model(x)
            preds.append(out.argmax(dim=1).cpu())
            labels.append(y.cpu())
    return torch.cat(preds), torch.cat(labels)

```

### backend\ai\baseline\tfidf_logreg_baseline.py
```py
# TF-IDF + Logistic Regression/SVM baseline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

class TfidfLogRegBaseline:
    def __init__(self, model_type='logreg'):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        if model_type == 'logreg':
            self.model = LogisticRegression(max_iter=200)
        else:
            self.model = SVC(probability=True)
    def train(self, X, y):
        X_vec = self.vectorizer.fit_transform(X)
        self.model.fit(X_vec, y)
    def predict(self, X):
        X_vec = self.vectorizer.transform(X)
        return self.model.predict(X_vec)
    def predict_proba(self, X):
        X_vec = self.vectorizer.transform(X)
        return self.model.predict_proba(X_vec)

```

### backend\ai\data_preparation\balance_training_data.py
```py
import json
import random
from collections import Counter, defaultdict

def load_json(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(data, filepath):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def balance_dataset(data, min_samples_per_class=25):
    samples = data['samples']
    # Group samples by different attributes
    by_purpose = defaultdict(list)
    by_sentiment = defaultdict(list)
    by_author = defaultdict(list)
    by_repo = defaultdict(list)
    
    # First pass - collect statistics and group samples
    for sample in samples:
        purpose = sample['purpose']
        sentiment = sample['sentiment']
        author = sample['metadata']['author']
        repo_id = sample['metadata']['source_repo']
        
        by_purpose[purpose].append(sample)
        by_sentiment[sentiment].append(sample)
        by_author[author].append(sample)
        by_repo[repo_id].append(sample)
      # Balance purposes first
    balanced_samples = []
    purposes = ['Feature Implementation', 'Bug Fix', 'Refactoring', 'Other']
    
    # Find minimum samples across purposes that have data
    min_purpose = float('inf')
    for purpose in purposes:
        if by_purpose[purpose]:  # Only consider non-empty categories
            min_purpose = min(min_purpose, len(by_purpose[purpose]))
    
    # Set target size per purpose, ensuring at least min_samples_per_class
    target_size = max(min_purpose, min_samples_per_class)
    
    for purpose in purposes:
        available = by_purpose[purpose]
        if not available:
            continue
            
        if len(available) < target_size:
            # If we don't have enough samples, use bootstrapping
            needed = target_size - len(available)
            selected = available + [random.choice(available) for _ in range(needed)]
        else:
            # Sample randomly
            selected = random.sample(available, target_size)
        balanced_samples.extend(selected)
      # Now balance sentiments while preserving purpose balance
    sentiments = ['positive', 'negative', 'neutral']
    sentiment_grouped = defaultdict(list)
    for sample in balanced_samples:
        sentiment = sample['sentiment']
        sentiment_grouped[sentiment].append(sample)
    
    # Find average sentiment count for non-empty categories
    total_samples = 0
    non_empty_cats = 0
    for sentiment in sentiments:
        if sentiment_grouped[sentiment]:
            total_samples += len(sentiment_grouped[sentiment])
            non_empty_cats += 1
    
    target_sentiment = max(total_samples // non_empty_cats, min_samples_per_class//2)
    
    final_samples = []
    for sentiment in sentiments:
        available = sentiment_grouped[sentiment]
        if len(available) < target_sentiment:
            final_samples.extend(available)
        else:
            final_samples.extend(random.sample(available, target_sentiment))
    
    # Shuffle final samples
    random.shuffle(final_samples)
    return final_samples

def main():
    # Load original and existing balanced data
    original_data = load_json('c:/SAN/KLTN/KLTN04/backend/ai/training_data/han_training_samples.json')
    
    # Balance the dataset
    balanced_samples = balance_dataset(original_data)
    
    # Save balanced dataset
    balanced_data = {'samples': balanced_samples}
    save_json(balanced_data, 'c:/SAN/KLTN/KLTN04/backend/ai/training_data/han_training_samples_balanced.json')
    
    # Print statistics
    print("\nDataset Statistics:")
    print(f"Total samples: {len(balanced_data['samples'])}")
    
    print("\nPurpose Distribution:")
    purposes = Counter(s['purpose'] for s in balanced_data['samples'])
    for purpose, count in purposes.most_common():
        print(f"{purpose}: {count} ({count/len(balanced_data['samples'])*100:.1f}%)")
    
    print("\nSentiment Distribution:")
    sentiments = Counter(s['sentiment'] for s in balanced_data['samples'])
    for sentiment, count in sentiments.most_common():
        print(f"{sentiment}: {count} ({count/len(balanced_data['samples'])*100:.1f}%)")
    
    print("\nTop Authors:")
    authors = Counter(s['metadata']['author'] for s in balanced_data['samples'])
    for author, count in authors.most_common(5):
        print(f"{author}: {count} ({count/len(balanced_data['samples'])*100:.1f}%)")
    
    print("\nRepository Distribution:")
    repos = Counter(s['metadata']['source_repo'] for s in balanced_data['samples'])
    for repo, count in repos.most_common():
        print(f"Repo {repo}: {count} ({count/len(balanced_data['samples'])*100:.1f}%)")

if __name__ == "__main__":
    main()
```

### backend\ai\data_preparation\build_training_data.py
```py
import os
import json
from ai.modelAi.generate_data import classify_commit_purpose, is_suspicious_commit, extract_tech_tags, classify_sentiment

def build_training_data(collected_dir, output_path):
    all_samples = []
    for file in os.listdir(collected_dir):
        if not file.endswith('.json'):
            continue
        with open(os.path.join(collected_dir, file), 'r', encoding='utf-8') as f:
            items = json.load(f)
        for item in items:
            text = item.get('raw_text', '')
            # Gán nhãn tự động
            labels = {
                'purpose': classify_commit_purpose(text),
                'suspicious': is_suspicious_commit(text),
                'tech_tag': extract_tech_tags(text),
                'sentiment': classify_sentiment(text)
            }
            item['labels'] = labels
            all_samples.append(item)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_samples, f, ensure_ascii=False, indent=2)
    print(f"Saved {len(all_samples)} samples to {output_path}")

if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    collected_dir = os.path.join(base_dir, "collected_data")
    output_path = os.path.join(base_dir, "training_data", "han_training_samples.json")
    build_training_data(
        collected_dir=collected_dir,
        output_path=output_path
    )

```

### backend\ai\data_preparation\clean_training_data.py
```py

```

### backend\ai\data_preprocessing\embedding_loader.py
```py
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch

# Tải embedding pre-trained (CodeBERT/FastText/GloVe)
class EmbeddingLoader:
    def __init__(self, embedding_type='codebert', model_name='microsoft/codebert-base'):
        self.embedding_type = embedding_type
        self.model_name = model_name
        self.embeddings = None
        self.tokenizer = None
        self.model = None

    def load(self, path=None):
        if self.embedding_type == 'codebert':
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
        elif self.embedding_type in ['glove', 'fasttext']:
            self.embeddings = {}
            with open(path, encoding='utf-8') as f:
                for line in f:
                    values = line.strip().split()
                    word = values[0]
                    vec = np.asarray(values[1:], dtype='float32')
                    self.embeddings[word] = vec
        else:
            raise ValueError('Unknown embedding type')

    def get_word_embedding(self, word):
        if self.embedding_type == 'codebert':
            # Trả về embedding của từ từ transformers
            tokens = self.tokenizer(word, return_tensors='pt')
            with torch.no_grad():
                output = self.model(**tokens)
            return output.last_hidden_state[0, 1:-1, :].mean(dim=0).numpy()
        elif self.embedding_type in ['glove', 'fasttext']:
            return self.embeddings.get(word, np.zeros(300))
        else:
            return None
        
    def get_embeddings_for_doc(self, doc):
        """Lấy embeddings cho toàn bộ document.
        
        Args:
            doc: numpy array có shape (num_sents, max_words) chứa các token id
            
        Returns:
            numpy array có shape (num_sents, max_words, embed_dim)
        """
        num_sents, max_words = doc.shape
        if self.embedding_type == 'codebert':
            embed_dim = 768
        else:
            embed_dim = 300
            
        embeddings = np.zeros((num_sents, max_words, embed_dim))
        
        for i in range(num_sents):
            for j in range(max_words):
                if doc[i,j] != 0:  # Không tính embedding cho padding token
                    word = str(doc[i,j])
                    embeddings[i,j] = self.get_word_embedding(word)
                    
        return embeddings

```

### backend\ai\data_preprocessing\text_processor.py
```py
import re
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

# Download required NLTK resources
resources = ['punkt', 'averaged_perceptron_tagger', 'stopwords']
for resource in resources:
    try:
        nltk.data.find(f'tokenizers/{resource}')
    except LookupError:
        nltk.download(resource, quiet=True)

# Xử lý văn bản: làm sạch, tách câu, token hóa, padding
class TextProcessor:
    def __init__(self, max_sent_len=30, max_word_len=20):
        self.max_sent_len = max_sent_len
        self.max_word_len = max_word_len

    def clean_text(self, text):
        text = text.lower()
        # Giữ lại chữ, số, dấu cách, dấu chấm, dấu gạch dưới, dấu ngoặc, dấu =, :, ;, #, @, /, \, ', ", <, >, -, +, *, %, &, |
        text = re.sub(r'[^\w\s\.\_\(\)\[\]\{\}\=\:\;\#\@\/\\\'\"\<\>\-\+\*\%\&\|]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def sent_tokenize(self, text):
        return sent_tokenize(text)

    def word_tokenize(self, sentence):
        return word_tokenize(sentence)

    def pad_sequences(self, sequences, maxlen, pad_value=0):
        padded = np.full((len(sequences), maxlen), pad_value)
        for i, seq in enumerate(sequences):
            trunc = seq[:maxlen]
            padded[i, :len(trunc)] = trunc
        return padded

    def chunk_text(self, text, max_tokens=500):
        """Chia văn bản dài thành các đoạn nhỏ hơn theo số lượng từ."""
        words = self.word_tokenize(text)
        chunks = []
        for i in range(0, len(words), max_tokens):
            chunk = ' '.join(words[i:i+max_tokens])
            chunks.append(chunk)
        return chunks

    def process_document(self, text, word2idx=None):
        text = self.clean_text(text)
        sents = self.sent_tokenize(text)[:self.max_sent_len]
        doc = []
        for sent in sents:
            words = self.word_tokenize(sent)[:self.max_word_len]
            if word2idx:
                words = [word2idx.get(w, word2idx.get('<unk>', 0)) for w in words]
            doc.append(words)
        # Padding từng câu
        doc = [w + [0]*(self.max_word_len-len(w)) if len(w)<self.max_word_len else w for w in doc]
        # Padding số câu
        if len(doc) < self.max_sent_len:
            doc += [[0]*self.max_word_len]*(self.max_sent_len-len(doc))
        return np.array(doc)

```

### backend\ai\evaluation\attention_visualizer.py
```py
# Visualize attention weights
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attn_weights, tokens, title='Attention', figsize=(10,2)):
    plt.figure(figsize=figsize)
    if len(attn_weights.shape) == 1:
        attn_weights = attn_weights.reshape(1, -1)
    sns.heatmap(attn_weights, annot=True, fmt='.2f', xticklabels=tokens, yticklabels=[''])
    plt.title(title)
    plt.tight_layout()
    plt.show()

```

### backend\ai\evaluation\metrics_calculator.py
```py
# Tính F1, Precision, Recall, AUC cho từng task
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
import numpy as np

def calc_metrics(y_true, y_pred, task_type='multiclass', average='macro'):
    metrics = {}
    if task_type == 'multiclass':
        metrics['f1'] = f1_score(y_true, y_pred, average=average)
        metrics['precision'] = precision_score(y_true, y_pred, average=average)
        metrics['recall'] = recall_score(y_true, y_pred, average=average)
        try:
            metrics['auc'] = roc_auc_score(y_true, y_pred, multi_class='ovr', average=average)
        except Exception:
            metrics['auc'] = None
    elif task_type == 'multilabel':
        metrics['f1'] = f1_score(y_true, y_pred, average=average)
        metrics['precision'] = precision_score(y_true, y_pred, average=average)
        metrics['recall'] = recall_score(y_true, y_pred, average=average)
        try:
            metrics['auc'] = roc_auc_score(y_true, y_pred, average=average)
        except Exception:
            metrics['auc'] = None
    return metrics

```

### backend\ai\modelAi\fetch_github_commits.py
```py

```

### backend\ai\modelAi\generate_data.py
```py
import json
from db.database import database
from db.models.commits import commits  # Đảm bảo bạn đã import đúng model commits
from datetime import datetime
import re

# Hàm để chuyển dữ liệu commit thành định dạng spaCy
async def convert_to_spacy_format():
    # Kết nối cơ sở dữ liệu trước khi truy vấn
    await database.connect()

    # Truy vấn tất cả dữ liệu commits từ database
    query = commits.select()  # Thay đổi nếu bạn sử dụng ORM khác như SQLAlchemy
    rows = await database.fetch_all(query)

    # Danh sách để lưu dữ liệu theo định dạng spaCy
    data = []

    for row in rows:
        # Lấy dữ liệu từ commit
        sha = row["sha"]
        message = row["message"]  # Lấy trường 'message' từ commit
        author_name = row["author_name"]
        author_email = row["author_email"]
        date = row["date"]  # Trường ngày giờ commit
        insertions = row["insertions"]
        deletions = row["deletions"]
        files_changed = row["files_changed"]
        repo_id = row["repo_id"]

        # Nếu date là kiểu datetime, chuyển nó thành chuỗi
        if isinstance(date, datetime):
            date = date.strftime('%Y-%m-%d %H:%M:%S')  # Chuyển đổi theo định dạng mong muốn
        
        # Kiểm tra commit có chứa trường message hay không
        if message:
            # Phân loại label từ message
            label = get_label_from_message(message)

            # Thêm vào danh sách data với định dạng spaCy
            data.append({
                "text": message,  # Nội dung commit
                "meta": {
                    "sha": sha,  # SHA của commit
                    "author_name": author_name,  # Tên tác giả
                    "author_email": author_email,  # Email tác giả
                    "date": date,  # Ngày commit (chuỗi)
                    "insertions": insertions,  # Số dòng thêm
                    "deletions": deletions,  # Số dòng xóa
                    "files_changed": files_changed,  # Số file thay đổi
                    "repo_id": repo_id  # ID của repository
                },
                "cats": label  # Thẻ phân loại của commit
            })

    # Ghi dữ liệu ra file JSON cho spaCy
    with open("ai/train_data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print("Dữ liệu đã được chuyển thành công!")

    # Ngắt kết nối cơ sở dữ liệu sau khi hoàn thành
    await database.disconnect()

# Hàm thu thập commit messages và lưu theo định dạng unified JSON
async def export_commit_messages_unified():
    await database.connect()
    query = commits.select()
    rows = await database.fetch_all(query)
    data = []
    for row in rows:
        sha = row["sha"]
        message = row["message"]
        author_name = row["author_name"]
        author_email = row["author_email"]
        date = row["date"]
        insertions = row["insertions"]
        deletions = row["deletions"]
        files_changed = row["files_changed"]
        repo_id = row["repo_id"]
        if isinstance(date, datetime):
            date = date.strftime('%Y-%m-%d %H:%M:%S')
        if message:
            data.append({
                "id": sha,
                "data_type": "commit_message",
                "raw_text": message,
                "source_info": {
                    "repo_id": repo_id,
                    "sha": sha,
                    "author_name": author_name,
                    "author_email": author_email,
                    "date": date,
                    "insertions": insertions,
                    "deletions": deletions,
                    "files_changed": files_changed
                },
                "labels": {
                    "purpose": None,
                    "suspicious": None,
                    "tech_tag": None,
                    "sentiment": None
                }
            })
    with open("ai/collected_data/commit_messages_raw.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print("Đã xuất commit messages sang collected_data/commit_messages_raw.json!")
    await database.disconnect()

# Hàm phân loại label từ message (cần phải viết logic phân loại của bạn)
def get_label_from_message(message):
    # Các từ khóa phổ biến trong commit
    categories = {
        "feat": {"feat": 1.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "thêm": {"feat": 1.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "fix": {"feat": 0.0, "fix": 1.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "sửa": {"feat": 0.0, "fix": 1.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "docs": {"feat": 0.0, "fix": 0.0, "docs": 1.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "style": {"feat": 0.0, "fix": 0.0, "docs": 0.0, "style": 1.0, "refactor": 0.0, "chore": 0.0, "test": 0.0},
        "refactor": {"feat": 0.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 1.0, "chore": 0.0, "test": 0.0},
        "chore": {"feat": 0.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 1.0, "test": 0.0},
        "test": {"feat": 0.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 1.0},
    }

    # Kiểm tra nếu commit chứa một trong các từ khóa trên
    for keyword, label in categories.items():
        if keyword in message.lower():
            return label
    
    # Nếu không có từ khóa nào trong message, phân loại là 'uncategorized'
    return {"feat": 0.0, "fix": 0.0, "docs": 0.0, "style": 0.0, "refactor": 0.0, "chore": 0.0, "test": 0.0, "uncategorized": 1.0}

def classify_commit_purpose(message: str) -> str:
    message = message.lower()
    # Bug Fix
    if re.search(r"\b(fix|bug|error|sửa lỗi|vá lỗi|issue #|problem|resolve|patch|hotfix|defect|repair|correct|debug)\b", message):
        return 'Bug Fix'
    # Feature Implementation
    if re.search(r"\b(feat|add|implement|thêm|phát triển|tính năng mới|feature|introduce|support|create|build)\b", message):
        return 'Feature Implementation'
    # Refactoring
    if re.search(r"\b(refactor|restructure|tái cấu trúc|optimi[sz]e|clean up|cleanup|improve structure|refactoring)\b", message):
        return 'Refactoring'
    # Documentation Update
    if re.search(r"\b(docs|update readme|document|tài liệu|hướng dẫn|readme|docstring|documentation|manual|guide)\b", message):
        return 'Documentation Update'
    # Test Update
    if re.search(r"\b(test|unit test|integration test|add test|update test|kiểm thử|bổ sung test|testcase|test case|testing)\b", message):
        return 'Test Update'
    # Security Patch
    if re.search(r"\b(security|bảo mật|vulnerability|cve-|patch security|fix security|secure|xss|csrf|injection|auth bypass|exploit)\b", message):
        return 'Security Patch'
    # Code Style/Formatting
    if re.search(r"\b(style|format|formatting|code style|reformat|lint|prettier|black|flake8|isort|format code|format lại|chuẩn hóa mã)\b", message):
        return 'Code Style/Formatting'
    # Build/CI/CD Script Update
    if re.search(r"\b(build|ci|cd|pipeline|deploy|release|workflow|github actions|jenkins|travis|circleci|update script|build script|ci/cd|docker|compose|build system|deployment)\b", message):
        return 'Build/CI/CD Script Update'
    # Default fallback
    return 'Other'

def is_suspicious_commit(message: str) -> int:
    if not message or len(message.strip()) < 5:
        return 1  # Quá ngắn hoặc không có nội dung
    if len(message) > 200:
        return 1  # Quá dài bất thường
    suspicious_keywords = [
        'hack', 'backdoor', 'temp fix', 'quick fix', 'todo: fix', 'xxx', 'hack', 'workaround',
        'bypass', 'disable security', 'hardcode', 'debug', 'remove check', 'skip test', 'patch quick', 'urgent fix'
    ]
    msg_lower = message.lower()
    for kw in suspicious_keywords:
        if kw in msg_lower:
            return 1
    # Toàn bộ viết hoa hoặc chứa nhiều ký tự đặc biệt
    if message.isupper() or sum(1 for c in message if not c.isalnum() and c not in ' .,:;') > len(message) * 0.3:
        return 1
    return 0

def extract_tech_tags(text: str) -> list:
    tech_vocab = [
        'python', 'fastapi', 'react', 'javascript', 'typescript', 'docker', 'sqlalchemy', 'pytorch', 'spacy',
        'css', 'html', 'postgresql', 'mysql', 'mongodb', 'redis', 'vue', 'angular', 'flask', 'django',
        'node', 'express', 'graphql', 'rest', 'api', 'gitlab', 'github', 'ci', 'cd', 'kubernetes', 'helm',
        'pytest', 'unittest', 'junit', 'cicd', 'github actions', 'travis', 'jenkins', 'circleci', 'webpack',
        'babel', 'vite', 'npm', 'yarn', 'pip', 'poetry', 'black', 'flake8', 'isort', 'prettier', 'eslint',
        'jwt', 'oauth', 'sso', 'celery', 'rabbitmq', 'kafka', 'grpc', 'protobuf', 'swagger', 'openapi',
        'sentry', 'prometheus', 'grafana', 'nginx', 'apache', 'linux', 'ubuntu', 'windows', 'macos',
        'aws', 'azure', 'gcp', 'firebase', 'heroku', 'netlify', 'vercel', 'tailwind', 'bootstrap', 'material ui'
    ]
    found = set()
    text_lower = text.lower()
    for tech in tech_vocab:
        if tech in text_lower:
            found.add(tech)
    return list(found)

def classify_sentiment(message: str) -> str:
    message = message.lower()
    positive_keywords = [
        'cải thiện', 'tốt', 'thành công', 'hoàn thành', 'ổn định', 'tối ưu', 'đẹp', 'gọn', 'sạch', 'great', 'awesome', 'improved', 'cleaned up', 'ok', 'hoàn tất', 'đã xong', 'resolved', 'fixed', 'passed', 'success', 'hoan thanh', 'tot', 'cam on', 'thanks', 'thank you', 'well done', 'hoàn thiện', 'đúng', 'đúng chức năng', 'đúng yêu cầu'
    ]
    negative_keywords = [
        'lỗi', 'fail', 'broke', 'revert', 'issue', 'bug', 'không chạy', 'không hoạt động', 'sai', 'chưa xong', 'chưa hoàn thành', 'chưa đúng', 'problem', 'error', 'crash', 'exception', 'bad', 'xóa bỏ', 'rollback', 'undo', 'fixme', 'todo', 'tạm thời', 'workaround', 'hack', 'tạm vá', 'không ổn', 'không tốt', 'chưa ổn', 'chưa tốt', 'chưa hoàn thiện', 'chưa đúng chức năng', 'chưa đúng yêu cầu'
    ]
    for word in positive_keywords:
        if word in message:
            return 'positive'
    for word in negative_keywords:
        if word in message:
            return 'negative'
    return 'neutral'

# Chạy hàm convert_to_spacy_format
if __name__ == "__main__":
    import asyncio
    #asyncio.run(convert_to_spacy_format())
    asyncio.run(export_commit_messages_unified())

```

### backend\ai\models\hierarchical_attention.py
```py
# Hierarchical Attention Network (HAN) - PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F

class WordAttention(nn.Module):
    def __init__(self, embed_dim, hidden_dim):
        super().__init__()
        self.bigru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.attn = nn.Linear(hidden_dim*2, 1)
        self.norm = nn.LayerNorm(hidden_dim*2)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x: (batch, num_word, embed_dim)
        h, _ = self.bigru(x)
        attn_weights = torch.softmax(self.attn(h), dim=1)  # (batch, num_word, 1)
        out = (h * attn_weights).sum(dim=1)  # (batch, hidden_dim*2)
        out = self.norm(out)
        out = self.dropout(out)
        return out, attn_weights.squeeze(-1)

class SentenceAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.bigru = nn.GRU(hidden_dim*2, hidden_dim, bidirectional=True, batch_first=True)
        self.attn = nn.Linear(hidden_dim*2, 1)
        self.norm = nn.LayerNorm(hidden_dim*2)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x: (batch, num_sent, hidden_dim*2)
        h, _ = self.bigru(x)
        attn_weights = torch.softmax(self.attn(h), dim=1)  # (batch, num_sent, 1)
        out = (h * attn_weights).sum(dim=1)  # (batch, hidden_dim*2)
        out = self.norm(out)
        out = self.dropout(out)
        return out, attn_weights.squeeze(-1)

class HierarchicalAttentionNetwork(nn.Module):
    def __init__(self, embed_dim, hidden_dim, num_classes_dict):
        super().__init__()
        self.word_attn = WordAttention(embed_dim, hidden_dim)
        self.sent_attn = SentenceAttention(hidden_dim)
        # Task-specific heads
        self.heads = nn.ModuleDict({
            'purpose': nn.Linear(hidden_dim*2, num_classes_dict['purpose']),
            'suspicious': nn.Linear(hidden_dim*2, 2),
            'tech_tag': nn.Linear(hidden_dim*2, num_classes_dict['tech_tag']),
            'sentiment': nn.Linear(hidden_dim*2, 3),
            'author': nn.Linear(hidden_dim*2, num_classes_dict['author']),
            'source_repo': nn.Linear(hidden_dim*2, num_classes_dict['source_repo']),
            'commit_type': nn.Linear(hidden_dim*2, num_classes_dict['commit_type'])
        })

    def forward(self, x):
        # x: (batch, num_sent, num_word, embed_dim)
        batch, num_sent, num_word, embed_dim = x.size()
        x = x.view(-1, num_word, embed_dim)  # (batch*num_sent, num_word, embed_dim)
        word_vecs, word_attn = self.word_attn(x)  # (batch*num_sent, hidden*2)
        word_vecs = word_vecs.view(batch, num_sent, -1)
        sent_vec, sent_attn = self.sent_attn(word_vecs)  # (batch, hidden*2)
        out = {
            'purpose': self.heads['purpose'](sent_vec),
            'suspicious': self.heads['suspicious'](sent_vec),
            'tech_tag': self.heads['tech_tag'](sent_vec),
            'sentiment': self.heads['sentiment'](sent_vec),
            'author': self.heads['author'](sent_vec),
            'source_repo': self.heads['source_repo'](sent_vec),
            'commit_type': self.heads['commit_type'](sent_vec)
        }
        return out, word_attn, sent_attn

```

### backend\ai\models\shared_layers.py
```py
# Các lớp dùng chung hoặc private cho multi-task
import torch.nn as nn

class SharedDense(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.linear = nn.Linear(in_dim, out_dim)
        self.relu = nn.ReLU()
    def forward(self, x):
        return self.relu(self.linear(x))

```

### backend\ai\scriptPhantich\analyze_data_balance.py
```py
import json
from collections import Counter
import pandas as pd
import os

# Đọc dữ liệu huấn luyện
file_path = "c:\\SAN\\KLTN\\KLTN04\\backend\\ai\\training_data\\han_training_samples.json"
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

samples = data['samples']

# Khởi tạo counters
purpose_counter = Counter()
sentiment_counter = Counter()
tech_tags_counter = Counter()
author_counter = Counter()
repo_counter = Counter()

# Đếm số lượng của mỗi nhãn
for sample in samples:
    purpose_counter[sample.get('purpose', 'Unknown')] += 1
    sentiment_counter[sample.get('sentiment', 'Unknown')] += 1
    
    tech_tags = sample.get('tech_tags', [])
    for tag in tech_tags:
        tech_tags_counter[tag] += 1
        
    metadata = sample.get('metadata', {})
    author_counter[metadata.get('author', 'Unknown')] += 1
    repo_counter[metadata.get('source_repo', 'Unknown')] += 1

# In phân tích
print("\n=== Phân tích cân bằng dữ liệu ===")
print(f"\nTổng số mẫu: {len(samples)}")

print("\n--- Purpose Distribution ---")
for purpose, count in purpose_counter.most_common():
    print(f"{purpose}: {count} ({count/len(samples)*100:.1f}%)")

print("\n--- Sentiment Distribution ---")
for sentiment, count in sentiment_counter.most_common():
    print(f"{sentiment}: {count} ({count/len(samples)*100:.1f}%)")

print("\n--- Top 10 Tech Tags ---")
for tag, count in tech_tags_counter.most_common(10):
    print(f"{tag}: {count}")

print("\n--- Author Distribution ---")
for author, count in author_counter.most_common():
    print(f"{author}: {count} ({count/len(samples)*100:.1f}%)")

print("\n--- Repository Distribution ---")
for repo, count in repo_counter.most_common():
    print(f"{repo}: {count} ({count/len(samples)*100:.1f}%)")

# Tính toán các chỉ số cân bằng
def calculate_imbalance(counter):
    total = sum(counter.values())
    proportions = [count/total for count in counter.values()]
    max_prop = max(proportions)
    min_prop = min(proportions)
    imbalance_ratio = max_prop / min_prop if min_prop > 0 else float('inf')
    return imbalance_ratio

print("\n=== Tỉ lệ mất cân bằng (max/min) ===")
print(f"Purpose Imbalance: {calculate_imbalance(purpose_counter):.2f}")
print(f"Sentiment Imbalance: {calculate_imbalance(sentiment_counter):.2f}")
print(f"Author Imbalance: {calculate_imbalance(author_counter):.2f}")
print(f"Repository Imbalance: {calculate_imbalance(repo_counter):.2f}")

```

### backend\ai\scripts\extract_code_comments.py
```py
import os
import re
import glob
import json

def extract_code_comments_from_file(filepath):
    comments = []
    ext = os.path.splitext(filepath)[1]
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
    except Exception:
        return comments  # Bỏ qua file không đọc được
    if ext in ['.py']:
        for line in lines:
            match = re.match(r'\s*#(.*)', line)
            if match:
                comments.append(match.group(1).strip())
    elif ext in ['.js', '.jsx', '.ts', '.tsx']:
        in_block = False
        for line in lines:
            if '/*' in line:
                in_block = True
            if in_block:
                comments.append(line.strip())
            if '*/' in line:
                in_block = False
            match = re.match(r'\s*//(.*)', line)
            if match:
                comments.append(match.group(1).strip())
    return comments

def extract_comments_from_repo(repo_path, exts=['.py', '.js', '.jsx', '.ts', '.tsx']):
    all_comments = []
    for ext in exts:
        for filepath in glob.glob(os.path.join(repo_path, f'**/*{ext}'), recursive=True):
            comments = extract_code_comments_from_file(filepath)
            for c in comments:
                if c:
                    all_comments.append({'text': c, 'source': filepath})
    return all_comments

def save_comments_to_json(comments, out_path, repo_name=None):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    output = []
    for idx, c in enumerate(comments):
        output.append({
            "id": f"comment_{idx}",
            "data_type": "code_comment",
            "raw_text": c["text"] if isinstance(c, dict) else c,
            "source_info": {
                "repo_name": repo_name or "KLTN04",
                "file_path": c["source"] if isinstance(c, dict) and "source" in c else None
            },
            "labels": {
                "purpose": None,
                "suspicious": None,
                "tech_tag": None,
                "sentiment": None
            }
        })
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    repo_path = '../../backend'  # Giới hạn quét trong backend
    comments = extract_comments_from_repo(repo_path)
    save_comments_to_json(comments, '../collected_data/code_comments.json')
    print(f'Extracted {len(comments)} comments.')

```

### backend\ai\scripts\extract_code_diffs.py
```py
import subprocess
import json
import re
import os

def extract_code_diffs(repo_path, max_commits=100):
    os.chdir(repo_path)
    # Lấy danh sách commit hash
    hashes = subprocess.check_output(['git', 'rev-list', '--max-count', str(max_commits), 'HEAD'], encoding='utf-8').splitlines()
    diffs = []
    for h in hashes:
        diff = subprocess.check_output(['git', 'show', h, '--unified=0', '--no-color'], encoding='utf-8', errors='ignore')
        added = re.findall(r'^\+[^+][^\n]*', diff, re.MULTILINE)
        removed = re.findall(r'^-[^-][^\n]*', diff, re.MULTILINE)
        if added or removed:
            diffs.append({'commit': h, 'added': added, 'removed': removed})
    return diffs

def save_diffs_to_json(diffs, out_path, repo_name=None):
    output = []
    for idx, d in enumerate(diffs):
        # Lưu từng dòng added/removed như một mục riêng biệt
        for i, line in enumerate(d.get('added', [])):
            output.append({
                "id": f"diff_{d['commit']}_add_{i}",
                "data_type": "code_diff_text",
                "raw_text": line,
                "source_info": {
                    "repo_name": repo_name or "KLTN04",
                    "commit_sha": d['commit'],
                    "diff_type": "added"
                },
                "labels": {
                    "purpose": None,
                    "suspicious": None,
                    "tech_tag": None,
                    "sentiment": None
                }
            })
        for i, line in enumerate(d.get('removed', [])):
            output.append({
                "id": f"diff_{d['commit']}_rm_{i}",
                "data_type": "code_diff_text",
                "raw_text": line,
                "source_info": {
                    "repo_name": repo_name or "KLTN04",
                    "commit_sha": d['commit'],
                    "diff_type": "removed"
                },
                "labels": {
                    "purpose": None,
                    "suspicious": None,
                    "tech_tag": None,
                    "sentiment": None
                }
            })
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    repo_path = './backend'  # Đường dẫn repo backend (tương đối từ workspace)
    diffs = extract_code_diffs(repo_path)
    os.makedirs('../collected_data', exist_ok=True)
    save_diffs_to_json(diffs, '../collected_data/code_diffs.json')
    print(f'Extracted {len(diffs)} code diffs.')

```

### backend\ai\scripts\extract_readme.py
```py
import os
import json

def extract_readme(repo_path):
    readme_path = None
    for fname in os.listdir(repo_path):
        if fname.lower().startswith('readme'):
            readme_path = os.path.join(repo_path, fname)
            break
    if not readme_path or not os.path.isfile(readme_path):
        return None
    with open(readme_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    return content

def save_readme_to_json(content, out_path, repo_name=None):
    output = [{
        "id": "readme_1",
        "data_type": "readme_content",
        "raw_text": content,
        "source_info": {
            "repo_name": repo_name or "KLTN04",
            "file_path": "README.md"
        },
        "labels": {
            "purpose": None,
            "suspicious": None,
            "tech_tag": None,
            "sentiment": None
        }
    }]
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    repo_path = '.'  # Đường dẫn tới thư mục gốc dự án
    content = extract_readme(repo_path)
    if content:
        save_readme_to_json(content, '../collected_data/readme_content.json')
        print('README extracted.')
    else:
        print('README not found.')

```

### backend\ai\taotukhoa\taotukhoa.py
```py
import json
from collections import defaultdict

# Danh sách nhãn
categories = [
    "auth", "search", "cart", "order", "profile", "product",
    "api", "ui", "notification", "dashboard", "fix", "feat",
    "refactor", "test", "style", "docs", "chore", "uncategorized"
]

# Từ khóa mẫu theo từng category
keyword_seed = {
    "auth": ["login", "logout", "register", "authentication", "authorization", "signin", "signup", "verify", "jwt", "token", "mã xác thực", "đăng nhập", "đăng ký"],
    "search": ["search", "filter", "query", "lookup", "tìm kiếm", "lọc"],
    "cart": ["cart", "add to cart", "remove from cart", "giỏ hàng", "thêm vào giỏ", "xóa khỏi giỏ"],
    "order": ["order", "checkout", "invoice", "bill", "đặt hàng", "thanh toán", "hóa đơn", "giao hàng"],
    "profile": ["profile", "account", "user info", "thông tin cá nhân", "hồ sơ", "avatar", "user"],
    "product": ["product", "item", "goods", "sản phẩm", "thêm sản phẩm", "chỉnh sửa sản phẩm"],
    "api": ["api", "endpoint", "request", "response", "swagger", "rest", "graphql"],
    "ui": ["ui", "interface", "layout", "giao diện", "font", "màu sắc", "hiển thị", "style", "responsive", "theme"],
    "notification": ["notification", "alert", "thông báo", "popup", "toast"],
    "dashboard": ["dashboard", "admin", "panel", "thống kê", "biểu đồ", "quản trị", "báo cáo"],

    "fix": ["fix", "bug", "sửa lỗi", "resolve", "patch", "hotfix", "lỗi", "error", "issue"],
    "feat": ["feat", "feature", "thêm chức năng", "implement", "function", "module", "new feature"],
    "refactor": ["refactor", "optimize", "tối ưu", "clean code", "cải tiến", "tái cấu trúc", "improve"],
    "test": ["test", "unit test", "integration test", "test case", "kiểm thử", "viết test"],
    "style": ["style", "format", "prettier", "indent", "reformat", "format lại", "chỉnh code"],
    "docs": ["docs", "readme", "tài liệu", "viết tài liệu", "hướng dẫn", "documentation"],
    "chore": ["chore", "cấu hình", "setup", "config", "update dependencies", "maintenance", "tooling"],
    "uncategorized": ["misc", "other", "khác", "thay đổi cấu trúc", "cập nhật"]
}

# Mở rộng mỗi nhãn thành nhiều từ khóa (dựa trên các gốc từ)
def expand_keywords(seed):
    expanded = set()
    for word in seed:
        expanded.add(word.lower())
        if " " in word:
            expanded.add(word.replace(" ", "-"))
            expanded.add(word.replace(" ", "_"))
        if word.isalpha():
            expanded.add(word + "s")
            expanded.add(word + "ed")
            expanded.add(word + "ing")
    return list(expanded)

# Xây dựng dict tổng hợp
keyword_dict = defaultdict(list)

for category, seed_words in keyword_seed.items():
    keyword_dict[category] = expand_keywords(seed_words)

# Xuất ra file JSON
with open("subintent_keywords.json", "w", encoding="utf-8") as f:
    json.dump(keyword_dict, f, ensure_ascii=False, indent=2)

print("✅ Đã tạo file 'subintent_keywords.json' với từ khóa mở rộng.")

```

### backend\ai\testmodel\test_han.py
```py
import torch
import sys
# Add workspace root to Python path
sys.path.append("c:\\SAN\\KLTN\\KLTN04\\backend")
sys.path.append("c:\\SAN\\KLTN\\KLTN04\\backend\\ai")
from ai.models.hierarchical_attention import HierarchicalAttentionNetwork
from data_preprocessing.text_processor import TextProcessor

# Load the trained HAN model
model_path = "c:\\SAN\\KLTN\\KLTN04\\backend\\ai\\models\\han_multitask_best.pth"
model = HierarchicalAttentionNetwork(embed_dim=768, hidden_dim=128, num_classes_dict={
    'purpose': 10, 'suspicious': 2, 'tech_tag': 15, 'sentiment': 3, 'author': 20, 'source_repo': 5
})

# Extract model_state_dict from checkpoint
checkpoint = torch.load(model_path)

# Extract num_classes_dict from checkpoint
num_classes_dict = checkpoint.get('num_classes_dict', {
    'purpose': 9, 'suspicious': 2, 'tech_tag': 79, 'sentiment': 3, 'author': 7, 'source_repo': 7
})

# Reinitialize model with correct num_classes_dict
model = HierarchicalAttentionNetwork(embed_dim=768, hidden_dim=128, num_classes_dict=num_classes_dict)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Initialize TextProcessor
text_processor = TextProcessor()

# Define a word2idx mapping for token conversion
word2idx = {'<unk>': 0}  # Add actual word-to-index mapping here

# Sample commit messages for testing
commit_messages = [
    "feat: Thêm tính năng xử lý lỗi",
    "fix: Sửa lỗi hiển thị trên giao diện người dùng",
    "docs: Cập nhật tài liệu API",
    "refactor: Tối ưu code module tìm kiếm",
    "style: Chỉnh sửa format code",
    "test: Thêm test case cho hàm login",
    "chore: Cập nhật dependencies",
    "uncategorized: Thay đổi cấu trúc thư mục",
    "feat: Thêm API thanh toán mới",
    "fix: Sửa layout bị cách ra ngoài",
]

# Load or generate embeddings for words
embedding_dim = 768  # Define embedding dimension
embedding_loader = torch.nn.Embedding(len(word2idx), embedding_dim)

# Ensure input tensor has the correct shape
max_sent_len = 10  # Define maximum number of sentences
max_word_len = 20  # Define maximum number of words per sentence

# Define label mappings for each task
label_mappings = {
    'purpose': ['Thêm tính năng', 'Sửa lỗi', 'Cập nhật tài liệu', 'Tối ưu code', 'Chỉnh sửa format', 'Thêm test case', 'Cập nhật dependencies', 'Thay đổi cấu trúc', 'Thêm API mới', 'Khác'],
    'sentiment': ['Tích cực', 'Tiêu cực', 'Trung lập'],
    'tech_tag': ['Python', 'JavaScript', 'HTML', 'CSS', 'SQL', 'Docker', 'React', 'Angular', 'Vue', 'Khác'],
    # Add mappings for other tasks as needed
}

# Preprocess and predict
for message in commit_messages:
    processed_message = text_processor.process_document(message, word2idx=word2idx)
    # Pad processed_message to match expected dimensions
    processed_message = [sent + [0]*(max_word_len-len(sent)) if len(sent)<max_word_len else sent for sent in processed_message]
    if len(processed_message) < max_sent_len:
        processed_message += [[0]*max_word_len]*(max_sent_len-len(processed_message))

    # Convert processed_message to embeddings
    embedded_message = [embedding_loader(torch.tensor(sent)) for sent in processed_message]
    embedded_message = torch.stack(embedded_message)

    # Ensure input tensor has the correct shape
    input_tensor = embedded_message.unsqueeze(0)  # Add batch dimension
    with torch.no_grad():
        predictions = model(input_tensor)

    # Extract predictions dictionary from model output
    predictions_dict = predictions[0]  # First element contains the task-specific outputs

    # Convert predictions to human-readable labels
    readable_predictions = {}
    for task, output in predictions_dict.items():
        predicted_label_idx = torch.argmax(output).item()
        # Ensure the index is within the range of label mappings
        if predicted_label_idx < len(label_mappings.get(task, [])):
            readable_predictions[task] = label_mappings.get(task, ['Unknown'])[predicted_label_idx]
        else:
            readable_predictions[task] = 'Unknown'

    print(f"Commit message: {message}")
    print("Predictions:")
    for task, label in readable_predictions.items():
        print(f"  {task}: {label}")
    print()

```

### backend\ai\testmodel\test_model.py
```py
import spacy

# Load mô hình đã train
nlp = spacy.load("modelAi")

# Các câu commit cần test
texts = [
    "feat: Thêm tính năng xử lý lỗi",
    "fix: Sửa lỗi hiển thị trên giao diện người dùng",
    "docs: Cập nhật tài liệu API",
    "refactor: Tối ưu code module tìm kiếm",
    "style: Chỉnh sửa format code",
    "test: Thêm test case cho hàm login",
    "chore: Cập nhật dependencies",
    "uncategorized: Thay đổi cấu trúc thư mục",
    "feat: Thêm API thanh toán mới",
    "fix: Sửa layout bị cách ra ngoài",
]

for text in texts:
    doc = nlp(text)
    print(f"Văn bản: {text}")
    for label, score in doc.cats.items():
        if score > 0.05:  # Tùy chỉnh ngưỡng lọc nhãn
            print(f"{label}: {score:.4f}")
    print()

```

### backend\ai\training\loss_functions.py
```py
# Dynamic loss weighting cho multi-task
import torch
import torch.nn as nn

def gradnorm_loss(losses, weights, grads, alpha=1.5):
    # losses: list of task losses
    # weights: nn.Parameter list
    # grads: list of grad norms
    avg_grad = torch.mean(torch.stack(grads))
    loss_gradnorm = 0
    for i, (l, w, g) in enumerate(zip(losses, weights, grads)):
        loss_gradnorm += torch.abs(g - avg_grad * (l.detach() / torch.mean(torch.stack(losses).detach())) ** alpha)
    return loss_gradnorm

class UncertaintyWeightingLoss(nn.Module):
    def __init__(self, num_tasks):
        super().__init__()
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))
    def forward(self, losses):
        weighted = 0
        for i, loss in enumerate(losses):
            precision = torch.exp(-self.log_vars[i])
            weighted += precision * loss + self.log_vars[i]
        return weighted

```

### backend\ai\training\multitask_trainer.py
```py
import torch
from tqdm import tqdm

# Huấn luyện đa nhiệm cho HAN
class MultiTaskTrainer:
    def __init__(self, model, optimizer, loss_fns, device, loss_weights=None):
        self.model = model
        self.optimizer = optimizer
        self.loss_fns = loss_fns  # dict: {task: loss_fn}
        self.device = device
        self.loss_weights = loss_weights or {k: 1.0 for k in loss_fns}

    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        for batch in tqdm(dataloader):
            x = batch['input'].to(self.device)
            y = {k: v.to(self.device) for k, v in batch['labels'].items()}
            self.optimizer.zero_grad()
            out, _, _ = self.model(x)
            loss = 0
            for task, loss_fn in self.loss_fns.items():
                task_loss = loss_fn(out[task], y[task])
                loss += self.loss_weights[task] * task_loss
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(dataloader)

    def validate(self, dataloader):
        self.model.eval()
        total_loss = 0
        all_preds = {k: [] for k in self.loss_fns}
        all_labels = {k: [] for k in self.loss_fns}
        with torch.no_grad():
            for batch in tqdm(dataloader):
                x = batch['input'].to(self.device)
                y = {k: v.to(self.device) for k, v in batch['labels'].items()}
                out, _, _ = self.model(x)
                loss = 0
                for task, loss_fn in self.loss_fns.items():
                    task_loss = loss_fn(out[task], y[task])
                    loss += self.loss_weights[task] * task_loss
                    all_preds[task].append(out[task].cpu())
                    all_labels[task].append(y[task].cpu())
                total_loss += loss.item()
        # Gộp lại
        for task in all_preds:
            all_preds[task] = torch.cat(all_preds[task])
            all_labels[task] = torch.cat(all_labels[task])
        return total_loss / len(dataloader), all_preds, all_labels

```

### backend\api\deps.py
```py
# KLTN04\backend\api\deps.py
# File chứa các dependencies (phụ thuộc) chung của ứng dụng

# Import AsyncSession từ SQLAlchemy để làm việc với database async
from sqlalchemy.ext.asyncio import AsyncSession

# Import kết nối database từ module database
from db.database import database

# Dependency (phụ thuộc) để lấy database session
async def get_db() -> AsyncSession:
    """
    Dependency tạo và quản lý database session
    
    Cách hoạt động:
    - Tạo một async session mới từ connection pool
    - Yield session để sử dụng trong request
    - Đảm bảo session được đóng sau khi request hoàn thành
    
    Returns:
        AsyncSession: Session database async để tương tác với DB
    """
    # Tạo và quản lý session thông qua context manager
    async with database.session() as session:
        # Yield session để sử dụng trong route
        yield session
        # Session sẽ tự động đóng khi ra khỏi block with
```

### backend\api\__init__.py
```py

```

### backend\api\routes\ai_suggestions.py
```py

```

### backend\api\routes\auth.py
```py
# KLTN04\backend\api\routes\auth.py
from fastapi import APIRouter, Request, HTTPException
from core.oauth import oauth
from fastapi.responses import RedirectResponse
from services.user_service import save_user  # Import hàm lưu người dùng
import os

auth_router = APIRouter()

# Endpoint /login để bắt đầu quá trình xác thực với GitHub
@auth_router.get("/login")
async def login(request: Request):
    # Lấy callback URL từ biến môi trường
    redirect_uri = os.getenv("GITHUB_CALLBACK_URL")
    
    # Chuyển hướng người dùng đến trang xác thực GitHub
    return await oauth.github.authorize_redirect(request, redirect_uri)

# Endpoint /auth/callback - GitHub sẽ gọi lại endpoint này sau khi xác thực thành công
@auth_router.get("/auth/callback")
async def auth_callback(request: Request):
    code = request.query_params.get("code")
    if not code:
        raise HTTPException(status_code=400, detail="Missing code")

    # Lấy access token từ GitHub
    token = await oauth.github.authorize_access_token(request)
    
    # Gọi API GitHub để lấy thông tin user cơ bản
    resp = await oauth.github.get("user", token=token)
    profile = resp.json()  # Chuyển response thành dictionary

    # Lấy email nếu không có trong profile
    if not profile.get("email"):
        # Gọi API riêng để lấy danh sách email
        emails_resp = await oauth.github.get("user/emails", token=token)
        emails = emails_resp.json()
        
        # Tìm email được đánh dấu là primary (chính)
        primary_email = next((e["email"] for e in emails if e["primary"]), None)
        
        # Gán email chính vào profile
        profile["email"] = primary_email

    # Kiểm tra thông tin bắt buộc
    if not profile.get("email") or not profile.get("login"):
        raise HTTPException(status_code=400, detail="Missing required user information")

    # Lưu thông tin người dùng vào cơ sở dữ liệu
    user_data = {
        "github_id": profile["id"],
        "github_username": profile["login"],
        "email": profile["email"],
        "avatar_url": profile["avatar_url"],
    }
    await save_user(user_data)

    # Redirect về frontend với token và thông tin người dùng
    redirect_url = (
        f"http://localhost:5173/auth-success"
        f"?token={token['access_token']}"
        f"&username={profile['login']}"
        f"&email={profile['email']}"
        f"&avatar_url={profile['avatar_url']}"
    )

    # Thực hiện chuyển hướng về frontend
    return RedirectResponse(redirect_url)
```

### backend\api\routes\commit_routes.py
```py
# File: backend/api/routes/commit_routes.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Header
from fastapi.responses import JSONResponse
from typing import List, Optional
import pandas as pd
from services.model_loader import predict_commit
from pathlib import Path
import tempfile
import httpx

router = APIRouter(prefix="/api/commits", tags=["Commit Analysis"])

@router.get("/analyze-github/{owner}/{repo}")
async def analyze_github_commits(
    owner: str,
    repo: str,
    authorization: str = Header(..., alias="Authorization"),
    per_page: int = 30,
    since: Optional[str] = None,
    until: Optional[str] = None
):
    """
    Phân tích commit từ repository GitHub
    
    Args:
        owner: Tên chủ repo
        repo: Tên repository
        authorization: Token GitHub (Format: Bearer <token>)
        per_page: Số commit tối đa cần phân tích (1-100)
        since: Lọc commit từ ngày (YYYY-MM-DDTHH:MM:SSZ)
        until: Lọc commit đến ngày (YYYY-MM-DDTHH:MM:SSZ)
    
    Returns:
        {
            "repo": f"{owner}/{repo}",
            "total": int,
            "critical": int,
            "critical_percentage": float,
            "details": List[dict],
            "analysis_date": str
        }
    """
    try:
        # Validate input
        if per_page < 1 or per_page > 100:
            raise HTTPException(
                status_code=400,
                detail="per_page must be between 1 and 100"
            )

        # Configure GitHub API request
        headers = {
            "Authorization": authorization,
            "Accept": "application/vnd.github.v3+json"
        }
        params = {
            "per_page": per_page,
            "since": since,
            "until": until
        }
        
        # Fetch commits from GitHub
        async with httpx.AsyncClient() as client:
            # Get first page to check repo accessibility
            initial_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(initial_url, headers=headers, params={**params, "per_page": 1})
            
            if response.status_code == 404:
                raise HTTPException(
                    status_code=404,
                    detail="Repository not found or access denied"
                )
            response.raise_for_status()

            # Get all requested commits
            full_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(full_url, headers=headers, params=params)
            response.raise_for_status()
            commits_data = response.json()

        # Prepare analysis data
        commits_for_analysis = [
            {
                "id": commit["sha"],
                "message": commit["commit"]["message"],
                "date": commit["commit"]["committer"]["date"] if commit["commit"]["committer"] else None
            }
            for commit in commits_data
            if commit.get("sha") and commit.get("commit", {}).get("message")
        ]

        # Analyze commits
        results = {
            "repo": f"{owner}/{repo}",
            "total": len(commits_for_analysis),
            "critical": 0,
            "critical_percentage": 0.0,
            "details": [],
            "analysis_date": datetime.utcnow().isoformat()
        }

        for commit in commits_for_analysis:
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
            
            results["details"].append({
                "id": commit["id"],
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message'],
                "date": commit["date"]
            })

        # Calculate percentage
        if results["total"] > 0:
            results["critical_percentage"] = round(
                (results["critical"] / results["total"]) * 100, 2
            )

        return results

    except httpx.HTTPStatusError as e:
        error_detail = "GitHub API error"
        if e.response.status_code == 403:
            error_detail = "API rate limit exceeded" if "rate limit" in str(e.response.content) else "Forbidden"
        elif e.response.status_code == 401:
            error_detail = "Invalid GitHub token"
        
        raise HTTPException(
            status_code=e.response.status_code,
            detail=f"{error_detail}: {e.response.text}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing GitHub commits: {str(e)}"
        )
@router.post("/analyze-text")
async def analyze_commit_text(message: str):
    """
    Phân tích một commit message dạng text
    
    Args:
        message: Nội dung commit message
    
    Returns:
        {"is_critical": 0|1, "message": string}
    """
    try:
        is_critical = predict_commit(message)
        return {
            "is_critical": is_critical,
            "message": "Phân tích thành công",
            "input_sample": message[:100] + "..." if len(message) > 100 else message
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lỗi phân tích: {str(e)}")

@router.post("/analyze-json")
async def analyze_commits_json(commits: List[dict]):
    """
    Phân tích nhiều commit từ JSON
    
    Args:
        commits: List[{"id": string, "message": string}]
    
    Returns:
        {"total": int, "critical": int, "details": List[dict]}
    """
    try:
        results = {
            "total": len(commits),
            "critical": 0,
            "details": []
        }
        
        for commit in commits:
            if not isinstance(commit, dict) or 'message' not in commit:
                continue
                
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
                
            results["details"].append({
                "id": commit.get("id", ""),
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message']
            })
            
        return JSONResponse(content=results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lỗi phân tích hàng loạt: {str(e)}")

@router.post("/analyze-csv", response_model=dict)
async def analyze_commits_csv(file: UploadFile = File(...)):
    """
    Phân tích commit từ file CSV
    
    Args:
        file: File CSV có cột 'message' hoặc 'commit_message'
    
    Returns:
        {"filename": string, "total": int, "critical": int}
    """
    try:
        # Lưu file tạm
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(await file.read())
            tmp_path = Path(tmp.name)
        
        # Đọc file CSV
        df = pd.read_csv(tmp_path)
        tmp_path.unlink()  # Xóa file tạm
        
        # Kiểm tra cột message
        message_col = 'message' if 'message' in df.columns else 'commit_message'
        if message_col not in df.columns:
            raise HTTPException(status_code=400, detail="File thiếu cột 'message' hoặc 'commit_message'")
        
        # Phân tích
        results = {
            "filename": file.filename,
            "total": len(df),
            "critical": 0,
            "sample_results": []
        }
        
        df['is_critical'] = df[message_col].apply(predict_commit)
        results["critical"] = int(df['is_critical'].sum())
        
        # Lấy 5 kết quả mẫu
        sample = df.head(5).to_dict('records')
        results["sample_results"] = [{
            "message": row[message_col][:100] + "..." if len(row[message_col]) > 100 else row[message_col],
            "is_critical": bool(row['is_critical'])
        } for row in sample]
        
        return results
        
    except Exception as e:
        if tmp_path.exists():
            tmp_path.unlink()
        raise HTTPException(status_code=500, detail=f"Lỗi xử lý file: {str(e)}")
```

### backend\api\routes\github.py
```py
# backend/api/routes/github.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.repo_service import get_repo_data
from services.commit_service import save_commit
from services.repo_service import get_repo_id_by_owner_and_name
from services.user_service import get_user_id_by_github_username
from services.branch_service import save_branch
from sqlalchemy.future import select
from fastapi import APIRouter, Depends
from services.repo_service import save_repository
from datetime import datetime
from sqlalchemy import select
from db.models.commits import commits
from db.models.repositories import repositories  # để lấy access token
from schemas.commit import CommitCreate  # schema
from services.github_service import fetch_commits  # hàm gọi GitHub API
from sqlalchemy.ext.asyncio import AsyncSession
from schemas.commit import CommitOut
from db.database import database

from services.branch_service import save_branches
from services.issue_service import save_issue, save_issues # Import cả hai hàm
github_router = APIRouter()

# Endpoint lấy thông tin repository cụ thể
@github_router.get("/github/{owner}/{repo}")
async def fetch_repo(owner: str, repo: str):
    return await get_repo_data(owner, repo)

@github_router.get("/github/repos")
async def get_user_repos(request: Request):
    # Lấy token từ header Authorization
    token = request.headers.get("Authorization")
    
    # Kiểm tra token hợp lệ (phải bắt đầu bằng "token ")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    # Gọi GitHub API để lấy danh sách repo
    async with httpx.AsyncClient() as client:
        resp = await client.get(
            "https://api.github.com/user/repos",
            headers={"Authorization": token}
        )
        # Nếu lỗi thì raise exception
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)
    
    # Trả về kết quả dạng JSON
    return resp.json()

# Endpoint lấy danh sách commit của một repository
@github_router.get("/github/{owner}/{repo}/commits")
async def get_commits(owner: str, repo: str, request: Request, branch: str = "main"):
    # Lấy token từ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Gọi GitHub API để lấy commit
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        # Xử lý trường hợp repository trống (409)
        if resp.status_code == 409:
            return []
        # Xử lý lỗi khác
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

def get_db():
    return database
# Lấy danh sách commit từ database
@github_router.get("/github/{owner}/{repo}/commits/db")
async def get_commits_from_db(owner: str, repo: str, db: AsyncSession = Depends(get_db)):
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    query = select(commits).where(commits.c.repo_id == repo_id)
    result = await db.fetch_all(query)
    return result

@github_router.get("/github/{owner}/{repo}/branches")
async def get_branches(owner: str, repo: str, request: Request):
    # Lấy token từ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Gọi GitHub API lấy danh sách branch
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

# Endpoint lưu commit vào database
@github_router.post("/github/{owner}/{repo}/save-commits")
async def save_repo_commits(owner: str, repo: str, request: Request, branch: str = None):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Nếu không truyền branch, lấy branch mặc định từ GitHub
    if not branch:
        async with httpx.AsyncClient() as client:
            repo_url = f"https://api.github.com/repos/{owner}/{repo}"
            headers = {"Authorization": token}
            repo_resp = await client.get(repo_url, headers=headers)
            if repo_resp.status_code != 200:
                raise HTTPException(status_code=repo_resp.status_code, detail=repo_resp.text)
            repo_data = repo_resp.json()
            branch = repo_data.get("default_branch", "main")

    # Lấy danh sách commit từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        commit_list = resp.json()

    # Lấy repo_id từ cơ sở dữ liệu
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    # Lưu từng commit vào cơ sở dữ liệu
    async with httpx.AsyncClient() as client:
        for commit in commit_list:
            # Lấy thông tin chi tiết của commit
            commit_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit['sha']}"
            commit_resp = await client.get(commit_url, headers={"Authorization": token})
            if commit_resp.status_code != 200:
                continue  # Bỏ qua commit nếu không lấy được thông tin chi tiết

            commit_details = commit_resp.json()
            stats = commit_details.get("stats", {})
            commit_data = {
                "sha": commit["sha"],
                "message": commit["commit"]["message"],
                "author_name": commit["commit"]["author"]["name"],
                "author_email": commit["commit"]["author"]["email"],
                "date": datetime.strptime(commit["commit"]["author"]["date"], "%Y-%m-%dT%H:%M:%SZ"),
                "insertions": stats.get("additions", 0),
                "deletions": stats.get("deletions", 0),
                "files_changed": stats.get("total", 0),
                "repo_id": repo_id,
            }
            await save_commit(commit_data)

    return {"message": "Commits saved successfully!"}
#lưu branchbranch vào database
@github_router.post("/github/{owner}/{repo}/save-branches")
async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Lấy danh sách branch từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()

    # Lưu branch vào database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for branch in branches:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
        }
        await save_branch(branch_data)

    return {"message": "Branches saved successfully!"}
# lưu issues vào database
@github_router.post("/github/{owner}/{repo}/save-issues")
async def save_issues(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Lấy danh sách issue từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        issues = resp.json()

    
    # Lưu issue vào database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for issue in issues:
        issue_data = {
            "title": issue["title"],
            "body": issue["body"],
            "state": issue["state"],
            "created_at": issue["created_at"],
            "updated_at": issue["updated_at"],
            "repo_id": repo_id,
        }
        await save_issue(issue_data)

    return {"message": "Issues saved successfully!"}
#save repo vào database
async def save_repo(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        repo_data = resp.json()

    repo_entry = {
        "github_id": repo_data["id"],
        "name": repo_data["name"],
        "owner": repo_data["owner"]["login"],
        "description": repo_data["description"],
        "stars": repo_data["stargazers_count"],
        "forks": repo_data["forks_count"],
        "language": repo_data["language"],
        "open_issues": repo_data["open_issues_count"],
        "url": repo_data["html_url"],
    }

    try:
        await save_repository(repo_entry)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving repository: {str(e)}")

def get_db():
    return database

# Endpoint lấy tất cả commit từ database
@github_router.get("/commits")
async def get_commits(db = Depends(get_db)):
    query = commits.select()  # Lấy tất cả commit
    result = await db.fetch_all(query)
    return result

# Endpoint đồng bộ commit từ GitHub về database
@github_router.get("/sync-commits")
async def sync_commits(
    repo_id: int,
    branch: str = "main",
    since: str = None,
    until: str = None,
    db: AsyncSession = Depends(get_db)
):
    # 1. Lấy thông tin repository từ database
    repo = await db.scalar(select(repositories).where(repositories.c.id == repo_id))
    if not repo:
        raise HTTPException(status_code=404, detail="Repository không tồn tại")

    # 2. Gọi GitHub API lấy commit với các tham số lọc
    commits_data = await fetch_commits(
        token=repo.token,  # Access token
        owner=repo.owner,  # Chủ repository
        name=repo.name,  # Tên repository
        branch=branch,  # Branch cần lấy
        since=since,  # Lọc từ thời gian
        until=until  # Lọc đến thời gian
    )

    # 3. Lưu commit mới vào database
    new_commits = []
    for item in commits_data:
        sha = item["sha"]
        # Kiểm tra commit đã tồn tại chưa
        existing = await db.scalar(select(commits).where(commits.c.sha == sha))
        if existing:
            continue  # Bỏ qua nếu đã tồn tại

        # Tạo commit mới
        new_commit = CommitCreate(
            sha=sha,
            message=item["commit"]["message"],
            author=item["commit"]["author"]["name"],
            date=item["commit"]["author"]["date"],
            repository_id=repo.id
        )
        commit_obj = commits.insert().values(**new_commit.dict())
        await db.execute(commit_obj)
        new_commits.append(new_commit)

    await db.commit()

    return {
        "message": f"Đồng bộ thành công {len(new_commits)} commit.",
        "data": [c.sha for c in new_commits]
    }
# đồng bộ toàn bộ dữ liệu
@github_router.post("/github/{owner}/{repo}/sync-all")
async def sync_all(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Đồng bộ repository
    await save_repo(owner, repo, request)

    # Đồng bộ branches
    await save_branches(owner, repo, request)

    # Đồng bộ commits
    await save_repo_commits(owner, repo, request)

    # Đồng bộ issues
    await save_issues(owner, repo, request)

    return {"message": "Đồng bộ toàn bộ dữ liệu thành công!"}
```

### backend\api\routes\gitlab.py
```py

```

### backend\api\routes\repo.py
```py

```

### backend\api\routes\users.py
```py

```

### backend\api\routes\__init__.py
```py

```

### backend\core\config.py
```py
# backend/core/config.py
# File cấu hình chính cho ứng dụng FastAPI

# Import các thư viện cần thiết
import os  # Làm việc với biến môi trường
from fastapi.middleware.cors import CORSMiddleware  # Middleware CORS
from starlette.middleware.sessions import SessionMiddleware  # Middleware quản lý session
from fastapi import FastAPI  # Framework chính
from api.routes.github import github_router  # Router cho GitHub API
from api.routes.auth import auth_router  # Router cho xác thực
from dotenv import load_dotenv  # Đọc file .env

# Nạp biến môi trường từ file .env
load_dotenv()

# Hàm cấu hình các middleware cho ứng dụng
def setup_middlewares(app: FastAPI):
    """
    Thiết lập các middleware cần thiết cho ứng dụng
    
    Args:
        app (FastAPI): Instance của FastAPI app
    """
    
    # Thêm middleware CORS (Cross-Origin Resource Sharing)
    app.add_middleware(
        CORSMiddleware,
        # Danh sách domain được phép truy cập
        allow_origins=[
            "http://localhost:5173",  # Frontend dev (Vite thường chạy ở port 5173)
            "http://localhost:3000"   # Frontend dev (React có thể chạy ở port 3000)
        ],
        allow_credentials=True,  # Cho phép gửi credential (cookies, auth headers)
        allow_methods=["*"],  # Cho phép tất cả HTTP methods
        allow_headers=["*"],  # Cho phép tất cả headers (bao gồm Authorization)
    )

    # Thêm middleware quản lý session
    app.add_middleware(
        SessionMiddleware,
        secret_key=os.getenv('SECRET_KEY')  # Khóa bí mật từ biến môi trường
    )


# Hàm cấu hình các router cho ứng dụng
def setup_routers(app: FastAPI):
    """
    Đăng ký các router chính của ứng dụng
    
    Args:
        app (FastAPI): Instance của FastAPI app
    """
    
    # Đăng ký auth router với prefix /auth
    app.include_router(auth_router, prefix="/auth")
    
    # Đăng ký github router với prefix /api
    app.include_router(github_router, prefix="/api")  # Gộp chung không bị đè lẫn nhau
```

### backend\core\lifespan.py
```py
from fastapi import FastAPI
from contextlib import asynccontextmanager
from db.database import database
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        await database.connect()
        logger.info("✅ Đã kết nối tới database thành công.")
        yield  # Chỉ yield nếu connect thành công
    except Exception as e:
        logger.error(f"❌ Kết nối database thất bại: {e}")
        raise e  # Dừng app nếu không kết nối được DB
    finally:
        try:
            await database.disconnect()
            logger.info("🛑 Đã ngắt kết nối database.")
        except Exception as e:
            logger.error(f"❌ Lỗi khi ngắt kết nối database: {e}")

```

### backend\core\logger.py
```py
# core/logger.py

import logging

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,  # Hiện log từ cấp INFO trở lên
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

```

### backend\core\oauth.py
```py
# backend/core/oauth.py
# File cấu hình OAuth cho ứng dụng, chủ yếu dùng cho GitHub OAuth

# Import các thư viện cần thiết
import os  # Để làm việc với biến môi trường
from dotenv import load_dotenv  # Để đọc file .env
from authlib.integrations.starlette_client import OAuth  # Thư viện OAuth cho Starlette/FastAPI

# Load các biến môi trường từ file .env
load_dotenv()

# Khởi tạo instance OAuth
oauth = OAuth()

# Đăng ký provider GitHub cho OAuth
oauth.register(
    name='github',  # Tên provider
    
    # Client ID từ ứng dụng GitHub OAuth App
    client_id=os.getenv('GITHUB_CLIENT_ID'),
    
    # Client Secret từ ứng dụng GitHub OAuth App
    client_secret=os.getenv('GITHUB_CLIENT_SECRET'),
    
    # URL để lấy access token
    access_token_url='https://github.com/login/oauth/access_token',
    
    # Các params thêm khi lấy access token (None nếu không có)
    access_token_params=None,
    
    # URL để xác thực
    authorize_url='https://github.com/login/oauth/authorize',
    
    # Các params thêm khi xác thực (None nếu không có)
    authorize_params=None,
    
    # Base URL cho API GitHub
    api_base_url='https://api.github.com/',
    
    # Các tham số bổ sung cho client
    client_kwargs={
        'scope': 'read:user user:email repo'  # Các quyền yêu cầu
        # read:user - Đọc thông tin user
        # user:email - Đọc email user
        # repo - Truy cập repository
    }
)
```

### backend\core\security.py
```py

```

### backend\migrations\env.py
```py
import os
from dotenv import load_dotenv
from sqlalchemy import engine_from_config, pool
from alembic import context
from db.metadata import metadata  # Import metadata từ metadata.py

# Nạp biến môi trường từ file .env
load_dotenv()

# Lấy DATABASE_URL từ biến môi trường
config = context.config
database_url = os.getenv("DATABASE_URL").replace("asyncpg", "psycopg2")
config.set_main_option("sqlalchemy.url", database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    from logging.config import fileConfig
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```

### backend\models\commit_model.py
```py
# KLTN04\backend\models\commit_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
import joblib

class CommitClassifier:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.model = RandomForestClassifier()
        self.labels = ['normal', 'critical']  # 0: normal, 1: critical/bugfix

    def train(self, df: pd.DataFrame):
        """Huấn luyện model từ dataframe"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical']  # Cột nhãn (0/1)
        self.model.fit(X, y)
        
    def predict(self, new_messages: list):
        """Dự đoán commit quan trọng cần review"""
        X_new = self.vectorizer.transform(new_messages)
        return self.model.predict(X_new)
    
    def save(self, path='models/commit_classifier.joblib'):
        """Lưu model"""
        joblib.dump({
            'vectorizer': self.vectorizer,
            'model': self.model
        }, path)
    
    @classmethod
    def load(cls, path='models/commit_classifier.joblib'):
        """Load model đã lưu"""
        data = joblib.load(path)
        classifier = cls()
        classifier.vectorizer = data['vectorizer']
        classifier.model = data['model']
        return classifier
```

### backend\models\task_model.py
```py
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class TaskAssigner:
    def __init__(self):
        self.skill_matrix = None
    
    def build_skill_matrix(self, developers: list, tasks: list):
        """Tạo ma trận kỹ năng developer-task"""
        # Vector hóa kỹ năng (ví dụ: [1,0,1] = biết Python, không biết SQL, biết Docker)
        dev_vectors = [d['skill_vector'] for d in developers]
        task_vectors = [t['required_skills'] for t in tasks]
        
        self.skill_matrix = cosine_similarity(task_vectors, dev_vectors)
        return self.skill_matrix
    
    def assign_tasks(self, developers: list, tasks: list):
        """Phân công công việc tối ưu"""
        if self.skill_matrix is None:
            self.build_skill_matrix(developers, tasks)
            
        assignments = []
        for task_idx, task in enumerate(tasks):
            best_dev_idx = np.argmax(self.skill_matrix[task_idx])
            assignments.append({
                'task_id': task['id'],
                'dev_id': developers[best_dev_idx]['id'],
                'fit_score': float(self.skill_matrix[task_idx][best_dev_idx])
            })
        return assignments
```

### backend\schemas\commit.py
```py
from pydantic import BaseModel
from datetime import datetime


class CommitCreate(BaseModel):
    commit_id: str
    message: str
    author_name: str
    author_email: str
    committed_date: datetime
    repository_id: int


class CommitOut(CommitCreate):
    id: int

    class Config:
        from_attributes = True  # Dành cho Pydantic V2 thay cho orm_mode

```

### backend\scripts\commit_analysis_system.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import warnings
warnings.filterwarnings('ignore')

class CommitAnalysisSystem:
    def __init__(self):
        """Khởi tạo hệ thống với cấu hình tối ưu"""
        self.vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            ngram_range=(1, 1)
        )
        self.model = RandomForestClassifier(
            n_estimators=30,
            max_depth=8,
            n_jobs=1,
            class_weight='balanced'
        )
        self.client = None

    def init_dask_client(self):
        """Khởi tạo Dask client"""
        self.client = Client(n_workers=2, threads_per_worker=1, memory_limit='2GB')

    @staticmethod
    def lightweight_heuristic(msg):
        """Hàm heuristic tĩnh để xử lý song song"""
        if not isinstance(msg, str) or not msg.strip():
            return 0
        msg = msg.lower()[:150]
        return int(any(kw in msg for kw in ['fix', 'bug', 'error', 'fail']))

    def process_large_file(self, input_path, output_dir):
        """Xử lý file lớn với Dask """
        try:
            if self.client:
                self.client.close()
            self.init_dask_client()

            # Đọc file với Dask
            ddf = dd.read_csv(
                str(input_path),
                blocksize="20MB",
                dtype={'message': 'string'},
                usecols=['commit', 'message'],
                na_values=['', 'NA', 'N/A', 'nan']
            )
            
            # Sửa lỗi: Thay .notna() bằng .notnull() cho Dask
            ddf = ddf[ddf['message'].notnull()]
            
            # Gán nhãn
            ddf['is_critical'] = ddf['message'].map(
                self.lightweight_heuristic,
                meta=('is_critical', 'int8')
            )
            
           # Lưu kết quả (đã sửa phần compute)
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # Sửa lỗi: Gọi compute() trực tiếp trên to_csv()
            ddf.to_csv(
                str(output_dir / "part_*.csv"),
                index=False
            )
            
            return True
        except Exception as e:
            print(f"🚨 Lỗi xử lý file: {str(e)}")
            return False
        finally:
            if self.client:
                self.client.close()

    def clean_data(self, df):
        """Làm sạch dữ liệu"""
        if 'message' not in df.columns:
            raise ValueError("Thiếu cột 'message' trong dữ liệu")
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df):
        """Gán nhãn tự động"""
        df = self.clean_data(df)
        df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
        return df

    def train_model(self, df):
        """Huấn luyện mô hình"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical'].values
        self.model.fit(X, y)

    def evaluate(self, test_df):
        """Đánh giá mô hình"""
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        print(classification_report(y_test, self.model.predict(X_test)))

    def save_model(self, path):
        """Lưu mô hình"""
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer
        }, str(path))

def main():
    print("🚀 Bắt đầu phân tích commit...")
    system = CommitAnalysisSystem()
    
    input_path = Path("D:/Project/KLTN04/data/oneline.csv")
    output_dir = Path("D:/Project/KLTN04/data/processed")
    
    if system.process_large_file(input_path, output_dir):
        print("✅ Đã xử lý file thành công")
        
        # Nạp và xử lý dữ liệu
        df = pd.concat([pd.read_csv(f) for f in output_dir.glob("part_*.csv")])
        df = system.auto_label(df)
        
        # Huấn luyện và đánh giá
        system.train_model(df)
        test_df = df.sample(frac=0.2, random_state=42)
        system.evaluate(test_df)
        
        # Lưu mô hình
        model_path = "backend/models/commit_classifier.joblib"
        system.save_model(model_path)
        print(f"💾 Đã lưu mô hình tại: {model_path}")

if __name__ == "__main__":
    main()
```

### backend\scripts\commit_analysis_system_v1.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import logging
from typing import Optional, Union
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CommitAnalysisSystem:
    """Hệ thống phân tích commit tự động với khả năng xử lý dữ liệu lớn"""
    
    VERSION = "1.0.0"
    
    def __init__(self, model_params: Optional[dict] = None, 
                 vectorizer_params: Optional[dict] = None):
        """
        Khởi tạo hệ thống phân tích commit
        
        Args:
            model_params: Tham số cho RandomForestClassifier
            vectorizer_params: Tham số cho TfidfVectorizer
        """
        # Cấu hình mặc định
        default_vectorizer_params = {
            'max_features': 1000,
            'stop_words': 'english',
            'ngram_range': (1, 2),  # Thêm bigram
            'min_df': 5,
            'max_df': 0.8
        }
        
        default_model_params = {
            'n_estimators': 100,
            'max_depth': 15,
            'class_weight': 'balanced',
            'random_state': 42
        }
        
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or default_vectorizer_params))
        self.model = RandomForestClassifier(**(model_params or default_model_params))
        self.client = None
        self._is_trained = False

    def init_dask_client(self, **kwargs):
        """Khởi tạo Dask client với cấu hình tùy chọn"""
        default_config = {
            'n_workers': 2,
            'threads_per_worker': 1,
            'memory_limit': '2GB',
            'silence_logs': logging.ERROR
        }
        config = {**default_config, **kwargs}
        
        try:
            self.client = Client(**config)
            logger.info(f"Dask client initialized with config: {config}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Dask client: {str(e)}")
            return False

    @staticmethod
    def lightweight_heuristic(msg: str) -> int:
        """Phân loại commit sử dụng heuristic đơn giản
        
        Args:
            msg: Nội dung commit message
            
        Returns:
            1 nếu là commit quan trọng (bugfix), 0 nếu không
        """
        if not isinstance(msg, str) or not msg.strip():
            return 0
            
        msg = msg.lower()[:200]  # Giới hạn độ dài xử lý
        keywords = {
            'fix', 'bug', 'error', 'fail', 'patch', 
            'resolve', 'crash', 'defect', 'issue'
        }
        return int(any(kw in msg for kw in keywords))

    def process_large_file(self, input_path: Union[str, Path], output_dir: Union[str, Path]) -> bool:
        """Xử lý file dữ liệu lớn bằng Dask"""
        try:
            input_path = Path(input_path)
            output_dir = Path(output_dir)

            if not input_path.exists():
                logger.error(f"Input file not found: {input_path}")
                return False

            logger.info(f"Starting processing large file: {input_path}")
            start_time = datetime.now()

            # Khởi tạo Dask client
            if not self.init_dask_client():
                return False

            try:
                # Đọc và xử lý dữ liệu
                ddf = dd.read_csv(
                    str(input_path),
                    blocksize="10MB",  # Giảm kích thước block để an toàn
                    dtype={'message': 'string'},
                    usecols=['commit', 'message'],
                    na_values=['', 'NA', 'N/A', 'nan']
                )

                # Lọc và gán nhãn
                ddf = ddf[ddf['message'].notnull()]
                ddf['is_critical'] = ddf['message'].map(
                    self.lightweight_heuristic,
                    meta=('is_critical', 'int8')
                )

                # Lưu kết quả
                output_dir.mkdir(exist_ok=True, parents=True)
                output_path = str(output_dir / f"processed_{input_path.stem}.csv")

                # Sử dụng dask.dataframe.to_csv với single_file=True
                ddf.to_csv(
                    output_path,
                    index=False,
                    single_file=True
                )

                logger.info(f"Processing completed in {datetime.now() - start_time}")
                logger.info(f"Results saved to: {output_path}")
                return True

            except Exception as e:
                logger.exception(f"Error during processing: {str(e)}")
                return False

        except Exception as e:
            logger.exception(f"System error: {str(e)}")
            return False

        finally:
            if self.client:
                self.client.close()
                self.client = None

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Làm sạch dữ liệu đầu vào
        
        Args:
            df: DataFrame chứa dữ liệu commit
            
        Returns:
            DataFrame đã được làm sạch
        """
        if 'message' not in df.columns:
            raise ValueError("Input data must contain 'message' column")
            
        df = df.copy()
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df: pd.DataFrame) -> pd.DataFrame:
        """Tự động gán nhãn cho dữ liệu commit
        
        Args:
            df: DataFrame chứa các commit message
            
        Returns:
            DataFrame đã được gán nhãn
        """
        try:
            df = self.clean_data(df)
            df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
            logger.info(f"Label distribution:\n{df['is_critical'].value_counts()}")
            return df
        except Exception as e:
            logger.error(f"Auto-labeling failed: {str(e)}")
            raise

    def train_model(self, df: pd.DataFrame) -> bool:
        """Huấn luyện mô hình phân loại commit
        
        Args:
            df: DataFrame đã được gán nhãn
            
        Returns:
            True nếu huấn luyện thành công
        """
        try:
            logger.info("Starting model training...")
            
            X = self.vectorizer.fit_transform(df['message'])
            y = df['is_critical'].values
            
            self.model.fit(X, y)
            self._is_trained = True
            
            logger.info("Model training completed successfully")
            return True
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return False

    def evaluate(self, test_df: pd.DataFrame) -> None:
        """Đánh giá hiệu suất mô hình
        
        Args:
            test_df: DataFrame chứa dữ liệu test
        """
        if not self._is_trained:
            logger.warning("Model has not been trained yet")
            return
            
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        y_pred = self.model.predict(X_test)
        
        report = classification_report(
            y_test, 
            y_pred, 
            target_names=['normal', 'critical']
        )
        logger.info(f"\nModel evaluation:\n{report}")

    def save_model(self, path: Union[str, Path]) -> bool:
        """Lưu mô hình và vectorizer
        
        Args:
            path: Đường dẫn lưu model
            
        Returns:
            True nếu lưu thành công
        """
        try:
            path = Path(path)
            path.parent.mkdir(exist_ok=True, parents=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'version': self.VERSION,
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, str(path))
            logger.info(f"Model saved to {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            return False

    @classmethod
    def load_model(cls, path: Union[str, Path]):
        """Tải mô hình đã lưu
        
        Args:
            path: Đường dẫn đến file model
            
        Returns:
            Instance của CommitAnalysisSystem với model đã tải
        """
        try:
            path = Path(path)
            model_data = joblib.load(str(path))
            
            system = cls()
            system.model = model_data['model']
            system.vectorizer = model_data['vectorizer']
            system._is_trained = True
            
            logger.info(f"Loaded model (v{model_data.get('version', 'unknown')} "
                       f"created at {model_data.get('timestamp', 'unknown')}")
            return system
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

def main():
    """Entry point cho ứng dụng"""
    try:
        logger.info("🚀 Starting commit analysis system")
        
        # Cấu hình đường dẫn
        input_path = Path("D:/Project/KLTN04/data/oneline.csv")
        output_dir = Path("D:/Project/KLTN04/data/processed")
        model_path = Path("backend/models/commit_classifier_v1.joblib")
        
        # Khởi tạo hệ thống
        system = CommitAnalysisSystem()
        
        # Xử lý dữ liệu lớn
        if system.process_large_file(input_path, output_dir):
            # Tổng hợp kết quả
            df = pd.concat([
                pd.read_csv(f) 
                for f in output_dir.glob("processed_*.csv")
            ])
            
            # Gán nhãn và huấn luyện
            labeled_data = system.auto_label(df)
            system.train_model(labeled_data)
            
            # Đánh giá trên tập test
            test_df = labeled_data.sample(frac=0.2, random_state=42)
            system.evaluate(test_df)
            
            # Lưu model
            if system.save_model(model_path):
                logger.info(f"✅ Pipeline completed successfully. Model saved to {model_path}")
        
    except Exception as e:
        logger.exception("❌ Critical error in main pipeline")
    finally:
        logger.info("🏁 System shutdown")

if __name__ == "__main__":
    main()
```

### backend\services\ai_model.py
```py

```

### backend\services\ai_service.py
```py
from models.commit_model import CommitClassifier
from models.task_model import TaskAssigner
from fastapi import APIRouter

router = APIRouter()
commit_model = CommitClassifier.load()
task_model = TaskAssigner()

@router.post("/analyze-commits")
async def analyze_commits(messages: list[str]):
    predictions = commit_model.predict(messages)
    return {"predictions": predictions.tolist()}

@router.post("/assign-tasks")
async def assign_tasks(developers: list, tasks: list):
    assignments = task_model.assign_tasks(developers, tasks)
    return {"assignments": assignments}
```

### backend\services\branch_service.py
```py
from db.models.branches import branches
from db.database import database
from fastapi import HTTPException, Request
import httpx
import logging

logger = logging.getLogger(__name__)

async def get_repo_id_by_owner_and_name(owner: str, repo: str):
    # Placeholder function for getting repository ID by owner and name
    pass

async def save_branch(branch_data):
    query = branches.insert().values(
        name=branch_data["name"],
        repo_id=branch_data["repo_id"],
    )
    await database.execute(query)

async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()
        logger.info(f"Branches data: {branches}")

    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for branch in branches:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
        }
        await save_branch(branch_data)
```

### backend\services\commit_service.py
```py
from db.models.commits import commits
from db.database import database
from sqlalchemy import select

async def save_commit(commit_data):
    # Kiểm tra commit đã tồn tại chưa
    query = select(commits).where(commits.c.sha == commit_data["sha"])
    existing_commit = await database.fetch_one(query)

    if existing_commit:
        return  # Bỏ qua nếu commit đã tồn tại

    # Chèn commit mới
    query = commits.insert().values(commit_data)
    await database.execute(query)
```

### backend\services\github_service.py
```py
# backend/services/github_service.py
# Service xử lý các tương tác với GitHub API

# Import các thư viện cần thiết
import httpx  # Thư viện HTTP client async
import os  # Làm việc với biến môi trường
from dotenv import load_dotenv  # Đọc file .env
from typing import Optional  # Để khai báo kiểu dữ liệu optional
load_dotenv()  # Nạp biến môi trường từ file .env

# Lấy GitHub token từ biến môi trường
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

# Base URL cho GitHub API
BASE_URL = "https://api.github.com"

# Headers mặc định cho các request GitHub API
headers = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",  # Token xác thực
    "Accept": "application/vnd.github+json",  # Loại response mong muốn
}

async def fetch_from_github(url: str):
    """
    Hàm tổng quát để fetch dữ liệu từ GitHub API
    
    Args:
        url (str): Phần cuối của URL (sau BASE_URL)
    
    Returns:
        dict: Dữ liệu JSON trả về từ GitHub API
    
    Raises:
        HTTPError: Nếu request lỗi
    """
    async with httpx.AsyncClient() as client:
        # Gọi GET request tới GitHub API
        response = await client.get(f"{BASE_URL}{url}", headers=headers)
        # Tự động raise exception nếu có lỗi HTTP
        response.raise_for_status()
        # Trả về dữ liệu dạng JSON
        return response.json()

async def fetch_commits(
    token: str, 
    owner: str, 
    name: str, 
    branch: str, 
    since: Optional[str], 
    until: Optional[str]
):
    """
    Lấy danh sách commit từ repository GitHub
    
    Args:
        token (str): GitHub access token
        owner (str): Chủ repository
        name (str): Tên repository
        branch (str): Tên branch
        since (Optional[str]): Lọc commit từ thời gian này (ISO format)
        until (Optional[str]): Lọc commit đến thời gian này (ISO format)
    
    Returns:
        list: Danh sách commit
    
    Raises:
        HTTPError: Nếu request lỗi
    """
    # Xây dựng URL API để lấy commit
    url = f"https://api.github.com/repos/{owner}/{name}/commits"
    
    # Headers cho request
    headers = {
        "Authorization": f"token {token}",  # Sử dụng token từ tham số
        "Accept": "application/vnd.github+json"  # Loại response mong muốn
    }
    
    # Parameters cho request
    params = {
        "sha": branch  # Lọc theo branch
    }
    
    # Thêm tham số lọc thời gian nếu có
    if since:
        params["since"] = since
    if until:
        params["until"] = until

    # Gọi API GitHub
    async with httpx.AsyncClient() as client:
        res = await client.get(url, headers=headers, params=params)
        # Kiểm tra lỗi HTTP
        res.raise_for_status()
        # Trả về dữ liệu dạng JSON
        return res.json()
```

### backend\services\gitlab_service.py
```py

```

### backend\services\issue_service.py
```py
from db.database import database
from db.models import issues

# Lưu một issue duy nhất
async def save_issue(issue_data):
    query = issues.insert().values(
        github_id=issue_data["github_id"],
        title=issue_data["title"],
        body=issue_data["body"],
        state=issue_data["state"],
        created_at=issue_data["created_at"],
        updated_at=issue_data["updated_at"],
        repo_id=issue_data["repo_id"],
    )
    await database.execute(query)

# Lưu danh sách nhiều issue
async def save_issues(issue_list):
    for issue in issue_list:
        await save_issue(issue)

```

### backend\services\model_loader.py
```py
# KLTN04\backend\services\model_loader.py
import joblib
from pathlib import Path
from typing import Optional, Union
import logging
from functools import lru_cache
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoader:
    _instance = None
    
    def __init__(self):
        try:
            model_path = self._get_model_path()
            logger.info(f"Loading model from {model_path}")
            
            self.model_data = joblib.load(model_path)
            self.model = self.model_data['model']
            self.vectorizer = self.model_data['vectorizer']
            
            # Warm-up predict
            self._warm_up()
            logger.info("Model loaded successfully")
            
        except Exception as e:
            logger.exception("Failed to load model")
            raise

    @staticmethod
    def _get_model_path() -> Path:
        """Validate and return model path"""
        model_path = Path(__file__).parent.parent / "models" / "commit_classifier_v1.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        return model_path

    def _warm_up(self):
        """Warm-up model with sample input"""
        sample = "fix: critical security vulnerability"
        self.predict(sample)
        
    @lru_cache(maxsize=1000)
    def vectorize(self, message: str) -> np.ndarray:
        """Cache vectorized results for frequent messages"""
        return self.vectorizer.transform([message])

    def predict(self, message: str) -> int:
        """Predict if commit is critical (with input validation)"""
        if not message or not isinstance(message, str):
            raise ValueError("Input must be non-empty string")
            
        X = self.vectorize(message.strip())
        return int(self.model.predict(X)[0])

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

def predict_commit(message: str) -> dict:
    """Public API for commit prediction
    
    Returns:
        {
            "prediction": 0|1,
            "confidence": float,
            "error": str|None
        }
    """
    try:
        loader = ModelLoader.get_instance()
        proba = loader.model.predict_proba(loader.vectorize(message))[0]
        return {
            "prediction": loader.predict(message),
            "confidence": float(np.max(proba)),
            "error": None
        }
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        return {
            "prediction": -1,
            "confidence": 0.0,
            "error": str(e)
        }
```

### backend\services\report_generator.py
```py

```

### backend\services\repo_service.py
```py
# backend/services/repo_service.py
from .github_service import fetch_from_github
from db.models.repositories import repositories
from sqlalchemy import select, update
from sqlalchemy.sql import func
from db.database import database

async def get_repo_data(owner: str, repo: str):
    url = f"/repos/{owner}/{repo}"
    data = await fetch_from_github(url)

    # Optionally: lọc data bạn muốn trả về
    return {
        "name": data.get("name"),
        "full_name": data.get("full_name"),
        "description": data.get("description"),
        "owner": data.get("owner", {}).get("login"),
        "stars": data.get("stargazers_count"),
        "forks": data.get("forks_count"),
        "watchers": data.get("watchers_count"),
        "language": data.get("language"),
        "open_issues": data.get("open_issues_count"),
        "url": data.get("html_url"),
        "created_at": data.get("created_at"),
        "updated_at": data.get("updated_at"),
    }


async def get_repo_id_by_owner_and_name(owner: str, repo_name: str):
    query = select(repositories).where(
        repositories.c.owner == owner,
        repositories.c.name == repo_name
    )
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None


async def save_repository(repo_entry):
    # Kiểm tra xem repository đã tồn tại chưa
    query = select(repositories).where(repositories.c.github_id == repo_entry["github_id"])
    existing_repo = await database.fetch_one(query)

    if existing_repo:
        # Nếu repository đã tồn tại, cập nhật thông tin (nếu cần)
        update_query = (
            update(repositories)
            .where(repositories.c.github_id == repo_entry["github_id"])
            .values(
                name=repo_entry["name"],
                owner=repo_entry["owner"],
                description=repo_entry["description"],
                stars=repo_entry["stars"],
                forks=repo_entry["forks"],
                language=repo_entry["language"],
                open_issues=repo_entry["open_issues"],
                url=repo_entry["url"],
                updated_at=func.now(),
            )
        )
        await database.execute(update_query)
    else:
        # Nếu repository chưa tồn tại, chèn mới
        query = repositories.insert().values(repo_entry)
        await database.execute(query)
```

### backend\services\user_service.py
```py
from db.models.users import users
from sqlalchemy import select, insert, update, func
from db.database import database

async def get_user_id_by_github_username(username: str):
    query = select(users).where(users.c.github_username == username)
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None

async def save_user(user_data):
    # Kiểm tra xem người dùng đã tồn tại chưa
    query = select(users).where(users.c.github_id == user_data["github_id"])
    existing_user = await database.fetch_one(query)

    if existing_user:
        # Nếu đã tồn tại, cập nhật thông tin
        query = (
            update(users)
            .where(users.c.github_id == user_data["github_id"])
            .values(
                github_username=user_data["github_username"],
                email=user_data["email"],
                avatar_url=user_data["avatar_url"],
                updated_at=func.now()  # Cập nhật thời gian
            )
        )
    else:
        # Nếu chưa tồn tại, thêm mới
        query = insert(users).values(
            github_id=user_data["github_id"],
            github_username=user_data["github_username"],
            email=user_data["email"],
            avatar_url=user_data["avatar_url"]
        )

    # Thực thi truy vấn
    await database.execute(query)

```

### backend\services\__init__.py
```py

```

### backend\utils\formatter.py
```py

```

### backend\utils\scheduler.py
```py

```

### backend\utils\__init__.py
```py

```

### frontend\eslint.config.js
```js
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]

```

### frontend\vite.config.js
```js
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})

```

### frontend\src\App.jsx
```jsx
import { BrowserRouter as Router, Routes, Route, Navigate } from "react-router-dom";
import Login from "./pages/Login";
import AuthSuccess from "./pages/AuthSuccess";
import Dashboard from "./pages/Dashboard"; 
import RepoDetails from "./pages/RepoDetails";
import CommitTable from './components/commits/CommitTable';

function App() {
  return (
    <Router>
      <Routes>
        {/* ✅ Trang mặc định là Login */}
        <Route path="/" element={<Navigate to="/login" />} />

        {/* Các route chính */}
        <Route path="/login" element={<Login />} />
        <Route path="/auth-success" element={<AuthSuccess />} />
        <Route path="/dashboard" element={<Dashboard />} />
        <Route path="/repo/:owner/:repo" element={<RepoDetails />} />
        <Route path="/commits" element={<CommitTable />} />

      </Routes>
    </Router>
  );
}

export default App;

```

### frontend\src\config.js
```js

```

### frontend\src\main.jsx
```jsx
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css';
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)

```

### frontend\src\api\github.js
```js

```

### frontend\src\components\Branchs\BranchSelector.jsx
```jsx
import { useEffect, useState } from "react";
import { Select, Spin, message, Tag, Typography, Divider } from "antd";
import { GithubOutlined, BranchesOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Option } = Select;
const { Text } = Typography;

const SelectContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 8px 12px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
`;

const StyledSelect = styled(Select)`
  min-width: 240px;
  
  .ant-select-selector {
    border-radius: 6px !important;
    border: 1px solid #d9d9d9 !important;
    transition: all 0.3s !important;
    
    &:hover {
      border-color: #1890ff !important;
    }
  }
  
  .ant-select-selection-item {
    font-weight: 500;
  }
`;

const BranchTag = styled(Tag)`
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  border-radius: 6px;
  background: #f0f5ff;
  color: #1890ff;
  border: 1px solid #d6e4ff;
`;

const BranchSelector = ({ owner, repo, onBranchChange }) => {
  const [branches, setBranches] = useState([]);
  const [loading, setLoading] = useState(true);
  const [selectedBranch, setSelectedBranch] = useState(null);

  useEffect(() => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchBranches = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setBranches(response.data);
        if (response.data.length > 0) {
          setSelectedBranch(response.data[0].name);
          onBranchChange(response.data[0].name);
        }
      } catch (err) {
        console.error(err);
        message.error("Không lấy được danh sách branch");
      } finally {
        setLoading(false);
      }
    };

    fetchBranches();
  }, [owner, repo]);

  const handleChange = (value) => {
    setSelectedBranch(value);
    onBranchChange(value);
  };

  if (loading) return <Spin size="small" />;

  return (
    <div style={{ marginBottom: 16 }}>
      {/* <Divider orientation="left" style={{ fontSize: 32, color: '#666' }}>
        Chọn branch
      </Divider> */}
      
      <SelectContainer>
        <BranchTag>
          <BranchesOutlined />
          <Text strong>Branch:</Text>
        </BranchTag>
        
        <StyledSelect
          value={selectedBranch}
          onChange={handleChange}
          suffixIcon={<GithubOutlined style={{ color: '#1890ff' }} />}
          dropdownMatchSelectWidth={false}
        >
          {branches.map((branch) => (
            <Option key={branch.name} value={branch.name}>
              <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                <BranchesOutlined style={{ color: '#52c41a' }} />
                <Text strong>{branch.name}</Text>
              </div>
            </Option>
          ))}
        </StyledSelect>
      </SelectContainer>
    </div>
  );
};

export default BranchSelector;
```

### frontend\src\components\commits\AnalyzeGitHubCommits.jsx
```jsx
import { useState } from 'react';
import { Button, Badge, Popover, List, Typography, Divider, Spin, Tag, Alert, Tooltip } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled, InfoCircleOutlined } from '@ant-design/icons';
import axios from 'axios';

const { Text, Title } = Typography;

const AnalyzeGitHubCommits = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [popoverVisible, setPopoverVisible] = useState(false);

  const analyzeCommits = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      
      if (!token) {
        throw new Error('Authentication required');
      }

      const response = await axios.get(
        `http://localhost:8000/api/commits/analyze-github/${repo.owner.login}/${repo.name}`,
        {
          headers: { 
            Authorization: `Bearer ${token}`,
            Accept: "application/json"
          },
          params: { 
            per_page: 10,
            // Add cache busting to avoid stale data
            timestamp: Date.now()
          },
          timeout: 10000 // 10 second timeout
        }
      );
      
      if (!response.data) {
        throw new Error('Invalid response data');
      }

      setAnalysis(response.data);
    } catch (err) {
      let errorMessage = 'Failed to analyze commits';
      
      if (err.response) {
        if (err.response.status === 401) {
          errorMessage = 'Please login to analyze commits';
        } else if (err.response.status === 403) {
          errorMessage = 'Access to this repository is denied';
        } else if (err.response.data?.detail) {
          errorMessage = err.response.data.detail;
        }
      } else if (err.message) {
        errorMessage = err.message;
      }

      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const handlePopoverOpen = (visible) => {
    setPopoverVisible(visible);
    if (visible && !analysis && !error) {
      analyzeCommits();
    }
  };

  const getStatusColor = () => {
    if (error) return 'warning';
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (error) return 'Error';
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical` 
      : 'No Issues';
  };

  const getStatusIcon = () => {
    if (error) return <InfoCircleOutlined />;
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const renderContent = () => {
    if (loading) {
      return <Spin size="small" tip="Analyzing commits..." />;
    }

    if (error) {
      return (
        <Alert
          message="Analysis Failed"
          description={error}
          type="error"
          showIcon
        />
      );
    }

    if (!analysis) {
      return <Text type="secondary">Click to analyze commits</Text>;
    }

    return (
      <>
        <div style={{ marginBottom: 16 }}>
          <Title level={5} style={{ marginBottom: 4 }}>
            Commit Analysis Summary
          </Title>
          <Text>
            <Tag color={analysis.critical > 0 ? 'error' : 'success'}>
              {analysis.critical > 0 ? 'Needs Review' : 'All Clear'}
            </Tag>
            {analysis.critical} of {analysis.total} commits are critical
          </Text>
        </div>

        <Divider style={{ margin: '12px 0' }} />

        <List
          size="small"
          dataSource={analysis.details.slice(0, 5)}
          renderItem={item => (
            <List.Item>
              <div style={{ width: '100%' }}>
                <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                  <Tag color={item.is_critical ? 'error' : 'success'}>
                    {item.is_critical ? 'CRITICAL' : 'Normal'}
                  </Tag>
                  <Tooltip title="Commit ID">
                    <Text code style={{ fontSize: 12 }}>
                      {item.id.substring(0, 7)}
                    </Text>
                  </Tooltip>
                </div>
                <Text
                  ellipsis={{ tooltip: item.message_preview }}
                  style={{ 
                    color: item.is_critical ? '#f5222d' : 'inherit',
                    marginTop: 4,
                    display: 'block'
                  }}
                >
                  {item.message_preview}
                </Text>
              </div>
            </List.Item>
          )}
        />

        {analysis.total > 5 && (
          <Text type="secondary" style={{ display: 'block', marginTop: 8 }}>
            Showing 5 of {analysis.total} commits
          </Text>
        )}
      </>
    );
  };

  return (
    <Popover 
      content={renderContent()}
      title={
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
          <span>Commit Analysis</span>
          {analysis && (
            <Badge 
              count={`${analysis.critical_percentage}%`} 
              style={{ 
                backgroundColor: analysis.critical > 0 ? '#f5222d' : '#52c41a'
              }} 
            />
          )}
        </div>
      }
      trigger="click"
      open={popoverVisible}
      onOpenChange={handlePopoverOpen}
      overlayStyle={{ width: 350 }}
      placement="bottomRight"
    >
      <Badge 
        count={analysis?.critical || 0} 
        color={getStatusColor()}
        offset={[-10, 10]}
      >
        <Button 
          type={error ? 'default' : analysis ? (analysis.critical ? 'danger' : 'success') : 'default'}
          icon={getStatusIcon()}
          loading={loading}
          onClick={(e) => e.stopPropagation()}
          style={{ 
            marginLeft: 'auto',
            fontWeight: 500,
            borderRadius: 20,
            padding: '0 16px',
            border: error ? '1px solid #faad14' : undefined
          }}
        >
          {getStatusText()}
        </Button>
      </Badge>
    </Popover>
  );
};

export default AnalyzeGitHubCommits;
```

### frontend\src\components\commits\CommitAnalysisBadge.jsx
```jsx
// components/CommitAnalysisBadge.jsx
import { Tag, Tooltip, Popover, List, Typography, Divider, Badge, Spin } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled } from '@ant-design/icons';
import { useState } from 'react';
import axios from 'axios';

const { Text } = Typography;

const CommitAnalysisBadge = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchCommitAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 5 } // Get last 5 commits for analysis
        }
      );
      
      // Analyze the commits
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  const getStatusColor = () => {
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical Commits` 
      : 'No Critical Commits';
  };

  const getStatusIcon = () => {
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const content = (
    <div style={{ maxWidth: 300 }}>
      {loading && <Spin size="small" />}
      {error && <Text type="danger">{error}</Text>}
      {analysis && (
        <>
          <Text strong>Recent Commits Analysis</Text>
          <Divider style={{ margin: '8px 0' }} />
          <List
            size="small"
            dataSource={analysis.details.slice(0, 5)}
            renderItem={item => (
              <List.Item>
                <div style={{ width: '100%' }}>
                  <div style={{ 
                    display: 'flex', 
                    justifyContent: 'space-between',
                    marginBottom: 4
                  }}>
                    <Text 
                      ellipsis 
                      style={{ 
                        maxWidth: 180,
                        color: item.is_critical ? '#f5222d' : 'inherit'
                      }}
                    >
                      {item.message_preview}
                    </Text>
                    <Tag color={item.is_critical ? 'error' : 'success'}>
                      {item.is_critical ? 'Critical' : 'Normal'}
                    </Tag>
                  </div>
                  <Text type="secondary" style={{ fontSize: 12 }}>
                    {item.id.substring(0, 7)}
                  </Text>
                </div>
              </List.Item>
            )}
          />
          <Divider style={{ margin: '8px 0' }} />
          <Text type="secondary">
            {analysis.critical} of {analysis.total} recent commits are critical
          </Text>
        </>
      )}
    </div>
  );

  return (
    <Popover 
      content={content}
      title="Commit Analysis"
      trigger="click"
      onVisibleChange={visible => visible && !analysis && fetchCommitAnalysis()}
    >
      <Badge 
        count={analysis?.critical || 0} 
        style={{ backgroundColor: getStatusColor() }}
      >
        <Tag 
          icon={getStatusIcon()}
          color={getStatusColor()}
          style={{ cursor: 'pointer' }}
        >
          {getStatusText()}
        </Tag>
      </Badge>
    </Popover>
  );
};

export default CommitAnalysisBadge;
```

### frontend\src\components\commits\CommitAnalysisModal.jsx
```jsx
// components/CommitAnalysisModal.jsx
import { Modal, List, Typography, Tag, Divider, Spin, Tabs, Progress, Alert } from 'antd';
import { 
  ExclamationCircleOutlined, 
  CheckCircleOutlined,
  BarChartOutlined,
  FileTextOutlined 
} from '@ant-design/icons';
import axios from 'axios';
import { useState, useEffect } from 'react';

const { Title, Text } = Typography;
const { TabPane } = Tabs;

const CommitAnalysisModal = ({ repo, visible, onCancel }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchFullAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 100 } // Get more commits for detailed analysis
        }
      );
      
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    if (visible) {
      fetchFullAnalysis();
    }
  }, [visible]);

  const criticalPercentage = analysis 
    ? Math.round((analysis.critical / analysis.total) * 100) 
    : 0;

  return (
    <Modal
      title={<><BarChartOutlined /> Commit Analysis for {repo.name}</>}
      visible={visible}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {loading && <Spin size="large" style={{ display: 'block', margin: '40px auto' }} />}
      
      {error && (
        <Alert 
          message="Error" 
          description={error} 
          type="error" 
          showIcon 
          style={{ marginBottom: 20 }}
        />
      )}
      
      {analysis && (
        <Tabs defaultActiveKey="1">
          <TabPane tab={<><FileTextOutlined /> Commits</>} key="1">
            <div style={{ marginBottom: 20 }}>
              <div style={{ display: 'flex', alignItems: 'center', marginBottom: 16 }}>
                <Progress
                  type="circle"
                  percent={criticalPercentage}
                  width={80}
                  format={percent => (
                    <Text strong style={{ fontSize: 24, color: percent > 0 ? '#f5222d' : '#52c41a' }}>
                      {percent}%
                    </Text>
                  )}
                  status={criticalPercentage > 0 ? 'exception' : 'success'}
                />
                <div style={{ marginLeft: 20 }}>
                  <Title level={4} style={{ marginBottom: 0 }}>
                    {analysis.critical} of {analysis.total} commits are critical
                  </Title>
                  <Text type="secondary">
                    {criticalPercentage > 0 
                      ? 'This repository contains potentially critical changes'
                      : 'No critical commits detected'}
                  </Text>
                </div>
              </div>
              
              <List
                size="large"
                dataSource={analysis.details}
                renderItem={item => (
                  <List.Item>
                    <div style={{ width: '100%' }}>
                      <div style={{ display: 'flex', justifyContent: 'space-between' }}>
                        <Tag color={item.is_critical ? 'error' : 'success'}>
                          {item.is_critical ? 'CRITICAL' : 'Normal'}
                        </Tag>
                        <Text type="secondary" copyable>
                          {item.id.substring(0, 7)}
                        </Text>
                      </div>
                      <Divider style={{ margin: '8px 0' }} />
                      <Text style={{ color: item.is_critical ? '#f5222d' : 'inherit' }}>
                        {item.message_preview}
                      </Text>
                    </div>
                  </List.Item>
                )}
              />
            </div>
          </TabPane>
          
          <TabPane tab={<><ExclamationCircleOutlined /> Critical Commits</>} key="2">
            {analysis.critical > 0 ? (
              <List
                dataSource={analysis.details.filter(c => c.is_critical)}
                renderItem={item => (
                  <List.Item>
                    <Alert
                      message="Critical Commit"
                      description={
                        <>
                          <Text strong style={{ display: 'block', marginBottom: 4 }}>
                            {item.message_preview}
                          </Text>
                          <Text type="secondary">Commit ID: {item.id.substring(0, 7)}</Text>
                        </>
                      }
                      type="error"
                      showIcon
                    />
                  </List.Item>
                )}
              />
            ) : (
              <div style={{ textAlign: 'center', padding: '40px 0' }}>
                <CheckCircleOutlined style={{ fontSize: 48, color: '#52c41a', marginBottom: 20 }} />
                <Title level={4} style={{ color: '#52c41a' }}>
                  No Critical Commits Found
                </Title>
                <Text type="secondary">
                  All analyzed commits appear to be normal changes
                </Text>
              </div>
            )}
          </TabPane>
        </Tabs>
      )}
    </Modal>
  );
};

export default CommitAnalysisModal;
```

### frontend\src\components\commits\CommitList.jsx
```jsx
import { useEffect, useState } from "react";
import { List, Avatar, Typography, Spin, message, Tooltip, Card, Tag, Pagination } from "antd";
import { GithubOutlined, BranchesOutlined, ClockCircleOutlined, UserOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Title, Text } = Typography;

const CommitCard = styled(Card)`
  margin-bottom: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
  }
`;

const CommitHeader = styled.div`
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
`;

const CommitMessage = styled.div`
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  font-weight: 500;
  
  &:hover {
    white-space: normal;
    overflow: visible;
  }
`;

const CommitMeta = styled.div`
  display: flex;
  align-items: center;
  gap: 12px;
  margin-top: 8px;
  color: #666;
  font-size: 13px;
`;

const PaginationContainer = styled.div`
  display: flex;
  justify-content: center;
  margin-top: 20px;
`;

const CommitList = ({ owner, repo, branch }) => {
  const [commits, setCommits] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const pageSize = 5;

  useEffect(() => {
    if (!branch) return;

    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchCommits = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/commits?branch=${branch}`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setCommits(response.data);
      } catch (err) {
        console.error(err);
        message.error("Lỗi khi lấy danh sách commit");
      } finally {
        setLoading(false);
      }
    };

    setLoading(true);
    fetchCommits();
  }, [owner, repo, branch]);

  const formatDate = (dateString) => {
    const options = { year: 'numeric', month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' };
    return new Date(dateString).toLocaleDateString('vi-VN', options);
  };

  // Tính toán dữ liệu hiển thị theo trang hiện tại
  const paginatedCommits = commits.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );

  if (loading) return <Spin tip="Đang tải commit..." size="large" />;

  return (
    <div style={{ padding: '16px' }}>
      <div style={{ display: 'flex', alignItems: 'center', marginBottom: '20px' }}>
        <Title level={4} style={{ margin: 0 }}>
          <BranchesOutlined style={{ marginRight: '8px', color: '#1890ff' }} />
          Commit trên branch: <Tag color="blue">{branch}</Tag>
          <Tag style={{ marginLeft: '8px' }}>{commits.length} commits</Tag>
        </Title>
      </div>
      
      <List
        itemLayout="vertical"
        dataSource={paginatedCommits}
        renderItem={(item) => (
          <List.Item>
            <CommitCard>
              <CommitHeader>
                <Tooltip title={item.sha} placement="topLeft">
                  <Tag icon={<GithubOutlined />} color="default">
                    {item.sha.substring(0, 7)}
                  </Tag>
                </Tooltip>
              </CommitHeader>
              
              <CommitMessage>
                <Tooltip title={item.commit.message} placement="topLeft">
                  {item.commit.message.split('\n')[0]}
                </Tooltip>
              </CommitMessage>
              
              <CommitMeta>
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <Avatar 
                    src={item.author?.avatar_url} 
                    size="small" 
                    icon={<UserOutlined />}
                    style={{ marginRight: '8px' }}
                  />
                  <Text>{item.commit.author.name}</Text>
                </div>
                
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <ClockCircleOutlined style={{ marginRight: '4px' }} />
                  <Text>{formatDate(item.commit.author.date)}</Text>
                </div>
              </CommitMeta>
            </CommitCard>
          </List.Item>
        )}
      />

      <PaginationContainer>
        <Pagination
          current={currentPage}
          pageSize={pageSize}
          total={commits.length}
          onChange={(page) => setCurrentPage(page)}
          showSizeChanger={false}
          showQuickJumper
          style={{ marginTop: '20px' }}
        />
      </PaginationContainer>
    </div>
  );
};

export default CommitList;
```

### frontend\src\components\commits\CommitTable.jsx
```jsx
//frontend\src\components\commits\CommitTable.jsxCommitTable.jsx

import { useEffect, useState } from 'react';
import { Table } from 'antd';
import axios from 'axios';

const CommitTable = () => {
  const [commits, setCommits] = useState([]);

  useEffect(() => {
    const fetchCommits = async () => {
      try {
        const response = await axios.get('http://localhost:8000/commits');
        setCommits(response.data);
      } catch (error) {
        console.error('Failed to fetch commits:', error);
      }
    };
    fetchCommits();
  }, []);

  const columns = [
    {
      title: 'ID',
      dataIndex: 'id',
    },
    {
      title: 'Repo ID',
      dataIndex: 'repo_id',
    },
    {
      title: 'User ID',
      dataIndex: 'user_id',
    },
    {
      title: 'Message',
      dataIndex: 'message',
    },
    {
      title: 'Hash',
      dataIndex: 'commit_hash',
    },
    {
      title: 'Date',
      dataIndex: 'commit_date',
    },
  ];

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-4">Lịch sử Commit</h2>
      <Table columns={columns} dataSource={commits} rowKey="id" />
    </div>
  );
};

export default CommitTable;
```

### frontend\src\components\Dashboard\AIInsightWidget.jsx
```jsx
import React from 'react';
import { Card, Space, Typography, Button, Tag } from 'antd';
import { BulbOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Title, Text } = Typography;

// Styled components
const InsightContainer = styled(Card)`
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  background: #ffffff;
  transition: all 0.3s ease;

  &:hover {
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.12);
    transform: translateY(-2px);
  }
`;

const InsightCard = styled(Card)`
  border-radius: 8px;
  border: 1px solid ${(props) => props.borderColor || '#f0f0f0'};
  background: #fff;
  transition: all 0.3s ease;
  padding: 12px;

  &:hover {
    border-color: ${(props) => props.borderColor || '#d9d9d9'};
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  }

  @media (max-width: 576px) {
    padding: 8px;
  }
`;

const IconWrapper = styled.div`
  display: flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background: ${(props) => props.bgColor || '#f0f0f0'};
`;

const ActionWrapper = styled.div`
  display: flex;
  justify-content: flex-end;
  gap: 8px;

  @media (max-width: 576px) {
    justify-content: flex-start;
    margin-top: 8px;
  }
`;

const AIInsightWidget = () => {
  const insights = [
    {
      id: 1,
      type: 'suggestion',
      title: 'Phân công đề xuất',
      description: 'Thêm 2 developer vào repo "frontend" để đảm bảo deadline 25/04/2025.',
    },
    {
      id: 2,
      type: 'warning',
      title: 'Dự đoán tiến độ',
      description: 'Repo "backend" có nguy cơ trễ hạn 3 ngày. Xem xét tăng tài nguyên.',
    },
  ];

  const getInsightStyle = (type) => {
    switch (type) {
      case 'suggestion':
        return {
          icon: <BulbOutlined style={{ fontSize: 20, color: '#1890ff' }} />,
          tag: <Tag color="blue">Đề xuất</Tag>,
          borderColor: '#e6f7ff',
          iconBg: '#e6f7ff',
        };
      case 'warning':
        return {
          icon: <WarningOutlined style={{ fontSize: 20, color: '#fa8c16' }} />,
          tag: <Tag color="orange">Cảnh báo</Tag>,
          borderColor: '#fff7e6',
          iconBg: '#fff7e6',
        };
      default:
        return {
          icon: null,
          tag: null,
          borderColor: '#f0f0f0',
          iconBg: '#f0f0f0',
        };
    }
  };

  return (
    <InsightContainer
      title={<Title level={4} style={{ margin: 0 }}>Gợi ý AI</Title>}
      bordered={false}
    >
      <Space direction="vertical" size="middle" style={{ width: '100%' }}>
        {insights.map((item) => {
          const { icon, tag, borderColor, iconBg } = getInsightStyle(item.type);
          return (
            <InsightCard key={item.id} borderColor={borderColor}>
              <Space direction="horizontal" size="middle" style={{ width: '100%', alignItems: 'center' }}>
                <IconWrapper bgColor={iconBg}>{icon}</IconWrapper>
                <Space direction="vertical" size={4} style={{ flex: 1 }}>
                  <Space>
                    <Title level={5} style={{ margin: 0 }}>{item.title}</Title>
                    {tag}
                  </Space>
                  <Text type="secondary">{item.description}</Text>
                </Space>
                <ActionWrapper>
                  <Button type="primary" size="small">Thực hiện</Button>
                  <Button size="small">Bỏ qua</Button>
                </ActionWrapper>
              </Space>
            </InsightCard>
          );
        })}
      </Space>
    </InsightContainer>
  );
};

export default AIInsightWidget;
```

### frontend\src\components\Dashboard\OverviewCard.jsx
```jsx
import React from 'react';
import { Card, Row, Col, Statistic } from 'antd';
import { ProjectOutlined, CheckCircleOutlined, WarningOutlined } from '@ant-design/icons';

const OverviewCard = ({ projects = 10, completedTasks = 50, overdueTasks = 5 }) => {
  return (
    <Card title="Tổng quan dự án" bordered={false}>
      <Row gutter={16}>
        <Col span={8}>
          <Statistic
            title="Số dự án"
            value={projects}
            prefix={<ProjectOutlined />}
            valueStyle={{ color: '#1890ff' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="Công việc hoàn thành"
            value={completedTasks}
            prefix={<CheckCircleOutlined />}
            valueStyle={{ color: '#52c41a' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="Công việc trễ hạn"
            value={overdueTasks}
            prefix={<WarningOutlined />}
            valueStyle={{ color: '#ff4d4f' }}
          />
        </Col>
      </Row>
    </Card>
  );
};

export default OverviewCard;
```

### frontend\src\components\Dashboard\RepoListFilter.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col, Input, Select, Button } from 'antd';
import { SearchOutlined } from '@ant-design/icons';

const { Option } = Select;

const RepoListFilter = ({ onFilterChange }) => {
  const [searchText, setSearchText] = useState('');
  const [status, setStatus] = useState('all');
  const [assignee, setAssignee] = useState('all');

  const handleApplyFilter = () => {
    onFilterChange({ searchText, status, assignee });
  };

  return (
    <Card title="Bộ lọc Repository" bordered={false}>
      <Row gutter={16}>
        <Col span={8}>
          <Input
            placeholder="Tìm kiếm repo"
            prefix={<SearchOutlined />}
            value={searchText}
            onChange={(e) => setSearchText(e.target.value)}
          />
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={status}
            onChange={(value) => setStatus(value)}
            placeholder="Trạng thái"
          >
            <Option value="all">Tất cả</Option>
            <Option value="active">Đang hoạt động</Option>
            <Option value="archived">Đã lưu trữ</Option>
          </Select>
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={assignee}
            onChange={(value) => setAssignee(value)}
            placeholder="Người phụ trách"
          >
            <Option value="all">Tất cả</Option>
            <Option value="user1">User 1</Option>
            <Option value="user2">User 2</Option>
          </Select>
        </Col>
        <Col span={4}>
          <Button type="primary" onClick={handleApplyFilter}>
            Áp dụng
          </Button>
        </Col>
      </Row>
    </Card>
  );
};

export default RepoListFilter;
```

### frontend\src\components\Dashboard\TaskBoard.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col } from 'antd';
import { DndContext, closestCenter } from '@dnd-kit/core';
import { SortableContext, useSortable, arrayMove } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { Task } from '../../utils/types';

const SortableTask = ({ task }) => {
  const { attributes, listeners, setNodeRef, transform, transition } = useSortable({ id: task.id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    marginBottom: 8,
  };

  return (
    <Card ref={setNodeRef} style={style} {...attributes} {...listeners}>
      <p>{task.title}</p>
      <p>Người phụ trách: {task.assignee}</p>
    </Card>
  );
};

const TaskBoard = ({ initialTasks = [] }) => {
  const [tasks, setTasks] = useState(initialTasks);

  const onDragEnd = (event) => {
    const { active, over } = event;
    if (active.id !== over.id) {
      setTasks((items) => {
        const oldIndex = items.findIndex((item) => item.id === active.id);
        const newIndex = items.findIndex((item) => item.id === over.id);
        return arrayMove(items, oldIndex, newIndex);
      });
    }
  };

  const columns = {
    todo: { title: 'Chờ xử lý', tasks: tasks.filter((task) => task.status === 'todo') },
    inProgress: { title: 'Đang thực hiện', tasks: tasks.filter((task) => task.status === 'inProgress') },
    done: { title: 'Hoàn thành', tasks: tasks.filter((task) => task.status === 'done') },
  };

  return (
    <Card title="Bảng công việc" bordered={false}>
      <DndContext collisionDetection={closestCenter} onDragEnd={onDragEnd}>
        <Row gutter={16}>
          {Object.keys(columns).map((columnId) => (
            <Col span={8} key={columnId}>
              <Card title={columns[columnId].title} bordered={false}>
                <SortableContext items={columns[columnId].tasks.map((task) => task.id)}>
                  {columns[columnId].tasks.map((task) => (
                    <SortableTask key={task.id} task={task} />
                  ))}
                </SortableContext>
              </Card>
            </Col>
          ))}
        </Row>
      </DndContext>
    </Card>
  );
};

export default TaskBoard;
```

### frontend\src\components\repo\RepoList.jsx
```jsx
import { useEffect, useState } from "react";
import { Avatar, Typography, Spin, message, Card, Tag, Pagination } from "antd";
import { useNavigate } from "react-router-dom";
import { GithubOutlined, StarFilled, EyeFilled, ForkOutlined, CalendarOutlined } from "@ant-design/icons";
import styled from "styled-components";
import axios from "axios";

const { Title, Text } = Typography;

const RepoContainer = styled.div`
  max-width: 900px;
  margin: 0 auto;
  padding: 24px;
`;

const RepoCard = styled(Card)`
  margin-bottom: 20px;
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
  cursor: pointer;
  border: none;
  
  &:hover {
    box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    transform: translateY(-5px);
  }
`;

const RepoHeader = styled.div`
  display: flex;
  align-items: flex-start;
  margin-bottom: 12px;
`;

const RepoTitle = styled.div`
  flex: 1;
  min-width: 0;
`;

const RepoName = styled(Text)`
  display: block;
  font-size: 18px;
  font-weight: 600;
  color: #24292e;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
`;

const RepoDescription = styled(Text)`
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
  color: #586069;
  margin: 8px 0;
`;

const RepoMeta = styled.div`
  display: flex;
  flex-wrap: wrap;
  gap: 16px;
  margin-top: 16px;
  align-items: center;
`;

const MetaItem = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 14px;
  color: #586069;
`;

const StyledPagination = styled(Pagination)`
  margin-top: 32px;
  text-align: center;
  
  .ant-pagination-item-active {
    border-color: #1890ff;
    background: #1890ff;
    
    a {
      color: white;
    }
  }
`;

const HighlightTag = styled(Tag)`
  font-weight: 500;
  border-radius: 12px;
  padding: 0 10px;
`;

const RepoList = () => {
  const [repos, setRepos] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const [totalRepos, setTotalRepos] = useState(0);
  const navigate = useNavigate();
  const pageSize = 8;

  useEffect(() => {
    const fetchRepos = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) return message.error("Vui lòng đăng nhập lại!");

      try {
        setLoading(true);
        const response = await axios.get("http://localhost:8000/api/github/repos", {
          headers: { Authorization: `token ${token}` },
          params: { sort: 'updated', direction: 'desc' } // Sắp xếp theo mới nhất
        });
        
        // Sắp xếp lại để đảm bảo mới nhất lên đầu
        const sortedRepos = response.data.sort((a, b) => 
          new Date(b.updated_at) - new Date(a.updated_at)
        );
        
        setRepos(sortedRepos);
        setTotalRepos(sortedRepos.length);
      } catch (error) {
        message.error("Không thể tải danh sách repository!");
        console.error(error);
      } finally {
        setLoading(false);
      }
    };

    fetchRepos();
  }, []);

  const formatDate = (dateString) => {
    return new Date(dateString).toLocaleDateString('vi-VN', {
      day: '2-digit',
      month: '2-digit',
      year: 'numeric'
    });
  };

  const paginatedRepos = repos.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );

  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', marginTop: '100px' }}>
        <Spin tip="Đang tải dữ liệu..." size="large" />
      </div>
    );
  }

  return (
    <RepoContainer>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '24px' }}>
        <Title level={2} style={{ margin: 0, color: '#24292e' }}>
          <GithubOutlined style={{ marginRight: '12px', color: '#1890ff' }} />
          GitHub Repositories
        </Title>
        <Text strong style={{ fontSize: '16px' }}>
          Tổng cộng: {totalRepos} repositories
        </Text>
      </div>

      {paginatedRepos.map((repo) => (
        <RepoCard 
          key={repo.id} 
          onClick={() => navigate(`/repo/${repo.owner.login}/${repo.name}`)}
        >
          <RepoHeader>
            <Avatar 
              src={repo.owner.avatar_url} 
              size={48}
              style={{ marginRight: '16px', flexShrink: 0 }}
            />
            <RepoTitle>
              <div style={{ display: 'flex', alignItems: 'center' }}>
                <RepoName>{repo.name}</RepoName>
                {repo.private ? (
                  <HighlightTag color="error" style={{ marginLeft: '12px' }}>
                    Private
                  </HighlightTag>
                ) : (
                  <HighlightTag color="success" style={{ marginLeft: '12px' }}>
                    Public
                  </HighlightTag>
                )}
              </div>
              
              <RepoDescription type="secondary">
                {repo.description || "Không có mô tả"}
              </RepoDescription>
            </RepoTitle>
          </RepoHeader>

          <RepoMeta>
            <MetaItem>
              <StarFilled style={{ color: '#ffc53d' }} />
              <Text strong>{repo.stargazers_count}</Text>
              <Text>stars</Text>
            </MetaItem>
            
            <MetaItem>
              <EyeFilled style={{ color: '#1890ff' }} />
              <Text strong>{repo.watchers_count}</Text>
              <Text>watchers</Text>
            </MetaItem>
            
            <MetaItem>
              <ForkOutlined style={{ color: '#73d13d' }} />
              <Text strong>{repo.forks_count}</Text>
              <Text>forks</Text>
            </MetaItem>
            
            {repo.language && (
              <MetaItem>
                <div style={{
                  width: 12,
                  height: 12,
                  borderRadius: '50%',
                  backgroundColor: '#1890ff',
                  marginRight: 6
                }} />
                <Text>{repo.language}</Text>
              </MetaItem>
            )}
            
            <MetaItem style={{ marginLeft: 'auto' }}>
              <CalendarOutlined />
              <Text>Cập nhật: {formatDate(repo.updated_at)}</Text>
            </MetaItem>
          </RepoMeta>
        </RepoCard>
      ))}

      <StyledPagination
        current={currentPage}
        pageSize={pageSize}
        total={totalRepos}
        onChange={(page) => setCurrentPage(page)}
        showSizeChanger={false}
        showQuickJumper
      />
    </RepoContainer>
  );
};

export default RepoList;
```

### frontend\src\features\github\GithubRepoFetcher.jsx
```jsx

```

### frontend\src\pages\AuthSuccess.jsx
```jsx
// src/pages/AuthSuccess.jsx
import React, { useEffect } from "react";
import { useNavigate, useLocation } from "react-router-dom";
import { message } from "antd";
import axios from "axios";

const AuthSuccess = () => {
  const navigate = useNavigate();
  const location = useLocation();

  useEffect(() => {
    const params = new URLSearchParams(location.search);
    const token = params.get("token");
    const username = params.get("username");
    const email = params.get("email");

    if (token) {
      const profile = {
        token,
        username,
        email,
        avatar_url: params.get("avatar_url"),
      };

      localStorage.setItem("github_profile", JSON.stringify(profile));
      localStorage.setItem("access_token", token);

      const syncAllRepositories = async () => {
        try {
          const response = await axios.get("http://localhost:8000/api/github/repos", {
            headers: {
              Authorization: `token ${token}`,
            },
          });

          const repositories = response.data;
          for (const repo of repositories) {
            await axios.post(
              `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
              {},
              {
                headers: {
                  Authorization: `token ${token}`,
                },
              }
            );
          }

          message.success("Đồng bộ dữ liệu thành công!");
        } catch (error) {
          console.error("Lỗi khi đồng bộ repository:", error);
          message.error("Không thể đồng bộ repository!");
        }
      };

      syncAllRepositories();
      navigate("/dashboard");
    } else {
      navigate("/login");
    }
  }, [location, navigate]);

  return (
    <div className="h-screen flex items-center justify-center">
      <p className="text-xl">Đang đồng bộ dữ liệu...</p>
    </div>
  );
};

export default AuthSuccess;
```

### frontend\src\pages\Dashboard.jsx
```jsx
import React, { useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button, Typography, Avatar, Card, Grid, Space, Divider, Badge, message, Spin } from 'antd';
import { LogoutOutlined, GithubOutlined, NotificationOutlined } from '@ant-design/icons';
import styled from 'styled-components';
import RepoList from '../components/repo/RepoList';
import OverviewCard from '../components/Dashboard/OverviewCard';
import AIInsightWidget from '../components/Dashboard/AIInsightWidget';
import RepoListFilter from '../components/Dashboard/RepoListFilter';
import TaskBoard from '../components/Dashboard/TaskBoard';
import axios from 'axios';

const { Title, Text } = Typography;
const { useBreakpoint } = Grid;

// Styled components với theme hiện đại
const DashboardContainer = styled.div`
  padding: 24px;
  max-width: 1440px;
  margin: 0 auto;
  background: #f8fafc;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  gap: 24px;

  @media (max-width: 768px) {
    padding: 16px;
    gap: 16px;
  }
`;

const HeaderCard = styled(Card)`
  border-radius: 16px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  
  .ant-card-body {
    padding: 24px;
  }
`;

const DashboardCard = styled(Card)`
  border-radius: 16px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  transition: all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);
  
  &:hover {
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
    transform: translateY(-2px);
  }

  .ant-card-head {
    border-bottom: 1px solid #f1f5f9;
    padding: 16px 24px;
  }

  .ant-card-body {
    padding: 24px;
  }

  @media (max-width: 768px) {
    .ant-card-body {
      padding: 16px;
    }
  }
`;

const PrimaryButton = styled(Button)`
  border-radius: 8px;
  font-weight: 500;
  height: 40px;
  padding: 0 20px;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const UserInfoContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 16px;
`;

const UserAvatar = styled(Avatar)`
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  border: 2px solid #ffffff;
`;

const WidgetsRow = styled.div`
  display: grid;
  grid-template-columns: 1.5fr 1fr;
  gap: 24px;

  @media (max-width: 992px) {
    grid-template-columns: 1fr;
  }
`;

const ContentSection = styled.section`
  display: flex;
  flex-direction: column;
  gap: 24px;
`;

const SectionTitle = styled(Title)`
  margin-bottom: 0 !important;
  font-weight: 600 !important;
  color: #1e293b !important;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const NotificationBadge = styled(Badge)`
  .ant-badge-count {
    background: #3b82f6;
    box-shadow: 0 0 0 1px #fff;
  }
`;

const Dashboard = () => {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(false);
  const navigate = useNavigate();
  const screens = useBreakpoint();

  const syncAllRepositories = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) {
      message.error('Vui lòng đăng nhập lại!');
      return;
    }

    try {
      setLoading(true);
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: {
          Authorization: `token ${token}`,
        },
      });

      const repositories = response.data;
      for (const repo of repositories) {
        await axios.post(
          `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
          {},
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
      }

      message.success('Đồng bộ tất cả repository thành công!');
    } catch (error) {
      console.error('Lỗi khi đồng bộ repository:', error);
      message.error('Không thể đồng bộ repository!');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    const storedProfile = localStorage.getItem('github_profile');
    if (!storedProfile) {
      navigate('/login');
    } else {
      setUser(JSON.parse(storedProfile));
    }
    syncAllRepositories();
  }, [navigate]);

  const handleLogout = () => {
    localStorage.removeItem('github_profile');
    localStorage.removeItem('access_token');
    navigate('/login');
  };

  const handleFilterChange = (filters) => {
    console.log('Applied filters:', filters);
  };

  const handleStatusChange = (taskId, newStatus) => {
    console.log(`Updated task ${taskId} status to ${newStatus}`);
  };

  if (loading) {
    return <Spin tip="Đang đồng bộ dữ liệu..." size="large" />;
  }

  return (
    <DashboardContainer>
      {/* Header Section */}
      <HeaderCard bordered={false}>
        <Space 
          direction={screens.md ? 'horizontal' : 'vertical'} 
          align={screens.md ? 'center' : 'start'}
          style={{ width: '100%', justifyContent: 'space-between' }}
        >
          <UserInfoContainer>
            <UserAvatar src={user?.avatar_url} size={screens.md ? 72 : 56} />
            <div>
              <Title level={4} style={{ margin: 0, color: '#1e293b' }}>
                Welcome back, {user?.username || 'User'}!
              </Title>
              <Text type="secondary" style={{ color: '#64748b' }}>
                {user?.email || 'No email provided'}
              </Text>
            </div>
          </UserInfoContainer>
          
          <Space size={screens.md ? 16 : 8}>
            <NotificationBadge count={3} size="small">
              <Button 
                icon={<NotificationOutlined />} 
                shape="circle" 
                style={{ border: 'none' }}
              />
            </NotificationBadge>
            <PrimaryButton 
              type="primary" 
              danger 
              onClick={handleLogout}
              icon={<LogoutOutlined />}
            >
              {screens.md ? 'Log Out' : ''}
            </PrimaryButton>
          </Space>
        </Space>
      </HeaderCard>

      {/* Overview Metrics */}
      <DashboardCard bodyStyle={{ padding: '16px' }}>
        <OverviewCard />
      </DashboardCard>

      {/* AI Insights and Filters */}
      <WidgetsRow>
        <DashboardCard 
          title={
            <SectionTitle level={5}>
              <GithubOutlined />
              Repository Analysis
            </SectionTitle>
          }
        >
          <AIInsightWidget />
        </DashboardCard>
        
        <DashboardCard 
          title={<SectionTitle level={5}>Filters & Settings</SectionTitle>}
        >
          <RepoListFilter onFilterChange={handleFilterChange} />
        </DashboardCard>
      </WidgetsRow>

      {/* Main Content Sections */}
      <ContentSection>
        <DashboardCard 
          title={
            <SectionTitle level={5}>
              My Repositories
              <Text type="secondary" style={{ fontSize: 14, marginLeft: 8 }}>
                (24 repositories)
              </Text>
            </SectionTitle>
          }
        >
          <RepoList />
        </DashboardCard>

        <DashboardCard 
          title={<SectionTitle level={5}>Project Tasks</SectionTitle>}
        >
          <TaskBoard onStatusChange={handleStatusChange} />
        </DashboardCard>
      </ContentSection>
    </DashboardContainer>
  );
};

export default Dashboard;
```

### frontend\src\pages\Login.jsx
```jsx
// src/pages/Login.jsx
import React from "react";
import { Button, Card, Typography } from "antd";
import { GithubOutlined } from "@ant-design/icons";

const { Title } = Typography;

const Login = () => {
  const handleGitHubLogin = () => {
    window.location.href = "http://localhost:8000/api/login"; // backend redirect to GitHub OAuth
  };

  return (
    <div className="h-screen flex items-center justify-center bg-gradient-to-br from-gray-100 to-white">
      <Card
        className="shadow-xl rounded-2xl w-full max-w-md"
        style={{ textAlign: "center", padding: "3rem 2rem" }}
      >
        <Title level={2} style={{ marginBottom: "2rem" }}>
          Đăng nhập vào <span style={{ color: "#1890ff" }}>TaskFlowAI</span>
        </Title>
        <Button
          type="primary"
          icon={<GithubOutlined />}
          size="large"
          onClick={handleGitHubLogin}
          style={{
            backgroundColor: "#000",
            borderColor: "#000",
            width: "100%",
          }}
        >
          Đăng nhập với GitHub
        </Button>
      </Card>
    </div>
  );
};

export default Login;
```

### frontend\src\pages\RepoDetails.jsx
```jsx
import { useEffect, useState } from "react";
import { useParams } from "react-router-dom";
import { message, Spin, Button } from "antd";
import BranchSelector from "../components/Branchs/BranchSelector";
import CommitList from "../components/commits/CommitList";
import axios from "axios";

const RepoDetails = () => {
  const { owner, repo } = useParams();
  const [branch, setBranch] = useState("");
  const [loading, setLoading] = useState(false);

  useEffect(() => {
    const syncAllData = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) {
        message.error("Vui lòng đăng nhập lại!");
        return;
      }

      try {
        setLoading(true);
        await axios.post(
          `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
          {},
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        message.success("Đồng bộ dữ liệu thành công!");
      } catch (error) {
        console.error("Lỗi khi đồng bộ dữ liệu:", error);
        message.error("Không thể đồng bộ dữ liệu!");
      } finally {
        setLoading(false);
      }
    };

    syncAllData();
  }, [owner, repo]);

  const saveCommits = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui lòng đăng nhập lại!");
      return;
    }

    try {
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/save-commits`,
        { branch },
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      message.success("Lưu commit thành công!");
    } catch (error) {
      console.error("Lỗi khi lưu commit:", error);
      message.error("Không thể lưu commit!");
    }
  };

  if (loading) {
    return <Spin tip="Đang đồng bộ dữ liệu..." size="large" />;
  }

  return (
    <div style={{ padding: 24 }}>
      <h2 style={{ fontWeight: "bold" }}>📁 Repository: {repo}</h2>
      <BranchSelector owner={owner} repo={repo} onBranchChange={setBranch} />
      <Button type="primary" onClick={saveCommits}>
        Lưu Commit
      </Button>
      <CommitList owner={owner} repo={repo} branch={branch} />
    </div>
  );
};

export default RepoDetails;
```

### frontend\src\utils\types.js
```js
export const Task = {
  id: '',
  title: '',
  assignee: '',
  status: '', // 'todo', 'inProgress', 'done'
};
```
