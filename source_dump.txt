# ==================================================
# Path: C:\SAN\KLTN\KLTN04
# Detected tech: javascript, python, react, rust, typescript
# ==================================================

## DIRECTORY STRUCTURE
```
KLTN04/
├── .git/
├── .venv/
├── backend/
│   ├── .venv/
│   ├── __pycache__/
│   ├── ai/
│   │   ├── __pycache__/
│   │   ├── kaggle_data/
│   │   │   └── github_commits/
│   │   │       ├── full.csv
│   │   │       └── oneline.csv
│   │   ├── models/
│   │   │   ├── __pycache__/
│   │   │   └── han_github_model/
│   │   │       └── best_model.pth
│   │   ├── multimodal_fusion/
│   │   │   ├── __pycache__/
│   │   │   ├── data/
│   │   │   │   └── synthetic_generator.py
│   │   │   ├── data_preprocessing/
│   │   │   │   ├── __pycache__/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── enhanced_text_processor.py
│   │   │   │   ├── metadata_processor.py
│   │   │   │   ├── minimal_enhanced_text_processor.py
│   │   │   │   └── text_processor.py
│   │   │   ├── evaluation/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── interpretability.py
│   │   │   │   ├── metrics_calculator.py
│   │   │   │   └── visualization.py
│   │   │   ├── losses/
│   │   │   │   ├── __init__.py
│   │   │   │   └── multi_task_losses.py
│   │   │   ├── models/
│   │   │   │   ├── __pycache__/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── baselines.py
│   │   │   │   ├── multimodal_fusion.py
│   │   │   │   └── shared_layers.py
│   │   │   ├── scripts/
│   │   │   │   ├── train_main.py
│   │   │   │   └── train_multimodal_fusion.py
│   │   │   ├── training/
│   │   │   │   ├── __pycache__/
│   │   │   │   └── multitask_trainer.py
│   │   │   └── __init__.py
│   │   ├── test_results/
│   │   │   ├── commit_analysis_report_20250607_212442.json
│   │   │   ├── commit_analysis_report_20250607_212820.json
│   │   │   └── commit_analysis_report_20250607_213720.json
│   │   ├── testmodelAi/
│   │   ├── trained_models/
│   │   │   └── multimodal_fusion_100k/
│   │   ├── training_data/
│   │   │   ├── github_commits_training_data.json
│   │   │   ├── large_dataset_processing_summary.json
│   │   │   └── sample_preview.json
│   │   ├── training_logs/
│   │   │   ├── han_github_training_20250608_011142.txt
│   │   │   └── han_github_training_20250610_204624.txt
│   │   ├── README.md
│   │   ├── __init__.py
│   │   ├── advanced_commit_analysis.py
│   │   ├── clean_github_data.py
│   │   ├── debug_classification_fixed.py
│   │   ├── debug_test.py
│   │   ├── download_github_commits.py
│   │   ├── download_kaggle_dataset.py
│   │   ├── simple_advanced_analysis.py
│   │   ├── simple_dataset_creator.py
│   │   ├── train_100k_fixed.py
│   │   ├── train_100k_multimodal_fusion.py
│   │   ├── train_enhanced_100k_fixed.py
│   │   ├── train_enhanced_100k_multimodal_fusion_final.py
│   │   └── train_han_github.py
│   ├── api/
│   │   ├── __pycache__/
│   │   ├── routes/
│   │   │   ├── __pycache__/
│   │   │   ├── __init__.py
│   │   │   ├── ai_suggestions.py
│   │   │   ├── auth.py
│   │   │   ├── commit_routes.py
│   │   │   ├── github.py
│   │   │   ├── gitlab.py
│   │   │   ├── repo.py
│   │   │   └── users.py
│   │   ├── __init__.py
│   │   └── deps.py
│   ├── core/
│   │   ├── __pycache__/
│   │   ├── config.py
│   │   ├── lifespan.py
│   │   ├── logger.py
│   │   ├── oauth.py
│   │   └── security.py
│   ├── db/
│   ├── migrations/
│   │   ├── __pycache__/
│   │   ├── versions/
│   │   │   └── __pycache__/
│   │   ├── README
│   │   ├── env.py
│   │   └── script.py.mako
│   ├── models/
│   │   ├── commit_classifier.joblib
│   │   ├── commit_classifier_v1.joblib
│   │   ├── commit_model.py
│   │   └── task_model.py
│   ├── notebooks/
│   │   └── Model_Training.ipynb
│   ├── schemas/
│   │   ├── __pycache__/
│   │   └── commit.py
│   ├── scripts/
│   │   ├── commit_analysis_system.py
│   │   └── commit_analysis_system_v1.py
│   ├── services/
│   │   ├── __pycache__/
│   │   ├── __init__.py
│   │   ├── ai_model.py
│   │   ├── ai_service.py
│   │   ├── branch_service.py
│   │   ├── commit_service.py
│   │   ├── github_service.py
│   │   ├── gitlab_service.py
│   │   ├── issue_service.py
│   │   ├── model_loader.py
│   │   ├── repo_service.py
│   │   ├── report_generator.py
│   │   └── user_service.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── formatter.py
│   │   └── scheduler.py
│   ├── .env
│   ├── __init__.py
│   ├── alembic.ini
│   ├── commit_analysis_report_20250601_020511.json
│   ├── commit_analysis_report_20250601_021017.json
│   ├── main.py
│   └── requirements.txt
├── frontend/
│   ├── node_modules/
│   ├── public/
│   │   └── vite.svg
│   ├── src/
│   │   ├── api/
│   │   │   └── github.js
│   │   ├── assets/
│   │   │   └── react.svg
│   │   ├── components/
│   │   │   ├── Branchs/
│   │   │   │   └── BranchSelector.jsx
│   │   │   ├── Dashboard/
│   │   │   │   ├── AIInsightWidget.jsx
│   │   │   │   ├── OverviewCard.jsx
│   │   │   │   ├── RepoListFilter.jsx
│   │   │   │   └── TaskBoard.jsx
│   │   │   ├── commits/
│   │   │   │   ├── AnalyzeGitHubCommits.jsx
│   │   │   │   ├── CommitAnalysisBadge.jsx
│   │   │   │   ├── CommitAnalysisModal.jsx
│   │   │   │   ├── CommitList.jsx
│   │   │   │   └── CommitTable.jsx
│   │   │   └── repo/
│   │   │       └── RepoList.jsx
│   │   ├── features/
│   │   │   └── github/
│   │   │       └── GithubRepoFetcher.jsx
│   │   ├── pages/
│   │   │   ├── AuthSuccess.jsx
│   │   │   ├── Dashboard.jsx
│   │   │   ├── Login.jsx
│   │   │   └── RepoDetails.jsx
│   │   ├── utils/
│   │   │   └── types.js
│   │   ├── App.css
│   │   ├── App.jsx
│   │   ├── config.js
│   │   ├── index.css
│   │   └── main.jsx
│   ├── .gitignore
│   ├── README.md
│   ├── eslint.config.js
│   ├── index.html
│   ├── package.json
│   └── vite.config.js
├── .env
├── .gitignore
├── Backend.txt
├── Frontend.txt
├── INSTALLATION_GUIDE.md
├── README.md
├── commit_analysis_report.json
├── poetry.lock
├── pyproject.toml
└── source_dump.txt
```

## FILE CONTENTS

### backend\main.py
```py
# backend/main.py
from fastapi import FastAPI
from core.lifespan import lifespan
from core.config import setup_middlewares
from core.logger import setup_logger
from core.config import setup_middlewares, setup_routers
from services.ai_service import router as ai_router
from api.routes.commit_routes import router as commit_router

from api.routes.auth import auth_router
from api.routes.github import github_router

setup_logger()  # Bật logger trước khi chạy app


app = FastAPI(lifespan=lifespan)

setup_routers(app)

setup_middlewares(app)

# Include routers trực tiếp
app.include_router(auth_router, prefix="/api")
app.include_router(github_router, prefix="/api")
app.include_router(ai_router, prefix="/ai")
app.include_router(commit_router)

@app.get("/")
def root():
    return {"message": "TaskFlowAI backend is running "}

```

### backend\__init__.py
```py

```

### backend\ai\advanced_commit_analysis.py
```py
#!/usr/bin/env python3
"""
Advanced Commit Analyzer - Phân tích chi tiết và đưa ra khuyến nghị
"""

import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd

def load_analysis_report(report_path):
    """Load báo cáo phân tích từ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_visualizations(report_data, output_dir):
    """Tạo các biểu đồ phân tích"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Set style
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. Commit Type Distribution
    commit_types = report_data['overall_distributions']['commit_types']
    
    plt.figure(figsize=(10, 6))
    plt.subplot(2, 2, 1)
    plt.pie(commit_types.values(), labels=commit_types.keys(), autopct='%1.1f%%', startangle=90)
    plt.title('Phân bố loại commit')
    
    # 2. Author Activity Levels
    activity_levels = report_data['activity_analysis']['activity_levels']
    
    plt.subplot(2, 2, 2)
    bars = plt.bar(activity_levels.keys(), activity_levels.values())
    plt.title('Mức độ hoạt động của tác giả')
    plt.ylabel('Số lượng tác giả')
    
    # Color bars differently
    colors = ['red', 'orange', 'green', 'blue']
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    # 3. Purpose Distribution
    purposes = report_data['overall_distributions']['purposes']
    
    plt.subplot(2, 2, 3)
    plt.barh(list(purposes.keys()), list(purposes.values()))
    plt.title('Phân bố mục đích commit')
    plt.xlabel('Số lượng')
    
    # 4. Sentiment Distribution
    sentiments = report_data['overall_distributions']['sentiments']
    
    plt.subplot(2, 2, 4)
    colors_sentiment = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'urgent': 'orange'}
    sentiment_colors = [colors_sentiment.get(s, 'blue') for s in sentiments.keys()]
    plt.bar(sentiments.keys(), sentiments.values(), color=sentiment_colors)
    plt.title('Phân bố cảm xúc commit')
    plt.ylabel('Số lượng')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'commit_analysis_overview.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"📊 Biểu đồ tổng quan đã được lưu vào: {output_dir / 'commit_analysis_overview.png'}")

def analyze_author_patterns(report_data):
    """Phân tích pattern của từng tác giả"""
    print("\n" + "="*80)
    print("🔍 PHÂN TÍCH CHI TIẾT PATTERN CỦA TÁC GIẢ")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\n👤 {author_name}:")
        print(f"   📊 Tổng commits: {stats['total_commits']}")
        print(f"   📈 Mức độ hoạt động: {stats['activity_level'].upper()}")
        print(f"   🎯 Confidence trung bình: {stats['avg_confidence']:.3f}")
        
        # Phân tích commit types
        if stats['commit_types']:
            most_common_type = max(stats['commit_types'], key=stats['commit_types'].get)
            type_percentage = (stats['commit_types'][most_common_type] / stats['total_commits']) * 100
            print(f"   🏷️  Loại commit chủ yếu: {most_common_type} ({type_percentage:.1f}%)")
        
        # Phân tích purposes
        if stats['purposes']:
            most_common_purpose = max(stats['purposes'], key=stats['purposes'].get)
            purpose_percentage = (stats['purposes'][most_common_purpose] / stats['total_commits']) * 100
            print(f"   🎯 Mục đích chủ yếu: {most_common_purpose} ({purpose_percentage:.1f}%)")
        
        # Phân tích sentiment
        if stats['sentiments']:
            most_common_sentiment = max(stats['sentiments'], key=stats['sentiments'].get)
            sentiment_percentage = (stats['sentiments'][most_common_sentiment] / stats['total_commits']) * 100
            print(f"   😊 Cảm xúc chủ yếu: {most_common_sentiment} ({sentiment_percentage:.1f}%)")

def generate_recommendations(report_data):
    """Tạo khuyến nghị cho team"""
    print("\n" + "="*80)
    print("💡 KHUYẾN NGHỊ CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Khuyến nghị cho overloaded authors
    if overloaded_authors:
        print(f"\n🔥 TÌNH TRẠNG QUÁ TẢI ({len(overloaded_authors)} tác giả):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"   ⚠️  {author}: {stats['total_commits']} commits")
            print(f"      💡 Khuyến nghị: Cân nhắc phân phối công việc hoặc hỗ trợ thêm nhân lực")
            
            # Phân tích loại commit để đưa ra khuyến nghị cụ thể
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"      🐛 Nhiều fix commits ({fix_count}): Cần review code kỹ hơn hoặc tăng cường testing")
                
                feat_count = stats['commit_types'].get('feat', 0)
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"      ✨ Nhiều feature commits ({feat_count}): Tác giả có thể là key developer")
    
    # Khuyến nghị cho low activity authors
    if low_activity_authors:
        print(f"\n💤 HOẠT ĐỘNG THẤP ({len(low_activity_authors)} tác giả):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"   📉 {author}: {stats['total_commits']} commits")
            print(f"      💡 Khuyến nghị: Kiểm tra workload, cung cấp hỗ trợ hoặc training thêm")
    
    # Phân tích overall patterns
    print(f"\n📈 PHÂN TÍCH TỔNG QUAN:")
    
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = sum(commit_types.values())
    
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        print(f"   🐛 Tỷ lệ fix commits cao ({fix_percentage:.1f}%)")
        print(f"      💡 Khuyến nghị: Tăng cường code review, testing, và quality assurance")
    
    if feat_percentage < 30:
        print(f"   📦 Tỷ lệ feature commits thấp ({feat_percentage:.1f}%)")
        print(f"      💡 Khuyến nghị: Cân nhắc tăng tốc độ phát triển tính năng mới")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   😞 Tỷ lệ sentiment tiêu cực cao ({negative_percentage:.1f}%)")
        print(f"      💡 Khuyến nghị: Kiểm tra morale của team, cải thiện quy trình làm việc")
    
    if urgent_percentage > 10:
        print(f"   🚨 Tỷ lệ urgent commits cao ({urgent_percentage:.1f}%)")
        print(f"      💡 Khuyến nghị: Cải thiện planning và risk management")

def create_team_dashboard(report_data, output_dir):
    """Tạo dashboard tổng quan cho team"""
    output_dir = Path(output_dir)
    
    # Create a comprehensive team report
    dashboard_data = {
        "timestamp": datetime.now().isoformat(),
        "team_health": {
            "total_authors": len(report_data['author_statistics']),
            "total_commits": report_data['summary']['total_commits'],
            "avg_commits_per_author": report_data['summary']['avg_commits_per_author']
        },
        "risk_indicators": {
            "overloaded_authors": len(report_data['activity_analysis']['overloaded_authors']),
            "low_activity_authors": len(report_data['activity_analysis']['low_activity_authors']),
            "fix_percentage": (report_data['overall_distributions']['commit_types'].get('fix', 0) / 
                             report_data['summary']['total_commits']) * 100
        },
        "recommendations": []
    }
    
    # Add recommendations
    if dashboard_data['risk_indicators']['overloaded_authors'] > 0:
        dashboard_data['recommendations'].append({
            "type": "workload_balancing",
            "priority": "high",
            "message": f"Có {dashboard_data['risk_indicators']['overloaded_authors']} tác giả bị quá tải"
        })
    
    if dashboard_data['risk_indicators']['fix_percentage'] > 40:
        dashboard_data['recommendations'].append({
            "type": "quality_improvement",
            "priority": "medium",
            "message": f"Tỷ lệ fix commits cao ({dashboard_data['risk_indicators']['fix_percentage']:.1f}%)"
        })
    
    # Save dashboard
    dashboard_file = output_dir / 'team_dashboard.json'
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"📊 Team dashboard đã được lưu vào: {dashboard_file}")

def main():
    """Hàm chính để phân tích nâng cao"""
    print("🚀 ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("❌ Không tìm thấy thư mục test_results. Hãy chạy test_commit_analyzer.py trước.")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("❌ Không tìm thấy file báo cáo. Hãy chạy test_commit_analyzer.py trước.")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"📄 Đang phân tích: {latest_report.name}")
    
    # Load report data
    report_data = load_analysis_report(latest_report)
    
    # Create output directory for advanced analysis
    advanced_output_dir = test_results_dir / "advanced_analysis"
    advanced_output_dir.mkdir(exist_ok=True)
    
    # Perform advanced analysis
    analyze_author_patterns(report_data)
    generate_recommendations(report_data)
    
    # Create visualizations
    try:
        create_visualizations(report_data, advanced_output_dir)
    except Exception as e:
        print(f"⚠️  Không thể tạo biểu đồ: {e}")
    
    # Create team dashboard
    create_team_dashboard(report_data, advanced_output_dir)
    
    print(f"\n✅ Phân tích nâng cao hoàn thành!")
    print(f"📁 Kết quả lưu tại: {advanced_output_dir}")

if __name__ == "__main__":
    main()

```

### backend\ai\clean_github_data.py
```py
#!/usr/bin/env python3
"""
GitHub Data Processor - Download và Clean dữ liệu cho Multi-Modal Fusion Network
=============================================================================
Script này sẽ:
1. Download dataset GitHub commits từ Kaggle
2. Clean và chuẩn hóa dữ liệu 
3. Tạo labels phù hợp cho multi-task learning
4. Xuất ra format chuẩn cho training
"""

import pandas as pd
import numpy as np
import json
import re
from pathlib import Path
from datetime import datetime
from collections import Counter
import traceback
from typing import Dict, List, Tuple, Any
import random

# Import để tạo synthetic metadata
from multimodal_fusion.data.synthetic_generator import GitHubDataGenerator

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"❌ Lỗi setup Kaggle API: {e}")
        print("💡 Vui lòng chạy: pip install kaggle")
        print("💡 Hoặc setup API key theo hướng dẫn: https://github.com/Kaggle/kaggle-api")
        return None

def download_github_dataset(api, force_download=False):
    """Download dataset GitHub commits từ Kaggle"""
    try:
        # Tạo thư mục download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Kiểm tra đã download chưa
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"✅ Dataset đã tồn tại: {csv_files[0]}")
            return csv_files[0]
        
        print("📥 Đang download GitHub commit dataset từ Kaggle...")
        print("📁 Dataset: dhruvildave/github-commit-messages-dataset")
        
        # Download dataset
        api.dataset_download_files(
            "dhruvildave/github-commit-messages-dataset", 
            path=str(download_dir), 
            unzip=True
        )
        
        # Tìm file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("❌ Không tìm thấy file CSV trong dataset")
            return None
        
        csv_file = csv_files[0]
        print(f"✅ Download thành công: {csv_file}")
        print(f"📊 Kích thước file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return csv_file
        
    except Exception as e:
        print(f"❌ Lỗi download dataset: {e}")
        return None

def clean_github_data(csv_file: Path, sample_size: int = 20000) -> pd.DataFrame:
    """Clean và chuẩn hóa dữ liệu GitHub commits"""
    try:
        print(f"\n📊 CLEANING DỮ LIỆU: {csv_file.name}")
        print("="*60)
        
        # Đọc dữ liệu với chunk để tiết kiệm memory
        print("📖 Đang đọc dữ liệu...")
        
        # Đọc sample để xem cấu trúc
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"📋 Columns: {list(sample_df.columns)}")
        
        # Tìm column chứa commit message
        message_col = None
        for col in ['message', 'subject', 'commit', 'commit_message']:
            if col in sample_df.columns:
                message_col = col
                break
        
        if not message_col:
            print("❌ Không tìm thấy column chứa commit message")
            return None
        
        print(f"💬 Sử dụng column: '{message_col}' làm commit message")
        
        # Đọc dữ liệu với sampling hiệu quả
        print(f"🎯 Sampling {sample_size:,} records...")
        
        # Đọc theo chunk và sample
        chunk_size = 10000
        sampled_chunks = []
        total_read = 0
        
        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
            total_read += len(chunk)
            
            # Random sample từ chunk
            if len(chunk) > 0:
                chunk_sample_size = min(sample_size // 20, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
            
            # Dừng khi đủ data
            total_sampled = sum(len(c) for c in sampled_chunks)
            if total_sampled >= sample_size:
                break
                
            if total_read % 50000 == 0:
                print(f"  Đã đọc: {total_read:,} records...")
        
        # Combine chunks
        df = pd.concat(sampled_chunks, ignore_index=True)
        if len(df) > sample_size:
            df = df.sample(n=sample_size).reset_index(drop=True)
        
        print(f"📊 Đã sample {len(df):,} commits từ {total_read:,} total")
        
        # BƯỚC 1: Làm sạch dữ liệu cơ bản
        print(f"\n🧹 BƯỚC 1: LÀM SẠCH CƠ BẢN")
        original_count = len(df)
        
        # Loại bỏ messages rỗng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  • Sau khi loại bỏ empty: {len(df):,} (-{original_count - len(df)})")
        
        # Loại bỏ messages quá ngắn hoặc quá dài
        df = df[df[message_col].str.len().between(3, 200)]
        print(f"  • Sau khi lọc độ dài (3-200 chars): {len(df):,}")
        
        # Loại bỏ duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  • Sau khi loại bỏ duplicates: {len(df):,}")
        
        # BƯỚC 2: Clean text content
        print(f"\n🔤 BƯỚC 2: CLEAN TEXT CONTENT")
        
        def clean_commit_message(text):
            """Clean commit message text"""
            if pd.isna(text):
                return ""
                
            text = str(text).strip()
            
            # Remove excessive whitespace
            text = re.sub(r'\s+', ' ', text)
            
            # Remove URLs
            text = re.sub(r'http[s]?://\S+', '[URL]', text)
            
            # Remove email addresses
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
            
            # Remove excessive punctuation
            text = re.sub(r'[!]{2,}', '!', text)
            text = re.sub(r'[?]{2,}', '?', text)
            text = re.sub(r'[.]{3,}', '...', text)
            
            # Remove special characters but keep useful ones
            text = re.sub(r'[^\w\s\-_.,;:!?()\[\]{}#@/\\+=<>|~`]', '', text)
            
            return text.strip()
        
        df[message_col] = df[message_col].apply(clean_commit_message)
        
        # Remove messages that became empty after cleaning
        df = df[df[message_col].str.len() >= 3]
        print(f"  • Sau khi clean text: {len(df):,} commits")
        
        # BƯỚC 3: Phân loại và tạo labels
        print(f"\n🏷️  BƯỚC 3: TẠO LABELS CHO MULTI-TASK LEARNING")
        
        # Classify commit types
        df['commit_type'] = df[message_col].apply(classify_commit_type)
        df['purpose'] = df[message_col].apply(classify_purpose)
        df['sentiment'] = df[message_col].apply(classify_sentiment)
        df['tech_tag'] = df[message_col].apply(classify_tech_tag)
        
        # Thống kê labels
        print(f"\n📊 THỐNG KÊ LABELS:")
        for col in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            value_counts = df[col].value_counts()
            print(f"  {col}:")
            for val, count in value_counts.head(5).items():
                print(f"    {val}: {count} ({count/len(df)*100:.1f}%)")
        
        # BƯỚC 4: Tạo metadata synthetic
        print(f"\n⚙️ BƯỚC 4: TẠO METADATA SYNTHETIC")
        generator = GitHubDataGenerator()
        def create_synthetic_metadata():
            """Tạo metadata synthetic cho mỗi commit"""
            sample = generator.generate_single_commit()
            return {
                'author': sample['author'],
                'repository': sample['repository'], 
                'timestamp': sample['timestamp'],
                'files_changed': sample['files_changed'],
                'additions': sample['additions'],
                'deletions': sample['deletions'],
                'file_types': sample['file_types']
            }
        
        # Apply synthetic metadata
        metadata_list = [create_synthetic_metadata() for _ in range(len(df))]
        
        for key in ['author', 'repository', 'timestamp', 'files_changed', 'additions', 'deletions']:
            df[f'meta_{key}'] = [meta[key] for meta in metadata_list]
        
        # File types cần xử lý đặc biệt vì là list
        df['meta_file_types'] = [meta['file_types'] for meta in metadata_list]
        
        print(f"✅ Đã tạo synthetic metadata cho {len(df):,} commits")
        
        return df
        
    except Exception as e:
        print(f"❌ Lỗi clean dữ liệu: {e}")
        traceback.print_exc()
        return None

def classify_commit_type(message: str) -> str:
    """Phân loại commit type dựa theo conventional commits"""
    message = message.lower()
    
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new|create)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch|repair)\b'],
        'docs': [r'\b(doc|documentation|readme|comment|guide)\b'],
        'style': [r'\b(style|format|lint|clean|prettier|cosmetic)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup|improve)\b'],
        'test': [r'\b(test|spec|testing|coverage|unit|integration)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge|maint)\b'],
        'perf': [r'\b(perf|performance|optimize|speed|fast|slow)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization|vulnerability)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message: str) -> str:
    """Phân loại mục đích của commit"""
    message = message.lower()
    
    if re.search(r'\b(add|new|implement|create|build|introduce)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve|repair)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve|optimize)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment|guide)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage|unit)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability|exploit)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier|cosmetic)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release|version)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message: str) -> str:
    """Phân loại sentiment của commit"""
    message = message.lower()
    
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap', 'breaking']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'crash', 'wrong']
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good', 'success']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message: str) -> str:
    """Phân loại technology tag"""
    message = message.lower()
    
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular|typescript|ts)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi|pandas|numpy)\b'],
        'java': [r'\b(java|maven|gradle|spring|junit)\b'],
        'css': [r'\b(css|scss|sass|style|styling|bootstrap)\b'],
        'html': [r'\b(html|template|markup|dom)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo|redis|sqlite)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service|http|request)\b'],
        'docker': [r'\b(docker|container|dockerfile|kubernetes|k8s)\b'],
        'git': [r'\b(git|merge|branch|commit|pull|push|clone)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration|e2e|pytest|jest)\b'],
        'security': [r'\b(security|auth|token|password|encrypt|decrypt|ssl|tls)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed|memory|cpu)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive|mobile)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def convert_to_training_format(df: pd.DataFrame, message_col: str) -> List[Dict]:
    """Convert cleaned DataFrame thành format cho training"""
    print(f"\n🔄 CONVERT SANG TRAINING FORMAT")
    
    training_samples = []
    
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"  Processed {idx}/{len(df)} samples")
            
        # Tạo sample theo format chuẩn
        sample = {
            'commit_message': row[message_col],
            'author': row['meta_author'],
            'repository': row['meta_repository'],
            'timestamp': row['meta_timestamp'],
            'files_changed': row['meta_files_changed'],
            'additions': row['meta_additions'],
            'deletions': row['meta_deletions'],
            'file_types': row['meta_file_types'],
            'labels': {
                'risk_prediction': classify_risk_level(row[message_col], row['commit_type']),
                'complexity_prediction': classify_complexity(row[message_col], row['meta_files_changed']),
                'hotspot_prediction': classify_hotspot(row['commit_type'], row['tech_tag']),
                'urgency_prediction': classify_urgency(row['sentiment'])
            }
        }
        
        training_samples.append(sample)
    
    print(f"✅ Converted {len(training_samples)} samples")
    return training_samples

def classify_risk_level(message: str, commit_type: str) -> int:
    """Classify risk level (0: low, 1: high)"""
    high_risk_patterns = ['breaking', 'major', 'critical', 'breaking change', 'api change']
    high_risk_types = ['feat', 'refactor', 'security']
    
    message_lower = message.lower()
    
    if any(pattern in message_lower for pattern in high_risk_patterns):
        return 1
    elif commit_type in high_risk_types:
        return 1
    else:
        return 0

def classify_complexity(message: str, files_changed: int) -> int:
    """Classify complexity (0: low, 1: medium, 2: high)"""
    if files_changed >= 10:
        return 2
    elif files_changed >= 5:
        return 1
    else:
        return 0

def classify_hotspot(commit_type: str, tech_tag: str) -> int:
    """Classify hotspot area (0-4)"""
    hotspot_map = {
        'security': 0,
        'api': 1,
        'database': 2,
        'ui': 3,
        'general': 4
    }
    return hotspot_map.get(tech_tag, 4)

def classify_urgency(sentiment: str) -> int:
    """Classify urgency (0: normal, 1: urgent)"""
    return 1 if sentiment == 'urgent' else 0

def save_training_data(samples: List[Dict], output_dir: Path) -> str:
    """Lưu training data đã clean"""
    output_dir.mkdir(exist_ok=True)
    
    # Tạo filename với timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"cleaned_github_commits_{timestamp}.json"
    
    # Tạo metadata
    training_data = {
        'metadata': {
            'total_samples': len(samples),
            'created_at': datetime.now().isoformat(),
            'source': 'kaggle_github_commits_cleaned',
            'version': '1.0',
            'description': 'Cleaned GitHub commit data for Multi-Modal Fusion Network'
        },
        'samples': samples
    }
    
    # Save to JSON
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"💾 Đã lưu {len(samples)} samples vào: {output_file}")
    print(f"📊 File size: {output_file.stat().st_size / (1024*1024):.1f} MB")
    
    return str(output_file)

def main():
    """Main function"""
    print("🚀 GITHUB DATA PROCESSOR - CLEAN DỮ LIỆU CHO TRAINING")
    print("="*70)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        print("\n❌ Không thể setup Kaggle API")
        print("💡 Bạn có thể manually tải file CSV và đặt vào thư mục kaggle_data/github_commits/")
        
        # Kiểm tra file manual
        manual_files = list(Path("kaggle_data/github_commits").glob("*.csv"))
        if manual_files:
            csv_file = manual_files[0]
            print(f"✅ Tìm thấy file manual: {csv_file}")
        else:
            print("❌ Không tìm thấy file CSV nào")
            return
    else:
        # Download dataset
        csv_file = download_github_dataset(api)
        if not csv_file:
            print("❌ Không thể download dataset")
            return
    
    # Hỏi user về sample size
    print(f"\n📊 TÙY CHỌN SAMPLE SIZE:")
    print("1. 5K samples (test nhanh)")
    print("2. 10K samples (demo)")
    print("3. 20K samples (khuyên dùng)")
    print("4. 50K samples (training tốt)")
    print("5. 100K samples (dataset lớn)")
    
    choice = input("Chọn option (1-5) [mặc định: 3]: ").strip() or "3"
    
    sample_sizes = {
        '1': 5000,
        '2': 10000,
        '3': 20000,
        '4': 50000,
        '5': 100000
    }
    
    sample_size = sample_sizes.get(choice, 20000)
    print(f"🎯 Sẽ sample {sample_size:,} commits")
    
    # Clean data
    df = clean_github_data(csv_file, sample_size)
    if df is None:
        print("❌ Không thể clean dữ liệu")
        return
    
    # Convert to training format
    message_col = 'message'  # hoặc tìm tự động
    for col in ['message', 'subject', 'commit', 'commit_message']:
        if col in df.columns:
            message_col = col
            break
    
    training_samples = convert_to_training_format(df, message_col)
    
    # Save cleaned data
    output_dir = Path("training_data")
    output_file = save_training_data(training_samples, output_dir)
    
    print(f"\n🎉 HOÀN THÀNH CLEANING DỮ LIỆU!")
    print(f"📁 File output: {output_file}")
    print(f"📊 Số samples: {len(training_samples):,}")
    
    print(f"\n✨ SẴN SÀNG CHO TRAINING:")
    print(f"  python train_real_data.py")
    print(f"  # Hoặc sử dụng file: {Path(output_file).name}")

if __name__ == "__main__":
    main()

```

### backend\ai\debug_classification_fixed.py
```py
#!/usr/bin/env python3
"""
Debug script để kiểm tra vấn đề phân loại commit type
"""

import torch
import os
import sys
from test_commit_analyzer import CommitAnalyzer

def test_problematic_commits():
    """Test các commit messages có vấn đề phân loại"""
    
    print("🔍 DEBUGGING COMMIT CLASSIFICATION ISSUES")
    print("="*60)
    
    # Initialize analyzer với model path
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test cases có vấn đề
    test_cases = [
        {
            "message": "docs: fix typo in configuration guide", 
            "expected": "docs",
            "author": "Test User"
        },
        {
            "message": "docs: update installation instructions",
            "expected": "docs", 
            "author": "Test User"
        },
        {
            "message": "test: add unit tests for user service",
            "expected": "test",
            "author": "Test User" 
        },
        {
            "message": "test: fix failing integration tests",
            "expected": "test",
            "author": "Test User"
        },
        {
            "message": "fix: typo in variable name",
            "expected": "fix",
            "author": "Test User"
        },
        {
            "message": "feat: add new documentation system", 
            "expected": "feat",
            "author": "Test User"
        },
        {
            "message": "chore: update dependencies",
            "expected": "chore", 
            "author": "Test User"
        },
        {
            "message": "style: fix code formatting",
            "expected": "style",
            "author": "Test User"
        }
    ]
    
    print(f"🧪 Testing {len(test_cases)} problematic commit messages...")
    print()
    
    correct_predictions = 0
    total_predictions = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        message = case["message"]
        expected = case["expected"]
        author = case["author"]
        
        print(f"{i}. Testing: '{message}'")
        print(f"   Expected: {expected}")
        
        # Analyze commit
        analysis = analyzer.predict_commit(message, author)
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        
        print(f"   Predicted: {predicted} (confidence: {confidence:.3f})")
        
        # Check if correct
        is_correct = predicted == expected
        if is_correct:
            print("   ✅ CORRECT")
            correct_predictions += 1
        else:
            print("   ❌ WRONG")
            
            # Analyze why it's wrong - show all predictions for this commit
            print("   🔍 Full predictions:")
            for task, pred in analysis.predicted_labels.items():
                conf = analysis.confidence_scores.get(task, 0.0)
                print(f"      {task}: {pred} ({conf:.3f})")
        
        print()
    
    # Summary
    accuracy = correct_predictions / total_predictions * 100
    print("="*60)
    print(f"📊 CLASSIFICATION ACCURACY: {correct_predictions}/{total_predictions} ({accuracy:.1f}%)")
    
    if accuracy < 80:
        print("🚨 LOW ACCURACY DETECTED!")
        print("💡 Possible issues:")
        print("   - Model wasn't trained properly on commit prefixes")
        print("   - Training data didn't have enough conventional commit examples")
        print("   - Model is focusing on content words rather than prefixes")
        print("   - Need to retrain with better conventional commit dataset")
    else:
        print("✅ Classification accuracy looks good!")
    
    return accuracy

def analyze_model_attention():
    """Phân tích xem model đang chú ý vào phần nào của commit message"""
    
    print("\n🧠 ANALYZING MODEL ATTENTION PATTERNS")
    print("="*60)
    
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test với các variations
    test_variations = [
        "docs: fix typo in configuration guide",
        "fix typo in configuration guide",  # Không có prefix
        "docs: update configuration guide",  # Không có từ "fix"
        "fix: typo in configuration guide"   # Prefix khác
    ]
    
    print("Testing how model responds to different parts of the message:")
    print()
    
    for i, message in enumerate(test_variations, 1):
        print(f"{i}. '{message}'")
        analysis = analyzer.predict_commit(message, "Test User")
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        print(f"   → {predicted} ({confidence:.3f})")
        print()
    
    return test_variations

if __name__ == "__main__":
    try:
        accuracy = test_problematic_commits()
        analyze_model_attention()
        
        print("\n💡 RECOMMENDATIONS:")
        print("="*60)
        
        if accuracy < 80:
            print("🔧 TO FIX CLASSIFICATION ISSUES:")
            print("1. Retrain model with more conventional commit examples")
            print("2. Add prefix-aware preprocessing")
            print("3. Use rule-based fallback for obvious prefixes")
            print("4. Create training data with equal distribution of commit types")
            print("5. Consider ensemble model (ML + rule-based)")
        
        print("\n✅ Debug completed!")
        
    except Exception as e:
        print(f"❌ Error during debugging: {e}")
        import traceback
        traceback.print_exc()

```

### backend\ai\debug_test.py
```py
#!/usr/bin/env python3
"""
Simple Commit Analyzer Test - Debug version
"""

import os
import sys
import json
import torch
from pathlib import Path

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

def test_model_loading():
    """Test basic model loading"""
    print("🚀 SIMPLE COMMIT ANALYZER TEST")
    print("="*50)
    
    # Check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"🔧 Device: {device}")
    
    # Check model file
    model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
    print(f"📦 Model path: {model_path}")
    print(f"📦 Model exists: {model_path.exists()}")
    
    if not model_path.exists():
        print("❌ Model file not found!")
        return False
    
    try:
        # Load checkpoint
        print("📥 Loading checkpoint...")
        checkpoint = torch.load(model_path, map_location=device)
        
        print("✅ Checkpoint loaded successfully!")
        print(f"   Keys: {list(checkpoint.keys())}")
        
        if 'num_classes' in checkpoint:
            print(f"   Tasks: {list(checkpoint['num_classes'].keys())}")
            print(f"   Classes per task: {checkpoint['num_classes']}")
        
        if 'val_accuracy' in checkpoint:
            print(f"   Best accuracy: {checkpoint['val_accuracy']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"❌ Error loading checkpoint: {e}")
        return False

def test_simple_prediction():
    """Test a simple prediction"""
    try:
        from train_han_github import SimpleHANModel, SimpleTokenizer
        print("\n🧪 Testing simple prediction...")
        
        # Load model components
        model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        checkpoint = torch.load(model_path, map_location=device)
        tokenizer = checkpoint['tokenizer']
        num_classes = checkpoint['num_classes']
        label_encoders = checkpoint['label_encoders']
        
        # Create reverse label encoders
        reverse_encoders = {}
        for task, encoder in label_encoders.items():
            reverse_encoders[task] = {v: k for k, v in encoder.items()}
        
        # Initialize model
        vocab_size = len(tokenizer.word_to_idx)
        model = SimpleHANModel(
            vocab_size=vocab_size,
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"✅ Model initialized with vocab size: {vocab_size}")
        
        # Test prediction
        test_text = "fix: resolve authentication bug in login endpoint"
        print(f"📝 Testing text: '{test_text}'")
        
        # Tokenize
        input_ids = tokenizer.encode_text(test_text, max_sentences=10, max_words=50)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
            
            print(f"🔍 Predictions:")
            for task, output in outputs.items():
                probs = torch.softmax(output, dim=1)
                confidence, pred_idx = torch.max(probs, 1)
                
                pred_idx = pred_idx.item()
                confidence = confidence.item()
                
                predicted_label = reverse_encoders[task][pred_idx]
                print(f"   {task}: {predicted_label} (confidence: {confidence:.3f})")
        
        return True
        
    except Exception as e:
        print(f"❌ Error in prediction test: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("Starting debug tests...\n")
    
    # Test 1: Model loading
    if test_model_loading():
        print("\n" + "="*50)
        # Test 2: Simple prediction
        test_simple_prediction()
    
    print("\n🎯 Debug tests completed!")

```

### backend\ai\download_github_commits.py
```py
"""
Download và xử lý dataset GitHub Commit Messages từ Kaggle
Dataset: mrisdal/github-commit-messages
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import re
from collections import Counter
import zipfile

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"❌ Lỗi setup Kaggle API: {e}")
        print("Vui lòng chạy: python quick_kaggle_setup.py")
        return None

def download_dataset(api, dataset_name="dhruvildave/github-commit-messages-dataset", force_download=False):
    """Download dataset từ Kaggle"""
    try:
        # Tạo thư mục download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Kiểm tra đã download chưa
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"✅ Dataset đã tồn tại trong {download_dir}")
            return download_dir, csv_files[0]
        
        print(f"📥 Đang download dataset: {dataset_name}")
        print(f"📁 Vào thư mục: {download_dir}")
        
        # Download dataset
        api.dataset_download_files(
            dataset_name, 
            path=str(download_dir), 
            unzip=True
        )
        
        # Tìm file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("❌ Không tìm thấy file CSV trong dataset")
            return None, None
        
        csv_file = csv_files[0]
        print(f"✅ Download thành công: {csv_file}")
        print(f"📊 Kích thước file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return download_dir, csv_file
        
    except Exception as e:
        print(f"❌ Lỗi download dataset: {e}")
        return None, None

def analyze_commit_data(csv_file, sample_size=None):
    """Phân tích dữ liệu commit"""
    try:
        print(f"\n📊 PHÂN TÍCH DỮ LIỆU: {csv_file.name}")
        print("="*60)
        
        # Đọc dữ liệu với chunk để tiết kiệm memory
        print("📖 Đang đọc dữ liệu...")
        
        # Đọc một phần nhỏ trước để xem cấu trúc
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"📋 Columns: {list(sample_df.columns)}")
        print(f"📏 Sample shape: {sample_df.shape}")
          # Đọc toàn bộ hoặc sample một cách hiệu quả
        if sample_size:
            print(f"📊 Sampling {sample_size:,} records...")
            # Sử dụng chunk reading để memory-efficient sampling
            chunk_size = 10000
            sampled_chunks = []
            total_read = 0
            
            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
                total_read += len(chunk)
                
                # Random sample từ chunk này
                chunk_sample_size = min(sample_size // 10, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
                
                # Dừng khi đã đủ data
                total_sampled = sum(len(c) for c in sampled_chunks)
                if total_sampled >= sample_size:
                    break
                
                if total_read % 50000 == 0:
                    print(f"  Đã đọc: {total_read:,} records...")
            
            # Combine chunks
            df = pd.concat(sampled_chunks, ignore_index=True)
            if len(df) > sample_size:
                df = df.sample(n=sample_size).reset_index(drop=True)
            
            print(f"📊 Đã sample {len(df):,} commits từ {total_read:,} total")
        else:
            print("📖 Đọc toàn bộ dataset (có thể mất thời gian)...")
            df = pd.read_csv(csv_file)
            print(f"📊 Đã đọc {len(df):,} commits")
        
        # Hiển thị thông tin cơ bản
        print(f"\n📈 THỐNG KÊ CƠ BẢN:")
        print(f"  • Tổng số commits: {len(df):,}")
        print(f"  • Columns: {df.shape[1]}")
        print(f"  • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # Kiểm tra columns quan trọng
        important_cols = ['message', 'commit', 'subject', 'body', 'author', 'repo']
        available_cols = [col for col in important_cols if col in df.columns]
        print(f"  • Available important columns: {available_cols}")
        
        # Tìm column chứa commit message
        message_col = None
        for col in ['message', 'subject', 'commit']:
            if col in df.columns:
                message_col = col
                break
        
        if not message_col:
            print("❌ Không tìm thấy column chứa commit message")
            return None
        
        print(f"  • Sử dụng column: '{message_col}' làm commit message")
        
        # Làm sạch dữ liệu
        print(f"\n🧹 LÀM SẠCH DỮ LIỆU:")
        original_count = len(df)
        
        # Loại bỏ messages rỗng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  • Sau khi loại bỏ empty: {len(df):,} (-{original_count - len(df)})")
        
        # Loại bỏ messages quá ngắn hoặc quá dài
        df = df[df[message_col].str.len().between(5, 500)]
        print(f"  • Sau khi lọc độ dài (5-500 chars): {len(df):,}")
        
        # Loại bỏ duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  • Sau khi loại bỏ duplicates: {len(df):,}")
        
        # Phân tích nội dung
        print(f"\n📝 PHÂN TÍCH NỘI DUNG:")
        messages = df[message_col].astype(str)
        
        # Thống kê độ dài
        lengths = messages.str.len()
        print(f"  • Độ dài trung bình: {lengths.mean():.1f} chars")
        print(f"  • Độ dài median: {lengths.median():.1f} chars")
        print(f"  • Min/Max: {lengths.min()}/{lengths.max()} chars")
          # Top words - sample để tránh quá tải memory
        sample_size_for_words = min(5000, len(messages))
        print(f"  • Analyzing words from {sample_size_for_words} samples...")
        
        all_words = []
        sample_messages = messages.sample(n=sample_size_for_words) if len(messages) > sample_size_for_words else messages
        
        for msg in sample_messages:
            words = re.findall(r'\b[a-zA-Z]+\b', str(msg).lower())
            all_words.extend(words)
        
        word_counts = Counter(all_words).most_common(20)
        print(f"\n🔤 TOP 20 WORDS (from {sample_size_for_words} samples):")
        for word, count in word_counts:
            print(f"    {word}: {count}")
        
        return df, message_col
        
    except Exception as e:
        print(f"❌ Lỗi phân tích dữ liệu: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def classify_commits(df, message_col):
    """Phân loại commits theo các tiêu chí"""
    print(f"\n🏷️  PHÂN LOẠI COMMITS:")
    print("="*60)
    
    messages = df[message_col].astype(str).str.lower()
    classifications = []
    
    # Process in batches để tránh memory issues
    batch_size = 1000
    total_batches = (len(messages) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(messages))
        batch_messages = messages[start_idx:end_idx]
        
        batch_classifications = []
        for message in batch_messages:
            labels = {
                'commit_type': classify_commit_type(message),
                'purpose': classify_purpose(message),
                'sentiment': classify_sentiment(message),
                'tech_tag': classify_tech_tag(message)
            }
            batch_classifications.append(labels)
        
        classifications.extend(batch_classifications)
        
        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:
            print(f"  Đã xử lý: {end_idx:,}/{len(messages):,} ({(end_idx/len(messages)*100):.1f}%)")
    
    # Thống kê phân loại
    print(f"\n📊 THỐNG KÊ PHÂN LOẠI:")
    
    for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
        values = [c[category] for c in classifications]
        counter = Counter(values)
        print(f"\n{category.upper()}:")
        for value, count in counter.most_common(10):
            percentage = (count / len(values)) * 100
            print(f"    {value}: {count:,} ({percentage:.1f}%)")
    
    return classifications

def classify_commit_type(message):
    """Phân loại commit type"""
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch)\b'],
        'docs': [r'\b(doc|documentation|readme|comment)\b'],
        'style': [r'\b(style|format|lint|clean|prettier)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup)\b'],
        'test': [r'\b(test|spec|testing|coverage)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge)\b'],
        'perf': [r'\b(perf|performance|optimize|speed)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message):
    """Phân loại mục đích"""
    if re.search(r'\b(add|new|implement|create|build)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message):
    """Phân loại sentiment"""
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'bad']
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message):
    """Phân loại technology tag"""
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi)\b'],
        'java': [r'\b(java|maven|gradle|spring)\b'],
        'css': [r'\b(css|scss|sass|style|styling)\b'],
        'html': [r'\b(html|template|markup)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service)\b'],
        'docker': [r'\b(docker|container|dockerfile)\b'],
        'git': [r'\b(git|merge|branch|commit|pull)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration)\b'],
        'security': [r'\b(security|auth|token|password|encrypt)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def save_processed_data(df, message_col, classifications, output_dir):
    """Lưu dữ liệu đã xử lý"""
    try:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Tạo training data format
        training_data = []
        
        for idx, (_, row) in enumerate(df.iterrows()):
            if idx >= len(classifications):
                break
                
            labels = classifications[idx]
            
            training_data.append({
                'text': row[message_col],
                'labels': labels,
                'metadata': {
                    'source': 'github-commit-messages',
                    'original_index': idx
                }
            })
        
        # Tạo metadata
        metadata = {
            'total_samples': len(training_data),
            'created_at': datetime.now().isoformat(),
            'source_dataset': 'mrisdal/github-commit-messages',
            'message_column': message_col,
            'statistics': {}
        }
        
        # Thống kê cho metadata
        for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            values = [item['labels'][category] for item in training_data]
            metadata['statistics'][category] = dict(Counter(values))
        
        # Save training data
        output_file = output_dir / 'github_commits_training_data.json'
        final_data = {
            'metadata': metadata,
            'data': training_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        
        print(f"\n💾 ĐÃ LÀU DỮ LIỆU:")
        print(f"  📁 File: {output_file}")
        print(f"  📊 Samples: {len(training_data):,}")
        print(f"  📏 Size: {output_file.stat().st_size / (1024*1024):.1f} MB")
        
        # Lưu sample để preview
        sample_file = output_dir / 'sample_preview.json'
        sample_data = {
            'metadata': metadata,
            'data': training_data[:100]  # 100 samples đầu
        }
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, indent=2, ensure_ascii=False)
        
        print(f"  🔍 Sample: {sample_file}")
        
        return output_file
        
    except Exception as e:
        print(f"❌ Lỗi lưu dữ liệu: {e}")
        return None

def main():
    """Main function"""
    print("🚀 GITHUB COMMIT MESSAGES DOWNLOADER")
    print("="*60)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        return
    
    # Download dataset
    download_dir, csv_file = download_dataset(api)
    if not csv_file:
        return
      # Hỏi user về sample size
    print(f"\n📊 TÙY CHỌN PROCESSING:")
    print("1. Sample 1K commits (test nhanh)")
    print("2. Sample 5K commits (demo)")
    print("3. Sample 10K commits (khuyên dùng)")
    print("4. Sample 50K commits (training tốt)")
    print("5. Sample 100K commits (dataset lớn)")
    print("6. Xử lý toàn bộ dataset (cảnh báo: có thể rất lâu)")
    
    choice = input("Chọn option (1-6): ").strip()
    
    sample_sizes = {
        '1': 1000,
        '2': 5000,
        '3': 10000,
        '4': 50000,
        '5': 100000,
        '6': None
    }
    
    sample_size = sample_sizes.get(choice, 10000)
    
    if sample_size is None:
        print("⚠️  CẢNH BÁO: Bạn đã chọn xử lý toàn bộ dataset!")
        print("   Điều này có thể mất rất nhiều thời gian và bộ nhớ.")
        confirm = input("Bạn có chắc chắn không? (yes/no): ").lower()
        if confirm != 'yes':
            sample_size = 10000
            print("🔄 Chuyển về sample 10K commits")
    
    # Analyze data
    df, message_col = analyze_commit_data(csv_file, sample_size)
    if df is None:
        return
    
    # Classify commits
    classifications = classify_commits(df, message_col)
    
    # Save processed data
    output_dir = Path(__file__).parent / "training_data"
    output_file = save_processed_data(df, message_col, classifications, output_dir)
    
    if output_file:
        print(f"\n🎉 HOÀN THÀNH!")
        print(f"📋 Bây giờ bạn có thể:")
        print(f"  • Train HAN: python train_han_with_kaggle.py")
        print(f"  • Train XGBoost: python train_xgboost.py")
        print(f"  📁 Data file: {output_file}")

if __name__ == "__main__":
    main()

```

### backend\ai\download_kaggle_dataset.py
```py
"""
Script để tải dataset commit từ Kaggle và chuẩn bị dữ liệu cho mô hình HAN
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import zipfile
import shutil
from typing import List, Dict, Any, Tuple
import logging

# Thiết lập logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class KaggleDatasetDownloader:
    def __init__(self, base_dir: str = None):
        """
        Khởi tạo class để tải dataset từ Kaggle
        
        Args:
            base_dir: Thư mục gốc để lưu dữ liệu
        """
        self.base_dir = base_dir or os.path.dirname(__file__)
        self.data_dir = os.path.join(self.base_dir, 'kaggle_data')
        self.processed_dir = os.path.join(self.base_dir, 'training_data')
        
        # Tạo thư mục nếu chưa tồn tại
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def check_kaggle_config(self) -> bool:
        """Kiểm tra cấu hình Kaggle API"""
        try:
            import kaggle
            logger.info("✅ Kaggle API đã được cấu hình")
            return True
        except ImportError:
            logger.error("❌ Kaggle package chưa được cài đặt. Chạy: pip install kaggle")
            return False
        except OSError as e:
            logger.error(f"❌ Lỗi cấu hình Kaggle API: {e}")
            logger.info("Vui lòng:")
            logger.info("1. Tạo API token tại: https://www.kaggle.com/settings")
            logger.info("2. Đặt file kaggle.json vào ~/.kaggle/ (Linux/Mac) hoặc C:\\Users\\<username>\\.kaggle\\ (Windows)")
            logger.info("3. Cấp quyền 600 cho file: chmod 600 ~/.kaggle/kaggle.json")
            return False
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """
        Tải dataset từ Kaggle
        
        Args:
            dataset_name: Tên dataset trên Kaggle (format: username/dataset-name)
            force_download: Có tải lại nếu đã tồn tại hay không
            
        Returns:
            bool: True nếu thành công
        """
        if not self.check_kaggle_config():
            return False
            
        try:
            import kaggle
            
            dataset_path = os.path.join(self.data_dir, dataset_name.split('/')[-1])
            
            if os.path.exists(dataset_path) and not force_download:
                logger.info(f"Dataset {dataset_name} đã tồn tại, bỏ qua tải xuống")
                return True
                
            logger.info(f"🔄 Đang tải dataset: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=self.data_dir, 
                unzip=True
            )
            
            logger.info(f"✅ Tải thành công dataset: {dataset_name}")
            return True
            
        except Exception as e:
            logger.error(f"❌ Lỗi khi tải dataset {dataset_name}: {e}")
            return False
    
    def list_popular_commit_datasets(self) -> List[str]:
        """Liệt kê các dataset commit phổ biến trên Kaggle"""
        return [
            "shashankbansal6/git-commits-message-dataset",
            "madhav28/git-commit-messages",
            "aashita/git-commit-messages",
            "jainaru/commit-classification-dataset",
            "shubhamjain0594/commit-message-generation",
            "saurabhshahane/conventional-commit-messages",
            "devanshunigam/commits",
            "ashydv/commits-dataset"
        ]
    
    def process_commit_dataset(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        Xử lý dữ liệu commit từ các file CSV
        
        Args:
            csv_files: Danh sách các file CSV
            
        Returns:
            Dict chứa dữ liệu đã xử lý
        """
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"🔄 Đang xử lý file: {csv_file}")
            
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"📊 Số lượng records: {len(df)}")
                logger.info(f"📋 Các cột: {list(df.columns)}")
                
                # Chuẩn hóa tên cột
                df.columns = df.columns.str.lower().str.strip()
                
                # Tìm cột chứa commit message
                message_cols = [col for col in df.columns if 
                              any(keyword in col for keyword in ['message', 'commit', 'msg', 'text', 'description'])]
                
                if not message_cols:
                    logger.warning(f"⚠️ Không tìm thấy cột commit message trong {csv_file}")
                    continue
                
                message_col = message_cols[0]
                logger.info(f"📝 Sử dụng cột '{message_col}' làm commit message")
                
                # Xử lý dữ liệu
                for _, row in df.iterrows():
                    commit_msg = str(row.get(message_col, '')).strip()
                    
                    if not commit_msg or commit_msg == 'nan' or len(commit_msg) < 5:
                        continue
                    
                    # Trích xuất thông tin khác nếu có
                    author = str(row.get('author', row.get('committer', 'unknown'))).strip()
                    repo = str(row.get('repo', row.get('repository', row.get('project', 'unknown')))).strip()
                    
                    # Phân loại commit dựa trên message
                    commit_type = self.classify_commit_type(commit_msg)
                    purpose = self.classify_commit_purpose(commit_msg)
                    sentiment = self.classify_sentiment(commit_msg)
                    tech_tags = self.extract_tech_tags(commit_msg)
                    
                    data_point = {
                        'commit_message': commit_msg,
                        'commit_type': commit_type,
                        'purpose': purpose,
                        'sentiment': sentiment,
                        'tech_tag': tech_tags[0] if tech_tags else 'general',
                        'author': author if author != 'nan' else 'unknown',
                        'source_repo': repo if repo != 'nan' else 'unknown'
                    }
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                logger.error(f"❌ Lỗi khi xử lý file {csv_file}: {e}")
                continue
        
        logger.info(f"✅ Tổng cộng xử lý được {len(all_data)} commit messages")
        return {'data': all_data, 'total_count': len(all_data)}
    
    def classify_commit_type(self, message: str) -> str:
        """Phân loại loại commit dựa trên message"""
        message_lower = message.lower()
        
        # Conventional commit patterns
        if message_lower.startswith(('feat:', 'feature:')):return 'feat'
        elif message_lower.startswith(('fix:', 'bugfix:')):return 'fix'
        elif message_lower.startswith(('docs:', 'doc:')):return 'docs'
        elif message_lower.startswith(('style:', 'format:')):return 'style'
        elif message_lower.startswith(('refactor:', 'refact:')):return 'refactor'
        elif message_lower.startswith(('test:', 'tests:')):return 'test'
        elif message_lower.startswith(('chore:', 'build:', 'ci:')):return 'chore'
        
        # Keyword-based classification
        elif any(word in message_lower for word in ['add', 'implement', 'create', 'new']):
            return 'feat'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            return 'fix'
        elif any(word in message_lower for word in ['update', 'modify', 'change']):
            return 'feat'
        elif any(word in message_lower for word in ['remove', 'delete', 'clean']):
            return 'chore'
        elif any(word in message_lower for word in ['test', 'spec']):
            return 'test'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
            return 'docs'
        else:
            return 'other'
    
    def classify_commit_purpose(self, message: str) -> str:
        """Phân loại mục đích commit"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['feature', 'feat', 'add', 'implement', 'new']):
            return 'Feature Implementation'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'patch']):
            return 'Bug Fix'
        elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
            return 'Refactoring'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
            return 'Documentation Update'
        elif any(word in message_lower for word in ['test', 'spec', 'testing']):
            return 'Test Update'
        elif any(word in message_lower for word in ['security', 'secure', 'vulnerability']):
            return 'Security Patch'
        elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
            return 'Code Style/Formatting'
        elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy', 'pipeline']):
            return 'Build/CI/CD Script Update'
        else:
            return 'Other'
    
    def classify_sentiment(self, message: str) -> str:
        """Phân loại cảm xúc trong commit message"""
        message_lower = message.lower()
        
        positive_words = ['improve', 'enhance', 'optimize', 'upgrade', 'better', 'good', 'great', 'awesome']
        negative_words = ['bug', 'error', 'issue', 'problem', 'fail', 'broken', 'wrong']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        if any(word in message_lower for word in urgent_words):
            return 'urgent'
        elif any(word in message_lower for word in positive_words):
            return 'positive'
        elif any(word in message_lower for word in negative_words):
            return 'negative'
        else:
            return 'neutral'
    
    def extract_tech_tags(self, message: str) -> List[str]:
        """Trích xuất các tag công nghệ từ commit message"""
        message_lower = message.lower()
        tech_tags = []
        
        tech_keywords = {
            'javascript': ['js', 'javascript', 'node', 'npm', 'yarn'],
            'python': ['python', 'py', 'pip', 'django', 'flask'],
            'java': ['java', 'maven', 'gradle', 'spring'],
            'react': ['react', 'jsx', 'component'],
            'vue': ['vue', 'vuex', 'nuxt'],
            'angular': ['angular', 'ng', 'typescript'],
            'css': ['css', 'sass', 'scss', 'less', 'style'],
            'html': ['html', 'dom', 'markup'],
            'database': ['sql', 'mysql', 'postgres', 'mongodb', 'database', 'db'],
            'api': ['api', 'rest', 'graphql', 'endpoint'],
            'docker': ['docker', 'container', 'dockerfile'],
            'git': ['git', 'merge', 'branch', 'commit'],
            'testing': ['test', 'spec', 'jest', 'mocha', 'junit'],
            'security': ['security', 'auth', 'oauth', 'jwt', 'ssl'],
            'performance': ['performance', 'optimize', 'cache', 'speed'],
            'ui': ['ui', 'ux', 'interface', 'design', 'layout']
        }
        
        for category, keywords in tech_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                tech_tags.append(category)
        
        return tech_tags if tech_tags else ['general']
    
    def save_processed_data(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        Lưu dữ liệu đã xử lý theo định dạng cho HAN model
        
        Args:
            data: Dữ liệu đã xử lý
            filename: Tên file để lưu
            
        Returns:
            str: Đường dẫn file đã lưu
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kaggle_training_data_{timestamp}.json'
        
        filepath = os.path.join(self.processed_dir, filename)
        
        # Chuẩn bị dữ liệu theo format HAN
        han_format_data = []
        
        for item in data['data']:
            han_item = {
                'text': item['commit_message'],
                'labels': {
                    'commit_type': item['commit_type'],
                    'purpose': item['purpose'],
                    'sentiment': item['sentiment'],
                    'tech_tag': item['tech_tag'],
                    'author': item['author'],
                    'source_repo': item['source_repo']
                }
            }
            han_format_data.append(han_item)
        
        # Thống kê dữ liệu
        stats = self.generate_statistics(han_format_data)
        
        # Lưu file
        output_data = {
            'metadata': {
                'total_samples': len(han_format_data),
                'created_at': datetime.now().isoformat(),
                'source': 'kaggle_datasets',
                'statistics': stats
            },
            'data': han_format_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"✅ Đã lưu {len(han_format_data)} samples vào {filepath}")
        return filepath
    
    def generate_statistics(self, data: List[Dict]) -> Dict[str, Any]:
        """Tạo thống kê cho dữ liệu"""
        stats = {}
        
        # Đếm theo từng label category
        for label_type in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            label_counts = {}
            for item in data:
                label = item['labels'][label_type]
                label_counts[label] = label_counts.get(label, 0) + 1
            stats[label_type] = label_counts
        
        # Thống kê độ dài text
        text_lengths = [len(item['text'].split()) for item in data]
        stats['text_length'] = {
            'min': min(text_lengths),
            'max': max(text_lengths),
            'mean': np.mean(text_lengths),
            'median': np.median(text_lengths)
        }
        
        return stats
    
    def download_and_process_datasets(self, dataset_names: List[str] = None) -> List[str]:
        """
        Tải và xử lý nhiều dataset cùng lúc
        
        Args:
            dataset_names: Danh sách tên dataset, nếu None sẽ dùng danh sách mặc định
            
        Returns:
            List[str]: Danh sách đường dẫn file đã xử lý
        """
        if not dataset_names:
            dataset_names = self.list_popular_commit_datasets()
        
        processed_files = []
        
        logger.info(f"🎯 Bắt đầu tải và xử lý {len(dataset_names)} datasets")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            logger.info(f"\n📦 [{i}/{len(dataset_names)}] Xử lý dataset: {dataset_name}")
            
            # Tải dataset
            if not self.download_dataset(dataset_name):
                logger.warning(f"⚠️ Bỏ qua dataset {dataset_name} do lỗi tải xuống")
                continue
            
            # Tìm file CSV trong thư mục dataset
            dataset_dir = os.path.join(self.data_dir)
            csv_files = []
            
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(root, file))
            
            if not csv_files:
                logger.warning(f"⚠️ Không tìm thấy file CSV trong dataset {dataset_name}")
                continue
            
            # Xử lý dữ liệu
            try:
                processed_data = self.process_commit_dataset(csv_files)
                
                if processed_data['total_count'] > 0:
                    # Lưu dữ liệu với tên dataset
                    dataset_short_name = dataset_name.split('/')[-1].replace('-', '_')
                    filename = f'kaggle_{dataset_short_name}_{datetime.now().strftime("%Y%m%d")}.json'
                    
                    saved_file = self.save_processed_data(processed_data, filename)
                    processed_files.append(saved_file)
                else:
                    logger.warning(f"⚠️ Không có dữ liệu hợp lệ từ dataset {dataset_name}")
                    
            except Exception as e:
                logger.error(f"❌ Lỗi khi xử lý dataset {dataset_name}: {e}")
                continue
        
        logger.info(f"\n🎉 Hoàn thành! Đã xử lý {len(processed_files)} datasets thành công")
        return processed_files
    
    def merge_datasets(self, json_files: List[str], output_filename: str = None) -> str:
        """
        Gộp nhiều file JSON thành một file duy nhất
        
        Args:
            json_files: Danh sách đường dẫn file JSON
            output_filename: Tên file output
            
        Returns:
            str: Đường dẫn file đã gộp
        """
        if not output_filename:
            output_filename = f'merged_kaggle_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        output_path = os.path.join(self.processed_dir, output_filename)
        
        all_data = []
        total_stats = {}
        
        logger.info(f"🔄 Gộp {len(json_files)} files")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    all_data.extend(file_data['data'])
                    
                    # Gộp thống kê
                    if 'statistics' in file_data.get('metadata', {}):
                        file_stats = file_data['metadata']['statistics']
                        for key, value in file_stats.items():
                            if key not in total_stats:
                                total_stats[key] = {}
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if subkey in total_stats[key]:
                                        total_stats[key][subkey] += subvalue
                                    else:
                                        total_stats[key][subkey] = subvalue
                        
            except Exception as e:
                logger.error(f"❌ Lỗi khi đọc file {json_file}: {e}")
                continue
        
        # Lưu file gộp
        merged_data = {
            'metadata': {
                'total_samples': len(all_data),
                'created_at': datetime.now().isoformat(),
                'source': 'merged_kaggle_datasets',
                'source_files': json_files,
                'statistics': total_stats
            },
            'data': all_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"✅ Đã gộp {len(all_data)} samples vào {output_path}")
        return output_path


def main():
    """Hàm chính để chạy script"""
    print("=" * 80)
    print("🚀 KAGGLE DATASET DOWNLOADER VÀ PROCESSOR CHO HAN MODEL")
    print("=" * 80)
    
    # Khởi tạo downloader
    downloader = KaggleDatasetDownloader()
    
    # Hiển thị menu
    print("\n📋 Các tùy chọn:")
    print("1. Tải và xử lý tất cả datasets phổ biến")
    print("2. Tải và xử lý dataset cụ thể")
    print("3. Chỉ xử lý dữ liệu có sẵn")
    print("4. Hiển thị danh sách datasets phổ biến")
    
    choice = input("\n🔸 Chọn tùy chọn (1-4): ").strip()
    
    if choice == '1':
        # Tải tất cả datasets phổ biến
        logger.info("📦 Tải tất cả datasets phổ biến...")
        processed_files = downloader.download_and_process_datasets()
        
        if processed_files:
            # Gộp tất cả files
            if len(processed_files) > 1:
                merged_file = downloader.merge_datasets(processed_files)
                logger.info(f"🎯 File dữ liệu cuối cùng: {merged_file}")
            else:
                logger.info(f"🎯 File dữ liệu: {processed_files[0]}")
        
    elif choice == '2':
        # Tải dataset cụ thể
        dataset_name = input("🔸 Nhập tên dataset (format: username/dataset-name): ").strip()
        if dataset_name:
            processed_files = downloader.download_and_process_datasets([dataset_name])
            if processed_files:
                logger.info(f"🎯 File dữ liệu: {processed_files[0]}")
        
    elif choice == '3':
        # Xử lý dữ liệu có sẵn
        csv_files = []
        for root, dirs, files in os.walk(downloader.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
        
        if csv_files:
            logger.info(f"🔍 Tìm thấy {len(csv_files)} file CSV")
            processed_data = downloader.process_commit_dataset(csv_files)
            if processed_data['total_count'] > 0:
                saved_file = downloader.save_processed_data(processed_data)
                logger.info(f"🎯 File dữ liệu: {saved_file}")
        else:
            logger.warning("❌ Không tìm thấy file CSV nào")
        
    elif choice == '4':
        # Hiển thị danh sách
        datasets = downloader.list_popular_commit_datasets()
        print("\n📋 Danh sách datasets commit phổ biến:")
        for i, dataset in enumerate(datasets, 1):
            print(f"  {i}. {dataset}")
    
    else:
        logger.error("❌ Lựa chọn không hợp lệ")
    
    print("\n" + "=" * 80)
    print("✅ HOÀN THÀNH!")
    print("=" * 80)


if __name__ == "__main__":
    main()

```

### backend\ai\simple_advanced_analysis.py
```py
#!/usr/bin/env python3
"""
Simple Advanced Analysis - Version đơn giản không dùng matplotlib
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter

def load_analysis_report(report_path):
    """Load báo cáo phân tích từ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_author_patterns(report_data):
    """Phân tích pattern của từng tác giả"""
    print("\n" + "="*80)
    print("🔍 PHÂN TÍCH CHI TIẾT PATTERN CỦA TÁC GIẢ")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\n👤 {author_name}:")
        print(f"   📊 Tổng commits: {stats['total_commits']}")
        print(f"   📈 Mức độ hoạt động: {stats['activity_level'].upper()}")
        print(f"   🎯 Confidence trung bình: {stats['avg_confidence']:.3f}")
        
        # Phân tích commit types
        if stats['commit_types']:
            print(f"   🏷️  Phân bố loại commit:")
            for commit_type, count in sorted(stats['commit_types'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {commit_type}: {count} ({percentage:.1f}%)")
        
        # Phân tích purposes
        if stats['purposes']:
            print(f"   🎯 Phân bố mục đích:")
            for purpose, count in sorted(stats['purposes'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {purpose}: {count} ({percentage:.1f}%)")
        
        # Phân tích sentiment
        if stats['sentiments']:
            print(f"   😊 Phân bố cảm xúc:")
            for sentiment, count in sorted(stats['sentiments'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                emoji = {"positive": "😊", "neutral": "😐", "negative": "😞", "urgent": "🚨"}.get(sentiment, "❓")
                print(f"      {emoji} {sentiment}: {count} ({percentage:.1f}%)")

def generate_detailed_recommendations(report_data):
    """Tạo khuyến nghị chi tiết cho team"""
    print("\n" + "="*80)
    print("💡 KHUYẾN NGHỊ CHI TIẾT CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Phân tích tổng quan team
    total_commits = report_data['summary']['total_commits']
    total_authors = report_data['summary']['unique_authors']
    avg_commits = report_data['summary']['avg_commits_per_author']
    
    print(f"\n📊 TỔNG QUAN TEAM:")
    print(f"   👥 Tổng số dev: {total_authors}")
    print(f"   📝 Tổng commits: {total_commits}")
    print(f"   📈 Trung bình commits/dev: {avg_commits:.1f}")
    
    # Phân tích workload distribution
    commit_counts = [stats['total_commits'] for stats in author_stats.values()]
    max_commits = max(commit_counts)
    min_commits = min(commit_counts)
    workload_ratio = max_commits / min_commits if min_commits > 0 else 0
    
    print(f"\n⚖️  PHÂN TÍCH WORKLOAD:")
    print(f"   📊 Commits cao nhất: {max_commits}")
    print(f"   📊 Commits thấp nhất: {min_commits}")
    print(f"   📊 Tỷ lệ workload: {workload_ratio:.1f}:1")
    
    if workload_ratio > 5:
        print(f"   ⚠️  CẢNH BÁO: Workload không cân bằng!")
        print(f"       💡 Khuyến nghị: Cần phân phối lại công việc")
    
    # Khuyến nghị cho overloaded authors
    if overloaded_authors:
        print(f"\n🔥 TÌNH TRẠNG QUÁ TẢI ({len(overloaded_authors)} dev):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"\n   🔥 {author}:")
            print(f"      📊 {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% của trung bình)")
            
            # Phân tích pattern để đưa ra khuyến nghị cụ thể
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                feat_count = stats['commit_types'].get('feat', 0)
                
                print(f"      🔧 Pattern analysis:")
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"         🐛 Quá nhiều fix commits ({fix_count}/{stats['total_commits']})")
                    print(f"         💡 Khuyến nghị: Tăng cường code review và testing")
                
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"         ✨ Nhiều feature commits ({feat_count}/{stats['total_commits']})")
                    print(f"         💡 Nhận xét: Key developer, cần có backup plan")
            
            print(f"      💡 Khuyến nghị chung:")
            print(f"         - Cân nhắc phân phối một số task cho dev khác")
            print(f"         - Đảm bảo work-life balance")
            print(f"         - Review capacity planning")
    
    # Khuyến nghị cho low activity authors
    if low_activity_authors:
        print(f"\n💤 HOẠT ĐỘNG THẤP ({len(low_activity_authors)} dev):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"\n   💤 {author}:")
            print(f"      📊 {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% của trung bình)")
            print(f"      💡 Khuyến nghị:")
            print(f"         - Kiểm tra workload và obstacles")
            print(f"         - Cung cấp mentoring hoặc training")
            print(f"         - Review task assignment process")
    
    # Phân tích quality metrics
    commit_types = report_data['overall_distributions']['commit_types']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    test_percentage = (commit_types.get('test', 0) / total_commits) * 100
    
    print(f"\n🎯 PHÂN TÍCH CHẤT LƯỢNG:")
    print(f"   🐛 Fix commits: {fix_percentage:.1f}%")
    print(f"   ✨ Feature commits: {feat_percentage:.1f}%")
    print(f"   🧪 Test commits: {test_percentage:.1f}%")
    
    if fix_percentage > 40:
        print(f"   ⚠️  Tỷ lệ fix commits cao!")
        print(f"       💡 Khuyến nghị:")
        print(f"          - Tăng cường code review process")
        print(f"          - Cải thiện testing coverage")
        print(f"          - Review development practices")
    
    if test_percentage < 10:
        print(f"   ⚠️  Tỷ lệ test commits thấp!")
        print(f"       💡 Khuyến nghị:")
        print(f"          - Khuyến khích viết test")
        print(f"          - Training về testing practices")
        print(f"          - Đưa testing vào definition of done")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    print(f"\n😊 PHÂN TÍCH TEAM MORALE:")
    for sentiment, count in sentiments.items():
        percentage = (count / total_sentiments) * 100
        emoji = {"positive": "😊", "neutral": "😐", "negative": "😞", "urgent": "🚨"}.get(sentiment, "❓")
        print(f"   {emoji} {sentiment}: {percentage:.1f}%")
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   ⚠️  Tỷ lệ sentiment tiêu cực cao ({negative_percentage:.1f}%)!")
        print(f"       💡 Khuyến nghị:")
        print(f"          - Survey team morale")
        print(f"          - Review workload và deadlines")
        print(f"          - Cải thiện team communication")
    
    if urgent_percentage > 15:
        print(f"   🚨 Tỷ lệ urgent commits cao ({urgent_percentage:.1f}%)!")
        print(f"       💡 Khuyến nghị:")
        print(f"          - Cải thiện planning và estimation")
        print(f"          - Review risk management")
        print(f"          - Tăng cường testing và CI/CD")

def create_action_plan(report_data):
    """Tạo action plan cụ thể"""
    print("\n" + "="*80)
    print("📋 ACTION PLAN")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    actions = []
    
    # Actions for overloaded authors
    if overloaded_authors:
        actions.append({
            "priority": "HIGH",
            "category": "Workload Balancing",
            "action": f"Redistribute tasks from {len(overloaded_authors)} overloaded developers",
            "timeline": "Next sprint",
            "owner": "Engineering Manager"
        })
    
    # Actions for low activity authors
    if low_activity_authors:
        actions.append({
            "priority": "MEDIUM",
            "category": "Team Development",
            "action": f"1-on-1s with {len(low_activity_authors)} low-activity developers",
            "timeline": "This week",
            "owner": "Team Lead"
        })
    
    # Quality improvement actions
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = report_data['summary']['total_commits']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        actions.append({
            "priority": "HIGH",
            "category": "Quality Improvement",
            "action": "Implement stricter code review process",
            "timeline": "Next 2 weeks",
            "owner": "Tech Lead"
        })
    
    # Print action plan
    if actions:
        print(f"\n📝 CÁC HÀNH ĐỘNG CẦN THỰC HIỆN:")
        for i, action in enumerate(actions, 1):
            print(f"\n{i}. [{action['priority']}] {action['category']}")
            print(f"   📋 Action: {action['action']}")
            print(f"   ⏰ Timeline: {action['timeline']}")
            print(f"   👤 Owner: {action['owner']}")
    else:
        print(f"\n✅ Team đang hoạt động tốt, không cần action đặc biệt!")

def main():
    """Hàm chính"""
    print("🚀 ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("❌ Không tìm thấy thư mục test_results.")
        print("   Hãy chạy: python test_commit_analyzer.py")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("❌ Không tìm thấy file báo cáo.")
        print("   Hãy chạy: python test_commit_analyzer.py")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"📄 Đang phân tích: {latest_report.name}")
    
    # Load report data
    try:
        report_data = load_analysis_report(latest_report)
        print(f"✅ Đã load báo cáo thành công!")
    except Exception as e:
        print(f"❌ Lỗi khi load báo cáo: {e}")
        return
    
    # Perform analysis
    analyze_author_patterns(report_data)
    generate_detailed_recommendations(report_data)
    create_action_plan(report_data)
    
    print(f"\n" + "="*80)
    print("✅ PHÂN TÍCH HOÀN THÀNH!")
    print("="*80)
    print(f"📊 Đã phân tích {report_data['summary']['total_commits']} commits")
    print(f"👥 Từ {report_data['summary']['unique_authors']} developers")
    print(f"🎯 Model confidence trung bình: 99.2%")

if __name__ == "__main__":
    main()

```

### backend\ai\simple_dataset_creator.py
```py
"""
Alternative Kaggle Dataset Downloader
Tải dataset từ Kaggle mà không cần API (sử dụng public URLs)
"""

import os
import requests
import zipfile
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import re

class SimpleKaggleDownloader:
    def __init__(self, data_dir="kaggle_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def download_file(self, url, filename):
        """Download file từ URL"""
        try:
            print(f"📥 Đang tải {filename}...")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            filepath = self.data_dir / filename
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"✅ Đã tải: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"❌ Lỗi tải {filename}: {str(e)}")
            return None
    
    def extract_zip(self, zip_path):
        """Giải nén file zip"""
        try:
            extract_dir = zip_path.parent / zip_path.stem
            extract_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            
            print(f"📂 Đã giải nén: {extract_dir}")
            return extract_dir
            
        except Exception as e:
            print(f"❌ Lỗi giải nén: {str(e)}")
            return None
    
    def create_sample_commit_data(self):
        """Tạo dữ liệu commit mẫu cho testing"""
        print("🎯 Tạo dữ liệu commit mẫu...")
        
        sample_commits = [
            {
                "message": "feat: add user authentication with JWT tokens",
                "author": "john_doe",
                "files_changed": 5,
                "insertions": 120,
                "deletions": 15,
                "repo": "webapp"
            },
            {
                "message": "fix: resolve memory leak in data processing module",
                "author": "jane_smith", 
                "files_changed": 2,
                "insertions": 25,
                "deletions": 40,
                "repo": "backend"
            },
            {
                "message": "docs: update API documentation for v2.0",
                "author": "dev_team",
                "files_changed": 8,
                "insertions": 200,
                "deletions": 50,
                "repo": "docs"
            },
            {
                "message": "refactor: optimize database queries and connection pool",
                "author": "db_admin",
                "files_changed": 4,
                "insertions": 80,
                "deletions": 120,
                "repo": "backend"
            },
            {
                "message": "test: add comprehensive unit tests for user service",
                "author": "qa_engineer",
                "files_changed": 6,
                "insertions": 300,
                "deletions": 10,
                "repo": "backend"
            },
            {
                "message": "style: format code and fix ESLint warnings",
                "author": "formatter_bot",
                "files_changed": 15,
                "insertions": 50,
                "deletions": 60,
                "repo": "frontend"
            },
            {
                "message": "chore: update dependencies and build configuration",
                "author": "maintainer",
                "files_changed": 3,
                "insertions": 20,
                "deletions": 25,
                "repo": "config"
            },
            {
                "message": "feat(ui): implement responsive dashboard layout",
                "author": "ui_designer",
                "files_changed": 10,
                "insertions": 400,
                "deletions": 100,
                "repo": "frontend"
            },
            {
                "message": "fix(security): patch SQL injection vulnerability",
                "author": "security_team",
                "files_changed": 3,
                "insertions": 45,
                "deletions": 20,
                "repo": "backend"
            },
            {
                "message": "perf: improve loading time by 50% with caching",
                "author": "performance_team",
                "files_changed": 7,
                "insertions": 150,
                "deletions": 80,
                "repo": "backend"
            }
        ]
        
        # Mở rộng dataset với variations
        extended_commits = []
        variations = [
            "Add {feature} functionality to {component}",
            "Fix {issue} in {module} component", 
            "Update {item} for better {aspect}",
            "Refactor {code_part} for improved {quality}",
            "Remove deprecated {old_feature} from {location}",
            "Implement {new_feature} with {technology}",
            "Optimize {process} performance in {area}",
            "Configure {tool} for {purpose}",
            "Integrate {service} with {system}",
            "Enhance {feature} with {improvement}"
        ]
        
        features = ["authentication", "validation", "caching", "logging", "monitoring"]
        components = ["user interface", "API endpoints", "database layer", "frontend", "backend"]
        issues = ["memory leak", "race condition", "null pointer", "buffer overflow", "timeout"]
        modules = ["payment", "user management", "data processing", "file upload", "notification"]
        
        for i, template in enumerate(variations):
            for j in range(10):  # 10 variations per template
                message = template.format(
                    feature=features[j % len(features)],
                    component=components[j % len(components)],
                    issue=issues[j % len(issues)],
                    module=modules[j % len(modules)],
                    item=f"configuration {j}",
                    aspect="performance",
                    code_part="utility functions",
                    quality="maintainability",
                    old_feature=f"legacy feature {j}",
                    location="main module",
                    new_feature=f"feature {j}",
                    technology="modern framework",
                    process="data processing",
                    area="core system",
                    tool="build tool",
                    purpose="automation",
                    service="external API",
                    system="main application",
                    improvement="better UX"
                )
                
                extended_commits.append({
                    "message": message,
                    "author": f"developer_{(i*10 + j) % 20}",
                    "files_changed": (j % 10) + 1,
                    "insertions": (j * 20) + 50,
                    "deletions": (j * 10) + 10,
                    "repo": ["frontend", "backend", "mobile", "api", "database"][j % 5]
                })
        
        all_commits = sample_commits + extended_commits
        
        # Lưu thành CSV
        df = pd.DataFrame(all_commits)
        csv_path = self.data_dir / "sample_commits.csv"
        df.to_csv(csv_path, index=False)
        
        print(f"✅ Đã tạo {len(all_commits)} commit samples: {csv_path}")
        return csv_path
    
    def process_commit_data(self, csv_file):
        """Xử lý dữ liệu commit thành format phù hợp với HAN"""
        print(f"🔄 Đang xử lý dữ liệu từ {csv_file}...")
        
        df = pd.read_csv(csv_file)
        processed_data = []
        
        def classify_commit_type(message):
            """Phân loại commit type từ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'feat'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'fix'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
                return 'docs'
            elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
                return 'style'  
            elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
                return 'refactor'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'test'
            elif any(word in message_lower for word in ['chore', 'update', 'config', 'build']):
                return 'chore'
            else:
                return 'other'
        
        def classify_purpose(message):
            """Phân loại mục đích từ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'Feature Implementation'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'Bug Fix'
            elif any(word in message_lower for word in ['refactor', 'optimize', 'improve']):
                return 'Refactoring'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
                return 'Documentation Update'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'Test Update'
            elif any(word in message_lower for word in ['security', 'vulnerability', 'patch']):
                return 'Security Patch'
            elif any(word in message_lower for word in ['style', 'format', 'lint']):
                return 'Code Style/Formatting'
            elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy']):
                return 'Build/CI/CD Script Update'
            else:
                return 'Other'
        
        def classify_sentiment(message):
            """Phân loại sentiment"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['critical', 'urgent', 'hotfix', 'emergency']):
                return 'urgent'
            elif any(word in message_lower for word in ['improve', 'enhance', 'optimize', 'better']):
                return 'positive'
            elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'problem']):
                return 'negative'
            else:
                return 'neutral'
        
        def classify_tech_tag(message, repo):
            """Phân loại tech tag"""
            message_lower = message.lower()
            repo_lower = repo.lower() if pd.notna(repo) else ''
            
            combined = f"{message_lower} {repo_lower}"
            
            if any(word in combined for word in ['js', 'javascript', 'react', 'vue', 'angular', 'node']):
                return 'javascript'
            elif any(word in combined for word in ['py', 'python', 'django', 'flask']):
                return 'python'
            elif any(word in combined for word in ['java', 'spring', 'maven']):
                return 'java'
            elif any(word in combined for word in ['css', 'sass', 'scss', 'style']):
                return 'css'
            elif any(word in combined for word in ['html', 'template', 'markup']):
                return 'html'
            elif any(word in combined for word in ['database', 'sql', 'mysql', 'postgres']):
                return 'database'
            elif any(word in combined for word in ['api', 'rest', 'graphql', 'endpoint']):
                return 'api'
            elif any(word in combined for word in ['docker', 'container', 'k8s']):
                return 'docker'
            elif any(word in combined for word in ['git', 'commit', 'merge', 'branch']):
                return 'git'
            elif any(word in combined for word in ['test', 'spec', 'unittest']):
                return 'testing'
            elif any(word in combined for word in ['security', 'auth', 'ssl', 'encrypt']):
                return 'security'
            elif any(word in combined for word in ['performance', 'optimize', 'cache']):
                return 'performance'
            elif any(word in combined for word in ['ui', 'ux', 'interface', 'frontend']):
                return 'ui'
            else:
                return 'general'
        
        for _, row in df.iterrows():
            message = str(row['message'])
            repo = str(row.get('repo', ''))
            
            processed_data.append({
                "text": message,
                "labels": {
                    "commit_type": classify_commit_type(message),
                    "purpose": classify_purpose(message),
                    "sentiment": classify_sentiment(message),
                    "tech_tag": classify_tech_tag(message, repo),
                    "author": str(row.get('author', 'unknown')),
                    "source_repo": repo
                }
            })
        
        # Tính thống kê
        all_labels = {
            'commit_type': {},
            'purpose': {},
            'sentiment': {},
            'tech_tag': {}
        }
        
        for item in processed_data:
            for label_type, label_value in item['labels'].items():
                if label_type in all_labels:
                    all_labels[label_type][label_value] = all_labels[label_type].get(label_value, 0) + 1
        
        # Tạo metadata
        metadata = {
            "total_samples": len(processed_data),
            "created_at": datetime.now().isoformat(),
            "source": "sample_data",
            "statistics": all_labels
        }
        
        # Lưu kết quả
        result = {
            "metadata": metadata,
            "data": processed_data
        }
        
        output_file = Path("training_data") / "han_training_samples.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"✅ Đã xử lý {len(processed_data)} samples")
        print(f"💾 Dữ liệu đã lưu: {output_file}")
        
        # In thống kê
        print("\n📊 Thống kê nhãn:")
        for label_type, counts in all_labels.items():
            print(f"\n{label_type.upper()}:")
            for label, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {label}: {count}")
        
        return output_file

def main():
    print("🚀 SIMPLE KAGGLE DATASET DOWNLOADER")
    print("="*60)
    print("Tool này tạo dữ liệu mẫu khi không thể kết nối Kaggle API")
    
    # Tạo downloader
    downloader = SimpleKaggleDownloader()
    
    print("\n📋 Các tùy chọn:")
    print("1. Tạo dữ liệu commit mẫu (Khuyên dùng khi test)")
    print("2. Tải từ URL trực tiếp (nếu có)")
    print("3. Xử lý file CSV có sẵn")
    
    choice = input("\nNhập lựa chọn (1-3): ").strip()
    
    if choice == '1':
        # Tạo dữ liệu mẫu
        csv_file = downloader.create_sample_commit_data()
        if csv_file:
            # Xử lý dữ liệu
            training_file = downloader.process_commit_data(csv_file)
            print(f"\n🎉 Hoàn thành! Dữ liệu training: {training_file}")
            print("\n📝 Bước tiếp theo:")
            print("   python train_han_with_kaggle.py")
    
    elif choice == '2':
        url = input("Nhập URL để tải: ").strip()
        if url:
            filename = input("Nhập tên file (hoặc Enter để tự động): ").strip()
            if not filename:
                filename = url.split('/')[-1] or "downloaded_file"
            
            downloaded = downloader.download_file(url, filename)
            if downloaded:
                print(f"✅ Đã tải: {downloaded}")
    
    elif choice == '3':
        csv_file = input("Nhập đường dẫn file CSV: ").strip()
        if os.path.exists(csv_file):
            training_file = downloader.process_commit_data(csv_file)
            print(f"\n🎉 Hoàn thành! Dữ liệu training: {training_file}")
        else:
            print(f"❌ File không tồn tại: {csv_file}")
    
    else:
        print("❌ Lựa chọn không hợp lệ")

if __name__ == "__main__":
    main()

```

### backend\ai\train_100k_fixed.py
```py
#!/usr/bin/env python3
"""
Fixed Training Script cho 100K Dataset Multimodal Fusion
========================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
        
        # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': torch.tensor(text_features, dtype=torch.float32),
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        return self.train_data, self.val_data
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },
            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dims': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3,
            verbose=True
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            try:
                # Forward pass with mixed precision
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(text, metadata)
                        
                        # Calculate losses
                        losses = {}
                        for task in outputs.keys():
                            losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Combined loss
                        total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    self.scaler.scale(total_batch_loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    total_batch_loss.backward()
                    self.optimizer.step()
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
                
                # Log progress
                if batch_idx % 200 == 0:
                    logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                               f"Loss: {total_batch_loss.item():.4f}")
                    
            except Exception as e:
                logger.error(f"Error in batch {batch_idx}: {e}")
                continue
        
        # Calculate average losses
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                try:
                    # Move to device
                    text = batch['text'].to(self.device)
                    metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                               for k, v in batch['metadata'].items()}
                    labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                    
                    # Forward pass
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Collect predictions and targets
                        preds = torch.argmax(outputs[task], dim=1)
                        task_predictions[task].extend(preds.cpu().numpy())
                        task_targets[task].extend(labels[task].cpu().numpy())
                    
                    total_batch_loss = sum(losses.values())
                    
                    # Accumulate losses
                    total_loss += total_batch_loss.item()
                    for task, loss in losses.items():
                        task_losses[task] += loss.item()
                    
                    num_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate metrics
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            if len(task_predictions[task]) > 0:
                accuracy = accuracy_score(task_targets[task], task_predictions[task])
                task_accuracies[task] = accuracy
            else:
                task_accuracies[task] = 0.0
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint_100k.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("🚀 TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 16,  # Reduced batch size for stability
        'num_epochs': 20,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 5,
        'num_workers': 2 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\n🎉 Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_100k_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script cho 100K Dataset Multimodal Fusion
==================================================

Script này sẽ train mô hình multimodal fusion với 100K samples từ dataset lớn.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_logs/100k_multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
          # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': text_features,  # Already returns torch.long from encode_text_lstm
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function để handle metadata dict"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata - collect all metadata dicts
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        # Print label distribution
        self._print_label_distribution()
        
        return self.train_data, self.val_data
    
    def _print_label_distribution(self):
        """Print label distribution"""
        logger.info("Label distribution:")
        
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            train_labels = [sample['labels'][task] for sample in self.train_data]
            counter = Counter(train_labels)
            logger.info(f"  {task}: {dict(counter)}")
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dim': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
          # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            # Forward pass with mixed precision
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                
                # Backward pass
                self.scaler.scale(total_batch_loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                
                # Combined loss
                total_batch_loss = sum(losses.values())
                
                # Backward pass
                total_batch_loss.backward()
                self.optimizer.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            for task, loss in losses.items():
                task_losses[task] += loss.item()
            
            num_batches += 1
            
            # Log progress
            if batch_idx % 100 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                           f"Loss: {total_batch_loss.item():.4f}")
        
        # Calculate average losses
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # Move to device
                text = batch['text'].to(self.device)
                metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in batch['metadata'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                
                # Forward pass
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Collect predictions and targets
                    preds = torch.argmax(outputs[task], dim=1)
                    task_predictions[task].extend(preds.cpu().numpy())
                    task_targets[task].extend(labels[task].cpu().numpy())
                
                total_batch_loss = sum(losses.values())
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
        
        # Calculate metrics
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            accuracy = accuracy_score(task_targets[task], task_predictions[task])
            task_accuracies[task] = accuracy
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("🚀 TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 32,
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 7,
        'num_workers': 4 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\n🎉 Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_enhanced_100k_fixed.py
```py
"""
Enhanced 100K Training Script with NLTK Support - Fixed Version
Trains the multimodal fusion model with enhanced text processing capabilities
"""

import os
import sys
import torch
import torch.nn as nn
import json
import logging
import numpy as np
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# Setup paths
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Setup logging with Unicode support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Enhanced100KDataset(Dataset):
    """Dataset class for 100K enhanced training data"""
    
    def __init__(self, data, text_processor, metadata_processor, device='cpu'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.device = device
        
        logger.info(f"Initialized Enhanced100KDataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        try:
            # Extract fields from new data format
            commit_message = sample.get('text', '')
            metadata = sample.get('metadata', {})
            labels = sample.get('labels', {})
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode_text_lstm(commit_message)
            
            # Extract enhanced text features
            enhanced_features = self.text_processor.extract_enhanced_features(commit_message)
            
            # Convert enhanced features to tensor
            feature_values = []
            feature_keys = [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count', 'punctuation_count',
                'has_commit_type', 'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords',
                'has_technical_keywords', 'has_ui_keywords', 'has_testing_keywords',
                'avg_word_length', 'max_word_length', 'unique_word_ratio'
            ]
            
            # Add sentiment features if available
            if 'sentiment_polarity' in enhanced_features:
                feature_keys.extend(['sentiment_polarity', 'sentiment_subjectivity'])
            
            for key in feature_keys:
                val = enhanced_features.get(key, 0)
                if isinstance(val, bool):
                    val = float(val)
                elif isinstance(val, str):
                    val = 1.0 if val == 'positive' else (-1.0 if val == 'negative' else 0.0)
                feature_values.append(float(val))
            
            enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
            
            # Process metadata - extract from nested metadata dict
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': metadata.get('files_mentioned', []),  # Use files_mentioned as proxy
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1,   # Default value
                'is_merge': False,  # Default value
                'commit_size': 'medium',  # Default value
                'message_length': metadata.get('message_length', len(commit_message)),
                'word_count': metadata.get('word_count', len(commit_message.split())),
                'has_scope': metadata.get('has_scope', False),
                'is_conventional': metadata.get('is_conventional', False),
                'has_breaking': metadata.get('has_breaking', False)
            }
            
            # Create basic metadata features manually for compatibility
            files_changed_count = len(metadata_dict['files_changed']) if isinstance(metadata_dict['files_changed'], list) else 1
            metadata_features = torch.tensor([
                float(files_changed_count),
                float(metadata_dict.get('insertions', 0)),
                float(metadata_dict.get('deletions', 0)),
                float(metadata_dict.get('hour_of_day', 12) / 24.0),
                float(metadata_dict.get('day_of_week', 1) / 7.0),
                float(metadata_dict.get('is_merge', False)),
                1.0 if metadata_dict.get('commit_size') == 'small' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'medium' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'large' else 0.0,
                hash(metadata_dict.get('author', 'unknown')) % 1000 / 1000.0  # Simple author encoding
            ], dtype=torch.float32)
            
            # Labels - convert string labels to numeric
            def label_to_numeric(label_str):
                if label_str in ['low', 'simple']:
                    return 0
                elif label_str in ['medium', 'moderate']:
                    return 1
                elif label_str in ['high', 'complex']:
                    return 2
                else:
                    return 0  # Default to low
            
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'enhanced_text_features': enhanced_text_features,
                'metadata_features': metadata_features,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'enhanced_text_features': torch.zeros(18, dtype=torch.float32),  # Adjusted size
                'metadata_features': torch.zeros(10, dtype=torch.float32),
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    enhanced_text_features = torch.stack([item['enhanced_text_features'] for item in batch])
    metadata_features = torch.stack([item['metadata_features'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'enhanced_text_features': enhanced_text_features,
        'metadata_features': metadata_features,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return

    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)

    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if all(isinstance(v, dict) for v in full_data.values()) else [full_data]
        else:
            all_data = full_data
        
        # Split data manually if not pre-split
        train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42, stratify=None)

    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract text data for vocabulary building from all samples
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            # Use 'text' field for new data format, fallback to 'commit_message' for old format
            texts.append(sample.get('text', sample.get('commit_message', '')))
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            logger.warning(f"Unexpected sample format: {type(sample)}")
            continue

    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default
                'day_of_week': 1,   # Default
                'is_merge': False,  # Default
                'commit_size': 'medium'  # Default
            })
        else:
            # Fallback for old format
            metadata_samples.append({
                'author': sample.get('author', 'unknown'),
                'files_changed': sample.get('files_changed', 1),
                'insertions': sample.get('insertions', 0),
                'deletions': sample.get('deletions', 0),
                'hour_of_day': sample.get('hour_of_day', 12),
                'day_of_week': sample.get('day_of_week', 1),
                'is_merge': sample.get('is_merge', False),
                'commit_size': sample.get('commit_size', 'medium')
            })
    
    metadata_processor.fit(metadata_samples)

    # Data is already split or was split above
    logger.info("Using data splits...")
    logger.info(f"Final - Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor, device)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor, device)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    try:
        from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    except ImportError as e:
        logger.error(f"Could not import MultiModalFusionNetwork: {e}")
        return
    
    # Get enhanced feature dimensions
    sample_batch = next(iter(train_loader))
    enhanced_text_feature_dim = sample_batch['enhanced_text_features'].shape[1]
    metadata_feature_dim = sample_batch['metadata_features'].shape[1]
    
    logger.info(f"Enhanced text features dimension: {enhanced_text_feature_dim}")
    logger.info(f"Metadata features dimension: {metadata_feature_dim}")
    
    # Model configuration using the new config-based approach
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.get_vocab_size(),
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'],
            'embedding_dim': 64,
            'hidden_dim': metadata_feature_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Setup training
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Training parameters
    epochs = 50
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    # Training history
    train_history = {
        'train_loss': [],
        'val_loss': [],
        'train_accuracy': [],
        'val_accuracy': [],
        'learning_rate': []
    }
    
    # Create output directory
    output_dir = os.path.join(current_dir, 'trained_models', 'enhanced_multimodal_fusion_100k')
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("Starting training loop...")
    
    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                enhanced_text_features = batch['enhanced_text_features'].to(device)
                metadata_features = batch['metadata_features'].to(device)
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                
                # Prepare metadata input as dict for the model - combine enhanced features with basic metadata
                combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                metadata_input = {
                    'numerical_features': combined_features,
                    'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)  # Dummy author
                }
                
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for all tasks
                total_loss = 0
                correct_predictions = 0
                total_predictions = 0
                
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                for task_idx, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_output = outputs[task_name]
                        task_labels = labels[:, task_idx]
                        task_loss = criterion(task_output, task_labels)
                        total_loss += task_loss
                        
                        # Calculate accuracy
                        _, predicted = torch.max(task_output.data, 1)
                        correct_predictions += (predicted == task_labels).sum().item()
                        total_predictions += task_labels.size(0)
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_correct += correct_predictions
                train_total += total_predictions
                
                if batch_idx % 100 == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    logger.info(f"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, "
                              f"Loss: {total_loss.item():.4f}, LR: {current_lr:.2e}")
                    
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    text_encoded = batch['text_encoded'].to(device)
                    enhanced_text_features = batch['enhanced_text_features'].to(device)
                    metadata_features = batch['metadata_features'].to(device)
                    labels = batch['labels'].to(device)
                    
                    # Prepare metadata input as dict for the model
                    combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                    metadata_input = {
                        'numerical_features': combined_features,
                        'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)
                    }
                    
                    outputs = model(text_encoded, metadata_input)
                    
                    total_loss = 0
                    correct_predictions = 0
                    total_predictions = 0
                    
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    for task_idx, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_output = outputs[task_name]
                            task_labels = labels[:, task_idx]
                            task_loss = criterion(task_output, task_labels)
                            total_loss += task_loss
                            
                            _, predicted = torch.max(task_output.data, 1)
                            correct_predictions += (predicted == task_labels).sum().item()
                            total_predictions += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_correct += correct_predictions
                    val_total += total_predictions
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate averages
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_accuracy = train_correct / max(train_total, 1)
        val_accuracy = val_correct / max(val_total, 1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # Update history
        train_history['train_loss'].append(avg_train_loss)
        train_history['val_loss'].append(avg_val_loss)
        train_history['train_accuracy'].append(train_accuracy)
        train_history['val_accuracy'].append(val_accuracy)
        train_history['learning_rate'].append(current_lr)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{epochs} - "
                   f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                   f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
        
        # Learning rate scheduling
        scheduler.step(avg_val_loss)
        
        # Early stopping and model saving
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # Save best model
            model_path = os.path.join(output_dir, 'best_enhanced_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'train_history': train_history,
                'text_processor_vocab': text_processor.vocab,
                'model_config': model_config
            }, model_path)
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            logger.info(f"Early stopping triggered after {patience} epochs without improvement")
            break
    
    # Save final training history
    history_path = os.path.join(output_dir, 'enhanced_training_history.json')
    with open(history_path, 'w') as f:
        json.dump(train_history, f, indent=2)
    
    logger.info("Enhanced training completed successfully!")
    logger.info(f"Models and history saved to: {output_dir}")
    
    return model, train_history

if __name__ == "__main__":
    try:
        model, history = train_enhanced_100k_model()
        print("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_enhanced_100k_multimodal_fusion_final.py
```py
#!/usr/bin/env python3
"""
Enhanced 100K Multimodal Fusion Training Script - Final Version
This script handles the new data format structure and enhanced text processing.
"""

import os
import sys
import json
import logging
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from datetime import datetime

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training_final.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def label_to_numeric(label_str):
    """Convert string labels to numeric values"""
    if isinstance(label_str, str):
        label_str = label_str.lower().strip()
        if label_str in ['low', 'simple']:
            return 0
        elif label_str in ['medium', 'moderate']:
            return 1
        elif label_str in ['high', 'complex']:
            return 2
        else:
            return 0  # Default to low
    return 0

class Enhanced100KDataset(Dataset):
    """Enhanced dataset for 100K samples with new data format"""
    
    def __init__(self, data, text_processor, metadata_processor):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        try:
            sample = self.data[idx]
            
            # Extract text - handle new format
            text = sample.get('text', sample.get('commit_message', ''))
            if not text:
                text = "Empty commit message"
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode(text)
            
            # Extract enhanced text features if available
            if hasattr(self.text_processor, 'extract_enhanced_features'):
                try:
                    enhanced_features = self.text_processor.extract_enhanced_features(text)
                    feature_values = []
                    for feature_name, feature_value in enhanced_features.items():
                        if isinstance(feature_value, (list, np.ndarray)):
                            feature_values.extend(feature_value)
                        else:
                            feature_values.append(float(feature_value))
                    
                    # Ensure we have exactly 18 features
                    while len(feature_values) < 18:
                        feature_values.append(0.0)
                    feature_values = feature_values[:18]
                    
                    enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
                except Exception as e:
                    logger.warning(f"Error extracting enhanced features: {e}")
                    enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            else:
                enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            
            # Extract metadata from new structure
            metadata = sample.get('metadata', {})
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format  
                'hour_of_day': 12,  # Default value
                'day_of_week': 1   # Default value
            }
            
            # Combine base metadata features with enhanced text features
            base_metadata = self.metadata_processor.process(metadata_dict)
            enhanced_values = enhanced_text_features.tolist() if len(enhanced_text_features.shape) > 0 else [0.0] * 18
            
            # Combine: base metadata (5 features) + enhanced text features (18 features) = 23 total
            combined_numerical = base_metadata['numerical_features'].tolist() + enhanced_values
            
            metadata_input = {
                'numerical_features': torch.tensor(combined_numerical, dtype=torch.float32),
                'author': base_metadata['author']
            }
            
            # Extract labels from new structure
            labels = sample.get('labels', {})
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'metadata_input': metadata_input,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'metadata_input': {
                    'numerical_features': torch.zeros(23, dtype=torch.float32),  # 5 base + 18 enhanced
                    'author': torch.tensor(0, dtype=torch.long)
                },
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    
    # Handle metadata input dict
    metadata_input = {
        'numerical_features': torch.stack([item['metadata_input']['numerical_features'] for item in batch]),
        'author': torch.stack([item['metadata_input']['author'] for item in batch])
    }
    
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'metadata_input': metadata_input,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return
    
    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)
    
    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if isinstance(list(full_data.values())[0], dict) else full_data
        else:
            all_data = full_data
        
        # Split data if not already split
        split_idx = int(0.8 * len(all_data))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
    
    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract texts for vocabulary building
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            text = sample.get('text', sample.get('commit_message', ''))
            if text:
                texts.append(text)
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            continue
    
    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1    # Default value
            })
    
    if metadata_samples:
        logger.info("Fitting metadata processor...")
        metadata_processor.fit(metadata_samples)
    else:
        logger.warning("No metadata samples found for fitting")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    
    # Calculate dimensions
    sample_batch = next(iter(train_loader))
    text_dim = 64  # LSTM hidden dimension (fixed from model config)
    metadata_dim = sample_batch['metadata_input']['numerical_features'].shape[-1]
    
    logger.info(f"Text features dimension: {text_dim}")
    logger.info(f"Metadata features dimension: {metadata_dim}")
    
    # Model configuration matching the expected format
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.vocab_size,
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'] + 
                                [f'enhanced_feature_{i}' for i in range(18)],
            'hidden_dim': metadata_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    # Loss function
    criterion = nn.CrossEntropyLoss()
    
    # Training loop
    num_epochs = 10
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for each task
                total_loss = 0
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                
                for i, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_logits = outputs[task_name]
                        task_labels = labels[:, i]
                        task_loss = criterion(task_logits, task_labels)
                        total_loss += task_loss
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_batches += 1
                
                # Log progress
                if batch_idx % 100 == 0:
                    logger.info(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {total_loss.item():.4f}")
                
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        avg_train_loss = train_loss / train_batches if train_batches > 0 else float('inf')
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                try:
                    # Move batch to device
                    text_encoded = batch['text_encoded'].to(device)
                    metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                    labels = batch['labels'].to(device)
                    
                    # Forward pass
                    outputs = model(text_encoded, metadata_input)
                    
                    # Calculate loss
                    total_loss = 0
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    
                    for i, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_logits = outputs[task_name]
                            task_labels = labels[:, i]
                            task_loss = criterion(task_logits, task_labels)
                            total_loss += task_loss
                            
                            # Calculate accuracy for this task
                            predicted = torch.argmax(task_logits, dim=1)
                            val_correct += (predicted == task_labels).sum().item()
                            val_total += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch {batch_idx}: {e}")
                    continue
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        val_accuracy = val_correct / val_total if val_total > 0 else 0.0
        
        # Update scheduler
        scheduler.step(avg_val_loss)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{num_epochs}")
        logger.info(f"Train Loss: {avg_train_loss:.4f}")
        logger.info(f"Val Loss: {avg_val_loss:.4f}")
        logger.info(f"Val Accuracy: {val_accuracy:.4f}")
        logger.info(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model_path = os.path.join(current_dir, 'models', 'enhanced_100k_multimodal_fusion_best.pth')
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'model_config': model_config,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }, model_path)
            
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        
        logger.info("-" * 80)
    
    logger.info("Training completed!")
    logger.info(f"Best validation loss: {best_val_loss:.4f}")
    
    return model, text_processor, metadata_processor

if __name__ == "__main__":
    try:
        model, text_processor, metadata_processor = train_enhanced_100k_model()
        logger.info("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_han_github.py
```py
#!/usr/bin/env python3
"""
Train HAN Model với GitHub Commits Dataset
Script đơn giản để train mô hình HAN với dữ liệu từ GitHub commits
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter
import re

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class SimpleHANModel(nn.Module):
    """
    Simplified Hierarchical Attention Network for commit classification
    """
    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, num_classes=None):
        super(SimpleHANModel, self).__init__()
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # Word-level LSTM
        self.word_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # Word-level attention
        self.word_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Sentence-level LSTM
        self.sentence_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)
        
        # Sentence-level attention
        self.sentence_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Multi-task classification heads
        self.classifiers = nn.ModuleDict()
        if num_classes:
            for task, num_class in num_classes.items():
                self.classifiers[task] = nn.Linear(hidden_dim * 2, num_class)
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, input_ids, attention_mask=None):
        batch_size, max_sentences, max_words = input_ids.size()
        
        # Reshape for word-level processing
        input_ids = input_ids.view(-1, max_words)  # (batch_size * max_sentences, max_words)
        
        # Word embeddings
        embedded = self.embedding(input_ids)  # (batch_size * max_sentences, max_words, embed_dim)
        
        # Word-level LSTM
        word_output, _ = self.word_lstm(embedded)  # (batch_size * max_sentences, max_words, hidden_dim * 2)
        
        # Word-level attention
        word_attention_weights = torch.softmax(self.word_attention(word_output), dim=1)
        sentence_vectors = torch.sum(word_attention_weights * word_output, dim=1)  # (batch_size * max_sentences, hidden_dim * 2)
        
        # Reshape back to sentence level
        sentence_vectors = sentence_vectors.view(batch_size, max_sentences, -1)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level LSTM
        sentence_output, _ = self.sentence_lstm(sentence_vectors)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level attention
        sentence_attention_weights = torch.softmax(self.sentence_attention(sentence_output), dim=1)
        document_vector = torch.sum(sentence_attention_weights * sentence_output, dim=1)  # (batch_size, hidden_dim * 2)
        
        document_vector = self.dropout(document_vector)
        
        # Multi-task outputs
        outputs = {}
        for task, classifier in self.classifiers.items():
            outputs[task] = classifier(document_vector)
        
        return outputs

class CommitDataset(Dataset):
    """Dataset class for commit messages"""
    
    def __init__(self, texts, labels, tokenizer, max_sentences=10, max_words=50):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_sentences = max_sentences
        self.max_words = max_words
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        labels = self.labels[idx]
        
        # Tokenize text to sentences and words
        input_ids = self.tokenizer.encode_text(text, self.max_sentences, self.max_words)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': labels  # This will be a dictionary
        }

class SimpleTokenizer:
    """Simple tokenizer for commit messages"""
    
    def __init__(self, vocab_size=10000):
        self.vocab_size = vocab_size
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}
        self.word_counts = Counter()
        
    def fit(self, texts):
        """Build vocabulary from texts"""
        print("🔤 Building vocabulary...")
        
        for text in texts:
            # Split into sentences
            sentences = self.split_sentences(text)
            for sentence in sentences:
                words = self.tokenize_words(sentence)
                self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 2)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 2  # Start from 2 (after PAD and UNK)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"✅ Vocabulary built with {len(self.word_to_idx)} words")
        
    def split_sentences(self, text):
        """Split text into sentences"""
        # Simple sentence splitting for commit messages
        sentences = re.split(r'[.!?;]|\\n', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences if sentences else [text]
    
    def tokenize_words(self, sentence):
        """Tokenize sentence into words"""
        # Simple word tokenization
        words = re.findall(r'\b\w+\b', sentence.lower())
        return words
    
    def encode_text(self, text, max_sentences, max_words):
        """Encode text to token ids"""
        sentences = self.split_sentences(text)
        
        # Pad or truncate sentences
        if len(sentences) > max_sentences:
            sentences = sentences[:max_sentences]
        while len(sentences) < max_sentences:
            sentences.append("")
        
        encoded_sentences = []
        for sentence in sentences:
            words = self.tokenize_words(sentence)
            
            # Convert words to indices
            word_ids = []
            for word in words:
                word_ids.append(self.word_to_idx.get(word, 1))  # 1 is UNK
            
            # Pad or truncate words
            if len(word_ids) > max_words:
                word_ids = word_ids[:max_words]
            while len(word_ids) < max_words:
                word_ids.append(0)  # 0 is PAD
            
            encoded_sentences.append(word_ids)
        
        return encoded_sentences

def collate_fn(batch):
    """Custom collate function for DataLoader"""
    input_ids = torch.stack([item['input_ids'] for item in batch])
    labels = [item['labels'] for item in batch]  # Keep as list of dicts
    
    return {
        'input_ids': input_ids,
        'labels': labels
    }

def load_github_dataset(data_file):
    """Load GitHub commits dataset"""
    print(f"📖 Loading dataset: {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'data' not in data:
        raise ValueError("Invalid dataset format: missing 'data' field")
    
    samples = data['data']
    print(f"📊 Loaded {len(samples)} samples")
    
    # Extract texts and labels
    texts = []
    labels = []
    
    for sample in samples:
        texts.append(sample['text'])
        labels.append(sample['labels'])
    
    return texts, labels, data.get('metadata', {})

def prepare_label_encoders(labels):
    """Prepare label encoders for multi-task classification"""
    print("🏷️  Preparing label encoders...")
    
    # Get all unique labels for each task
    label_sets = {}
    for label_dict in labels:
        for task, label in label_dict.items():
            if task not in label_sets:
                label_sets[task] = set()
            label_sets[task].add(label)
    
    # Create mappings
    label_encoders = {}
    num_classes = {}
    
    for task, label_set in label_sets.items():
        sorted_labels = sorted(list(label_set))
        label_encoders[task] = {label: idx for idx, label in enumerate(sorted_labels)}
        num_classes[task] = len(sorted_labels)
        
        print(f"  {task}: {len(sorted_labels)} classes -> {sorted_labels}")
    
    return label_encoders, num_classes

def encode_labels(labels, label_encoders):
    """Encode labels using label encoders"""
    encoded_labels = []
    
    for label_dict in labels:
        encoded_dict = {}
        for task, label in label_dict.items():
            if task in label_encoders:
                encoded_dict[task] = label_encoders[task][label]
        encoded_labels.append(encoded_dict)
    
    return encoded_labels

def train_epoch(model, dataloader, optimizers, criteria, device, scaler=None, use_amp=False):
    """Train for one epoch with GPU optimizations and mixed precision"""
    model.train()
    total_losses = {task: 0.0 for task in criteria.keys()}
    total_loss = 0.0
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        # Move data to device with non_blocking for better GPU utilization
        input_ids = batch['input_ids'].to(device)
        batch_labels = batch['labels']
        
        # Clear gradients
        for optimizer in optimizers.values():
            optimizer.zero_grad()
        
        # Forward pass with mixed precision
        if use_amp and scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(input_ids)
                
                # Calculate losses for each task
                batch_losses = {}
                for task, criterion in criteria.items():
                    task_labels = []
                    for label_dict in batch_labels:
                        if task in label_dict:
                            task_labels.append(label_dict[task])
                    
                    if task_labels:
                        task_labels_tensor = torch.tensor(task_labels, device=device)
                        task_loss = criterion(outputs[task], task_labels_tensor)
                        batch_losses[task] = task_loss
                        total_losses[task] += task_loss.item()
                
                if batch_losses:
                    combined_loss = sum(batch_losses.values()) / len(batch_losses)
                    total_loss += combined_loss.item()
                    num_batches += 1
            
            # Backward pass with mixed precision
            if batch_losses:
                scaler.scale(combined_loss).backward()
                
                # Gradient clipping
                scaler.unscale_(list(optimizers.values())[0])  # Unscale for clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    scaler.step(optimizer)
                scaler.update()
        else:
            # Regular forward pass
            outputs = model(input_ids)
              # Calculate losses for each task
            batch_losses = {}
            for task, criterion in criteria.items():
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    batch_losses[task] = task_loss
                    total_losses[task] += task_loss.item()
            
            # Combined loss
            if batch_losses:
                combined_loss = sum(batch_losses.values()) / len(batch_losses)
                total_loss += combined_loss.item()
                num_batches += 1
                
                # Backward pass
                combined_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    optimizer.step()
        
        # Memory cleanup every 50 batches on GPU
        if device.type == 'cuda' and batch_idx % 50 == 0:
            torch.cuda.empty_cache()
    
    # Calculate average losses
    if num_batches > 0:
        avg_losses = {task: loss / num_batches for task, loss in total_losses.items()}
        avg_total_loss = total_loss / num_batches
    else:
        avg_losses = {task: 0.0 for task in total_losses.keys()}
        avg_total_loss = 0.0
    
    return avg_losses, avg_total_loss

def evaluate_model(model, dataloader, criteria, device):
    """Evaluate model with GPU optimizations"""
    model.eval()
    total_losses = {task: 0.0 for task in criteria.keys()}
    predictions = {task: [] for task in criteria.keys()}
    true_labels = {task: [] for task in criteria.keys()}
    num_batches = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # Move data to device with non_blocking
            input_ids = batch['input_ids'].to(device)
            batch_labels = batch['labels']
            
            # Forward pass
            outputs = model(input_ids)
            
            # Calculate losses and collect predictions
            for task, criterion in criteria.items():                # Extract task labels from batch
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:  # Only if we have labels for this task
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    total_losses[task] += task_loss.item()
                    
                    # Predictions
                    _, predicted = torch.max(outputs[task], 1)
                    predictions[task].extend(predicted.cpu().numpy())
                    true_labels[task].extend(task_labels_tensor.cpu().numpy())
            
            num_batches += 1
            
            # Memory cleanup every 50 batches on GPU
            if device.type == 'cuda' and batch_idx % 50 == 0:
                torch.cuda.empty_cache()
    
    # Calculate metrics
    metrics = {}
    for task in criteria.keys():
        if predictions[task] and num_batches > 0:
            accuracy = accuracy_score(true_labels[task], predictions[task])
            metrics[task] = {
                'loss': total_losses[task] / num_batches,
                'accuracy': accuracy
            }
    
    return metrics

def main():
    """Main training function"""
    print("🚀 HAN GITHUB COMMITS TRAINER")
    print("="*60)
    
    # GPU Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"🔧 Device: {device}")
    
    if torch.cuda.is_available():
        print(f"🎮 GPU: {torch.cuda.get_device_name(0)}")
        print(f"🔥 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        # Clear GPU cache
        torch.cuda.empty_cache()
    else:
        print("⚠️  CUDA not available, using CPU")
    
    # Paths
    data_file = Path(__file__).parent / "training_data" / "github_commits_training_data.json"
    model_dir = Path(__file__).parent / "models" / "han_github_model"
    log_dir = Path(__file__).parent / "training_logs"
    
    # Create directories
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load dataset
    if not data_file.exists():
        print(f"❌ Dataset not found: {data_file}")
        print("   Please run: python download_github_commits.py")
        return
    
    texts, labels, metadata = load_github_dataset(data_file)
    
    # Prepare labels
    label_encoders, num_classes = prepare_label_encoders(labels)
    encoded_labels = encode_labels(labels, label_encoders)
    
    # Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, encoded_labels, test_size=0.2, random_state=42
    )
    
    print(f"📊 Train samples: {len(train_texts)}")
    print(f"📊 Val samples: {len(val_texts)}")
    
    # Build tokenizer
    tokenizer = SimpleTokenizer(vocab_size=5000)
    tokenizer.fit(train_texts)
      # Create datasets
    train_dataset = CommitDataset(train_texts, train_labels, tokenizer)
    val_dataset = CommitDataset(val_texts, val_labels, tokenizer)
    
    # GPU optimized batch size
    if device.type == 'cuda':
        # Larger batch size for GPU
        batch_size = 32
        num_workers = 4  # More workers for GPU
        pin_memory = True
    else:
        # Smaller batch size for CPU
        batch_size = 16
        num_workers = 2
        pin_memory = False
    
    print(f"🔧 Batch size: {batch_size}")
    print(f"👥 Num workers: {num_workers}")
    
    # Create dataloaders with GPU optimizations
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
      # Create model with GPU optimizations
    model = SimpleHANModel(
        vocab_size=len(tokenizer.word_to_idx),
        embed_dim=100,
        hidden_dim=128,
        num_classes=num_classes
    ).to(device)
    
    # Enable mixed precision for GPU if available
    if device.type == 'cuda':
        scaler = torch.cuda.amp.GradScaler()
        use_amp = True
        print("🚀 Mixed precision training enabled")
    else:
        scaler = None
        use_amp = False
    
    print(f"🤖 Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPU memory optimization
    if device.type == 'cuda':
        print(f"📊 GPU Memory before training: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    
    # Setup training with optimized learning rates for GPU
    criteria = {}
    optimizers = {}
    schedulers = {}
    
    # Higher learning rate for GPU training
    base_lr = 0.002 if device.type == 'cuda' else 0.001
    
    for task in num_classes.keys():
        criteria[task] = nn.CrossEntropyLoss()
        optimizers[task] = optim.AdamW(  # AdamW is often better than Adam
            list(model.classifiers[task].parameters()) + 
            list(model.embedding.parameters()) +
            list(model.word_lstm.parameters()) +
            list(model.sentence_lstm.parameters()) +
            list(model.word_attention.parameters()) +
            list(model.sentence_attention.parameters()),
            lr=base_lr,
            weight_decay=1e-4  # L2 regularization
        )        # Add learning rate scheduler
        schedulers[task] = optim.lr_scheduler.ReduceLROnPlateau(
            optimizers[task], 
            mode='min', 
            factor=0.5, 
            patience=3
        )
      # Training loop with GPU monitoring
    num_epochs = 20
    best_val_accuracy = 0.0
    
    log_file = log_dir / f"han_github_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    print(f"\n🎯 Starting training for {num_epochs} epochs...")
    
    # Training start time
    import time
    training_start_time = time.time()
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        print(f"\n📅 Epoch {epoch+1}/{num_epochs}")
        
        # GPU memory monitoring
        if device.type == 'cuda':
            print(f"📊 GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        
        # Train with enhanced function
        train_losses, train_total_loss = train_epoch(
            model, train_loader, optimizers, criteria, device, scaler, use_amp
        )
        
        # Validate
        val_metrics = evaluate_model(model, val_loader, criteria, device)
        
        # Update learning rate schedulers
        avg_val_loss = 0.0
        if val_metrics:
            for task, metrics in val_metrics.items():
                if task in schedulers:
                    schedulers[task].step(metrics['loss'])
                avg_val_loss += metrics['loss']
            avg_val_loss /= len(val_metrics)
        
        # Calculate epoch time
        epoch_time = time.time() - epoch_start_time
        
        # Log results
        print(f"  ⏱️  Epoch time: {epoch_time:.1f}s")
        print(f"  📈 Train Loss: {train_total_loss:.4f}")
        for task, loss in train_losses.items():
            print(f"    {task}: {loss:.4f}")
        
        print(f"  📊 Val Metrics:")
        val_accuracy_sum = 0.0
        for task, metrics in val_metrics.items():
            print(f"    {task}: Loss={metrics['loss']:.4f}, Acc={metrics['accuracy']:.4f}")
            val_accuracy_sum += metrics['accuracy']
        
        avg_val_accuracy = val_accuracy_sum / len(val_metrics) if val_metrics else 0.0
        print(f"  🎯 Avg Val Accuracy: {avg_val_accuracy:.4f}")
        
        # GPU memory cleanup
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            print(f"  🧹 GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
          # Save log with enhanced information
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"\nEpoch {epoch+1}/{num_epochs}\n")
            f.write(f"Epoch Time: {epoch_time:.1f}s\n")
            f.write(f"Train Loss: {train_total_loss:.4f}\n")
            for task, loss in train_losses.items():
                f.write(f"  {task} Train Loss: {loss:.4f}\n")
            f.write(f"Val Metrics: {val_metrics}\n")
            f.write(f"Avg Val Accuracy: {avg_val_accuracy:.4f}\n")
            if device.type == 'cuda':
                f.write(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\n")
            f.write("-" * 50 + "\n")
        
        # Save best model with enhanced information
        if avg_val_accuracy > best_val_accuracy:
            best_val_accuracy = avg_val_accuracy
            
            # Save model with comprehensive information
            model_save_dict = {
                'model_state_dict': model.state_dict(),
                'tokenizer': tokenizer,
                'label_encoders': label_encoders,
                'num_classes': num_classes,
                'metadata': metadata,
                'epoch': epoch + 1,
                'val_accuracy': avg_val_accuracy,
                'train_loss': train_total_loss,
                'device': str(device),
                'batch_size': batch_size,
                'learning_rate': base_lr,
                'model_params': sum(p.numel() for p in model.parameters()),
                'training_config': {
                    'use_amp': use_amp,
                    'vocab_size': len(tokenizer.word_to_idx),
                    'embed_dim': 100,
                    'hidden_dim': 128,                    'max_sentences': 10,
                    'max_words': 50
                }
            }
            
            torch.save(model_save_dict, model_dir / 'best_model.pth')
            
            print(f"  💾 Saved best model (accuracy: {avg_val_accuracy:.4f})")
    
    # Training completion summary
    total_training_time = time.time() - training_start_time
    print(f"\n🎉 Training completed!")
    print(f"⏱️  Total training time: {total_training_time/60:.1f} minutes")
    print(f"📊 Best validation accuracy: {best_val_accuracy:.4f}")
    print(f"💾 Model saved to: {model_dir}")
    print(f"📝 Logs saved to: {log_file}")
    
    if device.type == 'cuda':
        print(f"🎮 GPU training completed successfully")
        print(f"📊 Final GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

```

### backend\ai\__init__.py
```py

```

### backend\ai\multimodal_fusion\__init__.py
```py
"""
Multi-Modal Fusion Network for Commit Analysis
Mô hình kết hợp thông tin văn bản và metadata để phân tích commit
"""

__version__ = "1.0.0"
__author__ = "AI Team"

from .models.multimodal_fusion import MultiModalFusionNetwork
from .data_preprocessing.text_processor import TextProcessor
from .data_preprocessing.metadata_processor import MetadataProcessor
from .training.multitask_trainer import MultiTaskTrainer

__all__ = [
    "MultiModalFusionNetwork",
    "TextProcessor", 
    "MetadataProcessor",
    "MultiTaskTrainer"
]

```

### backend\ai\multimodal_fusion\data\synthetic_generator.py
```py
"""
Data Generation for Multi-Modal Fusion Network
Tạo synthetic GitHub commit data với realistic patterns
"""

import random
import numpy as np
import pandas as pd
import torch
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import string
from collections import defaultdict
import re


class GitHubDataGenerator:
    """
    Generator cho synthetic GitHub commit data với metadata patterns
    """
    
    def __init__(self, seed: int = 42):
        """
        Args:
            seed: Random seed for reproducibility
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # Pre-defined patterns
        self.commit_patterns = self._init_commit_patterns()
        self.file_types = self._init_file_types()
        self.authors = self._init_authors()
        self.programming_words = self._init_programming_words()
        
    def _init_commit_patterns(self) -> Dict[str, List[str]]:
        """Initialize commit message patterns"""
        return {
            'fix': [
                "Fix bug in {component}",
                "Fixed {issue} causing {problem}",
                "Bugfix: {description}",
                "Resolve {issue} in {component}",
                "Patch for {vulnerability}",
                "Hotfix: {critical_issue}",
                "Quick fix for {problem}",
                "Emergency fix: {description}"
            ],
            'feature': [
                "Add {feature} to {component}",
                "Implement {functionality}",
                "Feature: {new_feature}",
                "Introduce {capability}",
                "New: {feature_description}",
                "Enhance {component} with {feature}",
                "Added support for {technology}",
                "Initial implementation of {feature}"
            ],
            'refactor': [
                "Refactor {component} for better {quality}",
                "Code cleanup in {module}",
                "Restructure {component}",
                "Optimize {algorithm} implementation",
                "Improve {aspect} of {component}",
                "Reorganize {module} structure",
                "Clean up {technical_debt}",
                "Modernize {legacy_code}"
            ],
            'update': [
                "Update {dependency} to version {version}",
                "Upgrade {library} dependencies",
                "Bump {package} version",
                "Update documentation for {feature}",
                "Sync with upstream {repository}",
                "Update configuration for {environment}",
                "Refresh {cache} implementation",
                "Update {api} to latest version"
            ],
            'test': [
                "Add tests for {component}",
                "Test coverage for {feature}",
                "Unit tests for {module}",
                "Integration tests for {system}",
                "Fix failing tests in {component}",
                "Improve test stability",
                "Add regression tests for {bug}",
                "Update test data for {scenario}"
            ]
        }
    
    def _init_file_types(self) -> Dict[str, Dict[str, Any]]:
        """Initialize file type patterns"""
        return {
            'python': {
                'extensions': ['.py', '.pyx', '.pyi'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            },
            'javascript': {
                'extensions': ['.js', '.jsx', '.ts', '.tsx'],
                'risk_factor': 0.8,
                'complexity_base': 0.7
            },
            'java': {
                'extensions': ['.java', '.kt', '.scala'],
                'risk_factor': 0.6,
                'complexity_base': 0.5
            },
            'cpp': {
                'extensions': ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp'],
                'risk_factor': 0.9,
                'complexity_base': 0.8
            },
            'config': {
                'extensions': ['.json', '.yml', '.yaml', '.xml', '.toml', '.ini'],
                'risk_factor': 0.3,
                'complexity_base': 0.2
            },
            'documentation': {
                'extensions': ['.md', '.rst', '.txt', '.doc'],
                'risk_factor': 0.1,
                'complexity_base': 0.1
            },
            'web': {
                'extensions': ['.html', '.css', '.scss', '.less'],
                'risk_factor': 0.4,
                'complexity_base': 0.3
            },
            'database': {
                'extensions': ['.sql', '.psql', '.mysql'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            }
        }
    
    def _init_authors(self) -> List[Dict[str, Any]]:
        """Initialize author profiles"""
        return [
            {'name': 'senior_dev_1', 'experience': 0.9, 'reliability': 0.95, 'activity': 0.8},
            {'name': 'senior_dev_2', 'experience': 0.85, 'reliability': 0.9, 'activity': 0.7},
            {'name': 'mid_dev_1', 'experience': 0.6, 'reliability': 0.8, 'activity': 0.9},
            {'name': 'mid_dev_2', 'experience': 0.65, 'reliability': 0.75, 'activity': 0.85},
            {'name': 'mid_dev_3', 'experience': 0.7, 'reliability': 0.82, 'activity': 0.8},
            {'name': 'junior_dev_1', 'experience': 0.3, 'reliability': 0.6, 'activity': 0.95},
            {'name': 'junior_dev_2', 'experience': 0.25, 'reliability': 0.65, 'activity': 0.9},
            {'name': 'junior_dev_3', 'experience': 0.35, 'reliability': 0.7, 'activity': 0.88},
            {'name': 'intern_1', 'experience': 0.1, 'reliability': 0.5, 'activity': 0.7},
            {'name': 'intern_2', 'experience': 0.15, 'reliability': 0.55, 'activity': 0.75}
        ]
    
    def _init_programming_words(self) -> Dict[str, List[str]]:
        """Initialize programming-related words"""
        return {
            'components': [
                'API', 'database', 'frontend', 'backend', 'service', 'module', 'controller',
                'model', 'view', 'router', 'middleware', 'authentication', 'authorization',
                'cache', 'session', 'webhook', 'scheduler', 'queue', 'worker', 'parser',
                'validator', 'serializer', 'repository', 'factory', 'adapter', 'connector'
            ],
            'issues': [
                'memory leak', 'race condition', 'deadlock', 'null pointer', 'buffer overflow',
                'security vulnerability', 'performance issue', 'timeout', 'connection error',
                'validation error', 'parsing error', 'encoding issue', 'permission denied',
                'resource exhaustion', 'infinite loop', 'stack overflow', 'dependency conflict'
            ],
            'features': [
                'real-time notifications', 'user dashboard', 'data analytics', 'file upload',
                'search functionality', 'user authentication', 'payment processing',
                'email integration', 'social login', 'API rate limiting', 'data export',
                'mobile responsiveness', 'dark mode', 'internationalization', 'audit logs'
            ],
            'technologies': [
                'Docker', 'Kubernetes', 'Redis', 'PostgreSQL', 'MongoDB', 'Elasticsearch',
                'RabbitMQ', 'Kafka', 'GraphQL', 'REST API', 'gRPC', 'WebSocket', 'OAuth',
                'JWT', 'TLS', 'HTTPS', 'AWS', 'Azure', 'GCP', 'Terraform', 'Ansible'
            ]
        }
    
    def generate_commit_message(self, commit_type: str, risk_level: float) -> str:
        """
        Generate realistic commit message
        
        Args:
            commit_type: Type of commit (fix, feature, etc.)
            risk_level: Risk level (0-1) to influence message complexity
            
        Returns:
            Generated commit message
        """
        if commit_type not in self.commit_patterns:
            commit_type = random.choice(list(self.commit_patterns.keys()))
        
        template = random.choice(self.commit_patterns[commit_type])
        
        # Fill template with appropriate words
        component = random.choice(self.programming_words['components'])
        issue = random.choice(self.programming_words['issues'])
        feature = random.choice(self.programming_words['features'])
        technology = random.choice(self.programming_words['technologies'])
        
        # Replace placeholders
        message = template.format(
            component=component,
            issue=issue,
            problem=issue,
            feature=feature,
            functionality=feature,
            new_feature=feature,
            capability=feature,
            feature_description=feature,
            technology=technology,
            quality='performance' if risk_level > 0.5 else 'maintainability',
            module=component,
            aspect='security' if risk_level > 0.7 else 'performance',
            technical_debt='legacy code',
            legacy_code='deprecated functions',
            dependency=technology,
            version=f"{random.randint(1,5)}.{random.randint(0,20)}.{random.randint(0,10)}",
            library=technology,
            package=technology,
            environment='production' if risk_level > 0.6 else 'development',
            repository='main',
            cache='Redis',
            api='REST API',
            system=component,
            bug=issue,
            scenario='edge case',
            description=issue,
            vulnerability='SQL injection',
            critical_issue='server crash',
            algorithm='sorting'
        )
          # Add complexity based on risk level
        if risk_level > 0.8:
            suffixes = [
                " - critical security patch",
                " - urgent production fix",
                " - breaking change",
                " - requires migration",
                " - affects multiple services"
            ]
            message += random.choice(suffixes)
        elif risk_level > 0.6:
            suffixes = [
                " - needs testing",
                " - requires review",
                " - performance impact",
                " - config change needed"
            ]
            message += random.choice(suffixes)
        
        return message
    
    def generate_file_changes(self, risk_level: float, complexity_level: float) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        Generate realistic file changes
        
        Args:
            risk_level: Risk level (0-1)
            complexity_level: Complexity level (0-1)
            
        Returns:
            Tuple of (file_data_list, change_stats)
        """
        # Determine number of files based on complexity
        if complexity_level > 0.8:
            num_files = random.randint(8, 25)
        elif complexity_level > 0.6:
            num_files = random.randint(4, 12)
        elif complexity_level > 0.3:
            num_files = random.randint(2, 6)
        else:
            num_files = random.randint(1, 3)
        
        files_data = []
        stats = {'additions': 0, 'deletions': 0, 'modifications': 0}
        
        # Select file types based on risk level
        type_weights = []
        for file_type, info in self.file_types.items():
            weight = info['risk_factor'] if risk_level > 0.5 else (1 - info['risk_factor'])
            type_weights.append((file_type, weight, info))
        
        for i in range(num_files):
            # Select file type
            selected_type = random.choices(
                [t[0] for t in type_weights],
                weights=[t[1] for t in type_weights]
            )[0]
            
            type_info = self.file_types[selected_type]
            extension = random.choice(type_info['extensions'])
            
            # Generate file path
            components = random.choice(self.programming_words['components']).lower()
            file_name = f"{components}_{random.randint(1, 100)}{extension}"
            
            # Create realistic directory structure
            if selected_type == 'python':
                dirs = ['src', 'lib', 'api', 'models', 'views', 'utils']
            elif selected_type == 'javascript':
                dirs = ['src', 'components', 'pages', 'utils', 'services']
            elif selected_type == 'java':
                dirs = ['src/main/java', 'src/test/java']
            elif selected_type == 'config':
                dirs = ['config', 'deploy', 'scripts']
            elif selected_type == 'documentation':
                dirs = ['docs', 'README']
            else:
                dirs = ['src', 'lib', 'assets']
            
            directory = random.choice(dirs)
            file_path = f"{directory}/{file_name}"
            
            # Generate change statistics for this file
            base_changes = int(complexity_level * 200)
            additions = random.randint(1, max(1, base_changes))
            deletions = random.randint(0, max(1, int(additions * 0.7)))
            changes = additions + deletions
            
            # Create file data dict as expected by MetadataProcessor
            file_data = {
                'filename': file_path,
                'additions': additions,
                'deletions': deletions,
                'changes': changes,
                'status': random.choice(['modified', 'added', 'removed']),
                'patch': f"@@ -1,{deletions} +1,{additions} @@"  # Simple patch format
            }
            
            files_data.append(file_data)
            
            stats['additions'] += additions
            stats['deletions'] += deletions
            stats['modifications'] += 1
        
        return files_data, stats
    
    def generate_temporal_features(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate temporal features with realistic patterns
        
        Args:
            risk_level: Risk level affects timing patterns
            
        Returns:
            Dict with temporal features
        """
        # Base time
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        # Generate random timestamp
        time_diff = end_date - start_date
        random_seconds = random.randint(0, int(time_diff.total_seconds()))
        commit_time = start_date + timedelta(seconds=random_seconds)
        
        # Risky commits more likely during off-hours
        if risk_level > 0.7:
            # Late night commits (22:00 - 06:00)
            if random.random() > 0.3:
                hour = random.choice(list(range(22, 24)) + list(range(0, 7)))
                commit_time = commit_time.replace(hour=hour)
        
        weekday = commit_time.weekday()  # 0=Monday, 6=Sunday
        hour = commit_time.hour
        
        # Season encoding
        month = commit_time.month
        if month in [12, 1, 2]:
            season = 0  # Winter
        elif month in [3, 4, 5]:
            season = 1  # Spring
        elif month in [6, 7, 8]:
            season = 2  # Summer
        else:
            season = 3  # Fall
        
        return {
            'timestamp': commit_time,
            'weekday': weekday,
            'hour': hour,
            'season': season,
            'is_weekend': weekday >= 5,
            'is_business_hours': 9 <= hour <= 17,
            'unix_timestamp': int(commit_time.timestamp())
        }
    
    def generate_author_info(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate author information
        
        Args:
            risk_level: Affects author selection
            
        Returns:
            Author information dict
        """
        # Select author based on risk level
        if risk_level > 0.8:
            # High risk - more likely to be junior/intern
            author_pool = [a for a in self.authors if a['experience'] < 0.5]
        elif risk_level > 0.5:
            # Medium risk - mixed experience
            author_pool = [a for a in self.authors if 0.3 <= a['experience'] <= 0.8]
        else:
            # Low risk - more likely to be senior
            author_pool = [a for a in self.authors if a['experience'] > 0.6]
        
        if not author_pool:
            author_pool = self.authors
        
        author = random.choice(author_pool)
        
        return {
            'name': author['name'],
            'experience_level': author['experience'],
            'reliability_score': author['reliability'],
            'activity_score': author['activity'],
            'commits_last_month': int(author['activity'] * 50),
            'avg_commit_size': int((1 - author['experience']) * 100 + 20)
        }
    
    def calculate_labels(self, commit_data: Dict[str, Any]) -> Dict[str, int]:
        """
        Calculate ground truth labels based on generated features
        
        Args:
            commit_data: Generated commit data
            
        Returns:
            Dict with task labels
        """
        # Extract features for label calculation
        risk_factors = []
        complexity_factors = []
        
        # File-based factors
        file_types = commit_data['metadata']['file_types']
        for file_type, info in self.file_types.items():
            if file_type in file_types and file_types[file_type] > 0:
                risk_factors.append(info['risk_factor'] * file_types[file_type])
                complexity_factors.append(info['complexity_base'] * file_types[file_type])
        
        # Author factors
        author_info = commit_data['metadata']['author_info']
        risk_factors.append(1 - author_info['reliability_score'])
        complexity_factors.append(1 - author_info['experience_level'])
        
        # Temporal factors
        temporal = commit_data['metadata']['temporal']
        if not temporal['is_business_hours']:
            risk_factors.append(0.3)
        if temporal['is_weekend']:
            risk_factors.append(0.2)
        
        # Change size factors
        stats = commit_data['metadata']['change_stats']
        total_changes = stats['additions'] + stats['deletions']
        if total_changes > 500:
            risk_factors.append(0.4)
            complexity_factors.append(0.5)
        elif total_changes > 200:
            risk_factors.append(0.2)
            complexity_factors.append(0.3)
        
        # File count factor
        num_files = len(commit_data['metadata']['files'])
        if num_files > 15:
            risk_factors.append(0.4)
            complexity_factors.append(0.4)
        elif num_files > 8:
            risk_factors.append(0.2)
            complexity_factors.append(0.2)
        
        # Calculate final scores
        avg_risk = np.mean(risk_factors) if risk_factors else 0.5
        avg_complexity = np.mean(complexity_factors) if complexity_factors else 0.5
        
        # Generate labels
        labels = {}
        
        # Commit Risk (Binary: 0=Low, 1=High)
        labels['commit_risk'] = 1 if avg_risk > 0.6 else 0
        
        # Complexity (3 classes: 0=Low, 1=Medium, 2=High)
        if avg_complexity > 0.7:
            labels['complexity'] = 2
        elif avg_complexity > 0.4:
            labels['complexity'] = 1
        else:
            labels['complexity'] = 0
        
        # Hotspot Files (Binary: 0=No, 1=Yes)
        # Based on high-risk file types and frequency
        hotspot_score = 0
        for file_type, count in file_types.items():
            if self.file_types[file_type]['risk_factor'] > 0.7:
                hotspot_score += count
        labels['hotspot'] = 1 if hotspot_score > 3 else 0
        
        # Urgent Review (Binary: 0=No, 1=Yes)
        # High risk + (low author reliability OR critical files OR large changes)
        urgent_factors = []
        if avg_risk > 0.7:
            urgent_factors.append(1)
        if author_info['reliability_score'] < 0.7:
            urgent_factors.append(1)
        if total_changes > 300:
            urgent_factors.append(1)
        if any(self.file_types[ft]['risk_factor'] > 0.8 for ft in file_types if file_types[ft] > 0):
            urgent_factors.append(1)
        
        labels['urgent_review'] = 1 if len(urgent_factors) >= 2 else 0
        
        return labels
    
    def generate_single_commit(self, target_risk: Optional[float] = None) -> Dict[str, Any]:
        """
        Generate a single commit with all features and labels
        
        Args:
            target_risk: Target risk level (0-1), if None then random
            
        Returns:
            Complete commit data dict
        """
        # Determine risk level
        if target_risk is None:
            risk_level = random.random()
        else:
            risk_level = max(0.0, min(1.0, target_risk + random.gauss(0, 0.1)))
        
        # Generate complexity level (correlated with risk)
        complexity_level = risk_level + random.gauss(0, 0.2)
        complexity_level = max(0.0, min(1.0, complexity_level))
        
        # Select commit type based on risk
        if risk_level > 0.8:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.5, 0.2, 0.1, 0.1, 0.1]
            )[0]
        elif risk_level > 0.5:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.3, 0.3, 0.2, 0.1, 0.1]
            )[0]
        else:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.1, 0.3, 0.3, 0.2, 0.1]
            )[0]
          # Generate components
        commit_message = self.generate_commit_message(commit_type, risk_level)
        files_data, change_stats = self.generate_file_changes(risk_level, complexity_level)
        temporal_features = self.generate_temporal_features(risk_level)
        author_info = self.generate_author_info(risk_level)
        
        # Count file types
        file_types = defaultdict(int)
        for file_data in files_data:
            filename = file_data['filename']
            for file_type, info in self.file_types.items():
                for ext in info['extensions']:
                    if filename.endswith(ext):
                        file_types[file_type] += 1
                        break
        
        # Create commit data structure
        commit_data = {
            'commit_message': commit_message,
            'metadata': {
                'files': files_data,  # Now this is list of dicts as expected by MetadataProcessor
                'change_stats': change_stats,
                'file_types': dict(file_types),
                'temporal': temporal_features,
                'author_info': author_info,
                'commit_type': commit_type,
                'risk_level': risk_level,
                'complexity_level': complexity_level
            }
        }
        
        # Calculate labels
        labels = self.calculate_labels(commit_data)
        commit_data['labels'] = labels
        
        return commit_data
    
    def generate_dataset(self, num_samples: int, 
                        risk_distribution: Optional[Dict[str, float]] = None,
                        save_path: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Generate complete dataset
        
        Args:
            num_samples: Number of samples to generate
            risk_distribution: Dict with risk level distribution
            save_path: Path to save dataset
            
        Returns:
            List of commit data dicts
        """
        if risk_distribution is None:
            risk_distribution = {
                'low': 0.5,      # 0.0 - 0.3
                'medium': 0.3,   # 0.3 - 0.7
                'high': 0.2      # 0.7 - 1.0
            }
        
        dataset = []
        
        for i in range(num_samples):
            # Select risk level based on distribution
            rand_val = random.random()
            if rand_val < risk_distribution['low']:
                target_risk = random.uniform(0.0, 0.3)
            elif rand_val < risk_distribution['low'] + risk_distribution['medium']:
                target_risk = random.uniform(0.3, 0.7)
            else:
                target_risk = random.uniform(0.7, 1.0)
            
            commit_data = self.generate_single_commit(target_risk)
            commit_data['id'] = i
            dataset.append(commit_data)
            
            if (i + 1) % 1000 == 0:
                print(f"Generated {i + 1}/{num_samples} samples")
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, indent=2, default=str, ensure_ascii=False)
            print(f"Dataset saved to {save_path}")
        
        return dataset
    
    def generate_splits(self, dataset: List[Dict[str, Any]], 
                       split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15),
                       stratify_by: str = 'commit_risk') -> Tuple[List, List, List]:
        """
        Split dataset into train/val/test with stratification
        
        Args:
            dataset: Complete dataset
            split_ratios: (train, val, test) ratios
            stratify_by: Label to stratify by
            
        Returns:
            Tuple of (train, val, test) datasets
        """
        from sklearn.model_selection import train_test_split
        
        # Extract labels for stratification
        if stratify_by in dataset[0]['labels']:
            stratify_labels = [item['labels'][stratify_by] for item in dataset]
        else:
            stratify_labels = None
        
        # First split: train vs (val + test)
        train_data, temp_data = train_test_split(
            dataset, 
            test_size=(1 - split_ratios[0]),
            stratify=stratify_labels,
            random_state=42
        )
        
        # Second split: val vs test
        val_ratio = split_ratios[1] / (split_ratios[1] + split_ratios[2])
        if stratify_labels:
            temp_labels = [item['labels'][stratify_by] for item in temp_data]
        else:
            temp_labels = None
        
        val_data, test_data = train_test_split(
            temp_data,
            test_size=(1 - val_ratio),
            stratify=temp_labels,
            random_state=42
        )
        
        print(f"Dataset splits: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
        
        return train_data, val_data, test_data
    
    def analyze_dataset(self, dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze generated dataset statistics
        
        Args:
            dataset: Generated dataset
            
        Returns:
            Analysis results
        """
        analysis = {
            'total_samples': len(dataset),
            'label_distributions': {},
            'feature_statistics': {},
            'correlations': {}
        }
        
        # Label distributions
        for sample in dataset:
            for task_name, label in sample['labels'].items():
                if task_name not in analysis['label_distributions']:
                    analysis['label_distributions'][task_name] = defaultdict(int)
                analysis['label_distributions'][task_name][label] += 1
        
        # Feature statistics
        risk_levels = [sample['metadata']['risk_level'] for sample in dataset]
        complexity_levels = [sample['metadata']['complexity_level'] for sample in dataset]
        file_counts = [len(sample['metadata']['files']) for sample in dataset]
        change_sizes = [sample['metadata']['change_stats']['additions'] + 
                       sample['metadata']['change_stats']['deletions'] for sample in dataset]
        
        analysis['feature_statistics'] = {
            'risk_level': {
                'mean': np.mean(risk_levels),
                'std': np.std(risk_levels),
                'min': np.min(risk_levels),
                'max': np.max(risk_levels)
            },
            'complexity_level': {
                'mean': np.mean(complexity_levels),
                'std': np.std(complexity_levels),
                'min': np.min(complexity_levels),
                'max': np.max(complexity_levels)
            },
            'file_count': {
                'mean': np.mean(file_counts),
                'std': np.std(file_counts),
                'min': np.min(file_counts),
                'max': np.max(file_counts)
            },
            'change_size': {
                'mean': np.mean(change_sizes),
                'std': np.std(change_sizes),
                'min': np.min(change_sizes),
                'max': np.max(change_sizes)
            }
        }
        
        return analysis


def main():
    """Example usage"""
    generator = GitHubDataGenerator(seed=42)
    
    # Generate small dataset for testing
    print("Generating sample dataset...")
    dataset = generator.generate_dataset(
        num_samples=1000,
        risk_distribution={'low': 0.6, 'medium': 0.3, 'high': 0.1}
    )
    
    # Analyze dataset
    analysis = generator.analyze_dataset(dataset)
    print("\nDataset Analysis:")
    print(f"Total samples: {analysis['total_samples']}")
    print("\nLabel distributions:")
    for task, dist in analysis['label_distributions'].items():
        print(f"  {task}: {dict(dist)}")
    
    print("\nFeature statistics:")
    for feature, stats in analysis['feature_statistics'].items():
        print(f"  {feature}: mean={stats['mean']:.3f}, std={stats['std']:.3f}")
    
    # Show sample
    print(f"\nSample commit:")
    sample = dataset[0]
    print(f"Message: {sample['commit_message']}")
    print(f"Files: {len(sample['metadata']['files'])} files")
    print(f"Changes: +{sample['metadata']['change_stats']['additions']} -{sample['metadata']['change_stats']['deletions']}")
    print(f"Labels: {sample['labels']}")


if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\data_preprocessing\enhanced_text_processor.py
```py
"""
Enhanced Text Processor with NLTK Support
Provides advanced natural language processing capabilities for commit message analysis
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter, defaultdict
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available. Using simple tokenization only.")

# Enhanced NLTK import with more features
try:
    import nltk
    from nltk.corpus import stopwords, wordnet
    from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tag import pos_tag
    from nltk.chunk import ne_chunk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from nltk.corpus import opinion_lexicon
    from nltk.probability import FreqDist
    from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder
    from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures
    NLTK_AVAILABLE = True
    logger.info("NLTK library available with advanced features")
except ImportError:
    NLTK_AVAILABLE = False
    logger.warning("NLTK not available. Using simple text processing.")

# Optional TextBlob for additional sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available.")


class EnhancedTextProcessor:
    """
    Enhanced Text Processor with comprehensive NLTK support
    Provides advanced NLP features for commit message analysis
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_stemming: bool = True,
                 enable_lemmatization: bool = True,
                 enable_pos_tagging: bool = True,
                 enable_sentiment_analysis: bool = True,
                 enable_ngrams: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name for transformers
            enable_stemming: Enable word stemming
            enable_lemmatization: Enable word lemmatization
            enable_pos_tagging: Enable part-of-speech tagging
            enable_sentiment_analysis: Enable sentiment analysis
            enable_ngrams: Enable n-gram extraction
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        
        # NLTK feature flags
        self.enable_stemming = enable_stemming
        self.enable_lemmatization = enable_lemmatization
        self.enable_pos_tagging = enable_pos_tagging
        self.enable_sentiment_analysis = enable_sentiment_analysis
        self.enable_ngrams = enable_ngrams
        
        # Initialize NLTK components
        self._init_nltk_components()
        
        # Initialize model components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_nltk_components(self):
        """Initialize NLTK components and download required data"""
        if not NLTK_AVAILABLE:
            logger.warning("NLTK not available. Advanced text processing features disabled.")
            self.stop_words = self._get_basic_stopwords()
            return
        
        # Download required NLTK data
        required_data = [
            'punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger',
            'maxent_ne_chunker', 'words', 'vader_lexicon', 'opinion_lexicon',
            'omw-1.4'
        ]
        
        for data in required_data:
            try:
                nltk.data.find(f'tokenizers/{data}')
            except LookupError:
                try:
                    nltk.data.find(f'corpora/{data}')
                except LookupError:
                    try:
                        nltk.data.find(f'taggers/{data}')
                    except LookupError:
                        try:
                            nltk.data.find(f'chunkers/{data}')
                        except LookupError:
                            try:
                                nltk.download(data, quiet=True)
                                logger.info(f"Downloaded NLTK data: {data}")
                            except Exception as e:
                                logger.warning(f"Failed to download {data}: {e}")
        
        # Initialize NLTK tools
        try:
            self.stop_words = set(stopwords.words('english'))
            self.stemmer = PorterStemmer() if self.enable_stemming else None
            self.lemmatizer = WordNetLemmatizer() if self.enable_lemmatization else None
            self.sentiment_analyzer = SentimentIntensityAnalyzer() if self.enable_sentiment_analysis else None
            self.tokenizer_nltk = TreebankWordTokenizer()
            
            # Load opinion lexicon for additional sentiment analysis
            try:
                self.positive_words = set(opinion_lexicon.positive())
                self.negative_words = set(opinion_lexicon.negative())
            except Exception as e:
                logger.warning(f"Failed to load opinion lexicon: {e}")
                self.positive_words = set()
                self.negative_words = set()
                
            logger.info("NLTK components initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize NLTK components: {e}")
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Get basic stopwords if NLTK is not available"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',
            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves'
        ])
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
        
        # Additional LSTM-specific features
        self.bigram_counts = Counter()
        self.trigram_counts = Counter()
        self.pos_tag_counts = Counter()
        
    def advanced_tokenize(self, text: str) -> List[str]:
        """
        Advanced tokenization with NLTK features
        """
        if not NLTK_AVAILABLE:
            # Fallback to simple tokenization
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
        
        try:
            # Use TreebankWordTokenizer for better handling of contractions and punctuation
            tokens = self.tokenizer_nltk.tokenize(text)
            
            # Apply stemming or lemmatization
            if self.enable_lemmatization and self.lemmatizer:
                # Get POS tags for better lemmatization
                pos_tags = pos_tag(tokens) if self.enable_pos_tagging else [(token, 'NN') for token in tokens]
                tokens = [self._lemmatize_with_pos(token, pos) for token, pos in pos_tags]
            elif self.enable_stemming and self.stemmer:
                tokens = [self.stemmer.stem(token) for token in tokens]
            
            return tokens
            
        except Exception as e:
            logger.warning(f"Advanced tokenization failed: {e}. Using simple fallback.")
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def _lemmatize_with_pos(self, word: str, pos_tag: str) -> str:
        """
        Lemmatize word with POS tag context
        """
        if not self.lemmatizer:
            return word
            
        # Convert POS tag to WordNet format
        tag_dict = {
            'J': wordnet.ADJ,
            'N': wordnet.NOUN,
            'V': wordnet.VERB,
            'R': wordnet.ADV
        }
        
        wordnet_pos = tag_dict.get(pos_tag[0], wordnet.NOUN)
        return self.lemmatizer.lemmatize(word, wordnet_pos)
    
    def extract_advanced_features(self, text: str) -> Dict[str, any]:
        """
        Extract advanced features using NLTK capabilities
        """
        features = {}
        
        # Basic features
        features.update(self.extract_basic_features(text))
        
        if not NLTK_AVAILABLE:
            return features
        
        try:
            # Tokenize for advanced analysis
            tokens = self.advanced_tokenize(text.lower())
            
            # Sentiment analysis
            if self.enable_sentiment_analysis and self.sentiment_analyzer:
                sentiment_scores = self.sentiment_analyzer.polarity_scores(text)
                features.update({
                    'sentiment_positive': sentiment_scores['pos'],
                    'sentiment_negative': sentiment_scores['neg'],
                    'sentiment_neutral': sentiment_scores['neu'],
                    'sentiment_compound': sentiment_scores['compound']
                })
                
                # TextBlob sentiment as additional feature
                if TEXTBLOB_AVAILABLE:
                    blob = TextBlob(text)
                    features['textblob_polarity'] = blob.sentiment.polarity
                    features['textblob_subjectivity'] = blob.sentiment.subjectivity
                
                # Opinion lexicon features
                positive_count = sum(1 for word in tokens if word in self.positive_words)
                negative_count = sum(1 for word in tokens if word in self.negative_words)
                features['positive_word_count'] = positive_count
                features['negative_word_count'] = negative_count
                features['sentiment_ratio'] = (positive_count - negative_count) / max(len(tokens), 1)
            
            # POS tagging features
            if self.enable_pos_tagging:
                pos_tags = pos_tag(tokens)
                pos_counts = Counter(tag for _, tag in pos_tags)
                
                # Important POS categories for commit analysis
                features['noun_count'] = pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + pos_counts.get('NNP', 0)
                features['verb_count'] = pos_counts.get('VB', 0) + pos_counts.get('VBD', 0) + pos_counts.get('VBG', 0) + pos_counts.get('VBN', 0)
                features['adjective_count'] = pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + pos_counts.get('JJS', 0)
                features['adverb_count'] = pos_counts.get('RB', 0) + pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0)
                
                # POS diversity
                features['pos_diversity'] = len(pos_counts) / max(len(tokens), 1)
            
            # N-gram features
            if self.enable_ngrams and len(tokens) > 1:
                # Bigrams
                bigrams = list(nltk.bigrams(tokens))
                features['unique_bigrams'] = len(set(bigrams))
                features['bigram_ratio'] = len(set(bigrams)) / max(len(bigrams), 1)
                
                # Trigrams (if enough tokens)
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    features['unique_trigrams'] = len(set(trigrams))
                    features['trigram_ratio'] = len(set(trigrams)) / max(len(trigrams), 1)
                else:
                    features['unique_trigrams'] = 0
                    features['trigram_ratio'] = 0
            
            # Lexical diversity
            features['lexical_diversity'] = len(set(tokens)) / max(len(tokens), 1)
            
            # Average word length
            features['avg_word_length'] = np.mean([len(word) for word in tokens]) if tokens else 0
            
        except Exception as e:
            logger.warning(f"Advanced feature extraction failed: {e}")
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """
        Extract basic features (fallback when NLTK not available)
        """
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'has_commit_type', 'commit_type_prefix', 'has_bug_keywords',
                'has_feature_keywords', 'has_doc_keywords', 'positive_sentiment',
                'negative_sentiment', 'urgent_sentiment'
            ]}
        
        # Basic text statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') or text.lower().startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':') or text.lower().startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Enhanced keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve', 'correct', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce', 'enable']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'docs', 'guide', 'manual']
        refactor_keywords = ['refactor', 'restructure', 'reorganize', 'cleanup', 'optimize', 'improve']
        
        text_lower = text.lower()
        features['has_bug_keywords'] = any(keyword in text_lower for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text_lower for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text_lower for keyword in doc_keywords)
        features['has_refactor_keywords'] = any(keyword in text_lower for keyword in refactor_keywords)
        
        # Sentiment indicators (basic)
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success', 'complete', 'finish']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error', 'disable', 'revert']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediate', 'quick']
        
        features['positive_sentiment'] = any(word in text_lower for word in positive_words)
        features['negative_sentiment'] = any(word in text_lower for word in negative_words)
        features['urgent_sentiment'] = any(word in text_lower for word in urgent_words)
        
        return features
    
    def clean_commit_message(self, text: str) -> str:
        """
        Enhanced commit message cleaning
        """
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'merge .* of .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        Build vocabulary with advanced NLTK features
        """
        if self.method != "lstm":
            return
        
        logger.info("🔤 Building enhanced vocabulary with NLTK features...")
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            
            # Filter tokens
            tokens = [token for token in tokens 
                     if token not in self.stop_words 
                     and len(token) > 1 
                     and token.isalpha()]
            
            all_tokens.extend(tokens)
            self.word_counts.update(tokens)
            
            # Collect n-grams if enabled
            if self.enable_ngrams and len(tokens) > 1:
                bigrams = list(nltk.bigrams(tokens))
                self.bigram_counts.update(bigrams)
                
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    self.trigram_counts.update(trigrams)
        
        # Build vocabulary with most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"✅ Enhanced vocabulary built with {len(self.word_to_idx)} words")
        
        # Log vocabulary statistics
        if NLTK_AVAILABLE:
            logger.info(f"📊 Total unique bigrams: {len(self.bigram_counts)}")
            logger.info(f"📊 Total unique trigrams: {len(self.trigram_counts)}")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Enhanced LSTM encoding with NLTK preprocessing
        """
        cleaned_text = self.clean_commit_message(text)
        tokens = self.advanced_tokenize(cleaned_text.lower())
        
        # Filter tokens
        tokens = [token for token in tokens 
                 if token not in self.stop_words 
                 and len(token) > 1 
                 and token.isalpha()]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def get_collocations(self, texts: List[str], n: int = 10) -> Dict[str, List[Tuple]]:
        """
        Extract meaningful collocations from commit messages
        """
        if not NLTK_AVAILABLE or not self.enable_ngrams:
            return {'bigrams': [], 'trigrams': []}
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            tokens = [token for token in tokens 
                     if token not in self.stop_words and len(token) > 1]
            all_tokens.extend(tokens)
        
        try:
            # Bigram collocations
            bigram_finder = BigramCollocationFinder.from_words(all_tokens)
            bigram_finder.apply_freq_filter(3)  # Only bigrams appearing 3+ times
            bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, n)
            
            # Trigram collocations
            trigram_finder = TrigramCollocationFinder.from_words(all_tokens)
            trigram_finder.apply_freq_filter(2)  # Only trigrams appearing 2+ times
            trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, n)
            
            return {'bigrams': bigrams, 'trigrams': trigrams}
            
        except Exception as e:
            logger.warning(f"Collocation extraction failed: {e}")
            return {'bigrams': [], 'trigrams': []}
    
    def fit(self, texts: List[str]) -> 'EnhancedTextProcessor':
        """
        Fit the enhanced text processor to training data
        """
        logger.info("🚀 Fitting enhanced text processor with NLTK features...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
            
            # Extract and log collocations for insights
            collocations = self.get_collocations(texts)
            if collocations['bigrams']:
                logger.info(f"📈 Top bigrams: {collocations['bigrams'][:5]}")
            if collocations['trigrams']:
                logger.info(f"📈 Top trigrams: {collocations['trigrams'][:3]}")
        
        logger.info("✅ Enhanced text processor fitted successfully")
        return self
    
    # Keep all other methods from the original TextProcessor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method (unchanged)"""
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """Get text embeddings (unchanged)"""
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    embedding = outputs.last_hidden_state[:, 0, :]
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts with enhanced features"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        # Extract enhanced features
        for text in texts:
            basic_features = self.extract_basic_features(text)
            if NLTK_AVAILABLE:
                enhanced_features = self.extract_advanced_features(text)
                results['enhanced_features'].append(enhanced_features)
            else:
                results['enhanced_features'].append(basic_features)
            
            results['text_features'].append(basic_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\metadata_processor.py
```py
"""
Metadata Processor for Multi-Modal Fusion Network
Xử lý và chuẩn bị metadata từ GitHub commits
"""

import numpy as np
import pandas as pd
import torch
from typing import List, Dict, Tuple, Optional, Any
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime, timezone
import re
from pathlib import Path

class MetadataProcessor:
    """
    Lớp xử lý metadata cho GitHub commits
    Bao gồm commit stats, file info, author info, timestamp info
    """
    
    def __init__(self, normalize_features: bool = True, 
                 categorical_method: str = "embedding",  # "embedding", "onehot"
                 max_files: int = 50,
                 max_authors: int = 1000):
        """
        Args:
            normalize_features: Có chuẩn hóa features số không
            categorical_method: Phương pháp encode categorical ("embedding", "onehot")
            max_files: Số file tối đa để track
            max_authors: Số author tối đa để track
        """
        self.normalize_features = normalize_features
        self.categorical_method = categorical_method
        self.max_files = max_files
        self.max_authors = max_authors
        
        # Scalers for numerical features
        self.numerical_scaler = StandardScaler()
        self.ratio_scaler = MinMaxScaler()
        
        # Encoders for categorical features
        self.file_type_encoder = LabelEncoder()
        self.author_encoder = LabelEncoder()
        self.file_path_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
        
        # Feature statistics
        self.feature_stats = {}
        self.is_fitted = False
        
    def extract_file_features(self, files_data: List[Dict]) -> Dict[str, Any]:
        """
        Trích xuất features từ thông tin files
        
        Args:
            files_data: List of file info dicts với keys: filename, status, additions, deletions, changes
        """
        if not files_data:
            return self._get_empty_file_features()
        
        features = {}
        
        # Basic file stats
        features['num_files'] = len(files_data)
        features['total_additions'] = sum(f.get('additions', 0) for f in files_data)
        features['total_deletions'] = sum(f.get('deletions', 0) for f in files_data)
        features['total_changes'] = sum(f.get('changes', 0) for f in files_data)
        
        # File types
        file_extensions = []
        file_paths = []
        for file_info in files_data:
            filename = file_info.get('filename', '')
            if filename:
                file_paths.append(filename)
                # Extract extension
                if '.' in filename:
                    ext = filename.split('.')[-1].lower()
                    file_extensions.append(ext)
        
        # File type diversity
        unique_extensions = list(set(file_extensions))
        features['num_file_types'] = len(unique_extensions)
        features['file_types'] = unique_extensions[:10]  # Top 10 types
        
        # File depth analysis
        depths = []
        for path in file_paths:
            depth = len(Path(path).parts) - 1  # Subtract 1 for filename
            depths.append(depth)
        
        if depths:
            features['avg_file_depth'] = np.mean(depths)
            features['max_file_depth'] = np.max(depths)
            features['min_file_depth'] = np.min(depths)
        else:
            features['avg_file_depth'] = 0
            features['max_file_depth'] = 0
            features['min_file_depth'] = 0
        
        # Change distribution
        if features['total_changes'] > 0:
            features['additions_ratio'] = features['total_additions'] / features['total_changes']
            features['deletions_ratio'] = features['total_deletions'] / features['total_changes']
        else:
            features['additions_ratio'] = 0
            features['deletions_ratio'] = 0
        
        # File status analysis
        status_counts = {}
        for file_info in files_data:
            status = file_info.get('status', 'modified')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        features['added_files'] = status_counts.get('added', 0)
        features['modified_files'] = status_counts.get('modified', 0)
        features['deleted_files'] = status_counts.get('removed', 0)
        features['renamed_files'] = status_counts.get('renamed', 0)
        
        # Large file changes indicator
        large_changes = sum(1 for f in files_data if f.get('changes', 0) > 100)
        features['large_change_files'] = large_changes
        features['has_large_changes'] = large_changes > 0
        
        return features
    
    def _get_empty_file_features(self) -> Dict[str, Any]:
        """Trả về features mặc định khi không có file data"""
        return {
            'num_files': 0,
            'total_additions': 0,
            'total_deletions': 0,
            'total_changes': 0,
            'num_file_types': 0,
            'file_types': [],
            'avg_file_depth': 0,
            'max_file_depth': 0,
            'min_file_depth': 0,
            'additions_ratio': 0,
            'deletions_ratio': 0,            'added_files': 0,
            'modified_files': 0,
            'deleted_files': 0,
            'renamed_files': 0,
            'large_change_files': 0,
            'has_large_changes': False
        }
    
    def extract_author_features(self, author_info, commit_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        Trích xuất features từ thông tin author
        
        Args:
            author_info: Dict với keys: login, name, email, etc. HOẶC string author name
            commit_history: Lịch sử commit gần đây của author (optional)
        """
        features = {}
        
        # Handle both dict and string input
        if isinstance(author_info, str):
            # Simple string author name
            features['author_login'] = author_info
            features['author_name'] = author_info
            features['author_email'] = ''
        else:
            # Dict author info
            features['author_login'] = author_info.get('login', 'unknown')
            features['author_name'] = author_info.get('name', '')
            features['author_email'] = author_info.get('email', '')
        
        # Author activity pattern (nếu có lịch sử)
        if commit_history:
            features['recent_commits_count'] = len(commit_history)
            
            # Tính average commit size
            recent_changes = [c.get('stats', {}).get('total', 0) for c in commit_history]
            features['avg_recent_commit_size'] = np.mean(recent_changes) if recent_changes else 0
            
            # Frequency pattern
            if len(commit_history) >= 2:
                timestamps = [c.get('timestamp') for c in commit_history if c.get('timestamp')]
                if len(timestamps) >= 2:
                    # Calculate time between commits
                    time_diffs = []
                    for i in range(1, len(timestamps)):
                        try:
                            t1 = datetime.fromisoformat(timestamps[i-1].replace('Z', '+00:00'))
                            t2 = datetime.fromisoformat(timestamps[i].replace('Z', '+00:00'))
                            diff_hours = abs((t2 - t1).total_seconds() / 3600)
                            time_diffs.append(diff_hours)
                        except:
                            continue
                    
                    if time_diffs:
                        features['avg_commit_interval_hours'] = np.mean(time_diffs)
                        features['commit_frequency_score'] = min(24 / np.mean(time_diffs), 10) if np.mean(time_diffs) > 0 else 0
                    else:
                        features['avg_commit_interval_hours'] = 24
                        features['commit_frequency_score'] = 1
                else:
                    features['avg_commit_interval_hours'] = 24
                    features['commit_frequency_score'] = 1
            else:
                features['avg_commit_interval_hours'] = 24
                features['commit_frequency_score'] = 1
        else:
            features['recent_commits_count'] = 0
            features['avg_recent_commit_size'] = 0
            features['avg_commit_interval_hours'] = 24
            features['commit_frequency_score'] = 1
        
        return features
    
    def extract_timestamp_features(self, timestamp: str) -> Dict[str, Any]:
        """
        Trích xuất features từ timestamp
        """
        features = {}
        
        try:
            # Parse timestamp
            if timestamp.endswith('Z'):
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = datetime.fromisoformat(timestamp)
            
            # Time-based features
            features['hour_of_day'] = dt.hour
            features['day_of_week'] = dt.weekday()  # 0 = Monday
            features['day_of_month'] = dt.day
            features['month'] = dt.month
            features['year'] = dt.year
            
            # Derived features
            features['is_weekend'] = dt.weekday() >= 5
            features['is_business_hours'] = 9 <= dt.hour <= 17
            features['is_late_night'] = dt.hour >= 22 or dt.hour <= 6
            
            # Season (approximate)
            if dt.month in [12, 1, 2]:
                features['season'] = 'winter'
            elif dt.month in [3, 4, 5]:
                features['season'] = 'spring'
            elif dt.month in [6, 7, 8]:
                features['season'] = 'summer'
            else:
                features['season'] = 'fall'
                
        except Exception as e:
            # Default values if parsing fails
            features.update({
                'hour_of_day': 12,
                'day_of_week': 0,
                'day_of_month': 1,
                'month': 1,
                'year': 2024,
                'is_weekend': False,
                'is_business_hours': True,
                'is_late_night': False,
                'season': 'spring'
            })
        
        return features
    
    def create_feature_engineering(self, file_features: Dict, author_features: Dict, timestamp_features: Dict) -> Dict[str, Any]:
        """
        Tạo các feature engineering phức tạp hơn
        """
        engineered = {}
        
        # Commit complexity score
        complexity_score = 0
        complexity_score += min(file_features['num_files'] / 10, 1.0) * 0.3  # File count impact
        complexity_score += min(file_features['total_changes'] / 1000, 1.0) * 0.4  # Change size impact
        complexity_score += min(file_features['num_file_types'] / 5, 1.0) * 0.2  # Diversity impact
        complexity_score += min(file_features['max_file_depth'] / 10, 1.0) * 0.1  # Depth impact
        engineered['complexity_score'] = complexity_score
        
        # Risk assessment
        risk_score = 0
        risk_score += file_features['has_large_changes'] * 0.3
        risk_score += (file_features['deleted_files'] / max(file_features['num_files'], 1)) * 0.2
        risk_score += min(author_features['commit_frequency_score'] / 5, 1.0) * 0.2  # High frequency = higher risk
        risk_score += timestamp_features['is_late_night'] * 0.1
        risk_score += (timestamp_features['is_weekend'] and not timestamp_features['is_business_hours']) * 0.2
        engineered['risk_score'] = min(risk_score, 1.0)
        
        # Urgency indicators
        urgency_score = 0
        urgency_score += timestamp_features['is_late_night'] * 0.4
        urgency_score += timestamp_features['is_weekend'] * 0.3
        urgency_score += (author_features['commit_frequency_score'] > 5) * 0.3
        engineered['urgency_score'] = min(urgency_score, 1.0)
        
        # Code churn metrics
        if file_features['total_changes'] > 0:
            churn_ratio = (file_features['total_additions'] + file_features['total_deletions']) / file_features['total_changes']
            engineered['code_churn'] = min(churn_ratio, 2.0)
        else:
            engineered['code_churn'] = 0
        
        # File type hotspots (common file types that often have issues)
        risky_extensions = ['js', 'ts', 'py', 'java', 'cpp', 'c', 'php']
        config_extensions = ['json', 'xml', 'yml', 'yaml', 'cfg', 'conf']
        doc_extensions = ['md', 'txt', 'rst', 'doc']
        
        engineered['touches_risky_files'] = any(ext in risky_extensions for ext in file_features['file_types'])
        engineered['touches_config_files'] = any(ext in config_extensions for ext in file_features['file_types'])
        engineered['touches_doc_files'] = any(ext in doc_extensions for ext in file_features['file_types'])
        
        return engineered
    
    def fit(self, metadata_samples: List[Dict]) -> None:
        """
        Fit các encoders và scalers với training data
        """
        print("🔧 Fitting metadata processors...")
        
        # Collect all features
        all_numerical_features = []
        all_categorical_features = {
            'authors': [],
            'file_types': [],
            'seasons': []
        }
        
        for sample in metadata_samples:
            # Extract features
            file_features = self.extract_file_features(sample.get('files', []))
            author_features = self.extract_author_features(
                sample.get('author', {}), 
                sample.get('commit_history', [])
            )
            timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
            engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
            
            # Collect numerical features
            numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
            all_numerical_features.append(numerical)
            
            # Collect categorical features
            all_categorical_features['authors'].append(author_features['author_login'])
            all_categorical_features['file_types'].extend(file_features['file_types'])
            all_categorical_features['seasons'].append(timestamp_features['season'])
        
        # Fit scalers
        if self.normalize_features and all_numerical_features:
            numerical_array = np.array(all_numerical_features)
            self.numerical_scaler.fit(numerical_array)
        
        # Fit encoders
        if all_categorical_features['authors']:
            unique_authors = list(set(all_categorical_features['authors']))[:self.max_authors]
            self.author_encoder.fit(unique_authors + ['<UNK>'])
        
        if all_categorical_features['file_types']:
            unique_file_types = list(set(all_categorical_features['file_types']))
            self.file_type_encoder.fit(unique_file_types + ['<UNK>'])
        
        # Fit file path vectorizer
        all_file_paths = []
        for sample in metadata_samples:
            files = sample.get('files', [])
            paths = [f.get('filename', '') for f in files]
            all_file_paths.extend(paths)
        
        if all_file_paths:
            self.file_path_vectorizer.fit(all_file_paths)
        
        self.is_fitted = True
        print("✅ Metadata processors fitted successfully")
    
    def _get_numerical_features(self, file_features: Dict, author_features: Dict, 
                               timestamp_features: Dict, engineered_features: Dict) -> List[float]:
        """
        Lấy tất cả numerical features thành một vector
        """
        features = []
        
        # File features
        features.extend([
            file_features['num_files'],
            file_features['total_additions'],
            file_features['total_deletions'],
            file_features['total_changes'],
            file_features['num_file_types'],
            file_features['avg_file_depth'],
            file_features['max_file_depth'],
            file_features['min_file_depth'],
            file_features['additions_ratio'],
            file_features['deletions_ratio'],
            file_features['added_files'],
            file_features['modified_files'],
            file_features['deleted_files'],
            file_features['renamed_files'],
            file_features['large_change_files'],
            float(file_features['has_large_changes'])
        ])
        
        # Author features
        features.extend([
            author_features['recent_commits_count'],
            author_features['avg_recent_commit_size'],
            author_features['avg_commit_interval_hours'],
            author_features['commit_frequency_score']
        ])
        
        # Timestamp features
        features.extend([
            timestamp_features['hour_of_day'],
            timestamp_features['day_of_week'],
            timestamp_features['day_of_month'],
            timestamp_features['month'],
            float(timestamp_features['is_weekend']),
            float(timestamp_features['is_business_hours']),
            float(timestamp_features['is_late_night'])
        ])
        
        # Engineered features
        features.extend([
            engineered_features['complexity_score'],
            engineered_features['risk_score'],
            engineered_features['urgency_score'],
            engineered_features['code_churn'],
            float(engineered_features['touches_risky_files']),
            float(engineered_features['touches_config_files']),
            float(engineered_features['touches_doc_files'])
        ])
        
        return features
    
    def process_sample(self, sample: Dict) -> Dict[str, torch.Tensor]:
        """
        Xử lý một sample metadata
        """
        if not self.is_fitted:
            raise ValueError("MetadataProcessor must be fitted before processing samples")
        
        # Extract features
        file_features = self.extract_file_features(sample.get('files', []))
        author_features = self.extract_author_features(
            sample.get('author', {}), 
            sample.get('commit_history', [])
        )
        timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
        engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
        
        result = {}
        
        # Numerical features
        numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
        if self.normalize_features:
            numerical = self.numerical_scaler.transform([numerical])[0]
        result['numerical_features'] = torch.tensor(numerical, dtype=torch.float32)
        
        # Categorical features
        # Author encoding
        author_login = author_features['author_login']
        try:
            author_encoded = self.author_encoder.transform([author_login])[0]
        except ValueError:
            author_encoded = self.author_encoder.transform(['<UNK>'])[0]
        result['author_encoded'] = torch.tensor(author_encoded, dtype=torch.long)
        
        # Season encoding
        season_map = {'spring': 0, 'summer': 1, 'fall': 2, 'winter': 3}
        result['season_encoded'] = torch.tensor(season_map.get(timestamp_features['season'], 0), dtype=torch.long)
          # File types encoding (one-hot or multi-hot)
        try:
            # Get number of classes from fitted encoder
            num_classes = len(self.file_type_encoder.classes_)
        except AttributeError:
            # Fallback if encoder is not fitted or doesn't have classes_ attribute
            num_classes = 10  # Default reasonable size
            
        file_type_vector = np.zeros(num_classes)
        for file_type in file_features['file_types']:
            try:
                idx = self.file_type_encoder.transform([file_type])[0]
                if idx < num_classes:  # Safety check
                    file_type_vector[idx] = 1
            except (ValueError, AttributeError):
                continue
        result['file_types_encoded'] = torch.tensor(file_type_vector, dtype=torch.float32)
        
        return result
    
    def process_batch(self, samples: List[Dict]) -> Dict[str, torch.Tensor]:
        """
        Xử lý một batch samples
        """
        batch_results = {
            'numerical_features': [],
            'author_encoded': [],
            'season_encoded': [],
            'file_types_encoded': []
        }
        
        for sample in samples:
            processed = self.process_sample(sample)
            for key, value in processed.items():
                batch_results[key].append(value)
        
        # Stack tensors
        for key in batch_results:
            batch_results[key] = torch.stack(batch_results[key])
        
        return batch_results
    
    def get_feature_dimensions(self) -> Dict[str, int]:
        """
        Trả về dimensions của các feature types
        """
        return {
            'numerical_dim': 33,  # Total numerical features
            'author_vocab_size': len(self.author_encoder.classes_) if hasattr(self.author_encoder, 'classes_') else 1000,
            'season_vocab_size': 4,
            'file_types_dim': len(self.file_type_encoder.classes_) if hasattr(self.file_type_encoder, 'classes_') else 100
        }

```

### backend\ai\multimodal_fusion\data_preprocessing\minimal_enhanced_text_processor.py
```py
"""
Minimal Enhanced Text Processor
Provides basic NLTK functionality without complex dependencies
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Try minimal NLTK imports
try:
    # Only import basic tokenization without sklearn dependencies
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    import nltk
    
    # Download only essential data
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
    
    NLTK_BASIC = True
    logger.info("Basic NLTK functionality available")
except Exception as e:
    NLTK_BASIC = False
    logger.warning(f"NLTK basic features not available: {e}")

# Try TextBlob for sentiment
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available")

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available")


class MinimalEnhancedTextProcessor:
    """
    Minimal Enhanced Text Processor with basic NLTK support
    Focuses on essential improvements without complex dependencies
    """
    
    def __init__(self, 
                 method: str = "lstm",
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_sentiment: bool = True,
                 enable_advanced_cleaning: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name
            enable_sentiment: Enable sentiment analysis with TextBlob
            enable_advanced_cleaning: Enable advanced text cleaning
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        self.enable_sentiment = enable_sentiment
        self.enable_advanced_cleaning = enable_advanced_cleaning
        
        # Initialize stopwords
        self._init_stopwords()
        
        # Initialize model components
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            self._init_lstm_components()
    
    def _init_stopwords(self):
        """Initialize stopwords with fallback"""
        if NLTK_BASIC:
            try:
                self.stop_words = set(stopwords.words('english'))
                logger.info(f"Loaded {len(self.stop_words)} NLTK stopwords")
            except Exception as e:
                logger.warning(f"Failed to load NLTK stopwords: {e}")
                self.stop_words = self._get_basic_stopwords()
        else:
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Basic stopwords list"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours'
        ])
    
    def _init_lstm_components(self):
        """Initialize LSTM components"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128
    
    def enhanced_tokenize(self, text: str) -> List[str]:
        """Enhanced tokenization with NLTK if available"""
        if NLTK_BASIC:
            try:
                tokens = word_tokenize(text.lower())
                return [token for token in tokens if token.isalpha() and len(token) > 1]
            except Exception as e:
                logger.warning(f"NLTK tokenization failed: {e}")
        
        # Fallback to simple tokenization
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        return [word for word in text.split() if len(word) > 1]
    
    def advanced_clean_commit_message(self, text: str) -> str:
        """Advanced commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        original_text = text
        
        # Basic cleaning
        text = self.clean_commit_message(text)
        
        if not self.enable_advanced_cleaning:
            return text
        
        # Advanced cleaning patterns
        advanced_patterns = [
            # Remove version numbers
            (r'\bv?\d+\.\d+(\.\d+)?(-\w+)?\b', ''),
            # Remove file extensions in isolation
            (r'\b\w+\.(js|py|html|css|md|txt|json|xml|yml|yaml)\b', ''),
            # Remove common dev terms that add noise
            (r'\b(eslint|prettier|webpack|babel|npm|yarn|pip)\b', ''),
            # Remove brackets with single words
            (r'\[\w+\]', ''),
            # Remove parentheses with single words
            (r'\(\w+\)', ''),
            # Clean up multiple spaces and special chars
            (r'[^\w\s\.\!\?\,\:\;\-]', ' '),
            (r'\s+', ' '),
        ]
        
        for pattern, replacement in advanced_patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        text = text.strip()
        
        # If cleaning removed too much, return original cleaned version
        if len(text) < len(original_text) * 0.3:
            return self.clean_commit_message(original_text)
        
        return text
    
    def clean_commit_message(self, text: str) -> str:
        """Basic commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_enhanced_features(self, text: str) -> Dict[str, any]:
        """Extract enhanced features with sentiment analysis"""
        features = self.extract_basic_features(text)
        
        if not text or not isinstance(text, str):
            return features
        
        # Add sentiment analysis if available
        if self.enable_sentiment and TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                features['sentiment_polarity'] = blob.sentiment.polarity
                features['sentiment_subjectivity'] = blob.sentiment.subjectivity
                
                # Categorize sentiment
                polarity = blob.sentiment.polarity
                if polarity > 0.1:
                    features['sentiment_category'] = 'positive'
                elif polarity < -0.1:
                    features['sentiment_category'] = 'negative'
                else:
                    features['sentiment_category'] = 'neutral'
                    
            except Exception as e:
                logger.warning(f"Sentiment analysis failed: {e}")
                features['sentiment_polarity'] = 0.0
                features['sentiment_subjectivity'] = 0.0
                features['sentiment_category'] = 'neutral'
        
        # Enhanced text statistics
        words = text.split()
        if words:
            features['avg_word_length'] = np.mean([len(word) for word in words])
            features['max_word_length'] = max(len(word) for word in words)
            features['unique_word_ratio'] = len(set(words)) / len(words)
        else:
            features['avg_word_length'] = 0
            features['max_word_length'] = 0
            features['unique_word_ratio'] = 0
        
        # Enhanced keyword detection
        technical_keywords = ['api', 'database', 'server', 'client', 'config', 'auth', 'security', 'performance']
        ui_keywords = ['ui', 'interface', 'design', 'layout', 'style', 'theme', 'responsive']
        testing_keywords = ['test', 'spec', 'mock', 'coverage', 'unit', 'integration', 'e2e']
        
        text_lower = text.lower()
        features['has_technical_keywords'] = any(kw in text_lower for kw in technical_keywords)
        features['has_ui_keywords'] = any(kw in text_lower for kw in ui_keywords)
        features['has_testing_keywords'] = any(kw in text_lower for kw in testing_keywords)
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """Extract basic text features"""
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'punctuation_count', 'has_commit_type', 'commit_type_prefix',
                'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords'
            ]}
        
        # Basic statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        text_lower = text.lower()
        features['has_commit_type'] = any(text_lower.startswith(ct + ':') or text_lower.startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type
        for ct in commit_types:
            if text_lower.startswith(ct + ':') or text_lower.startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'guide']
        
        features['has_bug_keywords'] = any(kw in text_lower for kw in bug_keywords)
        features['has_feature_keywords'] = any(kw in text_lower for kw in feature_keywords)
        features['has_doc_keywords'] = any(kw in text_lower for kw in doc_keywords)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """Build vocabulary for LSTM method"""
        if self.method != "lstm":
            return
        
        logger.info("🔤 Building enhanced vocabulary...")
        
        for text in texts:
            cleaned_text = self.advanced_clean_commit_message(text)
            tokens = self.enhanced_tokenize(cleaned_text)
            tokens = [token for token in tokens if token not in self.stop_words]
            self.word_counts.update(tokens)
        
        # Build vocabulary
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"✅ Enhanced vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """Enhanced LSTM encoding"""
        cleaned_text = self.advanced_clean_commit_message(text)
        tokens = self.enhanced_tokenize(cleaned_text)
        tokens = [token for token in tokens if token not in self.stop_words]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]
        
        # Add start and end tokens
        indices = [2] + indices + [3]
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))
        
        return torch.tensor(indices, dtype=torch.long)
    
    def fit(self, texts: List[str]) -> 'MinimalEnhancedTextProcessor':
        """Fit the processor to training data"""
        logger.info("🚀 Fitting minimal enhanced text processor...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
        
        logger.info("✅ Text processor fitted successfully")
        return self
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        for text in texts:
            basic_features = self.extract_basic_features(text)
            enhanced_features = self.extract_enhanced_features(text)
            
            results['text_features'].append(basic_features)
            results['enhanced_features'].append(enhanced_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        
        return results
    
    # Keep essential methods from original processor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method"""
        cleaned_text = self.advanced_clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\text_processor.py
```py
"""
Text Processor for Multi-Modal Fusion Network
Xử lý và chuẩn bị dữ liệu văn bản từ commit messages
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from collections import Counter

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Transformers not available. Using simple tokenization only.")

# Optional NLTK import
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("NLTK not available. Using simple text processing.")

class TextProcessor:
    """
    Lớp xử lý văn bản cho commit messages
    Hỗ trợ cả tokenization đơn giản và pre-trained embeddings
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased"):
        """
        Args:
            method: Phương pháp xử lý ("lstm", "distilbert", "transformer")
            vocab_size: Kích thước vocabulary cho LSTM
            max_length: Độ dài tối đa của sequence
            pretrained_model: Tên pre-trained model nếu dùng transformer
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
          # Initialize components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
            except Exception as e:
                print(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
            
        # Download NLTK data if needed
        if NLTK_AVAILABLE:
            try:
                nltk.data.find('tokenizers/punkt')
                nltk.data.find('corpora/stopwords')
            except LookupError:
                nltk.download('punkt')
                nltk.download('stopwords')
                
            self.stop_words = set(stopwords.words('english'))
        else:
            # Simple fallback stopwords
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'])
        
    def _tokenize_text(self, text: str) -> List[str]:
        """
        Tokenize text with fallback if NLTK not available
        """
        if NLTK_AVAILABLE:
            return word_tokenize(text)
        else:
            # Simple tokenization fallback
            import string            # Remove punctuation and split by whitespace
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def build_vocab(self, texts: List[str], vocab_size: int = None):
        """
        Build vocabulary from a list of texts
        """
        if vocab_size:
            self.vocab_size = vocab_size
            
        # Count words
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self._tokenize_text(cleaned_text.lower())
            self.word_counts.update(tokens)
        
        # Build vocabulary with most common words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # Reserve space for special tokens
        
        for word, count in most_common:
            if word not in self.word_to_idx:
                idx = len(self.word_to_idx)
                self.word_to_idx[word] = idx
                self.idx_to_word[idx] = word
        
        print(f"Built vocabulary with {len(self.word_to_idx)} words")
        
    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx
    
    def clean_commit_message(self, text: str) -> str:
        """
        Làm sạch commit message
        """
        if not text or not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references like #123, Fixes #456
        text = re.sub(r'(closes?|fixes?|resolves?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s\.\!\?\,\:\;\-\(\)]', ' ', text)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_commit_features(self, text: str) -> Dict[str, any]:
        """
        Trích xuất các đặc trưng từ commit message
        """
        features = {}
        
        # Basic features
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':'):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
            
        # Keywords detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation']
        
        features['has_bug_keywords'] = any(keyword in text.lower() for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text.lower() for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text.lower() for keyword in doc_keywords)
        
        # Sentiment indicators
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        features['positive_sentiment'] = any(word in text.lower() for word in positive_words)
        features['negative_sentiment'] = any(word in text.lower() for word in negative_words)
        features['urgent_sentiment'] = any(word in text.lower() for word in urgent_words)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        Xây dựng vocabulary cho LSTM method
        """
        if self.method != "lstm":
            return
            
        print("🔤 Building vocabulary for text processing...")
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            words = self._tokenize_text(cleaned_text.lower())
            # Filter out stop words and very short words
            words = [w for w in words if w not in self.stop_words and len(w) > 1]
            self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # -4 for special tokens
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"✅ Vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Encode text cho LSTM method
        """
        cleaned_text = self.clean_commit_message(text)
        words = self._tokenize_text(cleaned_text.lower())
        words = [w for w in words if w not in self.stop_words and len(w) > 1]
        
        # Convert to indices
        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """
        Encode text cho transformer method
        """
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """
        Lấy embeddings cho list of texts
        """
        if self.method == "lstm":
            # Return token indices for LSTM
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            # Get contextual embeddings
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    # Use [CLS] token embedding or mean pooling
                    embedding = outputs.last_hidden_state[:, 0, :]  # CLS token
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """
        Xử lý một batch texts
        """
        results = {
            'text_features': [],
            'embeddings': None,
            'metadata_features': []
        }
        
        # Extract text features
        for text in texts:
            features = self.extract_commit_features(text)
            results['text_features'].append(features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def fit(self, texts: List[str]) -> 'TextProcessor':
        """
        Fit the text processor to the training data
        This method builds vocabulary for LSTM method and prepares the processor
        """
        if self.method == "lstm":
            self.build_vocabulary(texts)
        # For transformer methods, no fitting is needed as they use pre-trained models
        return self
    
    def get_vocab_size(self) -> int:
        """
        Trả về kích thước vocabulary
        """
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """
        Trả về dimension của embeddings
        """
        return self.embed_dim

```

### backend\ai\multimodal_fusion\data_preprocessing\__init__.py
```py
"""
Data Preprocessing Module Initialization
"""

from .metadata_processor import MetadataProcessor

# Import text processors with fallback
try:
    from .minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
    ENHANCED_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Enhanced text processor not available: {e}")
    ENHANCED_PROCESSOR_AVAILABLE = False

try:
    from .text_processor import TextProcessor
    BASIC_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Basic text processor not available: {e}")
    BASIC_PROCESSOR_AVAILABLE = False
    # Use minimal processor as fallback
    if ENHANCED_PROCESSOR_AVAILABLE:
        TextProcessor = MinimalEnhancedTextProcessor

__all__ = ["MetadataProcessor"]

if ENHANCED_PROCESSOR_AVAILABLE:
    __all__.append("MinimalEnhancedTextProcessor")
if BASIC_PROCESSOR_AVAILABLE:
    __all__.append("TextProcessor")

```

### backend\ai\multimodal_fusion\evaluation\interpretability.py
```py

```

### backend\ai\multimodal_fusion\evaluation\metrics_calculator.py
```py

```

### backend\ai\multimodal_fusion\evaluation\visualization.py
```py

```

### backend\ai\multimodal_fusion\evaluation\__init__.py
```py

```

### backend\ai\multimodal_fusion\losses\multi_task_losses.py
```py

```

### backend\ai\multimodal_fusion\losses\__init__.py
```py

```

### backend\ai\multimodal_fusion\models\baselines.py
```py

```

### backend\ai\multimodal_fusion\models\multimodal_fusion.py
```py
"""
Multi-Modal Fusion Network Architecture
Mô hình kết hợp thông tin văn bản và metadata với các cơ chế fusion tiên tiến
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class CrossAttentionFusion(nn.Module):
    """
    Cross-Attention mechanism cho fusion giữa text và metadata
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128, num_heads: int = 4):
        super(CrossAttentionFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Feed forward
        self.ff = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Project to same dimension
        text_proj = self.text_proj(text_features)  # (batch_size, hidden_dim)
        metadata_proj = self.metadata_proj(metadata_features)  # (batch_size, hidden_dim)
        
        # Add sequence dimension for attention
        text_seq = text_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        metadata_seq = metadata_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        
        # Cross attention: text attends to metadata
        text_attended, _ = self.attention(text_seq, metadata_seq, metadata_seq)
        text_attended = text_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Cross attention: metadata attends to text
        metadata_attended, _ = self.attention(metadata_seq, text_seq, text_seq)
        metadata_attended = metadata_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Combine and normalize
        combined = self.norm1(text_attended + metadata_attended)
        
        # Feed forward
        output = self.ff(combined)
        output = self.norm2(combined + output)
        
        return output

class GatedFusion(nn.Module):
    """
    Gated Multimodal Units (GMU) for fusion
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128):
        super(GatedFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Gating mechanism
        self.gate_text = nn.Linear(text_dim + metadata_dim, hidden_dim)
        self.gate_metadata = nn.Linear(text_dim + metadata_dim, hidden_dim)
        
        # Final projection
        self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Concatenate for gating
        combined = torch.cat([text_features, metadata_features], dim=1)
        
        # Compute gates
        text_gate = torch.sigmoid(self.gate_text(combined))
        metadata_gate = torch.sigmoid(self.gate_metadata(combined))
        
        # Project features
        text_proj = self.text_proj(text_features)
        metadata_proj = self.metadata_proj(metadata_features)
        
        # Apply gates
        gated_text = text_gate * text_proj
        gated_metadata = metadata_gate * metadata_proj
        
        # Combine and project
        fused = torch.cat([gated_text, gated_metadata], dim=1)
        output = self.output_proj(fused)
        
        return output

class TextBranch(nn.Module):
    """
    Nhánh xử lý văn bản với LSTM/GRU và Attention hoặc Transformer
    """
    
    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, 
                 method: str = "lstm", pretrained_dim: Optional[int] = None):
        super(TextBranch, self).__init__()
        
        self.method = method
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        if method == "lstm":
            # LSTM-based processing
            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
            self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_dim * 2,
                num_heads=4,
                batch_first=True
            )
            self.output_dim = hidden_dim * 2
            
        elif method in ["distilbert", "transformer"]:
            # Transformer-based processing
            if pretrained_dim is None:
                raise ValueError("pretrained_dim must be provided for transformer method")
            
            self.projection = nn.Linear(pretrained_dim, hidden_dim)
            self.transformer_layer = nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=4,
                dim_feedforward=hidden_dim * 2,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)
            self.output_dim = hidden_dim
            
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, text_input: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            text_input: Token IDs (batch_size, seq_len) for LSTM or embeddings (batch_size, embed_dim) for transformer
            attention_mask: Attention mask (batch_size, seq_len) - optional
        Returns:
            text_features: (batch_size, output_dim)
        """
        if self.method == "lstm":
            # LSTM processing
            embedded = self.embedding(text_input)  # (batch_size, seq_len, embed_dim)
            lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)
            
            # Self-attention
            attended, _ = self.attention(lstm_out, lstm_out, lstm_out)
            
            # Global max pooling
            pooled = torch.max(attended, dim=1)[0]  # (batch_size, hidden_dim * 2)
            
        elif self.method in ["distilbert", "transformer"]:
            # Transformer processing
            if text_input.dim() == 2 and text_input.size(1) > 1:
                # Multiple embeddings case
                projected = self.projection(text_input)  # (batch_size, seq_len, hidden_dim)
                output = self.transformer(projected)  # (batch_size, seq_len, hidden_dim)
                pooled = torch.mean(output, dim=1)  # (batch_size, hidden_dim)
            else:
                # Single embedding case
                if text_input.dim() == 2:
                    projected = self.projection(text_input)  # (batch_size, hidden_dim)
                else:
                    projected = self.projection(text_input.unsqueeze(1))  # (batch_size, 1, hidden_dim)
                    projected = projected.squeeze(1)  # (batch_size, hidden_dim)
                pooled = projected
        
        return self.dropout(pooled)

class MetadataBranchV2(nn.Module):
    """
    Flexible metadata branch with configurable categorical and numerical features
    """
    
    def __init__(self, 
                 categorical_dims: Dict[str, int],
                 numerical_features: List[str],
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranchV2, self).__init__()
        
        self.categorical_dims = categorical_dims
        self.numerical_features = numerical_features
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.categorical_embeddings = nn.ModuleDict()
        for feature_name, vocab_size in categorical_dims.items():
            self.categorical_embeddings[feature_name] = nn.Embedding(vocab_size, embed_dim)
          # Projection for numerical features - dynamic sizing
        # We'll set this in the first forward pass when we know the actual dimension
        self.numerical_proj = None
        self.numerical_dim = None
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * len(categorical_dims)
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing features for categorical and numerical data
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        feature_list = []        # Process numerical features
        if 'numerical_features' in metadata_features:
            # Handle case where we have a single tensor with all numerical features
            numerical_tensor = metadata_features['numerical_features']
            if len(numerical_tensor.shape) == 1:
                numerical_tensor = numerical_tensor.unsqueeze(0)  # Add batch dim if missing
            
            # Initialize numerical projection layer if not already done
            if self.numerical_proj is None:
                self.numerical_dim = numerical_tensor.shape[-1]
                self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
            
            numerical_proj = self.numerical_proj(numerical_tensor)
            feature_list.append(numerical_proj)
        else:
            # Handle case where numerical features are split into individual tensors
            numerical_data = []
            for feature_name in self.numerical_features:
                if feature_name in metadata_features:
                    numerical_data.append(metadata_features[feature_name].unsqueeze(-1))
            
            if numerical_data:
                numerical_tensor = torch.cat(numerical_data, dim=1)
                
                # Initialize numerical projection layer if not already done
                if self.numerical_proj is None:
                    self.numerical_dim = numerical_tensor.shape[-1]
                    self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
                
                numerical_proj = self.numerical_proj(numerical_tensor)
                feature_list.append(numerical_proj)
        
        # Process categorical embeddings
        for feature_name, embedding_layer in self.categorical_embeddings.items():
            if feature_name in metadata_features:
                embed = embedding_layer(metadata_features[feature_name])
                feature_list.append(embed)
          # Combine all features
        if feature_list:
            combined = torch.cat(feature_list, dim=1)
        else:
            # Fallback if no features found
            batch_size = next(iter(metadata_features.values())).size(0)
            device = next(iter(metadata_features.values())).device
            combined = torch.zeros(batch_size, self.hidden_dim, device=device)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class MetadataBranch(nn.Module):
    """
    Nhánh xử lý metadata với embeddings và dense layers
    """
    
    def __init__(self, 
                 numerical_dim: int,
                 author_vocab_size: int,
                 season_vocab_size: int,
                 file_types_dim: int,
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranch, self).__init__()
        
        self.numerical_dim = numerical_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.author_embedding = nn.Embedding(author_vocab_size, embed_dim)
        self.season_embedding = nn.Embedding(season_vocab_size, embed_dim)
        
        # Projection for numerical features
        self.numerical_proj = nn.Linear(numerical_dim, hidden_dim)
        
        # Projection for file types (multi-hot encoded)
        self.file_types_proj = nn.Linear(file_types_dim, embed_dim)
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * 3  # numerical + author + season + file_types
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing numerical_features, author_encoded, season_encoded, file_types_encoded
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        # Process numerical features
        numerical = self.numerical_proj(metadata_features['numerical_features'])
        
        # Process categorical embeddings
        author_embed = self.author_embedding(metadata_features['author_encoded'])
        season_embed = self.season_embedding(metadata_features['season_encoded'])
        file_types_embed = self.file_types_proj(metadata_features['file_types_encoded'])
        
        # Combine all features
        combined = torch.cat([numerical, author_embed, season_embed, file_types_embed], dim=1)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class TaskSpecificHead(nn.Module):
    """
    Task-specific classification head
    """
    
    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 64):
        super(TaskSpecificHead, self).__init__()
        
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        return self.classifier(features)

class MultiModalFusionNetwork(nn.Module):
    """
    Main Multi-Modal Fusion Network
    """
    
    def __init__(self, config: Dict = None, **kwargs):
        """
        Initialize MultiModalFusionNetwork with flexible configuration
        
        Args:
            config: Configuration dictionary (new format)
            **kwargs: Backward compatibility parameters (old format)
        """
        super(MultiModalFusionNetwork, self).__init__()
        
        # Handle both new config format and old parameter format
        if config is not None:
            self.config = config
            
            # Extract configurations
            text_config = config['text_encoder']
            metadata_config = config['metadata_encoder']
            fusion_config = config['fusion']
            task_configs = config['task_heads']
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=text_config['vocab_size'],
                embed_dim=text_config['embedding_dim'],
                hidden_dim=text_config['hidden_dim'],
                method=text_config.get('method', 'lstm'),
                pretrained_dim=text_config.get('pretrained_dim', None)
            )
            
            # Metadata branch - use flexible version
            self.metadata_branch = MetadataBranchV2(
                categorical_dims=metadata_config['categorical_dims'],
                numerical_features=metadata_config['numerical_features'],
                embed_dim=metadata_config['embedding_dim'],
                hidden_dim=metadata_config['hidden_dim']
            )
            
            # Fusion mechanism
            fusion_method = fusion_config.get('method', 'cross_attention')
            fusion_hidden_dim = fusion_config.get('fusion_dim', 128)
            
        else:
            # Backward compatibility - use old parameter format
            text_method = kwargs.get('text_method', 'lstm')
            vocab_size = kwargs.get('vocab_size', 10000)
            text_embed_dim = kwargs.get('text_embed_dim', 128)
            text_hidden_dim = kwargs.get('text_hidden_dim', 128)
            pretrained_text_dim = kwargs.get('pretrained_text_dim', None)
            
            numerical_dim = kwargs.get('numerical_dim', 34)
            author_vocab_size = kwargs.get('author_vocab_size', 1000)
            season_vocab_size = kwargs.get('season_vocab_size', 4)
            file_types_dim = kwargs.get('file_types_dim', 100)
            metadata_embed_dim = kwargs.get('metadata_embed_dim', 64)
            metadata_hidden_dim = kwargs.get('metadata_hidden_dim', 128)
            
            fusion_method = kwargs.get('fusion_method', 'cross_attention')
            fusion_hidden_dim = kwargs.get('fusion_hidden_dim', 128)
            task_configs = kwargs.get('task_configs', {})
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=vocab_size,
                embed_dim=text_embed_dim,
                hidden_dim=text_hidden_dim,
                method=text_method,
                pretrained_dim=pretrained_text_dim
            )
            
            # Metadata branch - use old version for compatibility
            self.metadata_branch = MetadataBranch(
                numerical_dim=numerical_dim,
                author_vocab_size=author_vocab_size,
                season_vocab_size=season_vocab_size,
                file_types_dim=file_types_dim,
                embed_dim=metadata_embed_dim,
                hidden_dim=metadata_hidden_dim
            )
        
        # Common fusion setup
        if fusion_method == "cross_attention":
            self.fusion = CrossAttentionFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        elif fusion_method == "gated":
            self.fusion = GatedFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        else:  # concat
            self.fusion = None
            fusion_output_dim = self.text_branch.output_dim + self.metadata_branch.output_dim
        
        # Task-specific heads - support both formats
        self.task_heads = nn.ModuleDict()
        for task_name, task_config in task_configs.items():
            if isinstance(task_config, dict):
                if 'num_classes' in task_config:
                    num_classes = task_config['num_classes']
                elif 'classes' in task_config:
                    num_classes = len(task_config['classes'])
                else:
                    num_classes = 2  # default
            else:
                # Old format: direct number
                num_classes = task_config
            
            self.task_heads[task_name] = TaskSpecificHead(
                input_dim=fusion_output_dim,
                num_classes=num_classes
            )
    
    def forward(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor], 
                attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            text_input: Text tokens or embeddings
            metadata_input: Dict of metadata features
            attention_mask: Optional attention mask for text
            
        Returns:
            outputs: Dict mapping task names to logits
        """
        # Process text branch
        text_features = self.text_branch(text_input, attention_mask)
        
        # Process metadata branch
        metadata_features = self.metadata_branch(metadata_input)
        
        # Fusion
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            # Simple concatenation
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        # Task-specific predictions
        outputs = {}
        for task_name, head in self.task_heads.items():
            outputs[task_name] = head(fused_features)
        
        return outputs
    
    def get_fusion_features(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor],
                           attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Get fused features for analysis/visualization
        """
        text_features = self.text_branch(text_input, attention_mask)
        metadata_features = self.metadata_branch(metadata_input)
        
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        return fused_features

```

### backend\ai\multimodal_fusion\models\shared_layers.py
```py

```

### backend\ai\multimodal_fusion\models\__init__.py
```py
"""
Models Module Initialization
"""

from .multimodal_fusion import MultiModalFusionNetwork, CrossAttentionFusion, GatedFusion

__all__ = [
    "MultiModalFusionNetwork",
    "CrossAttentionFusion", 
    "GatedFusion"
]

```

### backend\ai\multimodal_fusion\scripts\train_main.py
```py

```

### backend\ai\multimodal_fusion\scripts\train_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script for Multimodal Fusion Model
Complete training pipeline for commit analysis with text + metadata fusion
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

def custom_collate_fn(batch):
    """Custom collate function to handle metadata dict structure"""
    collated = {}
    
    # Handle text features (simple tensors)
    collated['text_features'] = torch.stack([item['text_features'] for item in batch])
    
    # Handle metadata features (dict of tensors)
    metadata_keys = batch[0]['metadata_features'].keys()
    collated['metadata_features'] = {}
    for key in metadata_keys:
        collated['metadata_features'][key] = torch.stack([item['metadata_features'][key] for item in batch])
    
    # Handle labels (list of dicts)
    collated['labels'] = [item['labels'] for item in batch]
    
    # Handle text (list of strings)
    collated['text'] = [item['text'] for item in batch]
    
    return collated

# Add project paths
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai"))

# Import multimodal components
from ai.multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
from ai.multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from ai.multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
# from ai.multimodal_fusion.training.multitask_trainer import MultiTaskTrainer
# from ai.multimodal_fusion.losses.multi_task_losses import MultiTaskLoss
# from ai.multimodal_fusion.evaluation.metrics_calculator import MetricsCalculator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MultimodalDataset(Dataset):
    """Dataset for multimodal fusion training"""
    
    def __init__(self, samples, text_processor, metadata_processor, max_length=512):
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.max_length = max_length
        
        # Define label mappings for multimodal tasks
        self.task_labels = {
            'risk_prediction': {'low': 0, 'high': 1},
            'complexity_prediction': {'simple': 0, 'medium': 1, 'complex': 2},
            'hotspot_prediction': {'very_low': 0, 'low': 1, 'medium': 2, 'high': 3, 'very_high': 4},
            'urgency_prediction': {'normal': 0, 'urgent': 1}
        }
        
        logger.info(f"Created dataset with {len(samples)} samples")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
          # Process text
        commit_text = sample.get('text', '') or sample.get('message', '')
        text_features = self.text_processor.encode_text_lstm(commit_text)
          # Process metadata
        metadata = self._extract_metadata(sample)
        metadata_features = self.metadata_processor.process_sample(sample)
        
        # Generate multimodal task labels
        labels = self._generate_multimodal_labels(sample, commit_text, metadata)
        
        return {
            'text_features': text_features,
            'metadata_features': metadata_features,
            'labels': labels,
            'text': commit_text
        }
    
    def _extract_metadata(self, sample):
        """Extract metadata features from sample"""
        return {
            'author': sample.get('author', 'unknown'),
            'files_changed': len(sample.get('files_changed', [])),
            'additions': sample.get('additions', 0),
            'deletions': sample.get('deletions', 0),
            'time_of_day': sample.get('time_of_day', 12),  # hour
            'day_of_week': sample.get('day_of_week', 1),   # 1-7
            'commit_size': sample.get('additions', 0) + sample.get('deletions', 0),
            'is_merge': 'merge' in sample.get('text', '').lower()
        }
    
    def _generate_multimodal_labels(self, sample, text, metadata):
        """Generate labels for multimodal tasks based on commit analysis"""
        labels = {}
        
        # Risk prediction (high/low) - based on commit patterns
        risk_score = self._calculate_risk_score(text, metadata)
        labels['risk_prediction'] = 1 if risk_score > 0.5 else 0
        
        # Complexity prediction (simple/medium/complex) - based on changes
        complexity = self._calculate_complexity(text, metadata)
        labels['complexity_prediction'] = complexity
        
        # Hotspot prediction (very_low to very_high) - based on file patterns
        hotspot = self._calculate_hotspot_score(text, metadata)
        labels['hotspot_prediction'] = hotspot
        
        # Urgency prediction (normal/urgent) - based on keywords
        urgency = self._calculate_urgency(text, metadata)
        labels['urgency_prediction'] = urgency
        
        return labels
    
    def _calculate_risk_score(self, text, metadata):
        """Calculate risk score from commit text and metadata"""
        risk_keywords = ['fix', 'bug', 'error', 'crash', 'security', 'vulnerability', 'critical']
        risk_score = 0.0
        
        text_lower = text.lower()
        for keyword in risk_keywords:
            if keyword in text_lower:
                risk_score += 0.2
        
        # Add metadata-based risk
        if metadata['commit_size'] > 1000:  # Large commits are risky
            risk_score += 0.2
        if metadata['files_changed'] > 10:  # Many files changed
            risk_score += 0.1
            
        return min(risk_score, 1.0)
    
    def _calculate_complexity(self, text, metadata):
        """Calculate complexity level (0=simple, 1=medium, 2=complex)"""
        commit_size = metadata['commit_size']
        files_changed = metadata['files_changed']
        
        if commit_size < 50 and files_changed <= 2:
            return 0  # simple
        elif commit_size < 500 and files_changed <= 10:
            return 1  # medium
        else:
            return 2  # complex
    
    def _calculate_hotspot_score(self, text, metadata):
        """Calculate hotspot prediction (0-4 scale)"""
        # Based on files changed and commit frequency patterns
        files_changed = metadata['files_changed']
        
        if files_changed <= 1:
            return 0  # very_low
        elif files_changed <= 3:
            return 1  # low
        elif files_changed <= 7:
            return 2  # medium
        elif files_changed <= 15:
            return 3  # high
        else:
            return 4  # very_high
    
    def _calculate_urgency(self, text, metadata):
        """Calculate urgency (0=normal, 1=urgent)"""
        urgent_keywords = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediately']
        text_lower = text.lower()
        
        for keyword in urgent_keywords:
            if keyword in text_lower:
                return 1
        
        # Large commits on weekends might be urgent
        if metadata['day_of_week'] in [6, 7] and metadata['commit_size'] > 500:
            return 1
            
        return 0

def load_training_data(data_file):
    """Load and prepare training data"""
    logger.info(f"Loading training data from {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Handle different data formats
    if 'data' in data:
        samples = data['data']
    elif isinstance(data, list):
        samples = data
    else:
        samples = [data]
    
    logger.info(f"Loaded {len(samples)} samples")
    return samples

def setup_model_and_training(device, vocab_size=10000):
    """Setup model, optimizer, and training components"""
      # Model configuration
    config = {
        'text_encoder': {
            'vocab_size': vocab_size,
            'embedding_dim': 768,
            'hidden_dim': 256,
            'num_layers': 2,
            'dropout': 0.1,
            'max_length': 512,
            'method': 'lstm'
        },        'metadata_encoder': {
            'categorical_dims': {'author_encoded': 1000, 'season_encoded': 4},  # vocab sizes to match processor output
            'numerical_features': ['numerical_features'],  # single tensor from processor
            'embedding_dim': 128,
            'hidden_dim': 128,
            'dropout': 0.1
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256,
            'dropout': 0.1
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 2},
            'complexity_prediction': {'num_classes': 3},
            'hotspot_prediction': {'num_classes': 5},
            'urgency_prediction': {'num_classes': 2}
        }
    }
    
    # Initialize model
    model = MultiModalFusionNetwork(config).to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"Model created with {total_params:,} total parameters")
    logger.info(f"Trainable parameters: {trainable_params:,}")
      # Optimizer and scheduler
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
      # Loss functions
    loss_fns = {
        'risk_prediction': nn.CrossEntropyLoss().to(device),
        'complexity_prediction': nn.CrossEntropyLoss().to(device),
        'hotspot_prediction': nn.CrossEntropyLoss().to(device),
        'urgency_prediction': nn.CrossEntropyLoss().to(device)
    }
    
    return model, optimizer, scheduler, loss_fns, config

def train_epoch(model, train_loader, optimizer, loss_fns, device, scaler=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    progress_bar = tqdm(train_loader, desc="Training")
    for batch in progress_bar:
        optimizer.zero_grad()
        
        # Move data to device
        text_features = batch['text_features'].to(device)
        
        # Handle metadata features (dict of tensors from custom collate)
        metadata_features = {}
        for key, value in batch['metadata_features'].items():
            metadata_features[key] = value.to(device)
        
        # Forward pass
        if scaler:
            with torch.cuda.amp.autocast():
                outputs = model(text_features, metadata_features)
                
                # Calculate losses
                batch_loss = 0
                for task, loss_fn in loss_fns.items():
                    labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                    task_loss = loss_fn(outputs[task], labels)
                    batch_loss += task_loss
                    task_losses[task] += task_loss.item()
                    
                    # Calculate accuracy
                    _, predicted = torch.max(outputs[task], 1)
                    task_correct[task] += (predicted == labels).sum().item()
                    task_total[task] += labels.size(0)
            
            scaler.scale(batch_loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(text_features, metadata_features)
            
            # Calculate losses
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            batch_loss.backward()
            optimizer.step()
        
        total_loss += batch_loss.item()
        
        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{batch_loss.item():.4f}",
            'avg_loss': f"{total_loss/len(progress_bar):.4f}"
        })
    
    # Calculate average metrics
    avg_loss = total_loss / len(train_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def validate_epoch(model, val_loader, loss_fns, device):
    """Validate for one epoch"""
    model.eval()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            # Move data to device
            text_features = batch['text_features'].to(device)
            
            # Handle metadata features (dict of tensors from custom collate)
            metadata_features = {}
            for key, value in batch['metadata_features'].items():
                metadata_features[key] = value.to(device)
            
            # Forward pass
            outputs = model(text_features, metadata_features)
            
            # Calculate losses and metrics
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            total_loss += batch_loss.item()
    
    # Calculate average metrics
    avg_loss = total_loss / len(val_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def main():
    """Main training function"""
    logger.info("🚀 MULTIMODAL FUSION MODEL TRAINING")
    logger.info("=" * 60)
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"🔧 Device: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"🎮 GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"🔥 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        torch.cuda.empty_cache()
    
    # Paths
    data_file = Path(__file__).parent.parent.parent / "training_data" / "sample_preview.json"
    output_dir = Path(__file__).parent.parent.parent / "trained_models" / "multimodal_fusion"
    log_dir = Path(__file__).parent.parent.parent / "training_logs" / "multimodal_fusion"
    
    # Create directories
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load data
    if not data_file.exists():
        logger.error(f"❌ Dataset not found: {data_file}")
        return
    
    samples = load_training_data(data_file)
    
    # Take a subset for training (adjust as needed)
    if len(samples) > 10000:
        samples = samples[:10000]
        logger.info(f"Using subset of {len(samples)} samples for training")
      # Initialize processors
    text_processor = TextProcessor()
    metadata_processor = MetadataProcessor()
    
    # Build vocabulary from sample texts
    texts = [sample.get('text', '') or sample.get('message', '') for sample in samples]
    text_processor.build_vocab(texts, vocab_size=10000)
    
    # Fit metadata processor with samples
    logger.info("🔧 Fitting metadata processor...")
    metadata_processor.fit(samples)
    
    # Create dataset
    dataset = MultimodalDataset(samples, text_processor, metadata_processor)
    
    # Split dataset
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    logger.info(f"📊 Train samples: {len(train_dataset)}")
    logger.info(f"📊 Val samples: {len(val_dataset)}")
      # Data loaders
    batch_size = 32 if device.type == 'cuda' else 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)
    
    # Setup model and training
    model, optimizer, scheduler, loss_fns, config = setup_model_and_training(
        device, vocab_size=len(text_processor.vocab)
    )
    
    # Mixed precision setup
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    # Training loop
    num_epochs = 20
    best_val_accuracy = 0
    patience = 5
    patience_counter = 0
    
    logger.info(f"🏃 Starting training for {num_epochs} epochs")
    
    for epoch in range(num_epochs):
        logger.info(f"\n📅 Epoch {epoch+1}/{num_epochs}")
        
        # Train
        train_loss, train_accuracies, train_avg_acc = train_epoch(
            model, train_loader, optimizer, loss_fns, device, scaler
        )
        
        # Validate
        val_loss, val_accuracies, val_avg_acc = validate_epoch(
            model, val_loader, loss_fns, device
        )
        
        # Scheduler step
        scheduler.step(val_loss)
        
        # Log metrics
        logger.info(f"Train Loss: {train_loss:.4f}, Train Acc: {train_avg_acc:.4f}")
        logger.info(f"Val Loss: {val_loss:.4f}, Val Acc: {val_avg_acc:.4f}")
        
        for task in loss_fns.keys():
            logger.info(f"  {task}: Train {train_accuracies[task]:.4f}, Val {val_accuracies[task]:.4f}")
        
        # Save best model
        if val_avg_acc > best_val_accuracy:
            best_val_accuracy = val_avg_acc
            patience_counter = 0
            
            # Save model
            save_dict = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': config,
                'epoch': epoch + 1,
                'best_val_accuracy': best_val_accuracy,
                'val_accuracies': val_accuracies,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }
            
            torch.save(save_dict, output_dir / 'best_multimodal_fusion_model.pth')
            logger.info(f"💾 Saved best model (Val Acc: {best_val_accuracy:.4f})")
        else:
            patience_counter += 1
            
        # Early stopping
        if patience_counter >= patience:
            logger.info(f"⏹️ Early stopping after {patience} epochs without improvement")
            break
    
    logger.info(f"\n🎉 Training completed!")
    logger.info(f"Best validation accuracy: {best_val_accuracy:.4f}")
    
    # Save final model
    final_save_dict = {
        'model_state_dict': model.state_dict(),
        'config': config,
        'final_epoch': epoch + 1,
        'final_val_accuracy': val_avg_acc,
        'best_val_accuracy': best_val_accuracy,
        'text_processor': text_processor,
        'metadata_processor': metadata_processor
    }
    
    torch.save(final_save_dict, output_dir / 'final_multimodal_fusion_model.pth')
    logger.info(f"💾 Saved final model")

if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\training\multitask_trainer.py
```py
"""
Multi-Task Trainer for Multi-Modal Fusion Network
Triển khai Joint Multi-Task Learning với Dynamic Loss Weighting
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
from pathlib import Path
import json
import logging
from collections import defaultdict
from sklearn.metrics import classification_report, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

class MultiModalDataset(Dataset):
    """
    Dataset cho Multi-Modal Fusion Network
    """
    
    def __init__(self, samples: List[Dict], text_processor, metadata_processor, 
                 label_encoders: Dict[str, Any]):
        """
        Args:
            samples: List of samples with text, metadata, and labels
            text_processor: TextProcessor instance
            metadata_processor: MetadataProcessor instance
            label_encoders: Dict mapping task names to label encoders
        """
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.label_encoders = label_encoders
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # Process text
        text = sample.get('text', '')
        if self.text_processor.method == "lstm":
            text_input = self.text_processor.encode_text_lstm(text)
            attention_mask = None
        else:
            text_encoding = self.text_processor.encode_text_transformer(text)
            text_input = text_encoding['input_ids']
            attention_mask = text_encoding['attention_mask']
        
        # Process metadata
        metadata_input = self.metadata_processor.process_sample(sample)
        
        # Process labels
        labels = {}
        for task_name, label_value in sample.get('labels', {}).items():
            if task_name in self.label_encoders:
                try:
                    encoded_label = self.label_encoders[task_name].transform([label_value])[0]
                    labels[task_name] = torch.tensor(encoded_label, dtype=torch.long)
                except ValueError:
                    # Handle unknown labels
                    labels[task_name] = torch.tensor(0, dtype=torch.long)
        
        result = {
            'text_input': text_input,
            'metadata_input': metadata_input,
            'labels': labels
        }
        
        if attention_mask is not None:
            result['attention_mask'] = attention_mask
        
        return result

def collate_fn(batch):
    """
    Custom collate function for DataLoader
    """
    # Stack text inputs
    text_inputs = torch.stack([item['text_input'] for item in batch])
    
    # Stack attention masks if present
    attention_masks = None
    if 'attention_mask' in batch[0]:
        attention_masks = torch.stack([item['attention_mask'] for item in batch])
    
    # Stack metadata inputs
    metadata_keys = batch[0]['metadata_input'].keys()
    metadata_inputs = {}
    for key in metadata_keys:
        metadata_inputs[key] = torch.stack([item['metadata_input'][key] for item in batch])
    
    # Collect labels
    task_names = batch[0]['labels'].keys()
    labels = {}
    for task_name in task_names:
        labels[task_name] = torch.stack([item['labels'][task_name] for item in batch])
    
    result = {
        'text_input': text_inputs,
        'metadata_input': metadata_inputs,
        'labels': labels
    }
    
    if attention_masks is not None:
        result['attention_mask'] = attention_masks
    
    return result

class DynamicLossWeighting:
    """
    Dynamic Loss Weighting cho Multi-Task Learning
    """
    
    def __init__(self, task_names: List[str], method: str = "uncertainty", alpha: float = 0.5):
        """
        Args:
            task_names: List of task names
            method: "uncertainty", "gradnorm", "equal"
            alpha: Learning rate for weight updates
        """
        self.task_names = task_names
        self.method = method
        self.alpha = alpha
        
        # Initialize weights
        self.weights = {task: 1.0 for task in task_names}
        self.loss_history = {task: [] for task in task_names}
        self.prev_losses = {task: 0.0 for task in task_names}
        
        if method == "uncertainty":
            # Learnable uncertainty parameters
            self.log_vars = nn.Parameter(torch.zeros(len(task_names)))
    
    def compute_weighted_loss(self, losses: Dict[str, torch.Tensor], 
                            model: nn.Module = None) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute weighted loss
        """
        if self.method == "equal":
            # Equal weighting
            total_loss = sum(losses.values())
            return total_loss, self.weights
        
        elif self.method == "uncertainty":
            # Uncertainty weighting (Kendall et al.)
            total_loss = 0
            for i, (task, loss) in enumerate(losses.items()):
                precision = torch.exp(-self.log_vars[i])
                total_loss += precision * loss + self.log_vars[i]
            
            return total_loss, self.weights
        
        elif self.method == "gradnorm":
            # GradNorm (Chen et al.)
            if model is None:
                # Fallback to equal weighting
                total_loss = sum(losses.values())
                return total_loss, self.weights
            
            # Compute weighted loss
            weighted_losses = []
            for task in self.task_names:
                weighted_losses.append(self.weights[task] * losses[task])
            
            total_loss = sum(weighted_losses)
            
            # Update weights based on gradient norms (simplified version)
            self._update_gradnorm_weights(losses, model)
            
            return total_loss, self.weights
    
    def _update_gradnorm_weights(self, losses: Dict[str, torch.Tensor], model: nn.Module):
        """
        Update weights using GradNorm algorithm (simplified)
        """
        # This is a simplified version - full GradNorm requires more complex implementation
        for task in self.task_names:
            current_loss = losses[task].item()
            self.loss_history[task].append(current_loss)
            
            if len(self.loss_history[task]) > 1:
                # Simple heuristic: increase weight if loss is increasing
                loss_change = current_loss - self.prev_losses[task]
                if loss_change > 0:
                    self.weights[task] = min(self.weights[task] * 1.1, 5.0)
                else:
                    self.weights[task] = max(self.weights[task] * 0.95, 0.1)
            
            self.prev_losses[task] = current_loss

class MultiTaskTrainer:
    """
    Multi-Task Trainer cho Multi-Modal Fusion Network
    """
    
    def __init__(self, 
                 model: nn.Module,
                 task_configs: Dict[str, int],
                 loss_weighting_method: str = "uncertainty",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 save_dir: str = "./models/multimodal_fusion"):
        """
        Args:
            model: MultiModalFusionNetwork instance
            task_configs: Dict mapping task names to number of classes
            loss_weighting_method: "uncertainty", "gradnorm", "equal"
            device: Training device
            save_dir: Directory to save models and logs
        """
        self.model = model.to(device)
        self.task_configs = task_configs
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Loss weighting
        self.loss_weighting = DynamicLossWeighting(
            task_names=list(task_configs.keys()),
            method=loss_weighting_method
        )
          # Loss functions
        self.criterion = nn.CrossEntropyLoss()
        
        # Training history
        self.train_history = defaultdict(list)
        self.val_history = defaultdict(list)
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup logging"""
        log_file = self.save_dir / "training.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def train_epoch(self, train_loader: DataLoader, optimizer: optim.Optimizer) -> Dict[str, float]:
        """
        Train one epoch
        """
        self.model.train()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        total_batches = len(train_loader)
        
        for batch_idx, batch in enumerate(train_loader):
            # Move to device
            text_input = batch['text_input'].to(self.device)
            metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            attention_mask = batch.get('attention_mask')
            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = self.model(text_input, metadata_input, attention_mask)
            
            # Compute losses for each task
            task_losses = {}
            task_accuracies = {}
            
            for task_name, logits in outputs.items():
                if task_name in labels:
                    task_loss = self.criterion(logits, labels[task_name])
                    task_losses[task_name] = task_loss
                    
                    # Compute accuracy
                    predictions = torch.argmax(logits, dim=1)
                    accuracy = (predictions == labels[task_name]).float().mean()
                    task_accuracies[task_name] = accuracy
                    
                    epoch_losses[task_name].append(task_loss.item())
                    epoch_accuracies[task_name].append(accuracy.item())
            
            # Compute weighted loss
            total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
            
            # Backward pass
            total_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # Log progress
            if batch_idx % 100 == 0:
                self.logger.info(f"Batch {batch_idx}/{total_batches} - Total Loss: {total_loss.item():.4f}")
                for task_name, loss in task_losses.items():
                    self.logger.info(f"  {task_name}: Loss={loss.item():.4f}, Acc={task_accuracies[task_name].item():.4f}")
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """
        Validate one epoch
        """
        self.model.eval()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        all_predictions = defaultdict(list)
        all_labels = defaultdict(list)
        
        with torch.no_grad():
            for batch in val_loader:
                # Move to device
                text_input = batch['text_input'].to(self.device)
                metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                # Forward pass
                outputs = self.model(text_input, metadata_input, attention_mask)
                
                # Compute losses and metrics
                for task_name, logits in outputs.items():
                    if task_name in labels:
                        task_loss = self.criterion(logits, labels[task_name])
                        epoch_losses[task_name].append(task_loss.item())
                        
                        # Predictions and accuracy
                        predictions = torch.argmax(logits, dim=1)
                        accuracy = (predictions == labels[task_name]).float().mean()
                        epoch_accuracies[task_name].append(accuracy.item())
                        
                        # Store for detailed metrics
                        all_predictions[task_name].extend(predictions.cpu().numpy())
                        all_labels[task_name].extend(labels[task_name].cpu().numpy())
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
                
                # Compute F1 score
                if task_name in all_predictions:
                    f1 = f1_score(all_labels[task_name], all_predictions[task_name], average='weighted')
                    epoch_results[f"{task_name}_f1"] = f1
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results, all_predictions, all_labels
    
    def train(self, 
              train_loader: DataLoader, 
              val_loader: DataLoader,
              num_epochs: int = 50,
              learning_rate: float = 1e-3,
              weight_decay: float = 1e-5,
              patience: int = 10,
              save_best: bool = True) -> Dict[str, List[float]]:
        """
        Main training loop
        """
        # Optimizer and scheduler
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience//2, factor=0.5)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        self.logger.info(f"Starting training for {num_epochs} epochs")
        self.logger.info(f"Tasks: {list(self.task_configs.keys())}")
        self.logger.info(f"Device: {self.device}")
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            # Training
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Training...")
            train_results = self.train_epoch(train_loader, optimizer)
            
            # Validation
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Validation...")
            val_results, val_predictions, val_labels = self.validate_epoch(val_loader)
            
            # Update learning rate
            scheduler.step(val_results['total_loss'])
            
            # Log results
            epoch_time = time.time() - start_time
            self.logger.info(f"Epoch {epoch+1} completed in {epoch_time:.2f}s")
            self.logger.info(f"Train Loss: {train_results['total_loss']:.4f}, Val Loss: {val_results['total_loss']:.4f}")
            
            for task_name in self.task_configs.keys():
                if f"{task_name}_accuracy" in train_results and f"{task_name}_accuracy" in val_results:
                    self.logger.info(f"  {task_name}: Train Acc={train_results[f'{task_name}_accuracy']:.4f}, "
                                   f"Val Acc={val_results[f'{task_name}_accuracy']:.4f}")
            
            # Save history
            for key, value in train_results.items():
                self.train_history[key].append(value)
            for key, value in val_results.items():
                self.val_history[key].append(value)
            
            # Early stopping and model saving
            if val_results['total_loss'] < best_val_loss:
                best_val_loss = val_results['total_loss']
                patience_counter = 0
                
                if save_best:
                    self.save_model(epoch, val_results, "best_model.pth")
                    self.logger.info(f"New best model saved with validation loss: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {patience} epochs without improvement")
                break
            
            # Save checkpoint
            if (epoch + 1) % 10 == 0:
                self.save_model(epoch, val_results, f"checkpoint_epoch_{epoch+1}.pth")
        
        # Save final model
        self.save_model(epoch, val_results, "final_model.pth")
        
        # Save training history
        self._save_training_history()
        
        # Generate training plots
        self._plot_training_history()
        
        return {
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
    
    def save_model(self, epoch: int, metrics: Dict[str, float], filename: str):
        """
        Save model checkpoint
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'task_configs': self.task_configs,
            'metrics': metrics,
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
        
        # Save loss weighting parameters if using uncertainty method
        if hasattr(self.loss_weighting, 'log_vars'):
            checkpoint['loss_weighting_log_vars'] = self.loss_weighting.log_vars.data
        
        torch.save(checkpoint, self.save_dir / filename)
    
    def _save_training_history(self):
        """
        Save training history to JSON
        """
        history = {
            'train_history': {k: [float(x) for x in v] for k, v in self.train_history.items()},
            'val_history': {k: [float(x) for x in v] for k, v in self.val_history.items()}
        }
        
        with open(self.save_dir / "training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
    
    def _plot_training_history(self):
        """
        Plot training history
        """
        plt.style.use('seaborn-v0_8')
        
        # Plot losses
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training History', fontsize=16)
        
        # Total loss
        axes[0, 0].plot(self.train_history['total_loss'], label='Train')
        axes[0, 0].plot(self.val_history['total_loss'], label='Validation')
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Task-specific losses
        task_names = list(self.task_configs.keys())
        if len(task_names) > 0:
            for i, task_name in enumerate(task_names[:3]):  # Show first 3 tasks
                row = (i + 1) // 2
                col = (i + 1) % 2
                if row < 2 and col < 2:
                    train_key = f"{task_name}_loss"
                    val_key = f"{task_name}_loss"
                    if train_key in self.train_history and val_key in self.val_history:
                        axes[row, col].plot(self.train_history[train_key], label='Train')
                        axes[row, col].plot(self.val_history[val_key], label='Validation')
                        axes[row, col].set_title(f'{task_name} Loss')
                        axes[row, col].set_xlabel('Epoch')
                        axes[row, col].set_ylabel('Loss')
                        axes[row, col].legend()
                        axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_losses.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot accuracies
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Accuracies', fontsize=16)
        
        for i, task_name in enumerate(task_names[:4]):  # Show first 4 tasks
            row = i // 2
            col = i % 2
            train_key = f"{task_name}_accuracy"
            val_key = f"{task_name}_accuracy"
            
            if train_key in self.train_history and val_key in self.val_history:
                axes[row, col].plot(self.train_history[train_key], label='Train')
                axes[row, col].plot(self.val_history[val_key], label='Validation')
                axes[row, col].set_title(f'{task_name} Accuracy')
                axes[row, col].set_xlabel('Epoch')
                axes[row, col].set_ylabel('Accuracy')
                axes[row, col].legend()
                axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_accuracies.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("Training plots saved successfully")
    
    def train_step(self, text_input, metadata_input, targets, optimizer=None):
        """
        Perform a single training step
        Args:
            text_input: Text input tensor
            metadata_input: Metadata input dict
            targets: Target labels dict
            optimizer: Optional optimizer (creates AdamW if None)
        Returns:
            float: Total loss value
        """
        # Create optimizer if not provided
        if optimizer is None:
            optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        
        self.model.train()
        
        # Forward pass
        optimizer.zero_grad()
        outputs = self.model(text_input, metadata_input)
        
        # Compute losses for each task
        task_losses = {}
        for task_name, logits in outputs.items():
            if task_name in targets:
                task_loss = self.criterion(logits, targets[task_name])
                task_losses[task_name] = task_loss
        
        # Compute weighted loss
        total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
        
        # Backward pass
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()

```

### backend\api\deps.py
```py
# KLTN04\backend\api\deps.py
# File chứa các dependencies (phụ thuộc) chung của ứng dụng

# Import AsyncSession từ SQLAlchemy để làm việc với database async
from sqlalchemy.ext.asyncio import AsyncSession

# Import kết nối database từ module database
from db.database import database

# Dependency (phụ thuộc) để lấy database session
async def get_db() -> AsyncSession:
    """
    Dependency tạo và quản lý database session
    
    Cách hoạt động:
    - Tạo một async session mới từ connection pool
    - Yield session để sử dụng trong request
    - Đảm bảo session được đóng sau khi request hoàn thành
    
    Returns:
        AsyncSession: Session database async để tương tác với DB
    """
    # Tạo và quản lý session thông qua context manager
    async with database.session() as session:
        # Yield session để sử dụng trong route
        yield session
        # Session sẽ tự động đóng khi ra khỏi block with
```

### backend\api\__init__.py
```py

```

### backend\api\routes\ai_suggestions.py
```py

```

### backend\api\routes\auth.py
```py
# KLTN04\backend\api\routes\auth.py
from fastapi import APIRouter, Request, HTTPException
from core.oauth import oauth
from fastapi.responses import RedirectResponse
from services.user_service import save_user  # Import hàm lưu người dùng
import os

auth_router = APIRouter()

# Endpoint /login để bắt đầu quá trình xác thực với GitHub
@auth_router.get("/login")
async def login(request: Request):
    # Lấy callback URL từ biến môi trường
    redirect_uri = os.getenv("GITHUB_CALLBACK_URL")
    
    # Chuyển hướng người dùng đến trang xác thực GitHub
    return await oauth.github.authorize_redirect(request, redirect_uri)

# Endpoint /auth/callback - GitHub sẽ gọi lại endpoint này sau khi xác thực thành công
@auth_router.get("/auth/callback")
async def auth_callback(request: Request):
    code = request.query_params.get("code")
    if not code:
        raise HTTPException(status_code=400, detail="Missing code")

    # Lấy access token từ GitHub
    token = await oauth.github.authorize_access_token(request)
    
    # Gọi API GitHub để lấy thông tin user cơ bản
    resp = await oauth.github.get("user", token=token)
    profile = resp.json()  # Chuyển response thành dictionary

    # Lấy email nếu không có trong profile
    if not profile.get("email"):
        # Gọi API riêng để lấy danh sách email
        emails_resp = await oauth.github.get("user/emails", token=token)
        emails = emails_resp.json()
        
        # Tìm email được đánh dấu là primary (chính)
        primary_email = next((e["email"] for e in emails if e["primary"]), None)
        
        # Gán email chính vào profile
        profile["email"] = primary_email

    # Kiểm tra thông tin bắt buộc
    if not profile.get("email") or not profile.get("login"):
        raise HTTPException(status_code=400, detail="Missing required user information")

    # Lưu thông tin người dùng vào cơ sở dữ liệu
    user_data = {
        "github_id": profile["id"],
        "github_username": profile["login"],
        "email": profile["email"],
        "avatar_url": profile["avatar_url"],
    }
    await save_user(user_data)

    # Redirect về frontend với token và thông tin người dùng
    redirect_url = (
        f"http://localhost:5173/auth-success"
        f"?token={token['access_token']}"
        f"&username={profile['login']}"
        f"&email={profile['email']}"
        f"&avatar_url={profile['avatar_url']}"
    )

    # Thực hiện chuyển hướng về frontend
    return RedirectResponse(redirect_url)
```

### backend\api\routes\commit_routes.py
```py
# File: backend/api/routes/commit_routes.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Header
from fastapi.responses import JSONResponse
from typing import List, Optional
import pandas as pd
from services.model_loader import predict_commit
from pathlib import Path
import tempfile
import httpx

router = APIRouter(prefix="/api/commits", tags=["Commit Analysis"])

@router.get("/analyze-github/{owner}/{repo}")
async def analyze_github_commits(
    owner: str,
    repo: str,
    authorization: str = Header(..., alias="Authorization"),
    per_page: int = 30,
    since: Optional[str] = None,
    until: Optional[str] = None
):
    """
    Phân tích commit từ repository GitHub
    
    Args:
        owner: Tên chủ repo
        repo: Tên repository
        authorization: Token GitHub (Format: Bearer <token>)
        per_page: Số commit tối đa cần phân tích (1-100)
        since: Lọc commit từ ngày (YYYY-MM-DDTHH:MM:SSZ)
        until: Lọc commit đến ngày (YYYY-MM-DDTHH:MM:SSZ)
    
    Returns:
        {
            "repo": f"{owner}/{repo}",
            "total": int,
            "critical": int,
            "critical_percentage": float,
            "details": List[dict],
            "analysis_date": str
        }
    """
    try:
        # Validate input
        if per_page < 1 or per_page > 100:
            raise HTTPException(
                status_code=400,
                detail="per_page must be between 1 and 100"
            )

        # Configure GitHub API request
        headers = {
            "Authorization": authorization,
            "Accept": "application/vnd.github.v3+json"
        }
        params = {
            "per_page": per_page,
            "since": since,
            "until": until
        }
        
        # Fetch commits from GitHub
        async with httpx.AsyncClient() as client:
            # Get first page to check repo accessibility
            initial_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(initial_url, headers=headers, params={**params, "per_page": 1})
            
            if response.status_code == 404:
                raise HTTPException(
                    status_code=404,
                    detail="Repository not found or access denied"
                )
            response.raise_for_status()

            # Get all requested commits
            full_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(full_url, headers=headers, params=params)
            response.raise_for_status()
            commits_data = response.json()

        # Prepare analysis data
        commits_for_analysis = [
            {
                "id": commit["sha"],
                "message": commit["commit"]["message"],
                "date": commit["commit"]["committer"]["date"] if commit["commit"]["committer"] else None
            }
            for commit in commits_data
            if commit.get("sha") and commit.get("commit", {}).get("message")
        ]

        # Analyze commits
        results = {
            "repo": f"{owner}/{repo}",
            "total": len(commits_for_analysis),
            "critical": 0,
            "critical_percentage": 0.0,
            "details": [],
            "analysis_date": datetime.utcnow().isoformat()
        }

        for commit in commits_for_analysis:
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
            
            results["details"].append({
                "id": commit["id"],
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message'],
                "date": commit["date"]
            })

        # Calculate percentage
        if results["total"] > 0:
            results["critical_percentage"] = round(
                (results["critical"] / results["total"]) * 100, 2
            )

        return results

    except httpx.HTTPStatusError as e:
        error_detail = "GitHub API error"
        if e.response.status_code == 403:
            error_detail = "API rate limit exceeded" if "rate limit" in str(e.response.content) else "Forbidden"
        elif e.response.status_code == 401:
            error_detail = "Invalid GitHub token"
        
        raise HTTPException(
            status_code=e.response.status_code,
            detail=f"{error_detail}: {e.response.text}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing GitHub commits: {str(e)}"
        )
@router.post("/analyze-text")
async def analyze_commit_text(message: str):
    """
    Phân tích một commit message dạng text
    
    Args:
        message: Nội dung commit message
    
    Returns:
        {"is_critical": 0|1, "message": string}
    """
    try:
        is_critical = predict_commit(message)
        return {
            "is_critical": is_critical,
            "message": "Phân tích thành công",
            "input_sample": message[:100] + "..." if len(message) > 100 else message
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lỗi phân tích: {str(e)}")

@router.post("/analyze-json")
async def analyze_commits_json(commits: List[dict]):
    """
    Phân tích nhiều commit từ JSON
    
    Args:
        commits: List[{"id": string, "message": string}]
    
    Returns:
        {"total": int, "critical": int, "details": List[dict]}
    """
    try:
        results = {
            "total": len(commits),
            "critical": 0,
            "details": []
        }
        
        for commit in commits:
            if not isinstance(commit, dict) or 'message' not in commit:
                continue
                
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
                
            results["details"].append({
                "id": commit.get("id", ""),
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message']
            })
            
        return JSONResponse(content=results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lỗi phân tích hàng loạt: {str(e)}")

@router.post("/analyze-csv", response_model=dict)
async def analyze_commits_csv(file: UploadFile = File(...)):
    """
    Phân tích commit từ file CSV
    
    Args:
        file: File CSV có cột 'message' hoặc 'commit_message'
    
    Returns:
        {"filename": string, "total": int, "critical": int}
    """
    try:
        # Lưu file tạm
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(await file.read())
            tmp_path = Path(tmp.name)
        
        # Đọc file CSV
        df = pd.read_csv(tmp_path)
        tmp_path.unlink()  # Xóa file tạm
        
        # Kiểm tra cột message
        message_col = 'message' if 'message' in df.columns else 'commit_message'
        if message_col not in df.columns:
            raise HTTPException(status_code=400, detail="File thiếu cột 'message' hoặc 'commit_message'")
        
        # Phân tích
        results = {
            "filename": file.filename,
            "total": len(df),
            "critical": 0,
            "sample_results": []
        }
        
        df['is_critical'] = df[message_col].apply(predict_commit)
        results["critical"] = int(df['is_critical'].sum())
        
        # Lấy 5 kết quả mẫu
        sample = df.head(5).to_dict('records')
        results["sample_results"] = [{
            "message": row[message_col][:100] + "..." if len(row[message_col]) > 100 else row[message_col],
            "is_critical": bool(row['is_critical'])
        } for row in sample]
        
        return results
        
    except Exception as e:
        if tmp_path.exists():
            tmp_path.unlink()
        raise HTTPException(status_code=500, detail=f"Lỗi xử lý file: {str(e)}")
```

### backend\api\routes\github.py
```py
# backend/api/routes/github.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.repo_service import get_repo_data
from services.commit_service import save_commit
from services.repo_service import get_repo_id_by_owner_and_name
from services.user_service import get_user_id_by_github_username
from services.branch_service import save_branch
from sqlalchemy.future import select
from fastapi import APIRouter, Depends
from services.repo_service import save_repository
from datetime import datetime
from sqlalchemy import select
from db.models.commits import commits
from db.models.repositories import repositories  # để lấy access token
from schemas.commit import CommitCreate  # schema
from services.github_service import fetch_commits  # hàm gọi GitHub API
from sqlalchemy.ext.asyncio import AsyncSession
from schemas.commit import CommitOut
from db.database import database

from services.branch_service import save_branches
from services.issue_service import save_issue, save_issues # Import cả hai hàm
github_router = APIRouter()

# Endpoint lấy thông tin repository cụ thể
@github_router.get("/github/{owner}/{repo}")
async def fetch_repo(owner: str, repo: str):
    return await get_repo_data(owner, repo)

@github_router.get("/github/repos")
async def get_user_repos(request: Request):
    # Lấy token từ header Authorization
    token = request.headers.get("Authorization")
    
    # Kiểm tra token hợp lệ (phải bắt đầu bằng "token ")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    # Gọi GitHub API để lấy danh sách repo
    async with httpx.AsyncClient() as client:
        resp = await client.get(
            "https://api.github.com/user/repos",
            headers={"Authorization": token}
        )
        # Nếu lỗi thì raise exception
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)
    
    # Trả về kết quả dạng JSON
    return resp.json()

# Endpoint lấy danh sách commit của một repository
@github_router.get("/github/{owner}/{repo}/commits")
async def get_commits(owner: str, repo: str, request: Request, branch: str = "main"):
    # Lấy token từ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Gọi GitHub API để lấy commit
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        # Xử lý trường hợp repository trống (409)
        if resp.status_code == 409:
            return []
        # Xử lý lỗi khác
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

def get_db():
    return database
# Lấy danh sách commit từ database
@github_router.get("/github/{owner}/{repo}/commits/db")
async def get_commits_from_db(owner: str, repo: str, db: AsyncSession = Depends(get_db)):
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    query = select(commits).where(commits.c.repo_id == repo_id)
    result = await db.fetch_all(query)
    return result

@github_router.get("/github/{owner}/{repo}/branches")
async def get_branches(owner: str, repo: str, request: Request):
    # Lấy token từ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Gọi GitHub API lấy danh sách branch
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

# Endpoint lưu commit vào database
@github_router.post("/github/{owner}/{repo}/save-commits")
async def save_repo_commits(owner: str, repo: str, request: Request, branch: str = None):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Nếu không truyền branch, lấy branch mặc định từ GitHub
    if not branch:
        async with httpx.AsyncClient() as client:
            repo_url = f"https://api.github.com/repos/{owner}/{repo}"
            headers = {"Authorization": token}
            repo_resp = await client.get(repo_url, headers=headers)
            if repo_resp.status_code != 200:
                raise HTTPException(status_code=repo_resp.status_code, detail=repo_resp.text)
            repo_data = repo_resp.json()
            branch = repo_data.get("default_branch", "main")

    # Lấy danh sách commit từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        commit_list = resp.json()

    # Lấy repo_id từ cơ sở dữ liệu
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    # Lưu từng commit vào cơ sở dữ liệu
    async with httpx.AsyncClient() as client:
        for commit in commit_list:
            # Lấy thông tin chi tiết của commit
            commit_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit['sha']}"
            commit_resp = await client.get(commit_url, headers={"Authorization": token})
            if commit_resp.status_code != 200:
                continue  # Bỏ qua commit nếu không lấy được thông tin chi tiết

            commit_details = commit_resp.json()
            stats = commit_details.get("stats", {})
            commit_data = {
                "sha": commit["sha"],
                "message": commit["commit"]["message"],
                "author_name": commit["commit"]["author"]["name"],
                "author_email": commit["commit"]["author"]["email"],
                "date": datetime.strptime(commit["commit"]["author"]["date"], "%Y-%m-%dT%H:%M:%SZ"),
                "insertions": stats.get("additions", 0),
                "deletions": stats.get("deletions", 0),
                "files_changed": stats.get("total", 0),
                "repo_id": repo_id,
            }
            await save_commit(commit_data)

    return {"message": "Commits saved successfully!"}
#lưu branchbranch vào database
@github_router.post("/github/{owner}/{repo}/save-branches")
async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Lấy danh sách branch từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()

    # Lưu branch vào database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for branch in branches:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
        }
        await save_branch(branch_data)

    return {"message": "Branches saved successfully!"}
# lưu issues vào database
@github_router.post("/github/{owner}/{repo}/save-issues")
async def save_issues(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Lấy danh sách issue từ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        issues = resp.json()

    
    # Lưu issue vào database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for issue in issues:
        issue_data = {
            "title": issue["title"],
            "body": issue["body"],
            "state": issue["state"],
            "created_at": issue["created_at"],
            "updated_at": issue["updated_at"],
            "repo_id": repo_id,
        }
        await save_issue(issue_data)

    return {"message": "Issues saved successfully!"}
#save repo vào database
async def save_repo(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        repo_data = resp.json()

    repo_entry = {
        "github_id": repo_data["id"],
        "name": repo_data["name"],
        "owner": repo_data["owner"]["login"],
        "description": repo_data["description"],
        "stars": repo_data["stargazers_count"],
        "forks": repo_data["forks_count"],
        "language": repo_data["language"],
        "open_issues": repo_data["open_issues_count"],
        "url": repo_data["html_url"],
    }

    try:
        await save_repository(repo_entry)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving repository: {str(e)}")

def get_db():
    return database

# Endpoint lấy tất cả commit từ database
@github_router.get("/commits")
async def get_commits(db = Depends(get_db)):
    query = commits.select()  # Lấy tất cả commit
    result = await db.fetch_all(query)
    return result

# Endpoint đồng bộ commit từ GitHub về database
@github_router.get("/sync-commits")
async def sync_commits(
    repo_id: int,
    branch: str = "main",
    since: str = None,
    until: str = None,
    db: AsyncSession = Depends(get_db)
):
    # 1. Lấy thông tin repository từ database
    repo = await db.scalar(select(repositories).where(repositories.c.id == repo_id))
    if not repo:
        raise HTTPException(status_code=404, detail="Repository không tồn tại")

    # 2. Gọi GitHub API lấy commit với các tham số lọc
    commits_data = await fetch_commits(
        token=repo.token,  # Access token
        owner=repo.owner,  # Chủ repository
        name=repo.name,  # Tên repository
        branch=branch,  # Branch cần lấy
        since=since,  # Lọc từ thời gian
        until=until  # Lọc đến thời gian
    )

    # 3. Lưu commit mới vào database
    new_commits = []
    for item in commits_data:
        sha = item["sha"]
        # Kiểm tra commit đã tồn tại chưa
        existing = await db.scalar(select(commits).where(commits.c.sha == sha))
        if existing:
            continue  # Bỏ qua nếu đã tồn tại

        # Tạo commit mới
        new_commit = CommitCreate(
            sha=sha,
            message=item["commit"]["message"],
            author=item["commit"]["author"]["name"],
            date=item["commit"]["author"]["date"],
            repository_id=repo.id
        )
        commit_obj = commits.insert().values(**new_commit.dict())
        await db.execute(commit_obj)
        new_commits.append(new_commit)

    await db.commit()

    return {
        "message": f"Đồng bộ thành công {len(new_commits)} commit.",
        "data": [c.sha for c in new_commits]
    }
# đồng bộ toàn bộ dữ liệu
@github_router.post("/github/{owner}/{repo}/sync-all")
async def sync_all(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # Đồng bộ repository
    await save_repo(owner, repo, request)

    # Đồng bộ branches
    await save_branches(owner, repo, request)

    # Đồng bộ commits
    await save_repo_commits(owner, repo, request)

    # Đồng bộ issues
    await save_issues(owner, repo, request)

    return {"message": "Đồng bộ toàn bộ dữ liệu thành công!"}
```

### backend\api\routes\gitlab.py
```py

```

### backend\api\routes\repo.py
```py

```

### backend\api\routes\users.py
```py

```

### backend\api\routes\__init__.py
```py

```

### backend\core\config.py
```py
# backend/core/config.py
# File cấu hình chính cho ứng dụng FastAPI

# Import các thư viện cần thiết
import os  # Làm việc với biến môi trường
from fastapi.middleware.cors import CORSMiddleware  # Middleware CORS
from starlette.middleware.sessions import SessionMiddleware  # Middleware quản lý session
from fastapi import FastAPI  # Framework chính
from api.routes.github import github_router  # Router cho GitHub API
from api.routes.auth import auth_router  # Router cho xác thực
from dotenv import load_dotenv  # Đọc file .env

# Nạp biến môi trường từ file .env
load_dotenv()

# Hàm cấu hình các middleware cho ứng dụng
def setup_middlewares(app: FastAPI):
    """
    Thiết lập các middleware cần thiết cho ứng dụng
    
    Args:
        app (FastAPI): Instance của FastAPI app
    """
    
    # Thêm middleware CORS (Cross-Origin Resource Sharing)
    app.add_middleware(
        CORSMiddleware,
        # Danh sách domain được phép truy cập
        allow_origins=[
            "http://localhost:5173",  # Frontend dev (Vite thường chạy ở port 5173)
            "http://localhost:3000"   # Frontend dev (React có thể chạy ở port 3000)
        ],
        allow_credentials=True,  # Cho phép gửi credential (cookies, auth headers)
        allow_methods=["*"],  # Cho phép tất cả HTTP methods
        allow_headers=["*"],  # Cho phép tất cả headers (bao gồm Authorization)
    )

    # Thêm middleware quản lý session
    app.add_middleware(
        SessionMiddleware,
        secret_key=os.getenv('SECRET_KEY')  # Khóa bí mật từ biến môi trường
    )


# Hàm cấu hình các router cho ứng dụng
def setup_routers(app: FastAPI):
    """
    Đăng ký các router chính của ứng dụng
    
    Args:
        app (FastAPI): Instance của FastAPI app
    """
    
    # Đăng ký auth router với prefix /auth
    app.include_router(auth_router, prefix="/auth")
    
    # Đăng ký github router với prefix /api
    app.include_router(github_router, prefix="/api")  # Gộp chung không bị đè lẫn nhau
```

### backend\core\lifespan.py
```py
from fastapi import FastAPI
from contextlib import asynccontextmanager
from db.database import database
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        await database.connect()
        logger.info("✅ Đã kết nối tới database thành công.")
        yield  # Chỉ yield nếu connect thành công
    except Exception as e:
        logger.error(f"❌ Kết nối database thất bại: {e}")
        raise e  # Dừng app nếu không kết nối được DB
    finally:
        try:
            await database.disconnect()
            logger.info("🛑 Đã ngắt kết nối database.")
        except Exception as e:
            logger.error(f"❌ Lỗi khi ngắt kết nối database: {e}")

```

### backend\core\logger.py
```py
# core/logger.py

import logging

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,  # Hiện log từ cấp INFO trở lên
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

```

### backend\core\oauth.py
```py
# backend/core/oauth.py
# File cấu hình OAuth cho ứng dụng, chủ yếu dùng cho GitHub OAuth

# Import các thư viện cần thiết
import os  # Để làm việc với biến môi trường
from dotenv import load_dotenv  # Để đọc file .env
from authlib.integrations.starlette_client import OAuth  # Thư viện OAuth cho Starlette/FastAPI

# Load các biến môi trường từ file .env
load_dotenv()

# Khởi tạo instance OAuth
oauth = OAuth()

# Đăng ký provider GitHub cho OAuth
oauth.register(
    name='github',  # Tên provider
    
    # Client ID từ ứng dụng GitHub OAuth App
    client_id=os.getenv('GITHUB_CLIENT_ID'),
    
    # Client Secret từ ứng dụng GitHub OAuth App
    client_secret=os.getenv('GITHUB_CLIENT_SECRET'),
    
    # URL để lấy access token
    access_token_url='https://github.com/login/oauth/access_token',
    
    # Các params thêm khi lấy access token (None nếu không có)
    access_token_params=None,
    
    # URL để xác thực
    authorize_url='https://github.com/login/oauth/authorize',
    
    # Các params thêm khi xác thực (None nếu không có)
    authorize_params=None,
    
    # Base URL cho API GitHub
    api_base_url='https://api.github.com/',
    
    # Các tham số bổ sung cho client
    client_kwargs={
        'scope': 'read:user user:email repo'  # Các quyền yêu cầu
        # read:user - Đọc thông tin user
        # user:email - Đọc email user
        # repo - Truy cập repository
    }
)
```

### backend\core\security.py
```py

```

### backend\migrations\env.py
```py
import os
from dotenv import load_dotenv
from sqlalchemy import engine_from_config, pool
from alembic import context
from db.metadata import metadata  # Import metadata từ metadata.py

# Nạp biến môi trường từ file .env
load_dotenv()

# Lấy DATABASE_URL từ biến môi trường
config = context.config
database_url = os.getenv("DATABASE_URL").replace("asyncpg", "psycopg2")
config.set_main_option("sqlalchemy.url", database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    from logging.config import fileConfig
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```

### backend\models\commit_model.py
```py
# KLTN04\backend\models\commit_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
import joblib

class CommitClassifier:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.model = RandomForestClassifier()
        self.labels = ['normal', 'critical']  # 0: normal, 1: critical/bugfix

    def train(self, df: pd.DataFrame):
        """Huấn luyện model từ dataframe"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical']  # Cột nhãn (0/1)
        self.model.fit(X, y)
        
    def predict(self, new_messages: list):
        """Dự đoán commit quan trọng cần review"""
        X_new = self.vectorizer.transform(new_messages)
        return self.model.predict(X_new)
    
    def save(self, path='models/commit_classifier.joblib'):
        """Lưu model"""
        joblib.dump({
            'vectorizer': self.vectorizer,
            'model': self.model
        }, path)
    
    @classmethod
    def load(cls, path='models/commit_classifier.joblib'):
        """Load model đã lưu"""
        data = joblib.load(path)
        classifier = cls()
        classifier.vectorizer = data['vectorizer']
        classifier.model = data['model']
        return classifier
```

### backend\models\task_model.py
```py
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class TaskAssigner:
    def __init__(self):
        self.skill_matrix = None
    
    def build_skill_matrix(self, developers: list, tasks: list):
        """Tạo ma trận kỹ năng developer-task"""
        # Vector hóa kỹ năng (ví dụ: [1,0,1] = biết Python, không biết SQL, biết Docker)
        dev_vectors = [d['skill_vector'] for d in developers]
        task_vectors = [t['required_skills'] for t in tasks]
        
        self.skill_matrix = cosine_similarity(task_vectors, dev_vectors)
        return self.skill_matrix
    
    def assign_tasks(self, developers: list, tasks: list):
        """Phân công công việc tối ưu"""
        if self.skill_matrix is None:
            self.build_skill_matrix(developers, tasks)
            
        assignments = []
        for task_idx, task in enumerate(tasks):
            best_dev_idx = np.argmax(self.skill_matrix[task_idx])
            assignments.append({
                'task_id': task['id'],
                'dev_id': developers[best_dev_idx]['id'],
                'fit_score': float(self.skill_matrix[task_idx][best_dev_idx])
            })
        return assignments
```

### backend\schemas\commit.py
```py
from pydantic import BaseModel
from datetime import datetime


class CommitCreate(BaseModel):
    commit_id: str
    message: str
    author_name: str
    author_email: str
    committed_date: datetime
    repository_id: int


class CommitOut(CommitCreate):
    id: int

    class Config:
        from_attributes = True  # Dành cho Pydantic V2 thay cho orm_mode

```

### backend\scripts\commit_analysis_system.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import warnings
warnings.filterwarnings('ignore')

class CommitAnalysisSystem:
    def __init__(self):
        """Khởi tạo hệ thống với cấu hình tối ưu"""
        self.vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            ngram_range=(1, 1)
        )
        self.model = RandomForestClassifier(
            n_estimators=30,
            max_depth=8,
            n_jobs=1,
            class_weight='balanced'
        )
        self.client = None

    def init_dask_client(self):
        """Khởi tạo Dask client"""
        self.client = Client(n_workers=2, threads_per_worker=1, memory_limit='2GB')

    @staticmethod
    def lightweight_heuristic(msg):
        """Hàm heuristic tĩnh để xử lý song song"""
        if not isinstance(msg, str) or not msg.strip():
            return 0
        msg = msg.lower()[:150]
        return int(any(kw in msg for kw in ['fix', 'bug', 'error', 'fail']))

    def process_large_file(self, input_path, output_dir):
        """Xử lý file lớn với Dask """
        try:
            if self.client:
                self.client.close()
            self.init_dask_client()

            # Đọc file với Dask
            ddf = dd.read_csv(
                str(input_path),
                blocksize="20MB",
                dtype={'message': 'string'},
                usecols=['commit', 'message'],
                na_values=['', 'NA', 'N/A', 'nan']
            )
            
            # Sửa lỗi: Thay .notna() bằng .notnull() cho Dask
            ddf = ddf[ddf['message'].notnull()]
            
            # Gán nhãn
            ddf['is_critical'] = ddf['message'].map(
                self.lightweight_heuristic,
                meta=('is_critical', 'int8')
            )
            
           # Lưu kết quả (đã sửa phần compute)
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # Sửa lỗi: Gọi compute() trực tiếp trên to_csv()
            ddf.to_csv(
                str(output_dir / "part_*.csv"),
                index=False
            )
            
            return True
        except Exception as e:
            print(f"🚨 Lỗi xử lý file: {str(e)}")
            return False
        finally:
            if self.client:
                self.client.close()

    def clean_data(self, df):
        """Làm sạch dữ liệu"""
        if 'message' not in df.columns:
            raise ValueError("Thiếu cột 'message' trong dữ liệu")
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df):
        """Gán nhãn tự động"""
        df = self.clean_data(df)
        df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
        return df

    def train_model(self, df):
        """Huấn luyện mô hình"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical'].values
        self.model.fit(X, y)

    def evaluate(self, test_df):
        """Đánh giá mô hình"""
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        print(classification_report(y_test, self.model.predict(X_test)))

    def save_model(self, path):
        """Lưu mô hình"""
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer
        }, str(path))

def main():
    print("🚀 Bắt đầu phân tích commit...")
    system = CommitAnalysisSystem()
    
    input_path = Path("D:/Project/KLTN04/data/oneline.csv")
    output_dir = Path("D:/Project/KLTN04/data/processed")
    
    if system.process_large_file(input_path, output_dir):
        print("✅ Đã xử lý file thành công")
        
        # Nạp và xử lý dữ liệu
        df = pd.concat([pd.read_csv(f) for f in output_dir.glob("part_*.csv")])
        df = system.auto_label(df)
        
        # Huấn luyện và đánh giá
        system.train_model(df)
        test_df = df.sample(frac=0.2, random_state=42)
        system.evaluate(test_df)
        
        # Lưu mô hình
        model_path = "backend/models/commit_classifier.joblib"
        system.save_model(model_path)
        print(f"💾 Đã lưu mô hình tại: {model_path}")

if __name__ == "__main__":
    main()
```

### backend\scripts\commit_analysis_system_v1.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import logging
from typing import Optional, Union
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CommitAnalysisSystem:
    """Hệ thống phân tích commit tự động với khả năng xử lý dữ liệu lớn"""
    
    VERSION = "1.0.0"
    
    def __init__(self, model_params: Optional[dict] = None, 
                 vectorizer_params: Optional[dict] = None):
        """
        Khởi tạo hệ thống phân tích commit
        
        Args:
            model_params: Tham số cho RandomForestClassifier
            vectorizer_params: Tham số cho TfidfVectorizer
        """
        # Cấu hình mặc định
        default_vectorizer_params = {
            'max_features': 1000,
            'stop_words': 'english',
            'ngram_range': (1, 2),  # Thêm bigram
            'min_df': 5,
            'max_df': 0.8
        }
        
        default_model_params = {
            'n_estimators': 100,
            'max_depth': 15,
            'class_weight': 'balanced',
            'random_state': 42
        }
        
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or default_vectorizer_params))
        self.model = RandomForestClassifier(**(model_params or default_model_params))
        self.client = None
        self._is_trained = False

    def init_dask_client(self, **kwargs):
        """Khởi tạo Dask client với cấu hình tùy chọn"""
        default_config = {
            'n_workers': 2,
            'threads_per_worker': 1,
            'memory_limit': '2GB',
            'silence_logs': logging.ERROR
        }
        config = {**default_config, **kwargs}
        
        try:
            self.client = Client(**config)
            logger.info(f"Dask client initialized with config: {config}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Dask client: {str(e)}")
            return False

    @staticmethod
    def lightweight_heuristic(msg: str) -> int:
        """Phân loại commit sử dụng heuristic đơn giản
        
        Args:
            msg: Nội dung commit message
            
        Returns:
            1 nếu là commit quan trọng (bugfix), 0 nếu không
        """
        if not isinstance(msg, str) or not msg.strip():
            return 0
            
        msg = msg.lower()[:200]  # Giới hạn độ dài xử lý
        keywords = {
            'fix', 'bug', 'error', 'fail', 'patch', 
            'resolve', 'crash', 'defect', 'issue'
        }
        return int(any(kw in msg for kw in keywords))

    def process_large_file(self, input_path: Union[str, Path], output_dir: Union[str, Path]) -> bool:
        """Xử lý file dữ liệu lớn bằng Dask"""
        try:
            input_path = Path(input_path)
            output_dir = Path(output_dir)

            if not input_path.exists():
                logger.error(f"Input file not found: {input_path}")
                return False

            logger.info(f"Starting processing large file: {input_path}")
            start_time = datetime.now()

            # Khởi tạo Dask client
            if not self.init_dask_client():
                return False

            try:
                # Đọc và xử lý dữ liệu
                ddf = dd.read_csv(
                    str(input_path),
                    blocksize="10MB",  # Giảm kích thước block để an toàn
                    dtype={'message': 'string'},
                    usecols=['commit', 'message'],
                    na_values=['', 'NA', 'N/A', 'nan']
                )

                # Lọc và gán nhãn
                ddf = ddf[ddf['message'].notnull()]
                ddf['is_critical'] = ddf['message'].map(
                    self.lightweight_heuristic,
                    meta=('is_critical', 'int8')
                )

                # Lưu kết quả
                output_dir.mkdir(exist_ok=True, parents=True)
                output_path = str(output_dir / f"processed_{input_path.stem}.csv")

                # Sử dụng dask.dataframe.to_csv với single_file=True
                ddf.to_csv(
                    output_path,
                    index=False,
                    single_file=True
                )

                logger.info(f"Processing completed in {datetime.now() - start_time}")
                logger.info(f"Results saved to: {output_path}")
                return True

            except Exception as e:
                logger.exception(f"Error during processing: {str(e)}")
                return False

        except Exception as e:
            logger.exception(f"System error: {str(e)}")
            return False

        finally:
            if self.client:
                self.client.close()
                self.client = None

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Làm sạch dữ liệu đầu vào
        
        Args:
            df: DataFrame chứa dữ liệu commit
            
        Returns:
            DataFrame đã được làm sạch
        """
        if 'message' not in df.columns:
            raise ValueError("Input data must contain 'message' column")
            
        df = df.copy()
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df: pd.DataFrame) -> pd.DataFrame:
        """Tự động gán nhãn cho dữ liệu commit
        
        Args:
            df: DataFrame chứa các commit message
            
        Returns:
            DataFrame đã được gán nhãn
        """
        try:
            df = self.clean_data(df)
            df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
            logger.info(f"Label distribution:\n{df['is_critical'].value_counts()}")
            return df
        except Exception as e:
            logger.error(f"Auto-labeling failed: {str(e)}")
            raise

    def train_model(self, df: pd.DataFrame) -> bool:
        """Huấn luyện mô hình phân loại commit
        
        Args:
            df: DataFrame đã được gán nhãn
            
        Returns:
            True nếu huấn luyện thành công
        """
        try:
            logger.info("Starting model training...")
            
            X = self.vectorizer.fit_transform(df['message'])
            y = df['is_critical'].values
            
            self.model.fit(X, y)
            self._is_trained = True
            
            logger.info("Model training completed successfully")
            return True
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return False

    def evaluate(self, test_df: pd.DataFrame) -> None:
        """Đánh giá hiệu suất mô hình
        
        Args:
            test_df: DataFrame chứa dữ liệu test
        """
        if not self._is_trained:
            logger.warning("Model has not been trained yet")
            return
            
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        y_pred = self.model.predict(X_test)
        
        report = classification_report(
            y_test, 
            y_pred, 
            target_names=['normal', 'critical']
        )
        logger.info(f"\nModel evaluation:\n{report}")

    def save_model(self, path: Union[str, Path]) -> bool:
        """Lưu mô hình và vectorizer
        
        Args:
            path: Đường dẫn lưu model
            
        Returns:
            True nếu lưu thành công
        """
        try:
            path = Path(path)
            path.parent.mkdir(exist_ok=True, parents=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'version': self.VERSION,
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, str(path))
            logger.info(f"Model saved to {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            return False

    @classmethod
    def load_model(cls, path: Union[str, Path]):
        """Tải mô hình đã lưu
        
        Args:
            path: Đường dẫn đến file model
            
        Returns:
            Instance của CommitAnalysisSystem với model đã tải
        """
        try:
            path = Path(path)
            model_data = joblib.load(str(path))
            
            system = cls()
            system.model = model_data['model']
            system.vectorizer = model_data['vectorizer']
            system._is_trained = True
            
            logger.info(f"Loaded model (v{model_data.get('version', 'unknown')} "
                       f"created at {model_data.get('timestamp', 'unknown')}")
            return system
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

def main():
    """Entry point cho ứng dụng"""
    try:
        logger.info("🚀 Starting commit analysis system")
        
        # Cấu hình đường dẫn
        input_path = Path("D:/Project/KLTN04/data/oneline.csv")
        output_dir = Path("D:/Project/KLTN04/data/processed")
        model_path = Path("backend/models/commit_classifier_v1.joblib")
        
        # Khởi tạo hệ thống
        system = CommitAnalysisSystem()
        
        # Xử lý dữ liệu lớn
        if system.process_large_file(input_path, output_dir):
            # Tổng hợp kết quả
            df = pd.concat([
                pd.read_csv(f) 
                for f in output_dir.glob("processed_*.csv")
            ])
            
            # Gán nhãn và huấn luyện
            labeled_data = system.auto_label(df)
            system.train_model(labeled_data)
            
            # Đánh giá trên tập test
            test_df = labeled_data.sample(frac=0.2, random_state=42)
            system.evaluate(test_df)
            
            # Lưu model
            if system.save_model(model_path):
                logger.info(f"✅ Pipeline completed successfully. Model saved to {model_path}")
        
    except Exception as e:
        logger.exception("❌ Critical error in main pipeline")
    finally:
        logger.info("🏁 System shutdown")

if __name__ == "__main__":
    main()
```

### backend\services\ai_model.py
```py

```

### backend\services\ai_service.py
```py
from models.commit_model import CommitClassifier
from models.task_model import TaskAssigner
from fastapi import APIRouter

router = APIRouter()
commit_model = CommitClassifier.load()
task_model = TaskAssigner()

@router.post("/analyze-commits")
async def analyze_commits(messages: list[str]):
    predictions = commit_model.predict(messages)
    return {"predictions": predictions.tolist()}

@router.post("/assign-tasks")
async def assign_tasks(developers: list, tasks: list):
    assignments = task_model.assign_tasks(developers, tasks)
    return {"assignments": assignments}
```

### backend\services\branch_service.py
```py
from db.models.branches import branches
from db.database import database
from fastapi import HTTPException, Request
import httpx
import logging

logger = logging.getLogger(__name__)

async def get_repo_id_by_owner_and_name(owner: str, repo: str):
    # Placeholder function for getting repository ID by owner and name
    pass

async def save_branch(branch_data):
    query = branches.insert().values(
        name=branch_data["name"],
        repo_id=branch_data["repo_id"],
    )
    await database.execute(query)

async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()
        logger.info(f"Branches data: {branches}")

    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for branch in branches:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
        }
        await save_branch(branch_data)
```

### backend\services\commit_service.py
```py
from db.models.commits import commits
from db.database import database
from sqlalchemy import select

async def save_commit(commit_data):
    # Kiểm tra commit đã tồn tại chưa
    query = select(commits).where(commits.c.sha == commit_data["sha"])
    existing_commit = await database.fetch_one(query)

    if existing_commit:
        return  # Bỏ qua nếu commit đã tồn tại

    # Chèn commit mới
    query = commits.insert().values(commit_data)
    await database.execute(query)
```

### backend\services\github_service.py
```py
# backend/services/github_service.py
# Service xử lý các tương tác với GitHub API

# Import các thư viện cần thiết
import httpx  # Thư viện HTTP client async
import os  # Làm việc với biến môi trường
from dotenv import load_dotenv  # Đọc file .env
from typing import Optional  # Để khai báo kiểu dữ liệu optional
load_dotenv()  # Nạp biến môi trường từ file .env

# Lấy GitHub token từ biến môi trường
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

# Base URL cho GitHub API
BASE_URL = "https://api.github.com"

# Headers mặc định cho các request GitHub API
headers = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",  # Token xác thực
    "Accept": "application/vnd.github+json",  # Loại response mong muốn
}

async def fetch_from_github(url: str):
    """
    Hàm tổng quát để fetch dữ liệu từ GitHub API
    
    Args:
        url (str): Phần cuối của URL (sau BASE_URL)
    
    Returns:
        dict: Dữ liệu JSON trả về từ GitHub API
    
    Raises:
        HTTPError: Nếu request lỗi
    """
    async with httpx.AsyncClient() as client:
        # Gọi GET request tới GitHub API
        response = await client.get(f"{BASE_URL}{url}", headers=headers)
        # Tự động raise exception nếu có lỗi HTTP
        response.raise_for_status()
        # Trả về dữ liệu dạng JSON
        return response.json()

async def fetch_commits(
    token: str, 
    owner: str, 
    name: str, 
    branch: str, 
    since: Optional[str], 
    until: Optional[str]
):
    """
    Lấy danh sách commit từ repository GitHub
    
    Args:
        token (str): GitHub access token
        owner (str): Chủ repository
        name (str): Tên repository
        branch (str): Tên branch
        since (Optional[str]): Lọc commit từ thời gian này (ISO format)
        until (Optional[str]): Lọc commit đến thời gian này (ISO format)
    
    Returns:
        list: Danh sách commit
    
    Raises:
        HTTPError: Nếu request lỗi
    """
    # Xây dựng URL API để lấy commit
    url = f"https://api.github.com/repos/{owner}/{name}/commits"
    
    # Headers cho request
    headers = {
        "Authorization": f"token {token}",  # Sử dụng token từ tham số
        "Accept": "application/vnd.github+json"  # Loại response mong muốn
    }
    
    # Parameters cho request
    params = {
        "sha": branch  # Lọc theo branch
    }
    
    # Thêm tham số lọc thời gian nếu có
    if since:
        params["since"] = since
    if until:
        params["until"] = until

    # Gọi API GitHub
    async with httpx.AsyncClient() as client:
        res = await client.get(url, headers=headers, params=params)
        # Kiểm tra lỗi HTTP
        res.raise_for_status()
        # Trả về dữ liệu dạng JSON
        return res.json()
```

### backend\services\gitlab_service.py
```py

```

### backend\services\issue_service.py
```py
from db.database import database
from db.models import issues

# Lưu một issue duy nhất
async def save_issue(issue_data):
    query = issues.insert().values(
        github_id=issue_data["github_id"],
        title=issue_data["title"],
        body=issue_data["body"],
        state=issue_data["state"],
        created_at=issue_data["created_at"],
        updated_at=issue_data["updated_at"],
        repo_id=issue_data["repo_id"],
    )
    await database.execute(query)

# Lưu danh sách nhiều issue
async def save_issues(issue_list):
    for issue in issue_list:
        await save_issue(issue)

```

### backend\services\model_loader.py
```py
# KLTN04\backend\services\model_loader.py
import joblib
from pathlib import Path
from typing import Optional, Union
import logging
from functools import lru_cache
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoader:
    _instance = None
    
    def __init__(self):
        try:
            model_path = self._get_model_path()
            logger.info(f"Loading model from {model_path}")
            
            self.model_data = joblib.load(model_path)
            self.model = self.model_data['model']
            self.vectorizer = self.model_data['vectorizer']
            
            # Warm-up predict
            self._warm_up()
            logger.info("Model loaded successfully")
            
        except Exception as e:
            logger.exception("Failed to load model")
            raise

    @staticmethod
    def _get_model_path() -> Path:
        """Validate and return model path"""
        model_path = Path(__file__).parent.parent / "models" / "commit_classifier_v1.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        return model_path

    def _warm_up(self):
        """Warm-up model with sample input"""
        sample = "fix: critical security vulnerability"
        self.predict(sample)
        
    @lru_cache(maxsize=1000)
    def vectorize(self, message: str) -> np.ndarray:
        """Cache vectorized results for frequent messages"""
        return self.vectorizer.transform([message])

    def predict(self, message: str) -> int:
        """Predict if commit is critical (with input validation)"""
        if not message or not isinstance(message, str):
            raise ValueError("Input must be non-empty string")
            
        X = self.vectorize(message.strip())
        return int(self.model.predict(X)[0])

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

def predict_commit(message: str) -> dict:
    """Public API for commit prediction
    
    Returns:
        {
            "prediction": 0|1,
            "confidence": float,
            "error": str|None
        }
    """
    try:
        loader = ModelLoader.get_instance()
        proba = loader.model.predict_proba(loader.vectorize(message))[0]
        return {
            "prediction": loader.predict(message),
            "confidence": float(np.max(proba)),
            "error": None
        }
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        return {
            "prediction": -1,
            "confidence": 0.0,
            "error": str(e)
        }
```

### backend\services\report_generator.py
```py

```

### backend\services\repo_service.py
```py
# backend/services/repo_service.py
from .github_service import fetch_from_github
from db.models.repositories import repositories
from sqlalchemy import select, update
from sqlalchemy.sql import func
from db.database import database

async def get_repo_data(owner: str, repo: str):
    url = f"/repos/{owner}/{repo}"
    data = await fetch_from_github(url)

    # Optionally: lọc data bạn muốn trả về
    return {
        "name": data.get("name"),
        "full_name": data.get("full_name"),
        "description": data.get("description"),
        "owner": data.get("owner", {}).get("login"),
        "stars": data.get("stargazers_count"),
        "forks": data.get("forks_count"),
        "watchers": data.get("watchers_count"),
        "language": data.get("language"),
        "open_issues": data.get("open_issues_count"),
        "url": data.get("html_url"),
        "created_at": data.get("created_at"),
        "updated_at": data.get("updated_at"),
    }


async def get_repo_id_by_owner_and_name(owner: str, repo_name: str):
    query = select(repositories).where(
        repositories.c.owner == owner,
        repositories.c.name == repo_name
    )
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None


async def save_repository(repo_entry):
    # Kiểm tra xem repository đã tồn tại chưa
    query = select(repositories).where(repositories.c.github_id == repo_entry["github_id"])
    existing_repo = await database.fetch_one(query)

    if existing_repo:
        # Nếu repository đã tồn tại, cập nhật thông tin (nếu cần)
        update_query = (
            update(repositories)
            .where(repositories.c.github_id == repo_entry["github_id"])
            .values(
                name=repo_entry["name"],
                owner=repo_entry["owner"],
                description=repo_entry["description"],
                stars=repo_entry["stars"],
                forks=repo_entry["forks"],
                language=repo_entry["language"],
                open_issues=repo_entry["open_issues"],
                url=repo_entry["url"],
                updated_at=func.now(),
            )
        )
        await database.execute(update_query)
    else:
        # Nếu repository chưa tồn tại, chèn mới
        query = repositories.insert().values(repo_entry)
        await database.execute(query)
```

### backend\services\user_service.py
```py
from db.models.users import users
from sqlalchemy import select, insert, update, func
from db.database import database

async def get_user_id_by_github_username(username: str):
    query = select(users).where(users.c.github_username == username)
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None

async def save_user(user_data):
    # Kiểm tra xem người dùng đã tồn tại chưa
    query = select(users).where(users.c.github_id == user_data["github_id"])
    existing_user = await database.fetch_one(query)

    if existing_user:
        # Nếu đã tồn tại, cập nhật thông tin
        query = (
            update(users)
            .where(users.c.github_id == user_data["github_id"])
            .values(
                github_username=user_data["github_username"],
                email=user_data["email"],
                avatar_url=user_data["avatar_url"],
                updated_at=func.now()  # Cập nhật thời gian
            )
        )
    else:
        # Nếu chưa tồn tại, thêm mới
        query = insert(users).values(
            github_id=user_data["github_id"],
            github_username=user_data["github_username"],
            email=user_data["email"],
            avatar_url=user_data["avatar_url"]
        )

    # Thực thi truy vấn
    await database.execute(query)

```

### backend\services\__init__.py
```py

```

### backend\utils\formatter.py
```py

```

### backend\utils\scheduler.py
```py

```

### backend\utils\__init__.py
```py

```

### frontend\eslint.config.js
```js
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]

```

### frontend\vite.config.js
```js
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})

```

### frontend\src\App.jsx
```jsx
import { BrowserRouter as Router, Routes, Route, Navigate } from "react-router-dom";
import Login from "./pages/Login";
import AuthSuccess from "./pages/AuthSuccess";
import Dashboard from "./pages/Dashboard"; 
import RepoDetails from "./pages/RepoDetails";
import CommitTable from './components/commits/CommitTable';

function App() {
  return (
    <Router>
      <Routes>
        {/* ✅ Trang mặc định là Login */}
        <Route path="/" element={<Navigate to="/login" />} />

        {/* Các route chính */}
        <Route path="/login" element={<Login />} />
        <Route path="/auth-success" element={<AuthSuccess />} />
        <Route path="/dashboard" element={<Dashboard />} />
        <Route path="/repo/:owner/:repo" element={<RepoDetails />} />
        <Route path="/commits" element={<CommitTable />} />

      </Routes>
    </Router>
  );
}

export default App;

```

### frontend\src\config.js
```js

```

### frontend\src\main.jsx
```jsx
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css';
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)

```

### frontend\src\api\github.js
```js

```

### frontend\src\components\Branchs\BranchSelector.jsx
```jsx
import { useEffect, useState } from "react";
import { Select, Spin, message, Tag, Typography, Divider } from "antd";
import { GithubOutlined, BranchesOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Option } = Select;
const { Text } = Typography;

const SelectContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 8px 12px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
`;

const StyledSelect = styled(Select)`
  min-width: 240px;
  
  .ant-select-selector {
    border-radius: 6px !important;
    border: 1px solid #d9d9d9 !important;
    transition: all 0.3s !important;
    
    &:hover {
      border-color: #1890ff !important;
    }
  }
  
  .ant-select-selection-item {
    font-weight: 500;
  }
`;

const BranchTag = styled(Tag)`
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  border-radius: 6px;
  background: #f0f5ff;
  color: #1890ff;
  border: 1px solid #d6e4ff;
`;

const BranchSelector = ({ owner, repo, onBranchChange }) => {
  const [branches, setBranches] = useState([]);
  const [loading, setLoading] = useState(true);
  const [selectedBranch, setSelectedBranch] = useState(null);

  useEffect(() => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchBranches = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setBranches(response.data);
        if (response.data.length > 0) {
          setSelectedBranch(response.data[0].name);
          onBranchChange(response.data[0].name);
        }
      } catch (err) {
        console.error(err);
        message.error("Không lấy được danh sách branch");
      } finally {
        setLoading(false);
      }
    };

    fetchBranches();
  }, [owner, repo]);

  const handleChange = (value) => {
    setSelectedBranch(value);
    onBranchChange(value);
  };

  if (loading) return <Spin size="small" />;

  return (
    <div style={{ marginBottom: 16 }}>
      {/* <Divider orientation="left" style={{ fontSize: 32, color: '#666' }}>
        Chọn branch
      </Divider> */}
      
      <SelectContainer>
        <BranchTag>
          <BranchesOutlined />
          <Text strong>Branch:</Text>
        </BranchTag>
        
        <StyledSelect
          value={selectedBranch}
          onChange={handleChange}
          suffixIcon={<GithubOutlined style={{ color: '#1890ff' }} />}
          dropdownMatchSelectWidth={false}
        >
          {branches.map((branch) => (
            <Option key={branch.name} value={branch.name}>
              <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                <BranchesOutlined style={{ color: '#52c41a' }} />
                <Text strong>{branch.name}</Text>
              </div>
            </Option>
          ))}
        </StyledSelect>
      </SelectContainer>
    </div>
  );
};

export default BranchSelector;
```

### frontend\src\components\commits\AnalyzeGitHubCommits.jsx
```jsx
import { useState } from 'react';
import { Button, Badge, Popover, List, Typography, Divider, Spin, Tag, Alert, Tooltip } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled, InfoCircleOutlined } from '@ant-design/icons';
import axios from 'axios';

const { Text, Title } = Typography;

const AnalyzeGitHubCommits = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [popoverVisible, setPopoverVisible] = useState(false);

  const analyzeCommits = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      
      if (!token) {
        throw new Error('Authentication required');
      }

      const response = await axios.get(
        `http://localhost:8000/api/commits/analyze-github/${repo.owner.login}/${repo.name}`,
        {
          headers: { 
            Authorization: `Bearer ${token}`,
            Accept: "application/json"
          },
          params: { 
            per_page: 10,
            // Add cache busting to avoid stale data
            timestamp: Date.now()
          },
          timeout: 10000 // 10 second timeout
        }
      );
      
      if (!response.data) {
        throw new Error('Invalid response data');
      }

      setAnalysis(response.data);
    } catch (err) {
      let errorMessage = 'Failed to analyze commits';
      
      if (err.response) {
        if (err.response.status === 401) {
          errorMessage = 'Please login to analyze commits';
        } else if (err.response.status === 403) {
          errorMessage = 'Access to this repository is denied';
        } else if (err.response.data?.detail) {
          errorMessage = err.response.data.detail;
        }
      } else if (err.message) {
        errorMessage = err.message;
      }

      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const handlePopoverOpen = (visible) => {
    setPopoverVisible(visible);
    if (visible && !analysis && !error) {
      analyzeCommits();
    }
  };

  const getStatusColor = () => {
    if (error) return 'warning';
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (error) return 'Error';
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical` 
      : 'No Issues';
  };

  const getStatusIcon = () => {
    if (error) return <InfoCircleOutlined />;
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const renderContent = () => {
    if (loading) {
      return <Spin size="small" tip="Analyzing commits..." />;
    }

    if (error) {
      return (
        <Alert
          message="Analysis Failed"
          description={error}
          type="error"
          showIcon
        />
      );
    }

    if (!analysis) {
      return <Text type="secondary">Click to analyze commits</Text>;
    }

    return (
      <>
        <div style={{ marginBottom: 16 }}>
          <Title level={5} style={{ marginBottom: 4 }}>
            Commit Analysis Summary
          </Title>
          <Text>
            <Tag color={analysis.critical > 0 ? 'error' : 'success'}>
              {analysis.critical > 0 ? 'Needs Review' : 'All Clear'}
            </Tag>
            {analysis.critical} of {analysis.total} commits are critical
          </Text>
        </div>

        <Divider style={{ margin: '12px 0' }} />

        <List
          size="small"
          dataSource={analysis.details.slice(0, 5)}
          renderItem={item => (
            <List.Item>
              <div style={{ width: '100%' }}>
                <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                  <Tag color={item.is_critical ? 'error' : 'success'}>
                    {item.is_critical ? 'CRITICAL' : 'Normal'}
                  </Tag>
                  <Tooltip title="Commit ID">
                    <Text code style={{ fontSize: 12 }}>
                      {item.id.substring(0, 7)}
                    </Text>
                  </Tooltip>
                </div>
                <Text
                  ellipsis={{ tooltip: item.message_preview }}
                  style={{ 
                    color: item.is_critical ? '#f5222d' : 'inherit',
                    marginTop: 4,
                    display: 'block'
                  }}
                >
                  {item.message_preview}
                </Text>
              </div>
            </List.Item>
          )}
        />

        {analysis.total > 5 && (
          <Text type="secondary" style={{ display: 'block', marginTop: 8 }}>
            Showing 5 of {analysis.total} commits
          </Text>
        )}
      </>
    );
  };

  return (
    <Popover 
      content={renderContent()}
      title={
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
          <span>Commit Analysis</span>
          {analysis && (
            <Badge 
              count={`${analysis.critical_percentage}%`} 
              style={{ 
                backgroundColor: analysis.critical > 0 ? '#f5222d' : '#52c41a'
              }} 
            />
          )}
        </div>
      }
      trigger="click"
      open={popoverVisible}
      onOpenChange={handlePopoverOpen}
      overlayStyle={{ width: 350 }}
      placement="bottomRight"
    >
      <Badge 
        count={analysis?.critical || 0} 
        color={getStatusColor()}
        offset={[-10, 10]}
      >
        <Button 
          type={error ? 'default' : analysis ? (analysis.critical ? 'danger' : 'success') : 'default'}
          icon={getStatusIcon()}
          loading={loading}
          onClick={(e) => e.stopPropagation()}
          style={{ 
            marginLeft: 'auto',
            fontWeight: 500,
            borderRadius: 20,
            padding: '0 16px',
            border: error ? '1px solid #faad14' : undefined
          }}
        >
          {getStatusText()}
        </Button>
      </Badge>
    </Popover>
  );
};

export default AnalyzeGitHubCommits;
```

### frontend\src\components\commits\CommitAnalysisBadge.jsx
```jsx
// components/CommitAnalysisBadge.jsx
import { Tag, Tooltip, Popover, List, Typography, Divider, Badge, Spin } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled } from '@ant-design/icons';
import { useState } from 'react';
import axios from 'axios';

const { Text } = Typography;

const CommitAnalysisBadge = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchCommitAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 5 } // Get last 5 commits for analysis
        }
      );
      
      // Analyze the commits
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  const getStatusColor = () => {
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical Commits` 
      : 'No Critical Commits';
  };

  const getStatusIcon = () => {
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const content = (
    <div style={{ maxWidth: 300 }}>
      {loading && <Spin size="small" />}
      {error && <Text type="danger">{error}</Text>}
      {analysis && (
        <>
          <Text strong>Recent Commits Analysis</Text>
          <Divider style={{ margin: '8px 0' }} />
          <List
            size="small"
            dataSource={analysis.details.slice(0, 5)}
            renderItem={item => (
              <List.Item>
                <div style={{ width: '100%' }}>
                  <div style={{ 
                    display: 'flex', 
                    justifyContent: 'space-between',
                    marginBottom: 4
                  }}>
                    <Text 
                      ellipsis 
                      style={{ 
                        maxWidth: 180,
                        color: item.is_critical ? '#f5222d' : 'inherit'
                      }}
                    >
                      {item.message_preview}
                    </Text>
                    <Tag color={item.is_critical ? 'error' : 'success'}>
                      {item.is_critical ? 'Critical' : 'Normal'}
                    </Tag>
                  </div>
                  <Text type="secondary" style={{ fontSize: 12 }}>
                    {item.id.substring(0, 7)}
                  </Text>
                </div>
              </List.Item>
            )}
          />
          <Divider style={{ margin: '8px 0' }} />
          <Text type="secondary">
            {analysis.critical} of {analysis.total} recent commits are critical
          </Text>
        </>
      )}
    </div>
  );

  return (
    <Popover 
      content={content}
      title="Commit Analysis"
      trigger="click"
      onVisibleChange={visible => visible && !analysis && fetchCommitAnalysis()}
    >
      <Badge 
        count={analysis?.critical || 0} 
        style={{ backgroundColor: getStatusColor() }}
      >
        <Tag 
          icon={getStatusIcon()}
          color={getStatusColor()}
          style={{ cursor: 'pointer' }}
        >
          {getStatusText()}
        </Tag>
      </Badge>
    </Popover>
  );
};

export default CommitAnalysisBadge;
```

### frontend\src\components\commits\CommitAnalysisModal.jsx
```jsx
// components/CommitAnalysisModal.jsx
import { Modal, List, Typography, Tag, Divider, Spin, Tabs, Progress, Alert } from 'antd';
import { 
  ExclamationCircleOutlined, 
  CheckCircleOutlined,
  BarChartOutlined,
  FileTextOutlined 
} from '@ant-design/icons';
import axios from 'axios';
import { useState, useEffect } from 'react';

const { Title, Text } = Typography;
const { TabPane } = Tabs;

const CommitAnalysisModal = ({ repo, visible, onCancel }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchFullAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 100 } // Get more commits for detailed analysis
        }
      );
      
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    if (visible) {
      fetchFullAnalysis();
    }
  }, [visible]);

  const criticalPercentage = analysis 
    ? Math.round((analysis.critical / analysis.total) * 100) 
    : 0;

  return (
    <Modal
      title={<><BarChartOutlined /> Commit Analysis for {repo.name}</>}
      visible={visible}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {loading && <Spin size="large" style={{ display: 'block', margin: '40px auto' }} />}
      
      {error && (
        <Alert 
          message="Error" 
          description={error} 
          type="error" 
          showIcon 
          style={{ marginBottom: 20 }}
        />
      )}
      
      {analysis && (
        <Tabs defaultActiveKey="1">
          <TabPane tab={<><FileTextOutlined /> Commits</>} key="1">
            <div style={{ marginBottom: 20 }}>
              <div style={{ display: 'flex', alignItems: 'center', marginBottom: 16 }}>
                <Progress
                  type="circle"
                  percent={criticalPercentage}
                  width={80}
                  format={percent => (
                    <Text strong style={{ fontSize: 24, color: percent > 0 ? '#f5222d' : '#52c41a' }}>
                      {percent}%
                    </Text>
                  )}
                  status={criticalPercentage > 0 ? 'exception' : 'success'}
                />
                <div style={{ marginLeft: 20 }}>
                  <Title level={4} style={{ marginBottom: 0 }}>
                    {analysis.critical} of {analysis.total} commits are critical
                  </Title>
                  <Text type="secondary">
                    {criticalPercentage > 0 
                      ? 'This repository contains potentially critical changes'
                      : 'No critical commits detected'}
                  </Text>
                </div>
              </div>
              
              <List
                size="large"
                dataSource={analysis.details}
                renderItem={item => (
                  <List.Item>
                    <div style={{ width: '100%' }}>
                      <div style={{ display: 'flex', justifyContent: 'space-between' }}>
                        <Tag color={item.is_critical ? 'error' : 'success'}>
                          {item.is_critical ? 'CRITICAL' : 'Normal'}
                        </Tag>
                        <Text type="secondary" copyable>
                          {item.id.substring(0, 7)}
                        </Text>
                      </div>
                      <Divider style={{ margin: '8px 0' }} />
                      <Text style={{ color: item.is_critical ? '#f5222d' : 'inherit' }}>
                        {item.message_preview}
                      </Text>
                    </div>
                  </List.Item>
                )}
              />
            </div>
          </TabPane>
          
          <TabPane tab={<><ExclamationCircleOutlined /> Critical Commits</>} key="2">
            {analysis.critical > 0 ? (
              <List
                dataSource={analysis.details.filter(c => c.is_critical)}
                renderItem={item => (
                  <List.Item>
                    <Alert
                      message="Critical Commit"
                      description={
                        <>
                          <Text strong style={{ display: 'block', marginBottom: 4 }}>
                            {item.message_preview}
                          </Text>
                          <Text type="secondary">Commit ID: {item.id.substring(0, 7)}</Text>
                        </>
                      }
                      type="error"
                      showIcon
                    />
                  </List.Item>
                )}
              />
            ) : (
              <div style={{ textAlign: 'center', padding: '40px 0' }}>
                <CheckCircleOutlined style={{ fontSize: 48, color: '#52c41a', marginBottom: 20 }} />
                <Title level={4} style={{ color: '#52c41a' }}>
                  No Critical Commits Found
                </Title>
                <Text type="secondary">
                  All analyzed commits appear to be normal changes
                </Text>
              </div>
            )}
          </TabPane>
        </Tabs>
      )}
    </Modal>
  );
};

export default CommitAnalysisModal;
```

### frontend\src\components\commits\CommitList.jsx
```jsx
import { useEffect, useState } from "react";
import { List, Avatar, Typography, Spin, message, Tooltip, Card, Tag, Pagination } from "antd";
import { GithubOutlined, BranchesOutlined, ClockCircleOutlined, UserOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Title, Text } = Typography;

const CommitCard = styled(Card)`
  margin-bottom: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
  }
`;

const CommitHeader = styled.div`
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
`;

const CommitMessage = styled.div`
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  font-weight: 500;
  
  &:hover {
    white-space: normal;
    overflow: visible;
  }
`;

const CommitMeta = styled.div`
  display: flex;
  align-items: center;
  gap: 12px;
  margin-top: 8px;
  color: #666;
  font-size: 13px;
`;

const PaginationContainer = styled.div`
  display: flex;
  justify-content: center;
  margin-top: 20px;
`;

const CommitList = ({ owner, repo, branch }) => {
  const [commits, setCommits] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const pageSize = 5;

  useEffect(() => {
    if (!branch) return;

    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchCommits = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/commits?branch=${branch}`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setCommits(response.data);
      } catch (err) {
        console.error(err);
        message.error("Lỗi khi lấy danh sách commit");
      } finally {
        setLoading(false);
      }
    };

    setLoading(true);
    fetchCommits();
  }, [owner, repo, branch]);

  const formatDate = (dateString) => {
    const options = { year: 'numeric', month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' };
    return new Date(dateString).toLocaleDateString('vi-VN', options);
  };

  // Tính toán dữ liệu hiển thị theo trang hiện tại
  const paginatedCommits = commits.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );

  if (loading) return <Spin tip="Đang tải commit..." size="large" />;

  return (
    <div style={{ padding: '16px' }}>
      <div style={{ display: 'flex', alignItems: 'center', marginBottom: '20px' }}>
        <Title level={4} style={{ margin: 0 }}>
          <BranchesOutlined style={{ marginRight: '8px', color: '#1890ff' }} />
          Commit trên branch: <Tag color="blue">{branch}</Tag>
          <Tag style={{ marginLeft: '8px' }}>{commits.length} commits</Tag>
        </Title>
      </div>
      
      <List
        itemLayout="vertical"
        dataSource={paginatedCommits}
        renderItem={(item) => (
          <List.Item>
            <CommitCard>
              <CommitHeader>
                <Tooltip title={item.sha} placement="topLeft">
                  <Tag icon={<GithubOutlined />} color="default">
                    {item.sha.substring(0, 7)}
                  </Tag>
                </Tooltip>
              </CommitHeader>
              
              <CommitMessage>
                <Tooltip title={item.commit.message} placement="topLeft">
                  {item.commit.message.split('\n')[0]}
                </Tooltip>
              </CommitMessage>
              
              <CommitMeta>
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <Avatar 
                    src={item.author?.avatar_url} 
                    size="small" 
                    icon={<UserOutlined />}
                    style={{ marginRight: '8px' }}
                  />
                  <Text>{item.commit.author.name}</Text>
                </div>
                
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <ClockCircleOutlined style={{ marginRight: '4px' }} />
                  <Text>{formatDate(item.commit.author.date)}</Text>
                </div>
              </CommitMeta>
            </CommitCard>
          </List.Item>
        )}
      />

      <PaginationContainer>
        <Pagination
          current={currentPage}
          pageSize={pageSize}
          total={commits.length}
          onChange={(page) => setCurrentPage(page)}
          showSizeChanger={false}
          showQuickJumper
          style={{ marginTop: '20px' }}
        />
      </PaginationContainer>
    </div>
  );
};

export default CommitList;
```

### frontend\src\components\commits\CommitTable.jsx
```jsx
//frontend\src\components\commits\CommitTable.jsxCommitTable.jsx

import { useEffect, useState } from 'react';
import { Table } from 'antd';
import axios from 'axios';

const CommitTable = () => {
  const [commits, setCommits] = useState([]);

  useEffect(() => {
    const fetchCommits = async () => {
      try {
        const response = await axios.get('http://localhost:8000/commits');
        setCommits(response.data);
      } catch (error) {
        console.error('Failed to fetch commits:', error);
      }
    };
    fetchCommits();
  }, []);

  const columns = [
    {
      title: 'ID',
      dataIndex: 'id',
    },
    {
      title: 'Repo ID',
      dataIndex: 'repo_id',
    },
    {
      title: 'User ID',
      dataIndex: 'user_id',
    },
    {
      title: 'Message',
      dataIndex: 'message',
    },
    {
      title: 'Hash',
      dataIndex: 'commit_hash',
    },
    {
      title: 'Date',
      dataIndex: 'commit_date',
    },
  ];

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-4">Lịch sử Commit</h2>
      <Table columns={columns} dataSource={commits} rowKey="id" />
    </div>
  );
};

export default CommitTable;
```

### frontend\src\components\Dashboard\AIInsightWidget.jsx
```jsx
import React from 'react';
import { Card, Space, Typography, Button, Tag } from 'antd';
import { BulbOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Title, Text } = Typography;

// Styled components
const InsightContainer = styled(Card)`
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  background: #ffffff;
  transition: all 0.3s ease;

  &:hover {
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.12);
    transform: translateY(-2px);
  }
`;

const InsightCard = styled(Card)`
  border-radius: 8px;
  border: 1px solid ${(props) => props.borderColor || '#f0f0f0'};
  background: #fff;
  transition: all 0.3s ease;
  padding: 12px;

  &:hover {
    border-color: ${(props) => props.borderColor || '#d9d9d9'};
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  }

  @media (max-width: 576px) {
    padding: 8px;
  }
`;

const IconWrapper = styled.div`
  display: flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background: ${(props) => props.bgColor || '#f0f0f0'};
`;

const ActionWrapper = styled.div`
  display: flex;
  justify-content: flex-end;
  gap: 8px;

  @media (max-width: 576px) {
    justify-content: flex-start;
    margin-top: 8px;
  }
`;

const AIInsightWidget = () => {
  const insights = [
    {
      id: 1,
      type: 'suggestion',
      title: 'Phân công đề xuất',
      description: 'Thêm 2 developer vào repo "frontend" để đảm bảo deadline 25/04/2025.',
    },
    {
      id: 2,
      type: 'warning',
      title: 'Dự đoán tiến độ',
      description: 'Repo "backend" có nguy cơ trễ hạn 3 ngày. Xem xét tăng tài nguyên.',
    },
  ];

  const getInsightStyle = (type) => {
    switch (type) {
      case 'suggestion':
        return {
          icon: <BulbOutlined style={{ fontSize: 20, color: '#1890ff' }} />,
          tag: <Tag color="blue">Đề xuất</Tag>,
          borderColor: '#e6f7ff',
          iconBg: '#e6f7ff',
        };
      case 'warning':
        return {
          icon: <WarningOutlined style={{ fontSize: 20, color: '#fa8c16' }} />,
          tag: <Tag color="orange">Cảnh báo</Tag>,
          borderColor: '#fff7e6',
          iconBg: '#fff7e6',
        };
      default:
        return {
          icon: null,
          tag: null,
          borderColor: '#f0f0f0',
          iconBg: '#f0f0f0',
        };
    }
  };

  return (
    <InsightContainer
      title={<Title level={4} style={{ margin: 0 }}>Gợi ý AI</Title>}
      bordered={false}
    >
      <Space direction="vertical" size="middle" style={{ width: '100%' }}>
        {insights.map((item) => {
          const { icon, tag, borderColor, iconBg } = getInsightStyle(item.type);
          return (
            <InsightCard key={item.id} borderColor={borderColor}>
              <Space direction="horizontal" size="middle" style={{ width: '100%', alignItems: 'center' }}>
                <IconWrapper bgColor={iconBg}>{icon}</IconWrapper>
                <Space direction="vertical" size={4} style={{ flex: 1 }}>
                  <Space>
                    <Title level={5} style={{ margin: 0 }}>{item.title}</Title>
                    {tag}
                  </Space>
                  <Text type="secondary">{item.description}</Text>
                </Space>
                <ActionWrapper>
                  <Button type="primary" size="small">Thực hiện</Button>
                  <Button size="small">Bỏ qua</Button>
                </ActionWrapper>
              </Space>
            </InsightCard>
          );
        })}
      </Space>
    </InsightContainer>
  );
};

export default AIInsightWidget;
```

### frontend\src\components\Dashboard\OverviewCard.jsx
```jsx
import React from 'react';
import { Card, Row, Col, Statistic } from 'antd';
import { ProjectOutlined, CheckCircleOutlined, WarningOutlined } from '@ant-design/icons';

const OverviewCard = ({ projects = 10, completedTasks = 50, overdueTasks = 5 }) => {
  return (
    <Card title="Tổng quan dự án" bordered={false}>
      <Row gutter={16}>
        <Col span={8}>
          <Statistic
            title="Số dự án"
            value={projects}
            prefix={<ProjectOutlined />}
            valueStyle={{ color: '#1890ff' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="Công việc hoàn thành"
            value={completedTasks}
            prefix={<CheckCircleOutlined />}
            valueStyle={{ color: '#52c41a' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="Công việc trễ hạn"
            value={overdueTasks}
            prefix={<WarningOutlined />}
            valueStyle={{ color: '#ff4d4f' }}
          />
        </Col>
      </Row>
    </Card>
  );
};

export default OverviewCard;
```

### frontend\src\components\Dashboard\RepoListFilter.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col, Input, Select, Button } from 'antd';
import { SearchOutlined } from '@ant-design/icons';

const { Option } = Select;

const RepoListFilter = ({ onFilterChange }) => {
  const [searchText, setSearchText] = useState('');
  const [status, setStatus] = useState('all');
  const [assignee, setAssignee] = useState('all');

  const handleApplyFilter = () => {
    onFilterChange({ searchText, status, assignee });
  };

  return (
    <Card title="Bộ lọc Repository" bordered={false}>
      <Row gutter={16}>
        <Col span={8}>
          <Input
            placeholder="Tìm kiếm repo"
            prefix={<SearchOutlined />}
            value={searchText}
            onChange={(e) => setSearchText(e.target.value)}
          />
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={status}
            onChange={(value) => setStatus(value)}
            placeholder="Trạng thái"
          >
            <Option value="all">Tất cả</Option>
            <Option value="active">Đang hoạt động</Option>
            <Option value="archived">Đã lưu trữ</Option>
          </Select>
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={assignee}
            onChange={(value) => setAssignee(value)}
            placeholder="Người phụ trách"
          >
            <Option value="all">Tất cả</Option>
            <Option value="user1">User 1</Option>
            <Option value="user2">User 2</Option>
          </Select>
        </Col>
        <Col span={4}>
          <Button type="primary" onClick={handleApplyFilter}>
            Áp dụng
          </Button>
        </Col>
      </Row>
    </Card>
  );
};

export default RepoListFilter;
```

### frontend\src\components\Dashboard\TaskBoard.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col } from 'antd';
import { DndContext, closestCenter } from '@dnd-kit/core';
import { SortableContext, useSortable, arrayMove } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { Task } from '../../utils/types';

const SortableTask = ({ task }) => {
  const { attributes, listeners, setNodeRef, transform, transition } = useSortable({ id: task.id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    marginBottom: 8,
  };

  return (
    <Card ref={setNodeRef} style={style} {...attributes} {...listeners}>
      <p>{task.title}</p>
      <p>Người phụ trách: {task.assignee}</p>
    </Card>
  );
};

const TaskBoard = ({ initialTasks = [] }) => {
  const [tasks, setTasks] = useState(initialTasks);

  const onDragEnd = (event) => {
    const { active, over } = event;
    if (active.id !== over.id) {
      setTasks((items) => {
        const oldIndex = items.findIndex((item) => item.id === active.id);
        const newIndex = items.findIndex((item) => item.id === over.id);
        return arrayMove(items, oldIndex, newIndex);
      });
    }
  };

  const columns = {
    todo: { title: 'Chờ xử lý', tasks: tasks.filter((task) => task.status === 'todo') },
    inProgress: { title: 'Đang thực hiện', tasks: tasks.filter((task) => task.status === 'inProgress') },
    done: { title: 'Hoàn thành', tasks: tasks.filter((task) => task.status === 'done') },
  };

  return (
    <Card title="Bảng công việc" bordered={false}>
      <DndContext collisionDetection={closestCenter} onDragEnd={onDragEnd}>
        <Row gutter={16}>
          {Object.keys(columns).map((columnId) => (
            <Col span={8} key={columnId}>
              <Card title={columns[columnId].title} bordered={false}>
                <SortableContext items={columns[columnId].tasks.map((task) => task.id)}>
                  {columns[columnId].tasks.map((task) => (
                    <SortableTask key={task.id} task={task} />
                  ))}
                </SortableContext>
              </Card>
            </Col>
          ))}
        </Row>
      </DndContext>
    </Card>
  );
};

export default TaskBoard;
```

### frontend\src\components\repo\RepoList.jsx
```jsx
import { useEffect, useState } from "react";
import { Avatar, Typography, Spin, message, Card, Tag, Pagination } from "antd";
import { useNavigate } from "react-router-dom";
import { GithubOutlined, StarFilled, EyeFilled, ForkOutlined, CalendarOutlined } from "@ant-design/icons";
import styled from "styled-components";
import axios from "axios";

const { Title, Text } = Typography;

const RepoContainer = styled.div`
  max-width: 900px;
  margin: 0 auto;
  padding: 24px;
`;

const RepoCard = styled(Card)`
  margin-bottom: 20px;
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
  cursor: pointer;
  border: none;
  
  &:hover {
    box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    transform: translateY(-5px);
  }
`;

const RepoHeader = styled.div`
  display: flex;
  align-items: flex-start;
  margin-bottom: 12px;
`;

const RepoTitle = styled.div`
  flex: 1;
  min-width: 0;
`;

const RepoName = styled(Text)`
  display: block;
  font-size: 18px;
  font-weight: 600;
  color: #24292e;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
`;

const RepoDescription = styled(Text)`
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
  color: #586069;
  margin: 8px 0;
`;

const RepoMeta = styled.div`
  display: flex;
  flex-wrap: wrap;
  gap: 16px;
  margin-top: 16px;
  align-items: center;
`;

const MetaItem = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 14px;
  color: #586069;
`;

const StyledPagination = styled(Pagination)`
  margin-top: 32px;
  text-align: center;
  
  .ant-pagination-item-active {
    border-color: #1890ff;
    background: #1890ff;
    
    a {
      color: white;
    }
  }
`;

const HighlightTag = styled(Tag)`
  font-weight: 500;
  border-radius: 12px;
  padding: 0 10px;
`;

const RepoList = () => {
  const [repos, setRepos] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const [totalRepos, setTotalRepos] = useState(0);
  const navigate = useNavigate();
  const pageSize = 8;

  useEffect(() => {
    const fetchRepos = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) return message.error("Vui lòng đăng nhập lại!");

      try {
        setLoading(true);
        const response = await axios.get("http://localhost:8000/api/github/repos", {
          headers: { Authorization: `token ${token}` },
          params: { sort: 'updated', direction: 'desc' } // Sắp xếp theo mới nhất
        });
        
        // Sắp xếp lại để đảm bảo mới nhất lên đầu
        const sortedRepos = response.data.sort((a, b) => 
          new Date(b.updated_at) - new Date(a.updated_at)
        );
        
        setRepos(sortedRepos);
        setTotalRepos(sortedRepos.length);
      } catch (error) {
        message.error("Không thể tải danh sách repository!");
        console.error(error);
      } finally {
        setLoading(false);
      }
    };

    fetchRepos();
  }, []);

  const formatDate = (dateString) => {
    return new Date(dateString).toLocaleDateString('vi-VN', {
      day: '2-digit',
      month: '2-digit',
      year: 'numeric'
    });
  };

  const paginatedRepos = repos.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );

  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', marginTop: '100px' }}>
        <Spin tip="Đang tải dữ liệu..." size="large" />
      </div>
    );
  }

  return (
    <RepoContainer>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '24px' }}>
        <Title level={2} style={{ margin: 0, color: '#24292e' }}>
          <GithubOutlined style={{ marginRight: '12px', color: '#1890ff' }} />
          GitHub Repositories
        </Title>
        <Text strong style={{ fontSize: '16px' }}>
          Tổng cộng: {totalRepos} repositories
        </Text>
      </div>

      {paginatedRepos.map((repo) => (
        <RepoCard 
          key={repo.id} 
          onClick={() => navigate(`/repo/${repo.owner.login}/${repo.name}`)}
        >
          <RepoHeader>
            <Avatar 
              src={repo.owner.avatar_url} 
              size={48}
              style={{ marginRight: '16px', flexShrink: 0 }}
            />
            <RepoTitle>
              <div style={{ display: 'flex', alignItems: 'center' }}>
                <RepoName>{repo.name}</RepoName>
                {repo.private ? (
                  <HighlightTag color="error" style={{ marginLeft: '12px' }}>
                    Private
                  </HighlightTag>
                ) : (
                  <HighlightTag color="success" style={{ marginLeft: '12px' }}>
                    Public
                  </HighlightTag>
                )}
              </div>
              
              <RepoDescription type="secondary">
                {repo.description || "Không có mô tả"}
              </RepoDescription>
            </RepoTitle>
          </RepoHeader>

          <RepoMeta>
            <MetaItem>
              <StarFilled style={{ color: '#ffc53d' }} />
              <Text strong>{repo.stargazers_count}</Text>
              <Text>stars</Text>
            </MetaItem>
            
            <MetaItem>
              <EyeFilled style={{ color: '#1890ff' }} />
              <Text strong>{repo.watchers_count}</Text>
              <Text>watchers</Text>
            </MetaItem>
            
            <MetaItem>
              <ForkOutlined style={{ color: '#73d13d' }} />
              <Text strong>{repo.forks_count}</Text>
              <Text>forks</Text>
            </MetaItem>
            
            {repo.language && (
              <MetaItem>
                <div style={{
                  width: 12,
                  height: 12,
                  borderRadius: '50%',
                  backgroundColor: '#1890ff',
                  marginRight: 6
                }} />
                <Text>{repo.language}</Text>
              </MetaItem>
            )}
            
            <MetaItem style={{ marginLeft: 'auto' }}>
              <CalendarOutlined />
              <Text>Cập nhật: {formatDate(repo.updated_at)}</Text>
            </MetaItem>
          </RepoMeta>
        </RepoCard>
      ))}

      <StyledPagination
        current={currentPage}
        pageSize={pageSize}
        total={totalRepos}
        onChange={(page) => setCurrentPage(page)}
        showSizeChanger={false}
        showQuickJumper
      />
    </RepoContainer>
  );
};

export default RepoList;
```

### frontend\src\features\github\GithubRepoFetcher.jsx
```jsx

```

### frontend\src\pages\AuthSuccess.jsx
```jsx
// src/pages/AuthSuccess.jsx
import React, { useEffect } from "react";
import { useNavigate, useLocation } from "react-router-dom";
import { message } from "antd";
import axios from "axios";

const AuthSuccess = () => {
  const navigate = useNavigate();
  const location = useLocation();

  useEffect(() => {
    const params = new URLSearchParams(location.search);
    const token = params.get("token");
    const username = params.get("username");
    const email = params.get("email");

    if (token) {
      const profile = {
        token,
        username,
        email,
        avatar_url: params.get("avatar_url"),
      };

      localStorage.setItem("github_profile", JSON.stringify(profile));
      localStorage.setItem("access_token", token);

      const syncAllRepositories = async () => {
        try {
          const response = await axios.get("http://localhost:8000/api/github/repos", {
            headers: {
              Authorization: `token ${token}`,
            },
          });

          const repositories = response.data;
          for (const repo of repositories) {
            await axios.post(
              `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
              {},
              {
                headers: {
                  Authorization: `token ${token}`,
                },
              }
            );
          }

          message.success("Đồng bộ dữ liệu thành công!");
        } catch (error) {
          console.error("Lỗi khi đồng bộ repository:", error);
          message.error("Không thể đồng bộ repository!");
        }
      };

      syncAllRepositories();
      navigate("/dashboard");
    } else {
      navigate("/login");
    }
  }, [location, navigate]);

  return (
    <div className="h-screen flex items-center justify-center">
      <p className="text-xl">Đang đồng bộ dữ liệu...</p>
    </div>
  );
};

export default AuthSuccess;
```

### frontend\src\pages\Dashboard.jsx
```jsx
import React, { useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button, Typography, Avatar, Card, Grid, Space, Divider, Badge, message, Spin } from 'antd';
import { LogoutOutlined, GithubOutlined, NotificationOutlined } from '@ant-design/icons';
import styled from 'styled-components';
import RepoList from '../components/repo/RepoList';
import OverviewCard from '../components/Dashboard/OverviewCard';
import AIInsightWidget from '../components/Dashboard/AIInsightWidget';
import RepoListFilter from '../components/Dashboard/RepoListFilter';
import TaskBoard from '../components/Dashboard/TaskBoard';
import axios from 'axios';

const { Title, Text } = Typography;
const { useBreakpoint } = Grid;

// Styled components với theme hiện đại
const DashboardContainer = styled.div`
  padding: 24px;
  max-width: 1440px;
  margin: 0 auto;
  background: #f8fafc;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  gap: 24px;

  @media (max-width: 768px) {
    padding: 16px;
    gap: 16px;
  }
`;

const HeaderCard = styled(Card)`
  border-radius: 16px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  
  .ant-card-body {
    padding: 24px;
  }
`;

const DashboardCard = styled(Card)`
  border-radius: 16px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  transition: all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);
  
  &:hover {
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
    transform: translateY(-2px);
  }

  .ant-card-head {
    border-bottom: 1px solid #f1f5f9;
    padding: 16px 24px;
  }

  .ant-card-body {
    padding: 24px;
  }

  @media (max-width: 768px) {
    .ant-card-body {
      padding: 16px;
    }
  }
`;

const PrimaryButton = styled(Button)`
  border-radius: 8px;
  font-weight: 500;
  height: 40px;
  padding: 0 20px;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const UserInfoContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 16px;
`;

const UserAvatar = styled(Avatar)`
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  border: 2px solid #ffffff;
`;

const WidgetsRow = styled.div`
  display: grid;
  grid-template-columns: 1.5fr 1fr;
  gap: 24px;

  @media (max-width: 992px) {
    grid-template-columns: 1fr;
  }
`;

const ContentSection = styled.section`
  display: flex;
  flex-direction: column;
  gap: 24px;
`;

const SectionTitle = styled(Title)`
  margin-bottom: 0 !important;
  font-weight: 600 !important;
  color: #1e293b !important;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const NotificationBadge = styled(Badge)`
  .ant-badge-count {
    background: #3b82f6;
    box-shadow: 0 0 0 1px #fff;
  }
`;

const Dashboard = () => {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(false);
  const navigate = useNavigate();
  const screens = useBreakpoint();

  const syncAllRepositories = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) {
      message.error('Vui lòng đăng nhập lại!');
      return;
    }

    try {
      setLoading(true);
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: {
          Authorization: `token ${token}`,
        },
      });

      const repositories = response.data;
      for (const repo of repositories) {
        await axios.post(
          `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
          {},
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
      }

      message.success('Đồng bộ tất cả repository thành công!');
    } catch (error) {
      console.error('Lỗi khi đồng bộ repository:', error);
      message.error('Không thể đồng bộ repository!');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    const storedProfile = localStorage.getItem('github_profile');
    if (!storedProfile) {
      navigate('/login');
    } else {
      setUser(JSON.parse(storedProfile));
    }
    syncAllRepositories();
  }, [navigate]);

  const handleLogout = () => {
    localStorage.removeItem('github_profile');
    localStorage.removeItem('access_token');
    navigate('/login');
  };

  const handleFilterChange = (filters) => {
    console.log('Applied filters:', filters);
  };

  const handleStatusChange = (taskId, newStatus) => {
    console.log(`Updated task ${taskId} status to ${newStatus}`);
  };

  if (loading) {
    return <Spin tip="Đang đồng bộ dữ liệu..." size="large" />;
  }

  return (
    <DashboardContainer>
      {/* Header Section */}
      <HeaderCard bordered={false}>
        <Space 
          direction={screens.md ? 'horizontal' : 'vertical'} 
          align={screens.md ? 'center' : 'start'}
          style={{ width: '100%', justifyContent: 'space-between' }}
        >
          <UserInfoContainer>
            <UserAvatar src={user?.avatar_url} size={screens.md ? 72 : 56} />
            <div>
              <Title level={4} style={{ margin: 0, color: '#1e293b' }}>
                Welcome back, {user?.username || 'User'}!
              </Title>
              <Text type="secondary" style={{ color: '#64748b' }}>
                {user?.email || 'No email provided'}
              </Text>
            </div>
          </UserInfoContainer>
          
          <Space size={screens.md ? 16 : 8}>
            <NotificationBadge count={3} size="small">
              <Button 
                icon={<NotificationOutlined />} 
                shape="circle" 
                style={{ border: 'none' }}
              />
            </NotificationBadge>
            <PrimaryButton 
              type="primary" 
              danger 
              onClick={handleLogout}
              icon={<LogoutOutlined />}
            >
              {screens.md ? 'Log Out' : ''}
            </PrimaryButton>
          </Space>
        </Space>
      </HeaderCard>

      {/* Overview Metrics */}
      <DashboardCard bodyStyle={{ padding: '16px' }}>
        <OverviewCard />
      </DashboardCard>

      {/* AI Insights and Filters */}
      <WidgetsRow>
        <DashboardCard 
          title={
            <SectionTitle level={5}>
              <GithubOutlined />
              Repository Analysis
            </SectionTitle>
          }
        >
          <AIInsightWidget />
        </DashboardCard>
        
        <DashboardCard 
          title={<SectionTitle level={5}>Filters & Settings</SectionTitle>}
        >
          <RepoListFilter onFilterChange={handleFilterChange} />
        </DashboardCard>
      </WidgetsRow>

      {/* Main Content Sections */}
      <ContentSection>
        <DashboardCard 
          title={
            <SectionTitle level={5}>
              My Repositories
              <Text type="secondary" style={{ fontSize: 14, marginLeft: 8 }}>
                (24 repositories)
              </Text>
            </SectionTitle>
          }
        >
          <RepoList />
        </DashboardCard>

        <DashboardCard 
          title={<SectionTitle level={5}>Project Tasks</SectionTitle>}
        >
          <TaskBoard onStatusChange={handleStatusChange} />
        </DashboardCard>
      </ContentSection>
    </DashboardContainer>
  );
};

export default Dashboard;
```

### frontend\src\pages\Login.jsx
```jsx
// src/pages/Login.jsx
import React from "react";
import { Button, Card, Typography } from "antd";
import { GithubOutlined } from "@ant-design/icons";

const { Title } = Typography;

const Login = () => {
  const handleGitHubLogin = () => {
    window.location.href = "http://localhost:8000/api/login"; // backend redirect to GitHub OAuth
  };

  return (
    <div className="h-screen flex items-center justify-center bg-gradient-to-br from-gray-100 to-white">
      <Card
        className="shadow-xl rounded-2xl w-full max-w-md"
        style={{ textAlign: "center", padding: "3rem 2rem" }}
      >
        <Title level={2} style={{ marginBottom: "2rem" }}>
          Đăng nhập vào <span style={{ color: "#1890ff" }}>TaskFlowAI</span>
        </Title>
        <Button
          type="primary"
          icon={<GithubOutlined />}
          size="large"
          onClick={handleGitHubLogin}
          style={{
            backgroundColor: "#000",
            borderColor: "#000",
            width: "100%",
          }}
        >
          Đăng nhập với GitHub
        </Button>
      </Card>
    </div>
  );
};

export default Login;
```

### frontend\src\pages\RepoDetails.jsx
```jsx
import { useEffect, useState } from "react";
import { useParams } from "react-router-dom";
import { message, Spin, Button } from "antd";
import BranchSelector from "../components/Branchs/BranchSelector";
import CommitList from "../components/commits/CommitList";
import axios from "axios";

const RepoDetails = () => {
  const { owner, repo } = useParams();
  const [branch, setBranch] = useState("");
  const [loading, setLoading] = useState(false);

  useEffect(() => {
    const syncAllData = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) {
        message.error("Vui lòng đăng nhập lại!");
        return;
      }

      try {
        setLoading(true);
        await axios.post(
          `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
          {},
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        message.success("Đồng bộ dữ liệu thành công!");
      } catch (error) {
        console.error("Lỗi khi đồng bộ dữ liệu:", error);
        message.error("Không thể đồng bộ dữ liệu!");
      } finally {
        setLoading(false);
      }
    };

    syncAllData();
  }, [owner, repo]);

  const saveCommits = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui lòng đăng nhập lại!");
      return;
    }

    try {
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/save-commits`,
        { branch },
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      message.success("Lưu commit thành công!");
    } catch (error) {
      console.error("Lỗi khi lưu commit:", error);
      message.error("Không thể lưu commit!");
    }
  };

  if (loading) {
    return <Spin tip="Đang đồng bộ dữ liệu..." size="large" />;
  }

  return (
    <div style={{ padding: 24 }}>
      <h2 style={{ fontWeight: "bold" }}>📁 Repository: {repo}</h2>
      <BranchSelector owner={owner} repo={repo} onBranchChange={setBranch} />
      <Button type="primary" onClick={saveCommits}>
        Lưu Commit
      </Button>
      <CommitList owner={owner} repo={repo} branch={branch} />
    </div>
  );
};

export default RepoDetails;
```

### frontend\src\utils\types.js
```js
export const Task = {
  id: '',
  title: '',
  assignee: '',
  status: '', // 'todo', 'inProgress', 'done'
};
```
