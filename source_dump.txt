# ==================================================
# Path: C:\SAN\KLTN\KLTN04
# Detected tech: javascript, python, react, rust, typescript
# ==================================================

## DIRECTORY STRUCTURE
```
KLTN04/
‚îú‚îÄ‚îÄ .git/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .vscode/
‚îú‚îÄ‚îÄ __pycache__/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github_commits/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ full.csv
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ oneline.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_github_model/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ best_model.pth
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_fusion/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ synthetic_generator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enhanced_text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ minimal_enhanced_text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interpretability.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_calculator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multi_task_losses.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baselines.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shared_layers.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_main.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multitask_trainer.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_results/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_report_20250607_212442.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_report_20250607_212820.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_analysis_report_20250607_213720.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testmodelAi/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_model_demo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_model_real_test_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trained_models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multimodal_fusion_100k/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_commits_training_data.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ large_dataset_processing_summary.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sample_preview.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_logs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_github_training_20250608_011142.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_github_training_20250610_204624.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_commit_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_github_data.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_classification_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_test.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_github_commits.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_kaggle_dataset.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_commit_analyzer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_advanced_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_dataset_creator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_100k_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_100k_multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_enhanced_100k_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_enhanced_100k_multimodal_fusion_final.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_han_github.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_suggestions.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ branch.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_routes.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contributors.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitlab.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ issue.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ member_analysis.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projects.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sync.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ users.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deps.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lifespan.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ oauth.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ security.py
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ a989fa2a380c_initial_migration_with_all_models.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ script.py.mako
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_model.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit.py
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_system.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_system_v1.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fix_commit_branch_consistency.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrate_collaborators.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ branch_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collaborator_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitlab_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ issue_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ member_analysis_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pull_request_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository_collaborator_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_service.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ alembic.ini
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ requirements_clean.txt
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ dist/
‚îÇ   ‚îú‚îÄ‚îÄ node_modules/
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vite.svg
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ react.svg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AI/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Branchs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BranchCommitList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BranchSelector.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BranchSelector_fixed.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProjectTaskManager/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DragOverlayContent.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DroppableColumn.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FiltersPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard.module.css
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoSelector.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SortableTaskCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ StatisticsPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskModal.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kanbanConstants.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kanbanUtils.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useKanbanDragDrop.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AIInsightWidget.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OverviewCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProjectTaskManager.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoListFilter.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepositoryMembers.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TaskBoard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commits/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnalyzeGitHubCommits.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitAnalysisBadge.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitAnalysisModal.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CommitTable.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SyncProgressNotification.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RepoList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AliasTest.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ErrorBoundary.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SimpleAliasTest.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SyncContext.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ GithubRepoFetcher.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useCommits.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useProjectData.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthSuccess.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoDetails.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TestPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ taskUtils.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ taskUtils.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.css
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.css
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.jsx
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ eslint.config.js
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ jsconfig.json
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ source_dump.txt
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.js
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Backend.txt
‚îú‚îÄ‚îÄ CommitApi.md
‚îú‚îÄ‚îÄ Frontend.txt
‚îú‚îÄ‚îÄ Project.md
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ insert_mock_collaborators.py
‚îú‚îÄ‚îÄ insert_mock_data_direct.py
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ quick_insert_mock.py
‚îú‚îÄ‚îÄ requirements_new.txt
‚îî‚îÄ‚îÄ source_dump.txt
```

## FILE CONTENTS

### insert_mock_collaborators.py
```py

```

### insert_mock_data_direct.py
```py

```

### quick_insert_mock.py
```py

```

### backend\main.py
```py
# backend/main.py
from fastapi import FastAPI
from core.lifespan import lifespan
from core.config import setup_middlewares
from core.logger import setup_logger

from api.routes.auth import auth_router
from api.routes.github import github_router
from api.routes.projects import router as projects_router
from api.routes.sync import sync_router
from api.routes.contributors import router as contributors_router
from api.routes.member_analysis import router as member_analysis_router
from api.routes.repositories import router as repositories_router

setup_logger()  # B·∫≠t logger tr∆∞·ªõc khi ch·∫°y app

app = FastAPI(lifespan=lifespan)

setup_middlewares(app)

# Include routers tr·ª±c ti·∫øp
app.include_router(auth_router, prefix="/api")
app.include_router(github_router, prefix="/api")
app.include_router(projects_router, prefix="/api")
app.include_router(sync_router, prefix="/api")
app.include_router(contributors_router, prefix="/api/contributors")
app.include_router(member_analysis_router)  # Already has /api prefix
app.include_router(repositories_router)  # Already has /api prefix

@app.get("/")
def root():
    return {"message": "TaskFlowAI backend is running "}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)

```

### backend\__init__.py
```py

```

### backend\ai\advanced_commit_analysis.py
```py
#!/usr/bin/env python3
"""
Advanced Commit Analyzer - Ph√¢n t√≠ch chi ti·∫øt v√† ƒë∆∞a ra khuy·∫øn ngh·ªã
"""

import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd

def load_analysis_report(report_path):
    """Load b√°o c√°o ph√¢n t√≠ch t·ª´ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_visualizations(report_data, output_dir):
    """T·∫°o c√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Set style
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. Commit Type Distribution
    commit_types = report_data['overall_distributions']['commit_types']
    
    plt.figure(figsize=(10, 6))
    plt.subplot(2, 2, 1)
    plt.pie(commit_types.values(), labels=commit_types.keys(), autopct='%1.1f%%', startangle=90)
    plt.title('Ph√¢n b·ªë lo·∫°i commit')
    
    # 2. Author Activity Levels
    activity_levels = report_data['activity_analysis']['activity_levels']
    
    plt.subplot(2, 2, 2)
    bars = plt.bar(activity_levels.keys(), activity_levels.values())
    plt.title('M·ª©c ƒë·ªô ho·∫°t ƒë·ªông c·ªßa t√°c gi·∫£')
    plt.ylabel('S·ªë l∆∞·ª£ng t√°c gi·∫£')
    
    # Color bars differently
    colors = ['red', 'orange', 'green', 'blue']
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    # 3. Purpose Distribution
    purposes = report_data['overall_distributions']['purposes']
    
    plt.subplot(2, 2, 3)
    plt.barh(list(purposes.keys()), list(purposes.values()))
    plt.title('Ph√¢n b·ªë m·ª•c ƒë√≠ch commit')
    plt.xlabel('S·ªë l∆∞·ª£ng')
    
    # 4. Sentiment Distribution
    sentiments = report_data['overall_distributions']['sentiments']
    
    plt.subplot(2, 2, 4)
    colors_sentiment = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'urgent': 'orange'}
    sentiment_colors = [colors_sentiment.get(s, 'blue') for s in sentiments.keys()]
    plt.bar(sentiments.keys(), sentiments.values(), color=sentiment_colors)
    plt.title('Ph√¢n b·ªë c·∫£m x√∫c commit')
    plt.ylabel('S·ªë l∆∞·ª£ng')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'commit_analysis_overview.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Bi·ªÉu ƒë·ªì t·ªïng quan ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_dir / 'commit_analysis_overview.png'}")

def analyze_author_patterns(report_data):
    """Ph√¢n t√≠ch pattern c·ªßa t·ª´ng t√°c gi·∫£"""
    print("\n" + "="*80)
    print("üîç PH√ÇN T√çCH CHI TI·∫æT PATTERN C·ª¶A T√ÅC GI·∫¢")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\nüë§ {author_name}:")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        print(f"   üìà M·ª©c ƒë·ªô ho·∫°t ƒë·ªông: {stats['activity_level'].upper()}")
        print(f"   üéØ Confidence trung b√¨nh: {stats['avg_confidence']:.3f}")
        
        # Ph√¢n t√≠ch commit types
        if stats['commit_types']:
            most_common_type = max(stats['commit_types'], key=stats['commit_types'].get)
            type_percentage = (stats['commit_types'][most_common_type] / stats['total_commits']) * 100
            print(f"   üè∑Ô∏è  Lo·∫°i commit ch·ªß y·∫øu: {most_common_type} ({type_percentage:.1f}%)")
        
        # Ph√¢n t√≠ch purposes
        if stats['purposes']:
            most_common_purpose = max(stats['purposes'], key=stats['purposes'].get)
            purpose_percentage = (stats['purposes'][most_common_purpose] / stats['total_commits']) * 100
            print(f"   üéØ M·ª•c ƒë√≠ch ch·ªß y·∫øu: {most_common_purpose} ({purpose_percentage:.1f}%)")
        
        # Ph√¢n t√≠ch sentiment
        if stats['sentiments']:
            most_common_sentiment = max(stats['sentiments'], key=stats['sentiments'].get)
            sentiment_percentage = (stats['sentiments'][most_common_sentiment] / stats['total_commits']) * 100
            print(f"   üòä C·∫£m x√∫c ch·ªß y·∫øu: {most_common_sentiment} ({sentiment_percentage:.1f}%)")

def generate_recommendations(report_data):
    """T·∫°o khuy·∫øn ngh·ªã cho team"""
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Khuy·∫øn ngh·ªã cho overloaded authors
    if overloaded_authors:
        print(f"\nüî• T√åNH TR·∫†NG QU√Å T·∫¢I ({len(overloaded_authors)} t√°c gi·∫£):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"   ‚ö†Ô∏è  {author}: {stats['total_commits']} commits")
            print(f"      üí° Khuy·∫øn ngh·ªã: C√¢n nh·∫Øc ph√¢n ph·ªëi c√¥ng vi·ªác ho·∫∑c h·ªó tr·ª£ th√™m nh√¢n l·ª±c")
            
            # Ph√¢n t√≠ch lo·∫°i commit ƒë·ªÉ ƒë∆∞a ra khuy·∫øn ngh·ªã c·ª• th·ªÉ
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"      üêõ Nhi·ªÅu fix commits ({fix_count}): C·∫ßn review code k·ªπ h∆°n ho·∫∑c tƒÉng c∆∞·ªùng testing")
                
                feat_count = stats['commit_types'].get('feat', 0)
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"      ‚ú® Nhi·ªÅu feature commits ({feat_count}): T√°c gi·∫£ c√≥ th·ªÉ l√† key developer")
    
    # Khuy·∫øn ngh·ªã cho low activity authors
    if low_activity_authors:
        print(f"\nüí§ HO·∫†T ƒê·ªòNG TH·∫§P ({len(low_activity_authors)} t√°c gi·∫£):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"   üìâ {author}: {stats['total_commits']} commits")
            print(f"      üí° Khuy·∫øn ngh·ªã: Ki·ªÉm tra workload, cung c·∫•p h·ªó tr·ª£ ho·∫∑c training th√™m")
    
    # Ph√¢n t√≠ch overall patterns
    print(f"\nüìà PH√ÇN T√çCH T·ªîNG QUAN:")
    
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = sum(commit_types.values())
    
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        print(f"   üêõ T·ª∑ l·ªá fix commits cao ({fix_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: TƒÉng c∆∞·ªùng code review, testing, v√† quality assurance")
    
    if feat_percentage < 30:
        print(f"   üì¶ T·ª∑ l·ªá feature commits th·∫•p ({feat_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: C√¢n nh·∫Øc tƒÉng t·ªëc ƒë·ªô ph√°t tri·ªÉn t√≠nh nƒÉng m·ªõi")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   üòû T·ª∑ l·ªá sentiment ti√™u c·ª±c cao ({negative_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: Ki·ªÉm tra morale c·ªßa team, c·∫£i thi·ªán quy tr√¨nh l√†m vi·ªác")
    
    if urgent_percentage > 10:
        print(f"   üö® T·ª∑ l·ªá urgent commits cao ({urgent_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: C·∫£i thi·ªán planning v√† risk management")

def create_team_dashboard(report_data, output_dir):
    """T·∫°o dashboard t·ªïng quan cho team"""
    output_dir = Path(output_dir)
    
    # Create a comprehensive team report
    dashboard_data = {
        "timestamp": datetime.now().isoformat(),
        "team_health": {
            "total_authors": len(report_data['author_statistics']),
            "total_commits": report_data['summary']['total_commits'],
            "avg_commits_per_author": report_data['summary']['avg_commits_per_author']
        },
        "risk_indicators": {
            "overloaded_authors": len(report_data['activity_analysis']['overloaded_authors']),
            "low_activity_authors": len(report_data['activity_analysis']['low_activity_authors']),
            "fix_percentage": (report_data['overall_distributions']['commit_types'].get('fix', 0) / 
                             report_data['summary']['total_commits']) * 100
        },
        "recommendations": []
    }
    
    # Add recommendations
    if dashboard_data['risk_indicators']['overloaded_authors'] > 0:
        dashboard_data['recommendations'].append({
            "type": "workload_balancing",
            "priority": "high",
            "message": f"C√≥ {dashboard_data['risk_indicators']['overloaded_authors']} t√°c gi·∫£ b·ªã qu√° t·∫£i"
        })
    
    if dashboard_data['risk_indicators']['fix_percentage'] > 40:
        dashboard_data['recommendations'].append({
            "type": "quality_improvement",
            "priority": "medium",
            "message": f"T·ª∑ l·ªá fix commits cao ({dashboard_data['risk_indicators']['fix_percentage']:.1f}%)"
        })
    
    # Save dashboard
    dashboard_file = output_dir / 'team_dashboard.json'
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"üìä Team dashboard ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {dashboard_file}")

def main():
    """H√†m ch√≠nh ƒë·ªÉ ph√¢n t√≠ch n√¢ng cao"""
    print("üöÄ ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c test_results. H√£y ch·∫°y test_commit_analyzer.py tr∆∞·ªõc.")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file b√°o c√°o. H√£y ch·∫°y test_commit_analyzer.py tr∆∞·ªõc.")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"üìÑ ƒêang ph√¢n t√≠ch: {latest_report.name}")
    
    # Load report data
    report_data = load_analysis_report(latest_report)
    
    # Create output directory for advanced analysis
    advanced_output_dir = test_results_dir / "advanced_analysis"
    advanced_output_dir.mkdir(exist_ok=True)
    
    # Perform advanced analysis
    analyze_author_patterns(report_data)
    generate_recommendations(report_data)
    
    # Create visualizations
    try:
        create_visualizations(report_data, advanced_output_dir)
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o bi·ªÉu ƒë·ªì: {e}")
    
    # Create team dashboard
    create_team_dashboard(report_data, advanced_output_dir)
    
    print(f"\n‚úÖ Ph√¢n t√≠ch n√¢ng cao ho√†n th√†nh!")
    print(f"üìÅ K·∫øt qu·∫£ l∆∞u t·∫°i: {advanced_output_dir}")

if __name__ == "__main__":
    main()

```

### backend\ai\clean_github_data.py
```py
#!/usr/bin/env python3
"""
GitHub Data Processor - Download v√† Clean d·ªØ li·ªáu cho Multi-Modal Fusion Network
=============================================================================
Script n√†y s·∫Ω:
1. Download dataset GitHub commits t·ª´ Kaggle
2. Clean v√† chu·∫©n h√≥a d·ªØ li·ªáu 
3. T·∫°o labels ph√π h·ª£p cho multi-task learning
4. Xu·∫•t ra format chu·∫©n cho training
"""

import pandas as pd
import numpy as np
import json
import re
from pathlib import Path
from datetime import datetime
from collections import Counter
import traceback
from typing import Dict, List, Tuple, Any
import random

# Import ƒë·ªÉ t·∫°o synthetic metadata
from multimodal_fusion.data.synthetic_generator import GitHubDataGenerator

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"‚ùå L·ªói setup Kaggle API: {e}")
        print("üí° Vui l√≤ng ch·∫°y: pip install kaggle")
        print("üí° Ho·∫∑c setup API key theo h∆∞·ªõng d·∫´n: https://github.com/Kaggle/kaggle-api")
        return None

def download_github_dataset(api, force_download=False):
    """Download dataset GitHub commits t·ª´ Kaggle"""
    try:
        # T·∫°o th∆∞ m·ª•c download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Ki·ªÉm tra ƒë√£ download ch∆∞a
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"‚úÖ Dataset ƒë√£ t·ªìn t·∫°i: {csv_files[0]}")
            return csv_files[0]
        
        print("üì• ƒêang download GitHub commit dataset t·ª´ Kaggle...")
        print("üìÅ Dataset: dhruvildave/github-commit-messages-dataset")
        
        # Download dataset
        api.dataset_download_files(
            "dhruvildave/github-commit-messages-dataset", 
            path=str(download_dir), 
            unzip=True
        )
        
        # T√¨m file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV trong dataset")
            return None
        
        csv_file = csv_files[0]
        print(f"‚úÖ Download th√†nh c√¥ng: {csv_file}")
        print(f"üìä K√≠ch th∆∞·ªõc file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return csv_file
        
    except Exception as e:
        print(f"‚ùå L·ªói download dataset: {e}")
        return None

def clean_github_data(csv_file: Path, sample_size: int = 20000) -> pd.DataFrame:
    """Clean v√† chu·∫©n h√≥a d·ªØ li·ªáu GitHub commits"""
    try:
        print(f"\nüìä CLEANING D·ªÆ LI·ªÜU: {csv_file.name}")
        print("="*60)
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi chunk ƒë·ªÉ ti·∫øt ki·ªám memory
        print("üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        
        # ƒê·ªçc sample ƒë·ªÉ xem c·∫•u tr√∫c
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"üìã Columns: {list(sample_df.columns)}")
        
        # T√¨m column ch·ª©a commit message
        message_col = None
        for col in ['message', 'subject', 'commit', 'commit_message']:
            if col in sample_df.columns:
                message_col = col
                break
        
        if not message_col:
            print("‚ùå Kh√¥ng t√¨m th·∫•y column ch·ª©a commit message")
            return None
        
        print(f"üí¨ S·ª≠ d·ª•ng column: '{message_col}' l√†m commit message")
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi sampling hi·ªáu qu·∫£
        print(f"üéØ Sampling {sample_size:,} records...")
        
        # ƒê·ªçc theo chunk v√† sample
        chunk_size = 10000
        sampled_chunks = []
        total_read = 0
        
        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
            total_read += len(chunk)
            
            # Random sample t·ª´ chunk
            if len(chunk) > 0:
                chunk_sample_size = min(sample_size // 20, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
            
            # D·ª´ng khi ƒë·ªß data
            total_sampled = sum(len(c) for c in sampled_chunks)
            if total_sampled >= sample_size:
                break
                
            if total_read % 50000 == 0:
                print(f"  ƒê√£ ƒë·ªçc: {total_read:,} records...")
        
        # Combine chunks
        df = pd.concat(sampled_chunks, ignore_index=True)
        if len(df) > sample_size:
            df = df.sample(n=sample_size).reset_index(drop=True)
        
        print(f"üìä ƒê√£ sample {len(df):,} commits t·ª´ {total_read:,} total")
        
        # B∆Ø·ªöC 1: L√†m s·∫°ch d·ªØ li·ªáu c∆° b·∫£n
        print(f"\nüßπ B∆Ø·ªöC 1: L√ÄM S·∫†CH C∆† B·∫¢N")
        original_count = len(df)
        
        # Lo·∫°i b·ªè messages r·ªóng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè empty: {len(df):,} (-{original_count - len(df)})")
        
        # Lo·∫°i b·ªè messages qu√° ng·∫Øn ho·∫∑c qu√° d√†i
        df = df[df[message_col].str.len().between(3, 200)]
        print(f"  ‚Ä¢ Sau khi l·ªçc ƒë·ªô d√†i (3-200 chars): {len(df):,}")
        
        # Lo·∫°i b·ªè duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè duplicates: {len(df):,}")
        
        # B∆Ø·ªöC 2: Clean text content
        print(f"\nüî§ B∆Ø·ªöC 2: CLEAN TEXT CONTENT")
        
        def clean_commit_message(text):
            """Clean commit message text"""
            if pd.isna(text):
                return ""
                
            text = str(text).strip()
            
            # Remove excessive whitespace
            text = re.sub(r'\s+', ' ', text)
            
            # Remove URLs
            text = re.sub(r'http[s]?://\S+', '[URL]', text)
            
            # Remove email addresses
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
            
            # Remove excessive punctuation
            text = re.sub(r'[!]{2,}', '!', text)
            text = re.sub(r'[?]{2,}', '?', text)
            text = re.sub(r'[.]{3,}', '...', text)
            
            # Remove special characters but keep useful ones
            text = re.sub(r'[^\w\s\-_.,;:!?()\[\]{}#@/\\+=<>|~`]', '', text)
            
            return text.strip()
        
        df[message_col] = df[message_col].apply(clean_commit_message)
        
        # Remove messages that became empty after cleaning
        df = df[df[message_col].str.len() >= 3]
        print(f"  ‚Ä¢ Sau khi clean text: {len(df):,} commits")
        
        # B∆Ø·ªöC 3: Ph√¢n lo·∫°i v√† t·∫°o labels
        print(f"\nüè∑Ô∏è  B∆Ø·ªöC 3: T·∫†O LABELS CHO MULTI-TASK LEARNING")
        
        # Classify commit types
        df['commit_type'] = df[message_col].apply(classify_commit_type)
        df['purpose'] = df[message_col].apply(classify_purpose)
        df['sentiment'] = df[message_col].apply(classify_sentiment)
        df['tech_tag'] = df[message_col].apply(classify_tech_tag)
        
        # Th·ªëng k√™ labels
        print(f"\nüìä TH·ªêNG K√ä LABELS:")
        for col in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            value_counts = df[col].value_counts()
            print(f"  {col}:")
            for val, count in value_counts.head(5).items():
                print(f"    {val}: {count} ({count/len(df)*100:.1f}%)")
        
        # B∆Ø·ªöC 4: T·∫°o metadata synthetic
        print(f"\n‚öôÔ∏è B∆Ø·ªöC 4: T·∫†O METADATA SYNTHETIC")
        generator = GitHubDataGenerator()
        def create_synthetic_metadata():
            """T·∫°o metadata synthetic cho m·ªói commit"""
            sample = generator.generate_single_commit()
            return {
                'author': sample['author'],
                'repository': sample['repository'], 
                'timestamp': sample['timestamp'],
                'files_changed': sample['files_changed'],
                'additions': sample['additions'],
                'deletions': sample['deletions'],
                'file_types': sample['file_types']
            }
        
        # Apply synthetic metadata
        metadata_list = [create_synthetic_metadata() for _ in range(len(df))]
        
        for key in ['author', 'repository', 'timestamp', 'files_changed', 'additions', 'deletions']:
            df[f'meta_{key}'] = [meta[key] for meta in metadata_list]
        
        # File types c·∫ßn x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ l√† list
        df['meta_file_types'] = [meta['file_types'] for meta in metadata_list]
        
        print(f"‚úÖ ƒê√£ t·∫°o synthetic metadata cho {len(df):,} commits")
        
        return df
        
    except Exception as e:
        print(f"‚ùå L·ªói clean d·ªØ li·ªáu: {e}")
        traceback.print_exc()
        return None

def classify_commit_type(message: str) -> str:
    """Ph√¢n lo·∫°i commit type d·ª±a theo conventional commits"""
    message = message.lower()
    
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new|create)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch|repair)\b'],
        'docs': [r'\b(doc|documentation|readme|comment|guide)\b'],
        'style': [r'\b(style|format|lint|clean|prettier|cosmetic)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup|improve)\b'],
        'test': [r'\b(test|spec|testing|coverage|unit|integration)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge|maint)\b'],
        'perf': [r'\b(perf|performance|optimize|speed|fast|slow)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization|vulnerability)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message: str) -> str:
    """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch c·ªßa commit"""
    message = message.lower()
    
    if re.search(r'\b(add|new|implement|create|build|introduce)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve|repair)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve|optimize)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment|guide)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage|unit)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability|exploit)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier|cosmetic)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release|version)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message: str) -> str:
    """Ph√¢n lo·∫°i sentiment c·ªßa commit"""
    message = message.lower()
    
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap', 'breaking']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'crash', 'wrong']
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good', 'success']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message: str) -> str:
    """Ph√¢n lo·∫°i technology tag"""
    message = message.lower()
    
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular|typescript|ts)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi|pandas|numpy)\b'],
        'java': [r'\b(java|maven|gradle|spring|junit)\b'],
        'css': [r'\b(css|scss|sass|style|styling|bootstrap)\b'],
        'html': [r'\b(html|template|markup|dom)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo|redis|sqlite)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service|http|request)\b'],
        'docker': [r'\b(docker|container|dockerfile|kubernetes|k8s)\b'],
        'git': [r'\b(git|merge|branch|commit|pull|push|clone)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration|e2e|pytest|jest)\b'],
        'security': [r'\b(security|auth|token|password|encrypt|decrypt|ssl|tls)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed|memory|cpu)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive|mobile)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def convert_to_training_format(df: pd.DataFrame, message_col: str) -> List[Dict]:
    """Convert cleaned DataFrame th√†nh format cho training"""
    print(f"\nüîÑ CONVERT SANG TRAINING FORMAT")
    
    training_samples = []
    
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"  Processed {idx}/{len(df)} samples")
            
        # T·∫°o sample theo format chu·∫©n
        sample = {
            'commit_message': row[message_col],
            'author': row['meta_author'],
            'repository': row['meta_repository'],
            'timestamp': row['meta_timestamp'],
            'files_changed': row['meta_files_changed'],
            'additions': row['meta_additions'],
            'deletions': row['meta_deletions'],
            'file_types': row['meta_file_types'],
            'labels': {
                'risk_prediction': classify_risk_level(row[message_col], row['commit_type']),
                'complexity_prediction': classify_complexity(row[message_col], row['meta_files_changed']),
                'hotspot_prediction': classify_hotspot(row['commit_type'], row['tech_tag']),
                'urgency_prediction': classify_urgency(row['sentiment'])
            }
        }
        
        training_samples.append(sample)
    
    print(f"‚úÖ Converted {len(training_samples)} samples")
    return training_samples

def classify_risk_level(message: str, commit_type: str) -> int:
    """Classify risk level (0: low, 1: high)"""
    high_risk_patterns = ['breaking', 'major', 'critical', 'breaking change', 'api change']
    high_risk_types = ['feat', 'refactor', 'security']
    
    message_lower = message.lower()
    
    if any(pattern in message_lower for pattern in high_risk_patterns):
        return 1
    elif commit_type in high_risk_types:
        return 1
    else:
        return 0

def classify_complexity(message: str, files_changed: int) -> int:
    """Classify complexity (0: low, 1: medium, 2: high)"""
    if files_changed >= 10:
        return 2
    elif files_changed >= 5:
        return 1
    else:
        return 0

def classify_hotspot(commit_type: str, tech_tag: str) -> int:
    """Classify hotspot area (0-4)"""
    hotspot_map = {
        'security': 0,
        'api': 1,
        'database': 2,
        'ui': 3,
        'general': 4
    }
    return hotspot_map.get(tech_tag, 4)

def classify_urgency(sentiment: str) -> int:
    """Classify urgency (0: normal, 1: urgent)"""
    return 1 if sentiment == 'urgent' else 0

def save_training_data(samples: List[Dict], output_dir: Path) -> str:
    """L∆∞u training data ƒë√£ clean"""
    output_dir.mkdir(exist_ok=True)
    
    # T·∫°o filename v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"cleaned_github_commits_{timestamp}.json"
    
    # T·∫°o metadata
    training_data = {
        'metadata': {
            'total_samples': len(samples),
            'created_at': datetime.now().isoformat(),
            'source': 'kaggle_github_commits_cleaned',
            'version': '1.0',
            'description': 'Cleaned GitHub commit data for Multi-Modal Fusion Network'
        },
        'samples': samples
    }
    
    # Save to JSON
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u {len(samples)} samples v√†o: {output_file}")
    print(f"üìä File size: {output_file.stat().st_size / (1024*1024):.1f} MB")
    
    return str(output_file)

def main():
    """Main function"""
    print("üöÄ GITHUB DATA PROCESSOR - CLEAN D·ªÆ LI·ªÜU CHO TRAINING")
    print("="*70)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        print("\n‚ùå Kh√¥ng th·ªÉ setup Kaggle API")
        print("üí° B·∫°n c√≥ th·ªÉ manually t·∫£i file CSV v√† ƒë·∫∑t v√†o th∆∞ m·ª•c kaggle_data/github_commits/")
        
        # Ki·ªÉm tra file manual
        manual_files = list(Path("kaggle_data/github_commits").glob("*.csv"))
        if manual_files:
            csv_file = manual_files[0]
            print(f"‚úÖ T√¨m th·∫•y file manual: {csv_file}")
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o")
            return
    else:
        # Download dataset
        csv_file = download_github_dataset(api)
        if not csv_file:
            print("‚ùå Kh√¥ng th·ªÉ download dataset")
            return
    
    # H·ªèi user v·ªÅ sample size
    print(f"\nüìä T√ôY CH·ªåN SAMPLE SIZE:")
    print("1. 5K samples (test nhanh)")
    print("2. 10K samples (demo)")
    print("3. 20K samples (khuy√™n d√πng)")
    print("4. 50K samples (training t·ªët)")
    print("5. 100K samples (dataset l·ªõn)")
    
    choice = input("Ch·ªçn option (1-5) [m·∫∑c ƒë·ªãnh: 3]: ").strip() or "3"
    
    sample_sizes = {
        '1': 5000,
        '2': 10000,
        '3': 20000,
        '4': 50000,
        '5': 100000
    }
    
    sample_size = sample_sizes.get(choice, 20000)
    print(f"üéØ S·∫Ω sample {sample_size:,} commits")
    
    # Clean data
    df = clean_github_data(csv_file, sample_size)
    if df is None:
        print("‚ùå Kh√¥ng th·ªÉ clean d·ªØ li·ªáu")
        return
    
    # Convert to training format
    message_col = 'message'  # ho·∫∑c t√¨m t·ª± ƒë·ªông
    for col in ['message', 'subject', 'commit', 'commit_message']:
        if col in df.columns:
            message_col = col
            break
    
    training_samples = convert_to_training_format(df, message_col)
    
    # Save cleaned data
    output_dir = Path("training_data")
    output_file = save_training_data(training_samples, output_dir)
    
    print(f"\nüéâ HO√ÄN TH√ÄNH CLEANING D·ªÆ LI·ªÜU!")
    print(f"üìÅ File output: {output_file}")
    print(f"üìä S·ªë samples: {len(training_samples):,}")
    
    print(f"\n‚ú® S·∫¥N S√ÄNG CHO TRAINING:")
    print(f"  python train_real_data.py")
    print(f"  # Ho·∫∑c s·ª≠ d·ª•ng file: {Path(output_file).name}")

if __name__ == "__main__":
    main()

```

### backend\ai\commit_model.py
```py
# AI commit model for TaskFlowAI
import os
import joblib
from pathlib import Path

class CommitClassifier:
    def __init__(self):
        self.model = None
        self.vectorizer = None
    
    @classmethod
    def load(cls):
        """Load the trained commit classifier model"""
        instance = cls()
        
        # ƒê∆∞·ªùng d·∫´n t·ªõi model trong th∆∞ m·ª•c AI
        model_dir = Path(__file__).parent / "trained_models"
        model_path = model_dir / "commit_classifier.joblib"
        
        try:
            if model_path.exists():
                data = joblib.load(model_path)
                if isinstance(data, dict):
                    instance.model = data.get('model')
                    instance.vectorizer = data.get('vectorizer')
                else:
                    instance.model = data
                print(f"‚úÖ Loaded commit classifier from {model_path}")
            else:
                print(f"‚ö†Ô∏è Model file not found at {model_path}")
                # T·∫°o mock model ƒë·ªÉ tr√°nh l·ªói
                instance._create_mock_model()
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            instance._create_mock_model()
        
        return instance
    
    def _create_mock_model(self):
        """Create a mock model for development"""
        print("üîß Creating mock commit classifier...")
        # Mock implementation
        self.model = None
        self.vectorizer = None
    
    def classify(self, commit_message):
        """Classify a commit message"""
        if self.model is None:
            # Mock classification
            return {
                'category': 'feature',
                'confidence': 0.85,
                'description': 'Mock classification result'
            }
        
        try:
            # Real classification logic would go here
            if self.vectorizer:
                features = self.vectorizer.transform([commit_message])
                prediction = self.model.predict(features)[0]
                confidence = self.model.predict_proba(features).max()
                
                return {
                    'category': prediction,
                    'confidence': confidence,
                    'description': f'Classified as {prediction}'
                }
        except Exception as e:
            print(f"Classification error: {e}")
        
        # Fallback
        return {
            'category': 'other',
            'confidence': 0.5,
            'description': 'Classification failed, using fallback'
        }
    
    def save(self, path=None):
        """Save the model"""
        if path is None:
            model_dir = Path(__file__).parent / "trained_models"
            model_dir.mkdir(exist_ok=True)
            path = model_dir / "commit_classifier.joblib"
        
        try:
            if self.model and self.vectorizer:
                data = {
                    'model': self.model,
                    'vectorizer': self.vectorizer
                }
                joblib.dump(data, path)
                print(f"‚úÖ Model saved to {path}")
            else:
                print("‚ö†Ô∏è No model to save")
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")

```

### backend\ai\debug_classification_fixed.py
```py
#!/usr/bin/env python3
"""
Debug script ƒë·ªÉ ki·ªÉm tra v·∫•n ƒë·ªÅ ph√¢n lo·∫°i commit type
"""

import torch
import os
import sys
from test_commit_analyzer import CommitAnalyzer

def test_problematic_commits():
    """Test c√°c commit messages c√≥ v·∫•n ƒë·ªÅ ph√¢n lo·∫°i"""
    
    print("üîç DEBUGGING COMMIT CLASSIFICATION ISSUES")
    print("="*60)
    
    # Initialize analyzer v·ªõi model path
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test cases c√≥ v·∫•n ƒë·ªÅ
    test_cases = [
        {
            "message": "docs: fix typo in configuration guide", 
            "expected": "docs",
            "author": "Test User"
        },
        {
            "message": "docs: update installation instructions",
            "expected": "docs", 
            "author": "Test User"
        },
        {
            "message": "test: add unit tests for user service",
            "expected": "test",
            "author": "Test User" 
        },
        {
            "message": "test: fix failing integration tests",
            "expected": "test",
            "author": "Test User"
        },
        {
            "message": "fix: typo in variable name",
            "expected": "fix",
            "author": "Test User"
        },
        {
            "message": "feat: add new documentation system", 
            "expected": "feat",
            "author": "Test User"
        },
        {
            "message": "chore: update dependencies",
            "expected": "chore", 
            "author": "Test User"
        },
        {
            "message": "style: fix code formatting",
            "expected": "style",
            "author": "Test User"
        }
    ]
    
    print(f"üß™ Testing {len(test_cases)} problematic commit messages...")
    print()
    
    correct_predictions = 0
    total_predictions = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        message = case["message"]
        expected = case["expected"]
        author = case["author"]
        
        print(f"{i}. Testing: '{message}'")
        print(f"   Expected: {expected}")
        
        # Analyze commit
        analysis = analyzer.predict_commit(message, author)
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        
        print(f"   Predicted: {predicted} (confidence: {confidence:.3f})")
        
        # Check if correct
        is_correct = predicted == expected
        if is_correct:
            print("   ‚úÖ CORRECT")
            correct_predictions += 1
        else:
            print("   ‚ùå WRONG")
            
            # Analyze why it's wrong - show all predictions for this commit
            print("   üîç Full predictions:")
            for task, pred in analysis.predicted_labels.items():
                conf = analysis.confidence_scores.get(task, 0.0)
                print(f"      {task}: {pred} ({conf:.3f})")
        
        print()
    
    # Summary
    accuracy = correct_predictions / total_predictions * 100
    print("="*60)
    print(f"üìä CLASSIFICATION ACCURACY: {correct_predictions}/{total_predictions} ({accuracy:.1f}%)")
    
    if accuracy < 80:
        print("üö® LOW ACCURACY DETECTED!")
        print("üí° Possible issues:")
        print("   - Model wasn't trained properly on commit prefixes")
        print("   - Training data didn't have enough conventional commit examples")
        print("   - Model is focusing on content words rather than prefixes")
        print("   - Need to retrain with better conventional commit dataset")
    else:
        print("‚úÖ Classification accuracy looks good!")
    
    return accuracy

def analyze_model_attention():
    """Ph√¢n t√≠ch xem model ƒëang ch√∫ √Ω v√†o ph·∫ßn n√†o c·ªßa commit message"""
    
    print("\nüß† ANALYZING MODEL ATTENTION PATTERNS")
    print("="*60)
    
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test v·ªõi c√°c variations
    test_variations = [
        "docs: fix typo in configuration guide",
        "fix typo in configuration guide",  # Kh√¥ng c√≥ prefix
        "docs: update configuration guide",  # Kh√¥ng c√≥ t·ª´ "fix"
        "fix: typo in configuration guide"   # Prefix kh√°c
    ]
    
    print("Testing how model responds to different parts of the message:")
    print()
    
    for i, message in enumerate(test_variations, 1):
        print(f"{i}. '{message}'")
        analysis = analyzer.predict_commit(message, "Test User")
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        print(f"   ‚Üí {predicted} ({confidence:.3f})")
        print()
    
    return test_variations

if __name__ == "__main__":
    try:
        accuracy = test_problematic_commits()
        analyze_model_attention()
        
        print("\nüí° RECOMMENDATIONS:")
        print("="*60)
        
        if accuracy < 80:
            print("üîß TO FIX CLASSIFICATION ISSUES:")
            print("1. Retrain model with more conventional commit examples")
            print("2. Add prefix-aware preprocessing")
            print("3. Use rule-based fallback for obvious prefixes")
            print("4. Create training data with equal distribution of commit types")
            print("5. Consider ensemble model (ML + rule-based)")
        
        print("\n‚úÖ Debug completed!")
        
    except Exception as e:
        print(f"‚ùå Error during debugging: {e}")
        import traceback
        traceback.print_exc()

```

### backend\ai\debug_test.py
```py
#!/usr/bin/env python3
"""
Simple Commit Analyzer Test - Debug version
"""

import os
import sys
import json
import torch
from pathlib import Path

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

def test_model_loading():
    """Test basic model loading"""
    print("üöÄ SIMPLE COMMIT ANALYZER TEST")
    print("="*50)
    
    # Check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Device: {device}")
    
    # Check model file
    model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
    print(f"üì¶ Model path: {model_path}")
    print(f"üì¶ Model exists: {model_path.exists()}")
    
    if not model_path.exists():
        print("‚ùå Model file not found!")
        return False
    
    try:
        # Load checkpoint
        print("üì• Loading checkpoint...")
        checkpoint = torch.load(model_path, map_location=device)
        
        print("‚úÖ Checkpoint loaded successfully!")
        print(f"   Keys: {list(checkpoint.keys())}")
        
        if 'num_classes' in checkpoint:
            print(f"   Tasks: {list(checkpoint['num_classes'].keys())}")
            print(f"   Classes per task: {checkpoint['num_classes']}")
        
        if 'val_accuracy' in checkpoint:
            print(f"   Best accuracy: {checkpoint['val_accuracy']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading checkpoint: {e}")
        return False

def test_simple_prediction():
    """Test a simple prediction"""
    try:
        from train_han_github import SimpleHANModel, SimpleTokenizer
        print("\nüß™ Testing simple prediction...")
        
        # Load model components
        model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        checkpoint = torch.load(model_path, map_location=device)
        tokenizer = checkpoint['tokenizer']
        num_classes = checkpoint['num_classes']
        label_encoders = checkpoint['label_encoders']
        
        # Create reverse label encoders
        reverse_encoders = {}
        for task, encoder in label_encoders.items():
            reverse_encoders[task] = {v: k for k, v in encoder.items()}
        
        # Initialize model
        vocab_size = len(tokenizer.word_to_idx)
        model = SimpleHANModel(
            vocab_size=vocab_size,
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"‚úÖ Model initialized with vocab size: {vocab_size}")
        
        # Test prediction
        test_text = "fix: resolve authentication bug in login endpoint"
        print(f"üìù Testing text: '{test_text}'")
        
        # Tokenize
        input_ids = tokenizer.encode_text(test_text, max_sentences=10, max_words=50)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
            
            print(f"üîç Predictions:")
            for task, output in outputs.items():
                probs = torch.softmax(output, dim=1)
                confidence, pred_idx = torch.max(probs, 1)
                
                pred_idx = pred_idx.item()
                confidence = confidence.item()
                
                predicted_label = reverse_encoders[task][pred_idx]
                print(f"   {task}: {predicted_label} (confidence: {confidence:.3f})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error in prediction test: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("Starting debug tests...\n")
    
    # Test 1: Model loading
    if test_model_loading():
        print("\n" + "="*50)
        # Test 2: Simple prediction
        test_simple_prediction()
    
    print("\nüéØ Debug tests completed!")

```

### backend\ai\download_github_commits.py
```py
"""
Download v√† x·ª≠ l√Ω dataset GitHub Commit Messages t·ª´ Kaggle
Dataset: mrisdal/github-commit-messages
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import re
from collections import Counter
import zipfile

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"‚ùå L·ªói setup Kaggle API: {e}")
        print("Vui l√≤ng ch·∫°y: python quick_kaggle_setup.py")
        return None

def download_dataset(api, dataset_name="dhruvildave/github-commit-messages-dataset", force_download=False):
    """Download dataset t·ª´ Kaggle"""
    try:
        # T·∫°o th∆∞ m·ª•c download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Ki·ªÉm tra ƒë√£ download ch∆∞a
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"‚úÖ Dataset ƒë√£ t·ªìn t·∫°i trong {download_dir}")
            return download_dir, csv_files[0]
        
        print(f"üì• ƒêang download dataset: {dataset_name}")
        print(f"üìÅ V√†o th∆∞ m·ª•c: {download_dir}")
        
        # Download dataset
        api.dataset_download_files(
            dataset_name, 
            path=str(download_dir), 
            unzip=True
        )
        
        # T√¨m file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV trong dataset")
            return None, None
        
        csv_file = csv_files[0]
        print(f"‚úÖ Download th√†nh c√¥ng: {csv_file}")
        print(f"üìä K√≠ch th∆∞·ªõc file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return download_dir, csv_file
        
    except Exception as e:
        print(f"‚ùå L·ªói download dataset: {e}")
        return None, None

def analyze_commit_data(csv_file, sample_size=None):
    """Ph√¢n t√≠ch d·ªØ li·ªáu commit"""
    try:
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU: {csv_file.name}")
        print("="*60)
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi chunk ƒë·ªÉ ti·∫øt ki·ªám memory
        print("üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        
        # ƒê·ªçc m·ªôt ph·∫ßn nh·ªè tr∆∞·ªõc ƒë·ªÉ xem c·∫•u tr√∫c
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"üìã Columns: {list(sample_df.columns)}")
        print(f"üìè Sample shape: {sample_df.shape}")
          # ƒê·ªçc to√†n b·ªô ho·∫∑c sample m·ªôt c√°ch hi·ªáu qu·∫£
        if sample_size:
            print(f"üìä Sampling {sample_size:,} records...")
            # S·ª≠ d·ª•ng chunk reading ƒë·ªÉ memory-efficient sampling
            chunk_size = 10000
            sampled_chunks = []
            total_read = 0
            
            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
                total_read += len(chunk)
                
                # Random sample t·ª´ chunk n√†y
                chunk_sample_size = min(sample_size // 10, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
                
                # D·ª´ng khi ƒë√£ ƒë·ªß data
                total_sampled = sum(len(c) for c in sampled_chunks)
                if total_sampled >= sample_size:
                    break
                
                if total_read % 50000 == 0:
                    print(f"  ƒê√£ ƒë·ªçc: {total_read:,} records...")
            
            # Combine chunks
            df = pd.concat(sampled_chunks, ignore_index=True)
            if len(df) > sample_size:
                df = df.sample(n=sample_size).reset_index(drop=True)
            
            print(f"üìä ƒê√£ sample {len(df):,} commits t·ª´ {total_read:,} total")
        else:
            print("üìñ ƒê·ªçc to√†n b·ªô dataset (c√≥ th·ªÉ m·∫•t th·ªùi gian)...")
            df = pd.read_csv(csv_file)
            print(f"üìä ƒê√£ ƒë·ªçc {len(df):,} commits")
        
        # Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n
        print(f"\nüìà TH·ªêNG K√ä C∆† B·∫¢N:")
        print(f"  ‚Ä¢ T·ªïng s·ªë commits: {len(df):,}")
        print(f"  ‚Ä¢ Columns: {df.shape[1]}")
        print(f"  ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # Ki·ªÉm tra columns quan tr·ªçng
        important_cols = ['message', 'commit', 'subject', 'body', 'author', 'repo']
        available_cols = [col for col in important_cols if col in df.columns]
        print(f"  ‚Ä¢ Available important columns: {available_cols}")
        
        # T√¨m column ch·ª©a commit message
        message_col = None
        for col in ['message', 'subject', 'commit']:
            if col in df.columns:
                message_col = col
                break
        
        if not message_col:
            print("‚ùå Kh√¥ng t√¨m th·∫•y column ch·ª©a commit message")
            return None
        
        print(f"  ‚Ä¢ S·ª≠ d·ª•ng column: '{message_col}' l√†m commit message")
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        print(f"\nüßπ L√ÄM S·∫†CH D·ªÆ LI·ªÜU:")
        original_count = len(df)
        
        # Lo·∫°i b·ªè messages r·ªóng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè empty: {len(df):,} (-{original_count - len(df)})")
        
        # Lo·∫°i b·ªè messages qu√° ng·∫Øn ho·∫∑c qu√° d√†i
        df = df[df[message_col].str.len().between(5, 500)]
        print(f"  ‚Ä¢ Sau khi l·ªçc ƒë·ªô d√†i (5-500 chars): {len(df):,}")
        
        # Lo·∫°i b·ªè duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè duplicates: {len(df):,}")
        
        # Ph√¢n t√≠ch n·ªôi dung
        print(f"\nüìù PH√ÇN T√çCH N·ªòI DUNG:")
        messages = df[message_col].astype(str)
        
        # Th·ªëng k√™ ƒë·ªô d√†i
        lengths = messages.str.len()
        print(f"  ‚Ä¢ ƒê·ªô d√†i trung b√¨nh: {lengths.mean():.1f} chars")
        print(f"  ‚Ä¢ ƒê·ªô d√†i median: {lengths.median():.1f} chars")
        print(f"  ‚Ä¢ Min/Max: {lengths.min()}/{lengths.max()} chars")
          # Top words - sample ƒë·ªÉ tr√°nh qu√° t·∫£i memory
        sample_size_for_words = min(5000, len(messages))
        print(f"  ‚Ä¢ Analyzing words from {sample_size_for_words} samples...")
        
        all_words = []
        sample_messages = messages.sample(n=sample_size_for_words) if len(messages) > sample_size_for_words else messages
        
        for msg in sample_messages:
            words = re.findall(r'\b[a-zA-Z]+\b', str(msg).lower())
            all_words.extend(words)
        
        word_counts = Counter(all_words).most_common(20)
        print(f"\nüî§ TOP 20 WORDS (from {sample_size_for_words} samples):")
        for word, count in word_counts:
            print(f"    {word}: {count}")
        
        return df, message_col
        
    except Exception as e:
        print(f"‚ùå L·ªói ph√¢n t√≠ch d·ªØ li·ªáu: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def classify_commits(df, message_col):
    """Ph√¢n lo·∫°i commits theo c√°c ti√™u ch√≠"""
    print(f"\nüè∑Ô∏è  PH√ÇN LO·∫†I COMMITS:")
    print("="*60)
    
    messages = df[message_col].astype(str).str.lower()
    classifications = []
    
    # Process in batches ƒë·ªÉ tr√°nh memory issues
    batch_size = 1000
    total_batches = (len(messages) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(messages))
        batch_messages = messages[start_idx:end_idx]
        
        batch_classifications = []
        for message in batch_messages:
            labels = {
                'commit_type': classify_commit_type(message),
                'purpose': classify_purpose(message),
                'sentiment': classify_sentiment(message),
                'tech_tag': classify_tech_tag(message)
            }
            batch_classifications.append(labels)
        
        classifications.extend(batch_classifications)
        
        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:
            print(f"  ƒê√£ x·ª≠ l√Ω: {end_idx:,}/{len(messages):,} ({(end_idx/len(messages)*100):.1f}%)")
    
    # Th·ªëng k√™ ph√¢n lo·∫°i
    print(f"\nüìä TH·ªêNG K√ä PH√ÇN LO·∫†I:")
    
    for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
        values = [c[category] for c in classifications]
        counter = Counter(values)
        print(f"\n{category.upper()}:")
        for value, count in counter.most_common(10):
            percentage = (count / len(values)) * 100
            print(f"    {value}: {count:,} ({percentage:.1f}%)")
    
    return classifications

def classify_commit_type(message):
    """Ph√¢n lo·∫°i commit type"""
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch)\b'],
        'docs': [r'\b(doc|documentation|readme|comment)\b'],
        'style': [r'\b(style|format|lint|clean|prettier)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup)\b'],
        'test': [r'\b(test|spec|testing|coverage)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge)\b'],
        'perf': [r'\b(perf|performance|optimize|speed)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message):
    """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch"""
    if re.search(r'\b(add|new|implement|create|build)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message):
    """Ph√¢n lo·∫°i sentiment"""
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'bad']
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message):
    """Ph√¢n lo·∫°i technology tag"""
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi)\b'],
        'java': [r'\b(java|maven|gradle|spring)\b'],
        'css': [r'\b(css|scss|sass|style|styling)\b'],
        'html': [r'\b(html|template|markup)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service)\b'],
        'docker': [r'\b(docker|container|dockerfile)\b'],
        'git': [r'\b(git|merge|branch|commit|pull)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration)\b'],
        'security': [r'\b(security|auth|token|password|encrypt)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def save_processed_data(df, message_col, classifications, output_dir):
    """L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"""
    try:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # T·∫°o training data format
        training_data = []
        
        for idx, (_, row) in enumerate(df.iterrows()):
            if idx >= len(classifications):
                break
                
            labels = classifications[idx]
            
            training_data.append({
                'text': row[message_col],
                'labels': labels,
                'metadata': {
                    'source': 'github-commit-messages',
                    'original_index': idx
                }
            })
        
        # T·∫°o metadata
        metadata = {
            'total_samples': len(training_data),
            'created_at': datetime.now().isoformat(),
            'source_dataset': 'mrisdal/github-commit-messages',
            'message_column': message_col,
            'statistics': {}
        }
        
        # Th·ªëng k√™ cho metadata
        for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            values = [item['labels'][category] for item in training_data]
            metadata['statistics'][category] = dict(Counter(values))
        
        # Save training data
        output_file = output_dir / 'github_commits_training_data.json'
        final_data = {
            'metadata': metadata,
            'data': training_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ ƒê√É L√ÄU D·ªÆ LI·ªÜU:")
        print(f"  üìÅ File: {output_file}")
        print(f"  üìä Samples: {len(training_data):,}")
        print(f"  üìè Size: {output_file.stat().st_size / (1024*1024):.1f} MB")
        
        # L∆∞u sample ƒë·ªÉ preview
        sample_file = output_dir / 'sample_preview.json'
        sample_data = {
            'metadata': metadata,
            'data': training_data[:100]  # 100 samples ƒë·∫ßu
        }
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, indent=2, ensure_ascii=False)
        
        print(f"  üîç Sample: {sample_file}")
        
        return output_file
        
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u d·ªØ li·ªáu: {e}")
        return None

def main():
    """Main function"""
    print("üöÄ GITHUB COMMIT MESSAGES DOWNLOADER")
    print("="*60)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        return
    
    # Download dataset
    download_dir, csv_file = download_dataset(api)
    if not csv_file:
        return
      # H·ªèi user v·ªÅ sample size
    print(f"\nüìä T√ôY CH·ªåN PROCESSING:")
    print("1. Sample 1K commits (test nhanh)")
    print("2. Sample 5K commits (demo)")
    print("3. Sample 10K commits (khuy√™n d√πng)")
    print("4. Sample 50K commits (training t·ªët)")
    print("5. Sample 100K commits (dataset l·ªõn)")
    print("6. X·ª≠ l√Ω to√†n b·ªô dataset (c·∫£nh b√°o: c√≥ th·ªÉ r·∫•t l√¢u)")
    
    choice = input("Ch·ªçn option (1-6): ").strip()
    
    sample_sizes = {
        '1': 1000,
        '2': 5000,
        '3': 10000,
        '4': 50000,
        '5': 100000,
        '6': None
    }
    
    sample_size = sample_sizes.get(choice, 10000)
    
    if sample_size is None:
        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: B·∫°n ƒë√£ ch·ªçn x·ª≠ l√Ω to√†n b·ªô dataset!")
        print("   ƒêi·ªÅu n√†y c√≥ th·ªÉ m·∫•t r·∫•t nhi·ªÅu th·ªùi gian v√† b·ªô nh·ªõ.")
        confirm = input("B·∫°n c√≥ ch·∫Øc ch·∫Øn kh√¥ng? (yes/no): ").lower()
        if confirm != 'yes':
            sample_size = 10000
            print("üîÑ Chuy·ªÉn v·ªÅ sample 10K commits")
    
    # Analyze data
    df, message_col = analyze_commit_data(csv_file, sample_size)
    if df is None:
        return
    
    # Classify commits
    classifications = classify_commits(df, message_col)
    
    # Save processed data
    output_dir = Path(__file__).parent / "training_data"
    output_file = save_processed_data(df, message_col, classifications, output_dir)
    
    if output_file:
        print(f"\nüéâ HO√ÄN TH√ÄNH!")
        print(f"üìã B√¢y gi·ªù b·∫°n c√≥ th·ªÉ:")
        print(f"  ‚Ä¢ Train HAN: python train_han_with_kaggle.py")
        print(f"  ‚Ä¢ Train XGBoost: python train_xgboost.py")
        print(f"  üìÅ Data file: {output_file}")

if __name__ == "__main__":
    main()

```

### backend\ai\download_kaggle_dataset.py
```py
"""
Script ƒë·ªÉ t·∫£i dataset commit t·ª´ Kaggle v√† chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh HAN
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import zipfile
import shutil
from typing import List, Dict, Any, Tuple
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class KaggleDatasetDownloader:
    def __init__(self, base_dir: str = None):
        """
        Kh·ªüi t·∫°o class ƒë·ªÉ t·∫£i dataset t·ª´ Kaggle
        
        Args:
            base_dir: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u d·ªØ li·ªáu
        """
        self.base_dir = base_dir or os.path.dirname(__file__)
        self.data_dir = os.path.join(self.base_dir, 'kaggle_data')
        self.processed_dir = os.path.join(self.base_dir, 'training_data')
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def check_kaggle_config(self) -> bool:
        """Ki·ªÉm tra c·∫•u h√¨nh Kaggle API"""
        try:
            import kaggle
            logger.info("‚úÖ Kaggle API ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh")
            return True
        except ImportError:
            logger.error("‚ùå Kaggle package ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install kaggle")
            return False
        except OSError as e:
            logger.error(f"‚ùå L·ªói c·∫•u h√¨nh Kaggle API: {e}")
            logger.info("Vui l√≤ng:")
            logger.info("1. T·∫°o API token t·∫°i: https://www.kaggle.com/settings")
            logger.info("2. ƒê·∫∑t file kaggle.json v√†o ~/.kaggle/ (Linux/Mac) ho·∫∑c C:\\Users\\<username>\\.kaggle\\ (Windows)")
            logger.info("3. C·∫•p quy·ªÅn 600 cho file: chmod 600 ~/.kaggle/kaggle.json")
            return False
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """
        T·∫£i dataset t·ª´ Kaggle
        
        Args:
            dataset_name: T√™n dataset tr√™n Kaggle (format: username/dataset-name)
            force_download: C√≥ t·∫£i l·∫°i n·∫øu ƒë√£ t·ªìn t·∫°i hay kh√¥ng
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        if not self.check_kaggle_config():
            return False
            
        try:
            import kaggle
            
            dataset_path = os.path.join(self.data_dir, dataset_name.split('/')[-1])
            
            if os.path.exists(dataset_path) and not force_download:
                logger.info(f"Dataset {dataset_name} ƒë√£ t·ªìn t·∫°i, b·ªè qua t·∫£i xu·ªëng")
                return True
                
            logger.info(f"üîÑ ƒêang t·∫£i dataset: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=self.data_dir, 
                unzip=True
            )
            
            logger.info(f"‚úÖ T·∫£i th√†nh c√¥ng dataset: {dataset_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫£i dataset {dataset_name}: {e}")
            return False
    
    def list_popular_commit_datasets(self) -> List[str]:
        """Li·ªát k√™ c√°c dataset commit ph·ªï bi·∫øn tr√™n Kaggle"""
        return [
            "shashankbansal6/git-commits-message-dataset",
            "madhav28/git-commit-messages",
            "aashita/git-commit-messages",
            "jainaru/commit-classification-dataset",
            "shubhamjain0594/commit-message-generation",
            "saurabhshahane/conventional-commit-messages",
            "devanshunigam/commits",
            "ashydv/commits-dataset"
        ]
    
    def process_commit_dataset(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu commit t·ª´ c√°c file CSV
        
        Args:
            csv_files: Danh s√°ch c√°c file CSV
            
        Returns:
            Dict ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        """
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"üîÑ ƒêang x·ª≠ l√Ω file: {csv_file}")
            
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"üìä S·ªë l∆∞·ª£ng records: {len(df)}")
                logger.info(f"üìã C√°c c·ªôt: {list(df.columns)}")
                
                # Chu·∫©n h√≥a t√™n c·ªôt
                df.columns = df.columns.str.lower().str.strip()
                
                # T√¨m c·ªôt ch·ª©a commit message
                message_cols = [col for col in df.columns if 
                              any(keyword in col for keyword in ['message', 'commit', 'msg', 'text', 'description'])]
                
                if not message_cols:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt commit message trong {csv_file}")
                    continue
                
                message_col = message_cols[0]
                logger.info(f"üìù S·ª≠ d·ª•ng c·ªôt '{message_col}' l√†m commit message")
                
                # X·ª≠ l√Ω d·ªØ li·ªáu
                for _, row in df.iterrows():
                    commit_msg = str(row.get(message_col, '')).strip()
                    
                    if not commit_msg or commit_msg == 'nan' or len(commit_msg) < 5:
                        continue
                    
                    # Tr√≠ch xu·∫•t th√¥ng tin kh√°c n·∫øu c√≥
                    author = str(row.get('author', row.get('committer', 'unknown'))).strip()
                    repo = str(row.get('repo', row.get('repository', row.get('project', 'unknown')))).strip()
                    
                    # Ph√¢n lo·∫°i commit d·ª±a tr√™n message
                    commit_type = self.classify_commit_type(commit_msg)
                    purpose = self.classify_commit_purpose(commit_msg)
                    sentiment = self.classify_sentiment(commit_msg)
                    tech_tags = self.extract_tech_tags(commit_msg)
                    
                    data_point = {
                        'commit_message': commit_msg,
                        'commit_type': commit_type,
                        'purpose': purpose,
                        'sentiment': sentiment,
                        'tech_tag': tech_tags[0] if tech_tags else 'general',
                        'author': author if author != 'nan' else 'unknown',
                        'source_repo': repo if repo != 'nan' else 'unknown'
                    }
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω file {csv_file}: {e}")
                continue
        
        logger.info(f"‚úÖ T·ªïng c·ªông x·ª≠ l√Ω ƒë∆∞·ª£c {len(all_data)} commit messages")
        return {'data': all_data, 'total_count': len(all_data)}
    
    def classify_commit_type(self, message: str) -> str:
        """Ph√¢n lo·∫°i lo·∫°i commit d·ª±a tr√™n message"""
        message_lower = message.lower()
        
        # Conventional commit patterns
        if message_lower.startswith(('feat:', 'feature:')):return 'feat'
        elif message_lower.startswith(('fix:', 'bugfix:')):return 'fix'
        elif message_lower.startswith(('docs:', 'doc:')):return 'docs'
        elif message_lower.startswith(('style:', 'format:')):return 'style'
        elif message_lower.startswith(('refactor:', 'refact:')):return 'refactor'
        elif message_lower.startswith(('test:', 'tests:')):return 'test'
        elif message_lower.startswith(('chore:', 'build:', 'ci:')):return 'chore'
        
        # Keyword-based classification
        elif any(word in message_lower for word in ['add', 'implement', 'create', 'new']):
            return 'feat'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            return 'fix'
        elif any(word in message_lower for word in ['update', 'modify', 'change']):
            return 'feat'
        elif any(word in message_lower for word in ['remove', 'delete', 'clean']):
            return 'chore'
        elif any(word in message_lower for word in ['test', 'spec']):
            return 'test'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
            return 'docs'
        else:
            return 'other'
    
    def classify_commit_purpose(self, message: str) -> str:
        """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch commit"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['feature', 'feat', 'add', 'implement', 'new']):
            return 'Feature Implementation'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'patch']):
            return 'Bug Fix'
        elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
            return 'Refactoring'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
            return 'Documentation Update'
        elif any(word in message_lower for word in ['test', 'spec', 'testing']):
            return 'Test Update'
        elif any(word in message_lower for word in ['security', 'secure', 'vulnerability']):
            return 'Security Patch'
        elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
            return 'Code Style/Formatting'
        elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy', 'pipeline']):
            return 'Build/CI/CD Script Update'
        else:
            return 'Other'
    
    def classify_sentiment(self, message: str) -> str:
        """Ph√¢n lo·∫°i c·∫£m x√∫c trong commit message"""
        message_lower = message.lower()
        
        positive_words = ['improve', 'enhance', 'optimize', 'upgrade', 'better', 'good', 'great', 'awesome']
        negative_words = ['bug', 'error', 'issue', 'problem', 'fail', 'broken', 'wrong']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        if any(word in message_lower for word in urgent_words):
            return 'urgent'
        elif any(word in message_lower for word in positive_words):
            return 'positive'
        elif any(word in message_lower for word in negative_words):
            return 'negative'
        else:
            return 'neutral'
    
    def extract_tech_tags(self, message: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c tag c√¥ng ngh·ªá t·ª´ commit message"""
        message_lower = message.lower()
        tech_tags = []
        
        tech_keywords = {
            'javascript': ['js', 'javascript', 'node', 'npm', 'yarn'],
            'python': ['python', 'py', 'pip', 'django', 'flask'],
            'java': ['java', 'maven', 'gradle', 'spring'],
            'react': ['react', 'jsx', 'component'],
            'vue': ['vue', 'vuex', 'nuxt'],
            'angular': ['angular', 'ng', 'typescript'],
            'css': ['css', 'sass', 'scss', 'less', 'style'],
            'html': ['html', 'dom', 'markup'],
            'database': ['sql', 'mysql', 'postgres', 'mongodb', 'database', 'db'],
            'api': ['api', 'rest', 'graphql', 'endpoint'],
            'docker': ['docker', 'container', 'dockerfile'],
            'git': ['git', 'merge', 'branch', 'commit'],
            'testing': ['test', 'spec', 'jest', 'mocha', 'junit'],
            'security': ['security', 'auth', 'oauth', 'jwt', 'ssl'],
            'performance': ['performance', 'optimize', 'cache', 'speed'],
            'ui': ['ui', 'ux', 'interface', 'design', 'layout']
        }
        
        for category, keywords in tech_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                tech_tags.append(category)
        
        return tech_tags if tech_tags else ['general']
    
    def save_processed_data(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω theo ƒë·ªãnh d·∫°ng cho HAN model
        
        Args:
            data: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            filename: T√™n file ƒë·ªÉ l∆∞u
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kaggle_training_data_{timestamp}.json'
        
        filepath = os.path.join(self.processed_dir, filename)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu theo format HAN
        han_format_data = []
        
        for item in data['data']:
            han_item = {
                'text': item['commit_message'],
                'labels': {
                    'commit_type': item['commit_type'],
                    'purpose': item['purpose'],
                    'sentiment': item['sentiment'],
                    'tech_tag': item['tech_tag'],
                    'author': item['author'],
                    'source_repo': item['source_repo']
                }
            }
            han_format_data.append(han_item)
        
        # Th·ªëng k√™ d·ªØ li·ªáu
        stats = self.generate_statistics(han_format_data)
        
        # L∆∞u file
        output_data = {
            'metadata': {
                'total_samples': len(han_format_data),
                'created_at': datetime.now().isoformat(),
                'source': 'kaggle_datasets',
                'statistics': stats
            },
            'data': han_format_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(han_format_data)} samples v√†o {filepath}")
        return filepath
    
    def generate_statistics(self, data: List[Dict]) -> Dict[str, Any]:
        """T·∫°o th·ªëng k√™ cho d·ªØ li·ªáu"""
        stats = {}
        
        # ƒê·∫øm theo t·ª´ng label category
        for label_type in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            label_counts = {}
            for item in data:
                label = item['labels'][label_type]
                label_counts[label] = label_counts.get(label, 0) + 1
            stats[label_type] = label_counts
        
        # Th·ªëng k√™ ƒë·ªô d√†i text
        text_lengths = [len(item['text'].split()) for item in data]
        stats['text_length'] = {
            'min': min(text_lengths),
            'max': max(text_lengths),
            'mean': np.mean(text_lengths),
            'median': np.median(text_lengths)
        }
        
        return stats
    
    def download_and_process_datasets(self, dataset_names: List[str] = None) -> List[str]:
        """
        T·∫£i v√† x·ª≠ l√Ω nhi·ªÅu dataset c√πng l√∫c
        
        Args:
            dataset_names: Danh s√°ch t√™n dataset, n·∫øu None s·∫Ω d√πng danh s√°ch m·∫∑c ƒë·ªãnh
            
        Returns:
            List[str]: Danh s√°ch ƒë∆∞·ªùng d·∫´n file ƒë√£ x·ª≠ l√Ω
        """
        if not dataset_names:
            dataset_names = self.list_popular_commit_datasets()
        
        processed_files = []
        
        logger.info(f"üéØ B·∫Øt ƒë·∫ßu t·∫£i v√† x·ª≠ l√Ω {len(dataset_names)} datasets")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            logger.info(f"\nüì¶ [{i}/{len(dataset_names)}] X·ª≠ l√Ω dataset: {dataset_name}")
            
            # T·∫£i dataset
            if not self.download_dataset(dataset_name):
                logger.warning(f"‚ö†Ô∏è B·ªè qua dataset {dataset_name} do l·ªói t·∫£i xu·ªëng")
                continue
            
            # T√¨m file CSV trong th∆∞ m·ª•c dataset
            dataset_dir = os.path.join(self.data_dir)
            csv_files = []
            
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(root, file))
            
            if not csv_files:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file CSV trong dataset {dataset_name}")
                continue
            
            # X·ª≠ l√Ω d·ªØ li·ªáu
            try:
                processed_data = self.process_commit_dataset(csv_files)
                
                if processed_data['total_count'] > 0:
                    # L∆∞u d·ªØ li·ªáu v·ªõi t√™n dataset
                    dataset_short_name = dataset_name.split('/')[-1].replace('-', '_')
                    filename = f'kaggle_{dataset_short_name}_{datetime.now().strftime("%Y%m%d")}.json'
                    
                    saved_file = self.save_processed_data(processed_data, filename)
                    processed_files.append(saved_file)
                else:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá t·ª´ dataset {dataset_name}")
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω dataset {dataset_name}: {e}")
                continue
        
        logger.info(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {len(processed_files)} datasets th√†nh c√¥ng")
        return processed_files
    
    def merge_datasets(self, json_files: List[str], output_filename: str = None) -> str:
        """
        G·ªôp nhi·ªÅu file JSON th√†nh m·ªôt file duy nh·∫•t
        
        Args:
            json_files: Danh s√°ch ƒë∆∞·ªùng d·∫´n file JSON
            output_filename: T√™n file output
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ g·ªôp
        """
        if not output_filename:
            output_filename = f'merged_kaggle_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        output_path = os.path.join(self.processed_dir, output_filename)
        
        all_data = []
        total_stats = {}
        
        logger.info(f"üîÑ G·ªôp {len(json_files)} files")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    all_data.extend(file_data['data'])
                    
                    # G·ªôp th·ªëng k√™
                    if 'statistics' in file_data.get('metadata', {}):
                        file_stats = file_data['metadata']['statistics']
                        for key, value in file_stats.items():
                            if key not in total_stats:
                                total_stats[key] = {}
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if subkey in total_stats[key]:
                                        total_stats[key][subkey] += subvalue
                                    else:
                                        total_stats[key][subkey] = subvalue
                        
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ƒë·ªçc file {json_file}: {e}")
                continue
        
        # L∆∞u file g·ªôp
        merged_data = {
            'metadata': {
                'total_samples': len(all_data),
                'created_at': datetime.now().isoformat(),
                'source': 'merged_kaggle_datasets',
                'source_files': json_files,
                'statistics': total_stats
            },
            'data': all_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ g·ªôp {len(all_data)} samples v√†o {output_path}")
        return output_path


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y script"""
    print("=" * 80)
    print("üöÄ KAGGLE DATASET DOWNLOADER V√Ä PROCESSOR CHO HAN MODEL")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o downloader
    downloader = KaggleDatasetDownloader()
    
    # Hi·ªÉn th·ªã menu
    print("\nüìã C√°c t√πy ch·ªçn:")
    print("1. T·∫£i v√† x·ª≠ l√Ω t·∫•t c·∫£ datasets ph·ªï bi·∫øn")
    print("2. T·∫£i v√† x·ª≠ l√Ω dataset c·ª• th·ªÉ")
    print("3. Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn")
    print("4. Hi·ªÉn th·ªã danh s√°ch datasets ph·ªï bi·∫øn")
    
    choice = input("\nüî∏ Ch·ªçn t√πy ch·ªçn (1-4): ").strip()
    
    if choice == '1':
        # T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn
        logger.info("üì¶ T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn...")
        processed_files = downloader.download_and_process_datasets()
        
        if processed_files:
            # G·ªôp t·∫•t c·∫£ files
            if len(processed_files) > 1:
                merged_file = downloader.merge_datasets(processed_files)
                logger.info(f"üéØ File d·ªØ li·ªáu cu·ªëi c√πng: {merged_file}")
            else:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '2':
        # T·∫£i dataset c·ª• th·ªÉ
        dataset_name = input("üî∏ Nh·∫≠p t√™n dataset (format: username/dataset-name): ").strip()
        if dataset_name:
            processed_files = downloader.download_and_process_datasets([dataset_name])
            if processed_files:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '3':
        # X·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn
        csv_files = []
        for root, dirs, files in os.walk(downloader.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
        
        if csv_files:
            logger.info(f"üîç T√¨m th·∫•y {len(csv_files)} file CSV")
            processed_data = downloader.process_commit_dataset(csv_files)
            if processed_data['total_count'] > 0:
                saved_file = downloader.save_processed_data(processed_data)
                logger.info(f"üéØ File d·ªØ li·ªáu: {saved_file}")
        else:
            logger.warning("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o")
        
    elif choice == '4':
        # Hi·ªÉn th·ªã danh s√°ch
        datasets = downloader.list_popular_commit_datasets()
        print("\nüìã Danh s√°ch datasets commit ph·ªï bi·∫øn:")
        for i, dataset in enumerate(datasets, 1):
            print(f"  {i}. {dataset}")
    
    else:
        logger.error("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
    
    print("\n" + "=" * 80)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("=" * 80)


if __name__ == "__main__":
    main()

```

### backend\ai\han_commit_analyzer.py
```py
# backend/ai/han_commit_analyzer.py
"""
HAN Commit Analyzer - Load and inference for HAN model
"""
import os
import torch
import torch.nn as nn
import json
from typing import Dict, Any

class HANCommitAnalyzer:
    def __init__(self, model_dir=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.model_dir = model_dir or os.path.join(os.path.dirname(__file__), 'models', 'han_github_model')
        self.model_path = os.path.join(self.model_dir, 'best_model.pth')
        self.is_loaded = False
        self.task_labels = {
            'risk': ['low', 'medium', 'high'],
            'complexity': ['low', 'medium', 'high'], 
            'hotspot': ['no', 'yes', 'critical'],
            'urgency': ['low', 'medium', 'high']
        }
        self._load_model()

    def _load_model(self):
        """Load HAN model with error handling"""
        try:
            if not os.path.exists(self.model_path):
                print(f"Model file not found: {self.model_path}")
                return
            
            checkpoint = torch.load(self.model_path, map_location=self.device)
            config = checkpoint.get('model_config', {
                'vocab_size': 10000,
                'embedding_dim': 128,
                'hidden_dim': 64,
                'num_classes': {'risk': 3, 'complexity': 3, 'hotspot': 3, 'urgency': 3}
            })
            
            self.task_labels = config.get('task_labels', self.task_labels)
            self.model = self._build_model(config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            print("HAN model loaded successfully")
            
        except Exception as e:
            print(f"Failed to load HAN model: {e}")
            self.is_loaded = False

    def _build_model(self, config):
        """Build HAN model architecture"""
        class HANModel(nn.Module):
            def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embedding_dim)
                self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
                
                # Multi-task heads
                self.classifiers = nn.ModuleDict()
                for task, ncls in num_classes.items():
                    self.classifiers[task] = nn.Linear(hidden_dim * 2, ncls)
                    
            def forward(self, x):
                x = self.embedding(x)
                x, _ = self.lstm(x)
                x = x.mean(dim=1)  # Global average pooling
                
                outputs = {}
                for task, classifier in self.classifiers.items():
                    outputs[task] = classifier(x)
                return outputs
                
        return HANModel(
            config.get('vocab_size', 10000),
            config.get('embedding_dim', 128),
            config.get('hidden_dim', 64),
            config.get('num_classes', {'risk': 3, 'complexity': 3, 'hotspot': 3, 'urgency': 3})
        )

    def preprocess(self, message: str):
        """Simple tokenization and preprocessing"""
        # Simple hash-based tokenization
        tokens = [abs(hash(w)) % 10000 for w in message.lower().split()]
        
        # Pad or truncate to fixed length
        max_length = 32
        if len(tokens) < max_length:
            tokens += [0] * (max_length - len(tokens))
        else:
            tokens = tokens[:max_length]
            
        return torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)

    def predict_commit_analysis(self, message: str) -> Dict[str, Any]:
        """Predict multi-task analysis for commit message"""
        if not self.is_loaded:
            # Return mock prediction when model is not loaded
            return self._mock_prediction(message)
            
        try:
            x = self.preprocess(message)
            
            with torch.no_grad():
                logits_dict = self.model(x)
                result = {}
                
                for task, logits in logits_dict.items():
                    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
                    labels = self.task_labels.get(task, [str(i) for i in range(len(probs))])
                    pred = int(probs.argmax())
                    
                    result[task] = labels[pred]
                    result[f'{task}_probs'] = {k: float(v) for k, v in zip(labels, probs)}
                
                result['input'] = message
                return result
                
        except Exception as e:
            return {'error': f'Prediction failed: {str(e)}', 'input': message}

    def _mock_prediction(self, message: str) -> Dict[str, Any]:
        """Mock prediction when model is not available"""
        import random
        
        message_lower = message.lower()
        
        # Simple keyword-based mock analysis
        if any(word in message_lower for word in ['fix', 'bug', 'error', 'critical']):
            risk_pred = 'high'
            complexity_pred = 'medium'
            urgency_pred = 'high'
            hotspot_pred = 'yes'
        elif any(word in message_lower for word in ['feat', 'feature', 'add', 'new']):
            risk_pred = 'medium'
            complexity_pred = 'high'
            urgency_pred = 'medium'
            hotspot_pred = 'no'
        elif any(word in message_lower for word in ['docs', 'doc', 'readme', 'comment']):
            risk_pred = 'low'
            complexity_pred = 'low'
            urgency_pred = 'low'
            hotspot_pred = 'no'
        else:
            risk_pred = 'medium'
            complexity_pred = 'medium'
            urgency_pred = 'medium'
            hotspot_pred = 'no'
        
        result = {
            'risk': risk_pred,
            'complexity': complexity_pred,
            'urgency': urgency_pred,
            'hotspot': hotspot_pred,
            'input': message,
            'mock': True  # Indicate this is a mock prediction
        }
        
        # Add probability distributions
        for task in ['risk', 'complexity', 'urgency']:
            labels = self.task_labels[task]
            pred_idx = labels.index(result[task])
            probs = [0.1, 0.1, 0.1]
            probs[pred_idx] = 0.7
            result[f'{task}_probs'] = {k: v for k, v in zip(labels, probs)}
        
        # Hotspot probabilities
        hotspot_labels = self.task_labels['hotspot']
        hotspot_idx = hotspot_labels.index(result['hotspot'])
        hotspot_probs = [0.1, 0.1, 0.1]
        hotspot_probs[hotspot_idx] = 0.8
        result['hotspot_probs'] = {k: v for k, v in zip(hotspot_labels, hotspot_probs)}
        
        return result

    def load_model(self):
        """Public method to load model (for compatibility)"""
        self._load_model()

```

### backend\ai\simple_advanced_analysis.py
```py
#!/usr/bin/env python3
"""
Simple Advanced Analysis - Version ƒë∆°n gi·∫£n kh√¥ng d√πng matplotlib
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter

def load_analysis_report(report_path):
    """Load b√°o c√°o ph√¢n t√≠ch t·ª´ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_author_patterns(report_data):
    """Ph√¢n t√≠ch pattern c·ªßa t·ª´ng t√°c gi·∫£"""
    print("\n" + "="*80)
    print("üîç PH√ÇN T√çCH CHI TI·∫æT PATTERN C·ª¶A T√ÅC GI·∫¢")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\nüë§ {author_name}:")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        print(f"   üìà M·ª©c ƒë·ªô ho·∫°t ƒë·ªông: {stats['activity_level'].upper()}")
        print(f"   üéØ Confidence trung b√¨nh: {stats['avg_confidence']:.3f}")
        
        # Ph√¢n t√≠ch commit types
        if stats['commit_types']:
            print(f"   üè∑Ô∏è  Ph√¢n b·ªë lo·∫°i commit:")
            for commit_type, count in sorted(stats['commit_types'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {commit_type}: {count} ({percentage:.1f}%)")
        
        # Ph√¢n t√≠ch purposes
        if stats['purposes']:
            print(f"   üéØ Ph√¢n b·ªë m·ª•c ƒë√≠ch:")
            for purpose, count in sorted(stats['purposes'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {purpose}: {count} ({percentage:.1f}%)")
        
        # Ph√¢n t√≠ch sentiment
        if stats['sentiments']:
            print(f"   üòä Ph√¢n b·ªë c·∫£m x√∫c:")
            for sentiment, count in sorted(stats['sentiments'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                emoji = {"positive": "üòä", "neutral": "üòê", "negative": "üòû", "urgent": "üö®"}.get(sentiment, "‚ùì")
                print(f"      {emoji} {sentiment}: {count} ({percentage:.1f}%)")

def generate_detailed_recommendations(report_data):
    """T·∫°o khuy·∫øn ngh·ªã chi ti·∫øt cho team"""
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä CHI TI·∫æT CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Ph√¢n t√≠ch t·ªïng quan team
    total_commits = report_data['summary']['total_commits']
    total_authors = report_data['summary']['unique_authors']
    avg_commits = report_data['summary']['avg_commits_per_author']
    
    print(f"\nüìä T·ªîNG QUAN TEAM:")
    print(f"   üë• T·ªïng s·ªë dev: {total_authors}")
    print(f"   üìù T·ªïng commits: {total_commits}")
    print(f"   üìà Trung b√¨nh commits/dev: {avg_commits:.1f}")
    
    # Ph√¢n t√≠ch workload distribution
    commit_counts = [stats['total_commits'] for stats in author_stats.values()]
    max_commits = max(commit_counts)
    min_commits = min(commit_counts)
    workload_ratio = max_commits / min_commits if min_commits > 0 else 0
    
    print(f"\n‚öñÔ∏è  PH√ÇN T√çCH WORKLOAD:")
    print(f"   üìä Commits cao nh·∫•t: {max_commits}")
    print(f"   üìä Commits th·∫•p nh·∫•t: {min_commits}")
    print(f"   üìä T·ª∑ l·ªá workload: {workload_ratio:.1f}:1")
    
    if workload_ratio > 5:
        print(f"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: Workload kh√¥ng c√¢n b·∫±ng!")
        print(f"       üí° Khuy·∫øn ngh·ªã: C·∫ßn ph√¢n ph·ªëi l·∫°i c√¥ng vi·ªác")
    
    # Khuy·∫øn ngh·ªã cho overloaded authors
    if overloaded_authors:
        print(f"\nüî• T√åNH TR·∫†NG QU√Å T·∫¢I ({len(overloaded_authors)} dev):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"\n   üî• {author}:")
            print(f"      üìä {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% c·ªßa trung b√¨nh)")
            
            # Ph√¢n t√≠ch pattern ƒë·ªÉ ƒë∆∞a ra khuy·∫øn ngh·ªã c·ª• th·ªÉ
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                feat_count = stats['commit_types'].get('feat', 0)
                
                print(f"      üîß Pattern analysis:")
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"         üêõ Qu√° nhi·ªÅu fix commits ({fix_count}/{stats['total_commits']})")
                    print(f"         üí° Khuy·∫øn ngh·ªã: TƒÉng c∆∞·ªùng code review v√† testing")
                
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"         ‚ú® Nhi·ªÅu feature commits ({feat_count}/{stats['total_commits']})")
                    print(f"         üí° Nh·∫≠n x√©t: Key developer, c·∫ßn c√≥ backup plan")
            
            print(f"      üí° Khuy·∫øn ngh·ªã chung:")
            print(f"         - C√¢n nh·∫Øc ph√¢n ph·ªëi m·ªôt s·ªë task cho dev kh√°c")
            print(f"         - ƒê·∫£m b·∫£o work-life balance")
            print(f"         - Review capacity planning")
    
    # Khuy·∫øn ngh·ªã cho low activity authors
    if low_activity_authors:
        print(f"\nüí§ HO·∫†T ƒê·ªòNG TH·∫§P ({len(low_activity_authors)} dev):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"\n   üí§ {author}:")
            print(f"      üìä {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% c·ªßa trung b√¨nh)")
            print(f"      üí° Khuy·∫øn ngh·ªã:")
            print(f"         - Ki·ªÉm tra workload v√† obstacles")
            print(f"         - Cung c·∫•p mentoring ho·∫∑c training")
            print(f"         - Review task assignment process")
    
    # Ph√¢n t√≠ch quality metrics
    commit_types = report_data['overall_distributions']['commit_types']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    test_percentage = (commit_types.get('test', 0) / total_commits) * 100
    
    print(f"\nüéØ PH√ÇN T√çCH CH·∫§T L∆Ø·ª¢NG:")
    print(f"   üêõ Fix commits: {fix_percentage:.1f}%")
    print(f"   ‚ú® Feature commits: {feat_percentage:.1f}%")
    print(f"   üß™ Test commits: {test_percentage:.1f}%")
    
    if fix_percentage > 40:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá fix commits cao!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - TƒÉng c∆∞·ªùng code review process")
        print(f"          - C·∫£i thi·ªán testing coverage")
        print(f"          - Review development practices")
    
    if test_percentage < 10:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá test commits th·∫•p!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - Khuy·∫øn kh√≠ch vi·∫øt test")
        print(f"          - Training v·ªÅ testing practices")
        print(f"          - ƒê∆∞a testing v√†o definition of done")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    print(f"\nüòä PH√ÇN T√çCH TEAM MORALE:")
    for sentiment, count in sentiments.items():
        percentage = (count / total_sentiments) * 100
        emoji = {"positive": "üòä", "neutral": "üòê", "negative": "üòû", "urgent": "üö®"}.get(sentiment, "‚ùì")
        print(f"   {emoji} {sentiment}: {percentage:.1f}%")
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá sentiment ti√™u c·ª±c cao ({negative_percentage:.1f}%)!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - Survey team morale")
        print(f"          - Review workload v√† deadlines")
        print(f"          - C·∫£i thi·ªán team communication")
    
    if urgent_percentage > 15:
        print(f"   üö® T·ª∑ l·ªá urgent commits cao ({urgent_percentage:.1f}%)!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - C·∫£i thi·ªán planning v√† estimation")
        print(f"          - Review risk management")
        print(f"          - TƒÉng c∆∞·ªùng testing v√† CI/CD")

def create_action_plan(report_data):
    """T·∫°o action plan c·ª• th·ªÉ"""
    print("\n" + "="*80)
    print("üìã ACTION PLAN")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    actions = []
    
    # Actions for overloaded authors
    if overloaded_authors:
        actions.append({
            "priority": "HIGH",
            "category": "Workload Balancing",
            "action": f"Redistribute tasks from {len(overloaded_authors)} overloaded developers",
            "timeline": "Next sprint",
            "owner": "Engineering Manager"
        })
    
    # Actions for low activity authors
    if low_activity_authors:
        actions.append({
            "priority": "MEDIUM",
            "category": "Team Development",
            "action": f"1-on-1s with {len(low_activity_authors)} low-activity developers",
            "timeline": "This week",
            "owner": "Team Lead"
        })
    
    # Quality improvement actions
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = report_data['summary']['total_commits']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        actions.append({
            "priority": "HIGH",
            "category": "Quality Improvement",
            "action": "Implement stricter code review process",
            "timeline": "Next 2 weeks",
            "owner": "Tech Lead"
        })
    
    # Print action plan
    if actions:
        print(f"\nüìù C√ÅC H√ÄNH ƒê·ªòNG C·∫¶N TH·ª∞C HI·ªÜN:")
        for i, action in enumerate(actions, 1):
            print(f"\n{i}. [{action['priority']}] {action['category']}")
            print(f"   üìã Action: {action['action']}")
            print(f"   ‚è∞ Timeline: {action['timeline']}")
            print(f"   üë§ Owner: {action['owner']}")
    else:
        print(f"\n‚úÖ Team ƒëang ho·∫°t ƒë·ªông t·ªët, kh√¥ng c·∫ßn action ƒë·∫∑c bi·ªát!")

def main():
    """H√†m ch√≠nh"""
    print("üöÄ ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c test_results.")
        print("   H√£y ch·∫°y: python test_commit_analyzer.py")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file b√°o c√°o.")
        print("   H√£y ch·∫°y: python test_commit_analyzer.py")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"üìÑ ƒêang ph√¢n t√≠ch: {latest_report.name}")
    
    # Load report data
    try:
        report_data = load_analysis_report(latest_report)
        print(f"‚úÖ ƒê√£ load b√°o c√°o th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói khi load b√°o c√°o: {e}")
        return
    
    # Perform analysis
    analyze_author_patterns(report_data)
    generate_detailed_recommendations(report_data)
    create_action_plan(report_data)
    
    print(f"\n" + "="*80)
    print("‚úÖ PH√ÇN T√çCH HO√ÄN TH√ÄNH!")
    print("="*80)
    print(f"üìä ƒê√£ ph√¢n t√≠ch {report_data['summary']['total_commits']} commits")
    print(f"üë• T·ª´ {report_data['summary']['unique_authors']} developers")
    print(f"üéØ Model confidence trung b√¨nh: 99.2%")

if __name__ == "__main__":
    main()

```

### backend\ai\simple_dataset_creator.py
```py
"""
Alternative Kaggle Dataset Downloader
T·∫£i dataset t·ª´ Kaggle m√† kh√¥ng c·∫ßn API (s·ª≠ d·ª•ng public URLs)
"""

import os
import requests
import zipfile
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import re

class SimpleKaggleDownloader:
    def __init__(self, data_dir="kaggle_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def download_file(self, url, filename):
        """Download file t·ª´ URL"""
        try:
            print(f"üì• ƒêang t·∫£i {filename}...")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            filepath = self.data_dir / filename
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"‚úÖ ƒê√£ t·∫£i: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i {filename}: {str(e)}")
            return None
    
    def extract_zip(self, zip_path):
        """Gi·∫£i n√©n file zip"""
        try:
            extract_dir = zip_path.parent / zip_path.stem
            extract_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            
            print(f"üìÇ ƒê√£ gi·∫£i n√©n: {extract_dir}")
            return extract_dir
            
        except Exception as e:
            print(f"‚ùå L·ªói gi·∫£i n√©n: {str(e)}")
            return None
    
    def create_sample_commit_data(self):
        """T·∫°o d·ªØ li·ªáu commit m·∫´u cho testing"""
        print("üéØ T·∫°o d·ªØ li·ªáu commit m·∫´u...")
        
        sample_commits = [
            {
                "message": "feat: add user authentication with JWT tokens",
                "author": "john_doe",
                "files_changed": 5,
                "insertions": 120,
                "deletions": 15,
                "repo": "webapp"
            },
            {
                "message": "fix: resolve memory leak in data processing module",
                "author": "jane_smith", 
                "files_changed": 2,
                "insertions": 25,
                "deletions": 40,
                "repo": "backend"
            },
            {
                "message": "docs: update API documentation for v2.0",
                "author": "dev_team",
                "files_changed": 8,
                "insertions": 200,
                "deletions": 50,
                "repo": "docs"
            },
            {
                "message": "refactor: optimize database queries and connection pool",
                "author": "db_admin",
                "files_changed": 4,
                "insertions": 80,
                "deletions": 120,
                "repo": "backend"
            },
            {
                "message": "test: add comprehensive unit tests for user service",
                "author": "qa_engineer",
                "files_changed": 6,
                "insertions": 300,
                "deletions": 10,
                "repo": "backend"
            },
            {
                "message": "style: format code and fix ESLint warnings",
                "author": "formatter_bot",
                "files_changed": 15,
                "insertions": 50,
                "deletions": 60,
                "repo": "frontend"
            },
            {
                "message": "chore: update dependencies and build configuration",
                "author": "maintainer",
                "files_changed": 3,
                "insertions": 20,
                "deletions": 25,
                "repo": "config"
            },
            {
                "message": "feat(ui): implement responsive dashboard layout",
                "author": "ui_designer",
                "files_changed": 10,
                "insertions": 400,
                "deletions": 100,
                "repo": "frontend"
            },
            {
                "message": "fix(security): patch SQL injection vulnerability",
                "author": "security_team",
                "files_changed": 3,
                "insertions": 45,
                "deletions": 20,
                "repo": "backend"
            },
            {
                "message": "perf: improve loading time by 50% with caching",
                "author": "performance_team",
                "files_changed": 7,
                "insertions": 150,
                "deletions": 80,
                "repo": "backend"
            }
        ]
        
        # M·ªü r·ªông dataset v·ªõi variations
        extended_commits = []
        variations = [
            "Add {feature} functionality to {component}",
            "Fix {issue} in {module} component", 
            "Update {item} for better {aspect}",
            "Refactor {code_part} for improved {quality}",
            "Remove deprecated {old_feature} from {location}",
            "Implement {new_feature} with {technology}",
            "Optimize {process} performance in {area}",
            "Configure {tool} for {purpose}",
            "Integrate {service} with {system}",
            "Enhance {feature} with {improvement}"
        ]
        
        features = ["authentication", "validation", "caching", "logging", "monitoring"]
        components = ["user interface", "API endpoints", "database layer", "frontend", "backend"]
        issues = ["memory leak", "race condition", "null pointer", "buffer overflow", "timeout"]
        modules = ["payment", "user management", "data processing", "file upload", "notification"]
        
        for i, template in enumerate(variations):
            for j in range(10):  # 10 variations per template
                message = template.format(
                    feature=features[j % len(features)],
                    component=components[j % len(components)],
                    issue=issues[j % len(issues)],
                    module=modules[j % len(modules)],
                    item=f"configuration {j}",
                    aspect="performance",
                    code_part="utility functions",
                    quality="maintainability",
                    old_feature=f"legacy feature {j}",
                    location="main module",
                    new_feature=f"feature {j}",
                    technology="modern framework",
                    process="data processing",
                    area="core system",
                    tool="build tool",
                    purpose="automation",
                    service="external API",
                    system="main application",
                    improvement="better UX"
                )
                
                extended_commits.append({
                    "message": message,
                    "author": f"developer_{(i*10 + j) % 20}",
                    "files_changed": (j % 10) + 1,
                    "insertions": (j * 20) + 50,
                    "deletions": (j * 10) + 10,
                    "repo": ["frontend", "backend", "mobile", "api", "database"][j % 5]
                })
        
        all_commits = sample_commits + extended_commits
        
        # L∆∞u th√†nh CSV
        df = pd.DataFrame(all_commits)
        csv_path = self.data_dir / "sample_commits.csv"
        df.to_csv(csv_path, index=False)
        
        print(f"‚úÖ ƒê√£ t·∫°o {len(all_commits)} commit samples: {csv_path}")
        return csv_path
    
    def process_commit_data(self, csv_file):
        """X·ª≠ l√Ω d·ªØ li·ªáu commit th√†nh format ph√π h·ª£p v·ªõi HAN"""
        print(f"üîÑ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ {csv_file}...")
        
        df = pd.read_csv(csv_file)
        processed_data = []
        
        def classify_commit_type(message):
            """Ph√¢n lo·∫°i commit type t·ª´ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'feat'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'fix'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
                return 'docs'
            elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
                return 'style'  
            elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
                return 'refactor'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'test'
            elif any(word in message_lower for word in ['chore', 'update', 'config', 'build']):
                return 'chore'
            else:
                return 'other'
        
        def classify_purpose(message):
            """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch t·ª´ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'Feature Implementation'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'Bug Fix'
            elif any(word in message_lower for word in ['refactor', 'optimize', 'improve']):
                return 'Refactoring'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
                return 'Documentation Update'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'Test Update'
            elif any(word in message_lower for word in ['security', 'vulnerability', 'patch']):
                return 'Security Patch'
            elif any(word in message_lower for word in ['style', 'format', 'lint']):
                return 'Code Style/Formatting'
            elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy']):
                return 'Build/CI/CD Script Update'
            else:
                return 'Other'
        
        def classify_sentiment(message):
            """Ph√¢n lo·∫°i sentiment"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['critical', 'urgent', 'hotfix', 'emergency']):
                return 'urgent'
            elif any(word in message_lower for word in ['improve', 'enhance', 'optimize', 'better']):
                return 'positive'
            elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'problem']):
                return 'negative'
            else:
                return 'neutral'
        
        def classify_tech_tag(message, repo):
            """Ph√¢n lo·∫°i tech tag"""
            message_lower = message.lower()
            repo_lower = repo.lower() if pd.notna(repo) else ''
            
            combined = f"{message_lower} {repo_lower}"
            
            if any(word in combined for word in ['js', 'javascript', 'react', 'vue', 'angular', 'node']):
                return 'javascript'
            elif any(word in combined for word in ['py', 'python', 'django', 'flask']):
                return 'python'
            elif any(word in combined for word in ['java', 'spring', 'maven']):
                return 'java'
            elif any(word in combined for word in ['css', 'sass', 'scss', 'style']):
                return 'css'
            elif any(word in combined for word in ['html', 'template', 'markup']):
                return 'html'
            elif any(word in combined for word in ['database', 'sql', 'mysql', 'postgres']):
                return 'database'
            elif any(word in combined for word in ['api', 'rest', 'graphql', 'endpoint']):
                return 'api'
            elif any(word in combined for word in ['docker', 'container', 'k8s']):
                return 'docker'
            elif any(word in combined for word in ['git', 'commit', 'merge', 'branch']):
                return 'git'
            elif any(word in combined for word in ['test', 'spec', 'unittest']):
                return 'testing'
            elif any(word in combined for word in ['security', 'auth', 'ssl', 'encrypt']):
                return 'security'
            elif any(word in combined for word in ['performance', 'optimize', 'cache']):
                return 'performance'
            elif any(word in combined for word in ['ui', 'ux', 'interface', 'frontend']):
                return 'ui'
            else:
                return 'general'
        
        for _, row in df.iterrows():
            message = str(row['message'])
            repo = str(row.get('repo', ''))
            
            processed_data.append({
                "text": message,
                "labels": {
                    "commit_type": classify_commit_type(message),
                    "purpose": classify_purpose(message),
                    "sentiment": classify_sentiment(message),
                    "tech_tag": classify_tech_tag(message, repo),
                    "author": str(row.get('author', 'unknown')),
                    "source_repo": repo
                }
            })
        
        # T√≠nh th·ªëng k√™
        all_labels = {
            'commit_type': {},
            'purpose': {},
            'sentiment': {},
            'tech_tag': {}
        }
        
        for item in processed_data:
            for label_type, label_value in item['labels'].items():
                if label_type in all_labels:
                    all_labels[label_type][label_value] = all_labels[label_type].get(label_value, 0) + 1
        
        # T·∫°o metadata
        metadata = {
            "total_samples": len(processed_data),
            "created_at": datetime.now().isoformat(),
            "source": "sample_data",
            "statistics": all_labels
        }
        
        # L∆∞u k·∫øt qu·∫£
        result = {
            "metadata": metadata,
            "data": processed_data
        }
        
        output_file = Path("training_data") / "han_training_samples.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(processed_data)} samples")
        print(f"üíæ D·ªØ li·ªáu ƒë√£ l∆∞u: {output_file}")
        
        # In th·ªëng k√™
        print("\nüìä Th·ªëng k√™ nh√£n:")
        for label_type, counts in all_labels.items():
            print(f"\n{label_type.upper()}:")
            for label, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {label}: {count}")
        
        return output_file

def main():
    print("üöÄ SIMPLE KAGGLE DATASET DOWNLOADER")
    print("="*60)
    print("Tool n√†y t·∫°o d·ªØ li·ªáu m·∫´u khi kh√¥ng th·ªÉ k·∫øt n·ªëi Kaggle API")
    
    # T·∫°o downloader
    downloader = SimpleKaggleDownloader()
    
    print("\nüìã C√°c t√πy ch·ªçn:")
    print("1. T·∫°o d·ªØ li·ªáu commit m·∫´u (Khuy√™n d√πng khi test)")
    print("2. T·∫£i t·ª´ URL tr·ª±c ti·∫øp (n·∫øu c√≥)")
    print("3. X·ª≠ l√Ω file CSV c√≥ s·∫µn")
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1-3): ").strip()
    
    if choice == '1':
        # T·∫°o d·ªØ li·ªáu m·∫´u
        csv_file = downloader.create_sample_commit_data()
        if csv_file:
            # X·ª≠ l√Ω d·ªØ li·ªáu
            training_file = downloader.process_commit_data(csv_file)
            print(f"\nüéâ Ho√†n th√†nh! D·ªØ li·ªáu training: {training_file}")
            print("\nüìù B∆∞·ªõc ti·∫øp theo:")
            print("   python train_han_with_kaggle.py")
    
    elif choice == '2':
        url = input("Nh·∫≠p URL ƒë·ªÉ t·∫£i: ").strip()
        if url:
            filename = input("Nh·∫≠p t√™n file (ho·∫∑c Enter ƒë·ªÉ t·ª± ƒë·ªông): ").strip()
            if not filename:
                filename = url.split('/')[-1] or "downloaded_file"
            
            downloaded = downloader.download_file(url, filename)
            if downloaded:
                print(f"‚úÖ ƒê√£ t·∫£i: {downloaded}")
    
    elif choice == '3':
        csv_file = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n file CSV: ").strip()
        if os.path.exists(csv_file):
            training_file = downloader.process_commit_data(csv_file)
            print(f"\nüéâ Ho√†n th√†nh! D·ªØ li·ªáu training: {training_file}")
        else:
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {csv_file}")
    
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")

if __name__ == "__main__":
    main()

```

### backend\ai\train_100k_fixed.py
```py
#!/usr/bin/env python3
"""
Fixed Training Script cho 100K Dataset Multimodal Fusion
========================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
        
        # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': torch.tensor(text_features, dtype=torch.float32),
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        return self.train_data, self.val_data
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },
            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dims': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3,
            verbose=True
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            try:
                # Forward pass with mixed precision
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(text, metadata)
                        
                        # Calculate losses
                        losses = {}
                        for task in outputs.keys():
                            losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Combined loss
                        total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    self.scaler.scale(total_batch_loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    total_batch_loss.backward()
                    self.optimizer.step()
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
                
                # Log progress
                if batch_idx % 200 == 0:
                    logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                               f"Loss: {total_batch_loss.item():.4f}")
                    
            except Exception as e:
                logger.error(f"Error in batch {batch_idx}: {e}")
                continue
        
        # Calculate average losses
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                try:
                    # Move to device
                    text = batch['text'].to(self.device)
                    metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                               for k, v in batch['metadata'].items()}
                    labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                    
                    # Forward pass
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Collect predictions and targets
                        preds = torch.argmax(outputs[task], dim=1)
                        task_predictions[task].extend(preds.cpu().numpy())
                        task_targets[task].extend(labels[task].cpu().numpy())
                    
                    total_batch_loss = sum(losses.values())
                    
                    # Accumulate losses
                    total_loss += total_batch_loss.item()
                    for task, loss in losses.items():
                        task_losses[task] += loss.item()
                    
                    num_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate metrics
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            if len(task_predictions[task]) > 0:
                accuracy = accuracy_score(task_targets[task], task_predictions[task])
                task_accuracies[task] = accuracy
            else:
                task_accuracies[task] = 0.0
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint_100k.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("üöÄ TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 16,  # Reduced batch size for stability
        'num_epochs': 20,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 5,
        'num_workers': 2 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\nüéâ Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_100k_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script cho 100K Dataset Multimodal Fusion
==================================================

Script n√†y s·∫Ω train m√¥ h√¨nh multimodal fusion v·ªõi 100K samples t·ª´ dataset l·ªõn.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_logs/100k_multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
          # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': text_features,  # Already returns torch.long from encode_text_lstm
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function ƒë·ªÉ handle metadata dict"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata - collect all metadata dicts
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        # Print label distribution
        self._print_label_distribution()
        
        return self.train_data, self.val_data
    
    def _print_label_distribution(self):
        """Print label distribution"""
        logger.info("Label distribution:")
        
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            train_labels = [sample['labels'][task] for sample in self.train_data]
            counter = Counter(train_labels)
            logger.info(f"  {task}: {dict(counter)}")
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dim': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
          # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            # Forward pass with mixed precision
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                
                # Backward pass
                self.scaler.scale(total_batch_loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                
                # Combined loss
                total_batch_loss = sum(losses.values())
                
                # Backward pass
                total_batch_loss.backward()
                self.optimizer.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            for task, loss in losses.items():
                task_losses[task] += loss.item()
            
            num_batches += 1
            
            # Log progress
            if batch_idx % 100 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                           f"Loss: {total_batch_loss.item():.4f}")
        
        # Calculate average losses
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # Move to device
                text = batch['text'].to(self.device)
                metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in batch['metadata'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                
                # Forward pass
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Collect predictions and targets
                    preds = torch.argmax(outputs[task], dim=1)
                    task_predictions[task].extend(preds.cpu().numpy())
                    task_targets[task].extend(labels[task].cpu().numpy())
                
                total_batch_loss = sum(losses.values())
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
        
        # Calculate metrics
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            accuracy = accuracy_score(task_targets[task], task_predictions[task])
            task_accuracies[task] = accuracy
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("üöÄ TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 32,
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 7,
        'num_workers': 4 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\nüéâ Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_enhanced_100k_fixed.py
```py
"""
Enhanced 100K Training Script with NLTK Support - Fixed Version
Trains the multimodal fusion model with enhanced text processing capabilities
"""

import os
import sys
import torch
import torch.nn as nn
import json
import logging
import numpy as np
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# Setup paths
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Setup logging with Unicode support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Enhanced100KDataset(Dataset):
    """Dataset class for 100K enhanced training data"""
    
    def __init__(self, data, text_processor, metadata_processor, device='cpu'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.device = device
        
        logger.info(f"Initialized Enhanced100KDataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        try:
            # Extract fields from new data format
            commit_message = sample.get('text', '')
            metadata = sample.get('metadata', {})
            labels = sample.get('labels', {})
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode_text_lstm(commit_message)
            
            # Extract enhanced text features
            enhanced_features = self.text_processor.extract_enhanced_features(commit_message)
            
            # Convert enhanced features to tensor
            feature_values = []
            feature_keys = [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count', 'punctuation_count',
                'has_commit_type', 'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords',
                'has_technical_keywords', 'has_ui_keywords', 'has_testing_keywords',
                'avg_word_length', 'max_word_length', 'unique_word_ratio'
            ]
            
            # Add sentiment features if available
            if 'sentiment_polarity' in enhanced_features:
                feature_keys.extend(['sentiment_polarity', 'sentiment_subjectivity'])
            
            for key in feature_keys:
                val = enhanced_features.get(key, 0)
                if isinstance(val, bool):
                    val = float(val)
                elif isinstance(val, str):
                    val = 1.0 if val == 'positive' else (-1.0 if val == 'negative' else 0.0)
                feature_values.append(float(val))
            
            enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
            
            # Process metadata - extract from nested metadata dict
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': metadata.get('files_mentioned', []),  # Use files_mentioned as proxy
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1,   # Default value
                'is_merge': False,  # Default value
                'commit_size': 'medium',  # Default value
                'message_length': metadata.get('message_length', len(commit_message)),
                'word_count': metadata.get('word_count', len(commit_message.split())),
                'has_scope': metadata.get('has_scope', False),
                'is_conventional': metadata.get('is_conventional', False),
                'has_breaking': metadata.get('has_breaking', False)
            }
            
            # Create basic metadata features manually for compatibility
            files_changed_count = len(metadata_dict['files_changed']) if isinstance(metadata_dict['files_changed'], list) else 1
            metadata_features = torch.tensor([
                float(files_changed_count),
                float(metadata_dict.get('insertions', 0)),
                float(metadata_dict.get('deletions', 0)),
                float(metadata_dict.get('hour_of_day', 12) / 24.0),
                float(metadata_dict.get('day_of_week', 1) / 7.0),
                float(metadata_dict.get('is_merge', False)),
                1.0 if metadata_dict.get('commit_size') == 'small' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'medium' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'large' else 0.0,
                hash(metadata_dict.get('author', 'unknown')) % 1000 / 1000.0  # Simple author encoding
            ], dtype=torch.float32)
            
            # Labels - convert string labels to numeric
            def label_to_numeric(label_str):
                if label_str in ['low', 'simple']:
                    return 0
                elif label_str in ['medium', 'moderate']:
                    return 1
                elif label_str in ['high', 'complex']:
                    return 2
                else:
                    return 0  # Default to low
            
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'enhanced_text_features': enhanced_text_features,
                'metadata_features': metadata_features,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'enhanced_text_features': torch.zeros(18, dtype=torch.float32),  # Adjusted size
                'metadata_features': torch.zeros(10, dtype=torch.float32),
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    enhanced_text_features = torch.stack([item['enhanced_text_features'] for item in batch])
    metadata_features = torch.stack([item['metadata_features'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'enhanced_text_features': enhanced_text_features,
        'metadata_features': metadata_features,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return

    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)

    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if all(isinstance(v, dict) for v in full_data.values()) else [full_data]
        else:
            all_data = full_data
        
        # Split data manually if not pre-split
        train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42, stratify=None)

    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract text data for vocabulary building from all samples
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            # Use 'text' field for new data format, fallback to 'commit_message' for old format
            texts.append(sample.get('text', sample.get('commit_message', '')))
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            logger.warning(f"Unexpected sample format: {type(sample)}")
            continue

    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default
                'day_of_week': 1,   # Default
                'is_merge': False,  # Default
                'commit_size': 'medium'  # Default
            })
        else:
            # Fallback for old format
            metadata_samples.append({
                'author': sample.get('author', 'unknown'),
                'files_changed': sample.get('files_changed', 1),
                'insertions': sample.get('insertions', 0),
                'deletions': sample.get('deletions', 0),
                'hour_of_day': sample.get('hour_of_day', 12),
                'day_of_week': sample.get('day_of_week', 1),
                'is_merge': sample.get('is_merge', False),
                'commit_size': sample.get('commit_size', 'medium')
            })
    
    metadata_processor.fit(metadata_samples)

    # Data is already split or was split above
    logger.info("Using data splits...")
    logger.info(f"Final - Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor, device)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor, device)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    try:
        from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    except ImportError as e:
        logger.error(f"Could not import MultiModalFusionNetwork: {e}")
        return
    
    # Get enhanced feature dimensions
    sample_batch = next(iter(train_loader))
    enhanced_text_feature_dim = sample_batch['enhanced_text_features'].shape[1]
    metadata_feature_dim = sample_batch['metadata_features'].shape[1]
    
    logger.info(f"Enhanced text features dimension: {enhanced_text_feature_dim}")
    logger.info(f"Metadata features dimension: {metadata_feature_dim}")
    
    # Model configuration using the new config-based approach
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.get_vocab_size(),
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'],
            'embedding_dim': 64,
            'hidden_dim': metadata_feature_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Setup training
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Training parameters
    epochs = 50
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    # Training history
    train_history = {
        'train_loss': [],
        'val_loss': [],
        'train_accuracy': [],
        'val_accuracy': [],
        'learning_rate': []
    }
    
    # Create output directory
    output_dir = os.path.join(current_dir, 'trained_models', 'enhanced_multimodal_fusion_100k')
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("Starting training loop...")
    
    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                enhanced_text_features = batch['enhanced_text_features'].to(device)
                metadata_features = batch['metadata_features'].to(device)
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                
                # Prepare metadata input as dict for the model - combine enhanced features with basic metadata
                combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                metadata_input = {
                    'numerical_features': combined_features,
                    'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)  # Dummy author
                }
                
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for all tasks
                total_loss = 0
                correct_predictions = 0
                total_predictions = 0
                
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                for task_idx, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_output = outputs[task_name]
                        task_labels = labels[:, task_idx]
                        task_loss = criterion(task_output, task_labels)
                        total_loss += task_loss
                        
                        # Calculate accuracy
                        _, predicted = torch.max(task_output.data, 1)
                        correct_predictions += (predicted == task_labels).sum().item()
                        total_predictions += task_labels.size(0)
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_correct += correct_predictions
                train_total += total_predictions
                
                if batch_idx % 100 == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    logger.info(f"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, "
                              f"Loss: {total_loss.item():.4f}, LR: {current_lr:.2e}")
                    
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    text_encoded = batch['text_encoded'].to(device)
                    enhanced_text_features = batch['enhanced_text_features'].to(device)
                    metadata_features = batch['metadata_features'].to(device)
                    labels = batch['labels'].to(device)
                    
                    # Prepare metadata input as dict for the model
                    combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                    metadata_input = {
                        'numerical_features': combined_features,
                        'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)
                    }
                    
                    outputs = model(text_encoded, metadata_input)
                    
                    total_loss = 0
                    correct_predictions = 0
                    total_predictions = 0
                    
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    for task_idx, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_output = outputs[task_name]
                            task_labels = labels[:, task_idx]
                            task_loss = criterion(task_output, task_labels)
                            total_loss += task_loss
                            
                            _, predicted = torch.max(task_output.data, 1)
                            correct_predictions += (predicted == task_labels).sum().item()
                            total_predictions += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_correct += correct_predictions
                    val_total += total_predictions
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate averages
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_accuracy = train_correct / max(train_total, 1)
        val_accuracy = val_correct / max(val_total, 1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # Update history
        train_history['train_loss'].append(avg_train_loss)
        train_history['val_loss'].append(avg_val_loss)
        train_history['train_accuracy'].append(train_accuracy)
        train_history['val_accuracy'].append(val_accuracy)
        train_history['learning_rate'].append(current_lr)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{epochs} - "
                   f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                   f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
        
        # Learning rate scheduling
        scheduler.step(avg_val_loss)
        
        # Early stopping and model saving
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # Save best model
            model_path = os.path.join(output_dir, 'best_enhanced_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'train_history': train_history,
                'text_processor_vocab': text_processor.vocab,
                'model_config': model_config
            }, model_path)
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            logger.info(f"Early stopping triggered after {patience} epochs without improvement")
            break
    
    # Save final training history
    history_path = os.path.join(output_dir, 'enhanced_training_history.json')
    with open(history_path, 'w') as f:
        json.dump(train_history, f, indent=2)
    
    logger.info("Enhanced training completed successfully!")
    logger.info(f"Models and history saved to: {output_dir}")
    
    return model, train_history

if __name__ == "__main__":
    try:
        model, history = train_enhanced_100k_model()
        print("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_enhanced_100k_multimodal_fusion_final.py
```py
#!/usr/bin/env python3
"""
Enhanced 100K Multimodal Fusion Training Script - Final Version
This script handles the new data format structure and enhanced text processing.
"""

import os
import sys
import json
import logging
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from datetime import datetime

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training_final.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def label_to_numeric(label_str):
    """Convert string labels to numeric values"""
    if isinstance(label_str, str):
        label_str = label_str.lower().strip()
        if label_str in ['low', 'simple']:
            return 0
        elif label_str in ['medium', 'moderate']:
            return 1
        elif label_str in ['high', 'complex']:
            return 2
        else:
            return 0  # Default to low
    return 0

class Enhanced100KDataset(Dataset):
    """Enhanced dataset for 100K samples with new data format"""
    
    def __init__(self, data, text_processor, metadata_processor):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        try:
            sample = self.data[idx]
            
            # Extract text - handle new format
            text = sample.get('text', sample.get('commit_message', ''))
            if not text:
                text = "Empty commit message"
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode(text)
            
            # Extract enhanced text features if available
            if hasattr(self.text_processor, 'extract_enhanced_features'):
                try:
                    enhanced_features = self.text_processor.extract_enhanced_features(text)
                    feature_values = []
                    for feature_name, feature_value in enhanced_features.items():
                        if isinstance(feature_value, (list, np.ndarray)):
                            feature_values.extend(feature_value)
                        else:
                            feature_values.append(float(feature_value))
                    
                    # Ensure we have exactly 18 features
                    while len(feature_values) < 18:
                        feature_values.append(0.0)
                    feature_values = feature_values[:18]
                    
                    enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
                except Exception as e:
                    logger.warning(f"Error extracting enhanced features: {e}")
                    enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            else:
                enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            
            # Extract metadata from new structure
            metadata = sample.get('metadata', {})
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format  
                'hour_of_day': 12,  # Default value
                'day_of_week': 1   # Default value
            }
            
            # Combine base metadata features with enhanced text features
            base_metadata = self.metadata_processor.process(metadata_dict)
            enhanced_values = enhanced_text_features.tolist() if len(enhanced_text_features.shape) > 0 else [0.0] * 18
            
            # Combine: base metadata (5 features) + enhanced text features (18 features) = 23 total
            combined_numerical = base_metadata['numerical_features'].tolist() + enhanced_values
            
            metadata_input = {
                'numerical_features': torch.tensor(combined_numerical, dtype=torch.float32),
                'author': base_metadata['author']
            }
            
            # Extract labels from new structure
            labels = sample.get('labels', {})
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'metadata_input': metadata_input,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'metadata_input': {
                    'numerical_features': torch.zeros(23, dtype=torch.float32),  # 5 base + 18 enhanced
                    'author': torch.tensor(0, dtype=torch.long)
                },
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    
    # Handle metadata input dict
    metadata_input = {
        'numerical_features': torch.stack([item['metadata_input']['numerical_features'] for item in batch]),
        'author': torch.stack([item['metadata_input']['author'] for item in batch])
    }
    
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'metadata_input': metadata_input,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return
    
    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)
    
    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if isinstance(list(full_data.values())[0], dict) else full_data
        else:
            all_data = full_data
        
        # Split data if not already split
        split_idx = int(0.8 * len(all_data))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
    
    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract texts for vocabulary building
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            text = sample.get('text', sample.get('commit_message', ''))
            if text:
                texts.append(text)
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            continue
    
    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1    # Default value
            })
    
    if metadata_samples:
        logger.info("Fitting metadata processor...")
        metadata_processor.fit(metadata_samples)
    else:
        logger.warning("No metadata samples found for fitting")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    
    # Calculate dimensions
    sample_batch = next(iter(train_loader))
    text_dim = 64  # LSTM hidden dimension (fixed from model config)
    metadata_dim = sample_batch['metadata_input']['numerical_features'].shape[-1]
    
    logger.info(f"Text features dimension: {text_dim}")
    logger.info(f"Metadata features dimension: {metadata_dim}")
    
    # Model configuration matching the expected format
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.vocab_size,
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'] + 
                                [f'enhanced_feature_{i}' for i in range(18)],
            'hidden_dim': metadata_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    # Loss function
    criterion = nn.CrossEntropyLoss()
    
    # Training loop
    num_epochs = 10
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for each task
                total_loss = 0
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                
                for i, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_logits = outputs[task_name]
                        task_labels = labels[:, i]
                        task_loss = criterion(task_logits, task_labels)
                        total_loss += task_loss
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_batches += 1
                
                # Log progress
                if batch_idx % 100 == 0:
                    logger.info(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {total_loss.item():.4f}")
                
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        avg_train_loss = train_loss / train_batches if train_batches > 0 else float('inf')
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                try:
                    # Move batch to device
                    text_encoded = batch['text_encoded'].to(device)
                    metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                    labels = batch['labels'].to(device)
                    
                    # Forward pass
                    outputs = model(text_encoded, metadata_input)
                    
                    # Calculate loss
                    total_loss = 0
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    
                    for i, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_logits = outputs[task_name]
                            task_labels = labels[:, i]
                            task_loss = criterion(task_logits, task_labels)
                            total_loss += task_loss
                            
                            # Calculate accuracy for this task
                            predicted = torch.argmax(task_logits, dim=1)
                            val_correct += (predicted == task_labels).sum().item()
                            val_total += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch {batch_idx}: {e}")
                    continue
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        val_accuracy = val_correct / val_total if val_total > 0 else 0.0
        
        # Update scheduler
        scheduler.step(avg_val_loss)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{num_epochs}")
        logger.info(f"Train Loss: {avg_train_loss:.4f}")
        logger.info(f"Val Loss: {avg_val_loss:.4f}")
        logger.info(f"Val Accuracy: {val_accuracy:.4f}")
        logger.info(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model_path = os.path.join(current_dir, 'models', 'enhanced_100k_multimodal_fusion_best.pth')
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'model_config': model_config,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }, model_path)
            
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        
        logger.info("-" * 80)
    
    logger.info("Training completed!")
    logger.info(f"Best validation loss: {best_val_loss:.4f}")
    
    return model, text_processor, metadata_processor

if __name__ == "__main__":
    try:
        model, text_processor, metadata_processor = train_enhanced_100k_model()
        logger.info("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_han_github.py
```py
#!/usr/bin/env python3
"""
Train HAN Model v·ªõi GitHub Commits Dataset
Script ƒë∆°n gi·∫£n ƒë·ªÉ train m√¥ h√¨nh HAN v·ªõi d·ªØ li·ªáu t·ª´ GitHub commits
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter
import re

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class SimpleHANModel(nn.Module):
    """
    Simplified Hierarchical Attention Network for commit classification
    """
    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, num_classes=None):
        super(SimpleHANModel, self).__init__()
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # Word-level LSTM
        self.word_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # Word-level attention
        self.word_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Sentence-level LSTM
        self.sentence_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)
        
        # Sentence-level attention
        self.sentence_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Multi-task classification heads
        self.classifiers = nn.ModuleDict()
        if num_classes:
            for task, num_class in num_classes.items():
                self.classifiers[task] = nn.Linear(hidden_dim * 2, num_class)
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, input_ids, attention_mask=None):
        batch_size, max_sentences, max_words = input_ids.size()
        
        # Reshape for word-level processing
        input_ids = input_ids.view(-1, max_words)  # (batch_size * max_sentences, max_words)
        
        # Word embeddings
        embedded = self.embedding(input_ids)  # (batch_size * max_sentences, max_words, embed_dim)
        
        # Word-level LSTM
        word_output, _ = self.word_lstm(embedded)  # (batch_size * max_sentences, max_words, hidden_dim * 2)
        
        # Word-level attention
        word_attention_weights = torch.softmax(self.word_attention(word_output), dim=1)
        sentence_vectors = torch.sum(word_attention_weights * word_output, dim=1)  # (batch_size * max_sentences, hidden_dim * 2)
        
        # Reshape back to sentence level
        sentence_vectors = sentence_vectors.view(batch_size, max_sentences, -1)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level LSTM
        sentence_output, _ = self.sentence_lstm(sentence_vectors)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level attention
        sentence_attention_weights = torch.softmax(self.sentence_attention(sentence_output), dim=1)
        document_vector = torch.sum(sentence_attention_weights * sentence_output, dim=1)  # (batch_size, hidden_dim * 2)
        
        document_vector = self.dropout(document_vector)
        
        # Multi-task outputs
        outputs = {}
        for task, classifier in self.classifiers.items():
            outputs[task] = classifier(document_vector)
        
        return outputs

class CommitDataset(Dataset):
    """Dataset class for commit messages"""
    
    def __init__(self, texts, labels, tokenizer, max_sentences=10, max_words=50):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_sentences = max_sentences
        self.max_words = max_words
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        labels = self.labels[idx]
        
        # Tokenize text to sentences and words
        input_ids = self.tokenizer.encode_text(text, self.max_sentences, self.max_words)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': labels  # This will be a dictionary
        }

class SimpleTokenizer:
    """Simple tokenizer for commit messages"""
    
    def __init__(self, vocab_size=10000):
        self.vocab_size = vocab_size
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}
        self.word_counts = Counter()
        
    def fit(self, texts):
        """Build vocabulary from texts"""
        print("üî§ Building vocabulary...")
        
        for text in texts:
            # Split into sentences
            sentences = self.split_sentences(text)
            for sentence in sentences:
                words = self.tokenize_words(sentence)
                self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 2)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 2  # Start from 2 (after PAD and UNK)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"‚úÖ Vocabulary built with {len(self.word_to_idx)} words")
        
    def split_sentences(self, text):
        """Split text into sentences"""
        # Simple sentence splitting for commit messages
        sentences = re.split(r'[.!?;]|\\n', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences if sentences else [text]
    
    def tokenize_words(self, sentence):
        """Tokenize sentence into words"""
        # Simple word tokenization
        words = re.findall(r'\b\w+\b', sentence.lower())
        return words
    
    def encode_text(self, text, max_sentences, max_words):
        """Encode text to token ids"""
        sentences = self.split_sentences(text)
        
        # Pad or truncate sentences
        if len(sentences) > max_sentences:
            sentences = sentences[:max_sentences]
        while len(sentences) < max_sentences:
            sentences.append("")
        
        encoded_sentences = []
        for sentence in sentences:
            words = self.tokenize_words(sentence)
            
            # Convert words to indices
            word_ids = []
            for word in words:
                word_ids.append(self.word_to_idx.get(word, 1))  # 1 is UNK
            
            # Pad or truncate words
            if len(word_ids) > max_words:
                word_ids = word_ids[:max_words]
            while len(word_ids) < max_words:
                word_ids.append(0)  # 0 is PAD
            
            encoded_sentences.append(word_ids)
        
        return encoded_sentences

def collate_fn(batch):
    """Custom collate function for DataLoader"""
    input_ids = torch.stack([item['input_ids'] for item in batch])
    labels = [item['labels'] for item in batch]  # Keep as list of dicts
    
    return {
        'input_ids': input_ids,
        'labels': labels
    }

def load_github_dataset(data_file):
    """Load GitHub commits dataset"""
    print(f"üìñ Loading dataset: {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'data' not in data:
        raise ValueError("Invalid dataset format: missing 'data' field")
    
    samples = data['data']
    print(f"üìä Loaded {len(samples)} samples")
    
    # Extract texts and labels
    texts = []
    labels = []
    
    for sample in samples:
        texts.append(sample['text'])
        labels.append(sample['labels'])
    
    return texts, labels, data.get('metadata', {})

def prepare_label_encoders(labels):
    """Prepare label encoders for multi-task classification"""
    print("üè∑Ô∏è  Preparing label encoders...")
    
    # Get all unique labels for each task
    label_sets = {}
    for label_dict in labels:
        for task, label in label_dict.items():
            if task not in label_sets:
                label_sets[task] = set()
            label_sets[task].add(label)
    
    # Create mappings
    label_encoders = {}
    num_classes = {}
    
    for task, label_set in label_sets.items():
        sorted_labels = sorted(list(label_set))
        label_encoders[task] = {label: idx for idx, label in enumerate(sorted_labels)}
        num_classes[task] = len(sorted_labels)
        
        print(f"  {task}: {len(sorted_labels)} classes -> {sorted_labels}")
    
    return label_encoders, num_classes

def encode_labels(labels, label_encoders):
    """Encode labels using label encoders"""
    encoded_labels = []
    
    for label_dict in labels:
        encoded_dict = {}
        for task, label in label_dict.items():
            if task in label_encoders:
                encoded_dict[task] = label_encoders[task][label]
        encoded_labels.append(encoded_dict)
    
    return encoded_labels

def train_epoch(model, dataloader, optimizers, criteria, device, scaler=None, use_amp=False):
    """Train for one epoch with GPU optimizations and mixed precision"""
    model.train()
    total_losses = {task: 0.0 for task in criteria.keys()}
    total_loss = 0.0
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        # Move data to device with non_blocking for better GPU utilization
        input_ids = batch['input_ids'].to(device)
        batch_labels = batch['labels']
        
        # Clear gradients
        for optimizer in optimizers.values():
            optimizer.zero_grad()
        
        # Forward pass with mixed precision
        if use_amp and scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(input_ids)
                
                # Calculate losses for each task
                batch_losses = {}
                for task, criterion in criteria.items():
                    task_labels = []
                    for label_dict in batch_labels:
                        if task in label_dict:
                            task_labels.append(label_dict[task])
                    
                    if task_labels:
                        task_labels_tensor = torch.tensor(task_labels, device=device)
                        task_loss = criterion(outputs[task], task_labels_tensor)
                        batch_losses[task] = task_loss
                        total_losses[task] += task_loss.item()
                
                if batch_losses:
                    combined_loss = sum(batch_losses.values()) / len(batch_losses)
                    total_loss += combined_loss.item()
                    num_batches += 1
            
            # Backward pass with mixed precision
            if batch_losses:
                scaler.scale(combined_loss).backward()
                
                # Gradient clipping
                scaler.unscale_(list(optimizers.values())[0])  # Unscale for clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    scaler.step(optimizer)
                scaler.update()
        else:
            # Regular forward pass
            outputs = model(input_ids)
              # Calculate losses for each task
            batch_losses = {}
            for task, criterion in criteria.items():
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    batch_losses[task] = task_loss
                    total_losses[task] += task_loss.item()
            
            # Combined loss
            if batch_losses:
                combined_loss = sum(batch_losses.values()) / len(batch_losses)
                total_loss += combined_loss.item()
                num_batches += 1
                
                # Backward pass
                combined_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    optimizer.step()
        
        # Memory cleanup every 50 batches on GPU
        if device.type == 'cuda' and batch_idx % 50 == 0:
            torch.cuda.empty_cache()
    
    # Calculate average losses
    if num_batches > 0:
        avg_losses = {task: loss / num_batches for task, loss in total_losses.items()}
        avg_total_loss = total_loss / num_batches
    else:
        avg_losses = {task: 0.0 for task in total_losses.keys()}
        avg_total_loss = 0.0
    
    return avg_losses, avg_total_loss

def evaluate_model(model, dataloader, criteria, device):
    """Evaluate model with GPU optimizations"""
    model.eval()
    total_losses = {task: 0.0 for task in criteria.keys()}
    predictions = {task: [] for task in criteria.keys()}
    true_labels = {task: [] for task in criteria.keys()}
    num_batches = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # Move data to device with non_blocking
            input_ids = batch['input_ids'].to(device)
            batch_labels = batch['labels']
            
            # Forward pass
            outputs = model(input_ids)
            
            # Calculate losses and collect predictions
            for task, criterion in criteria.items():                # Extract task labels from batch
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:  # Only if we have labels for this task
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    total_losses[task] += task_loss.item()
                    
                    # Predictions
                    _, predicted = torch.max(outputs[task], 1)
                    predictions[task].extend(predicted.cpu().numpy())
                    true_labels[task].extend(task_labels_tensor.cpu().numpy())
            
            num_batches += 1
            
            # Memory cleanup every 50 batches on GPU
            if device.type == 'cuda' and batch_idx % 50 == 0:
                torch.cuda.empty_cache()
    
    # Calculate metrics
    metrics = {}
    for task in criteria.keys():
        if predictions[task] and num_batches > 0:
            accuracy = accuracy_score(true_labels[task], predictions[task])
            metrics[task] = {
                'loss': total_losses[task] / num_batches,
                'accuracy': accuracy
            }
    
    return metrics

def main():
    """Main training function"""
    print("üöÄ HAN GITHUB COMMITS TRAINER")
    print("="*60)
    
    # GPU Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Device: {device}")
    
    if torch.cuda.is_available():
        print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        # Clear GPU cache
        torch.cuda.empty_cache()
    else:
        print("‚ö†Ô∏è  CUDA not available, using CPU")
    
    # Paths
    data_file = Path(__file__).parent / "training_data" / "github_commits_training_data.json"
    model_dir = Path(__file__).parent / "models" / "han_github_model"
    log_dir = Path(__file__).parent / "training_logs"
    
    # Create directories
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load dataset
    if not data_file.exists():
        print(f"‚ùå Dataset not found: {data_file}")
        print("   Please run: python download_github_commits.py")
        return
    
    texts, labels, metadata = load_github_dataset(data_file)
    
    # Prepare labels
    label_encoders, num_classes = prepare_label_encoders(labels)
    encoded_labels = encode_labels(labels, label_encoders)
    
    # Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, encoded_labels, test_size=0.2, random_state=42
    )
    
    print(f"üìä Train samples: {len(train_texts)}")
    print(f"üìä Val samples: {len(val_texts)}")
    
    # Build tokenizer
    tokenizer = SimpleTokenizer(vocab_size=5000)
    tokenizer.fit(train_texts)
      # Create datasets
    train_dataset = CommitDataset(train_texts, train_labels, tokenizer)
    val_dataset = CommitDataset(val_texts, val_labels, tokenizer)
    
    # GPU optimized batch size
    if device.type == 'cuda':
        # Larger batch size for GPU
        batch_size = 32
        num_workers = 4  # More workers for GPU
        pin_memory = True
    else:
        # Smaller batch size for CPU
        batch_size = 16
        num_workers = 2
        pin_memory = False
    
    print(f"üîß Batch size: {batch_size}")
    print(f"üë• Num workers: {num_workers}")
    
    # Create dataloaders with GPU optimizations
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
      # Create model with GPU optimizations
    model = SimpleHANModel(
        vocab_size=len(tokenizer.word_to_idx),
        embed_dim=100,
        hidden_dim=128,
        num_classes=num_classes
    ).to(device)
    
    # Enable mixed precision for GPU if available
    if device.type == 'cuda':
        scaler = torch.cuda.amp.GradScaler()
        use_amp = True
        print("üöÄ Mixed precision training enabled")
    else:
        scaler = None
        use_amp = False
    
    print(f"ü§ñ Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPU memory optimization
    if device.type == 'cuda':
        print(f"üìä GPU Memory before training: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    
    # Setup training with optimized learning rates for GPU
    criteria = {}
    optimizers = {}
    schedulers = {}
    
    # Higher learning rate for GPU training
    base_lr = 0.002 if device.type == 'cuda' else 0.001
    
    for task in num_classes.keys():
        criteria[task] = nn.CrossEntropyLoss()
        optimizers[task] = optim.AdamW(  # AdamW is often better than Adam
            list(model.classifiers[task].parameters()) + 
            list(model.embedding.parameters()) +
            list(model.word_lstm.parameters()) +
            list(model.sentence_lstm.parameters()) +
            list(model.word_attention.parameters()) +
            list(model.sentence_attention.parameters()),
            lr=base_lr,
            weight_decay=1e-4  # L2 regularization
        )        # Add learning rate scheduler
        schedulers[task] = optim.lr_scheduler.ReduceLROnPlateau(
            optimizers[task], 
            mode='min', 
            factor=0.5, 
            patience=3
        )
      # Training loop with GPU monitoring
    num_epochs = 20
    best_val_accuracy = 0.0
    
    log_file = log_dir / f"han_github_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    print(f"\nüéØ Starting training for {num_epochs} epochs...")
    
    # Training start time
    import time
    training_start_time = time.time()
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        print(f"\nüìÖ Epoch {epoch+1}/{num_epochs}")
        
        # GPU memory monitoring
        if device.type == 'cuda':
            print(f"üìä GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        
        # Train with enhanced function
        train_losses, train_total_loss = train_epoch(
            model, train_loader, optimizers, criteria, device, scaler, use_amp
        )
        
        # Validate
        val_metrics = evaluate_model(model, val_loader, criteria, device)
        
        # Update learning rate schedulers
        avg_val_loss = 0.0
        if val_metrics:
            for task, metrics in val_metrics.items():
                if task in schedulers:
                    schedulers[task].step(metrics['loss'])
                avg_val_loss += metrics['loss']
            avg_val_loss /= len(val_metrics)
        
        # Calculate epoch time
        epoch_time = time.time() - epoch_start_time
        
        # Log results
        print(f"  ‚è±Ô∏è  Epoch time: {epoch_time:.1f}s")
        print(f"  üìà Train Loss: {train_total_loss:.4f}")
        for task, loss in train_losses.items():
            print(f"    {task}: {loss:.4f}")
        
        print(f"  üìä Val Metrics:")
        val_accuracy_sum = 0.0
        for task, metrics in val_metrics.items():
            print(f"    {task}: Loss={metrics['loss']:.4f}, Acc={metrics['accuracy']:.4f}")
            val_accuracy_sum += metrics['accuracy']
        
        avg_val_accuracy = val_accuracy_sum / len(val_metrics) if val_metrics else 0.0
        print(f"  üéØ Avg Val Accuracy: {avg_val_accuracy:.4f}")
        
        # GPU memory cleanup
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            print(f"  üßπ GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
          # Save log with enhanced information
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"\nEpoch {epoch+1}/{num_epochs}\n")
            f.write(f"Epoch Time: {epoch_time:.1f}s\n")
            f.write(f"Train Loss: {train_total_loss:.4f}\n")
            for task, loss in train_losses.items():
                f.write(f"  {task} Train Loss: {loss:.4f}\n")
            f.write(f"Val Metrics: {val_metrics}\n")
            f.write(f"Avg Val Accuracy: {avg_val_accuracy:.4f}\n")
            if device.type == 'cuda':
                f.write(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\n")
            f.write("-" * 50 + "\n")
        
        # Save best model with enhanced information
        if avg_val_accuracy > best_val_accuracy:
            best_val_accuracy = avg_val_accuracy
            
            # Save model with comprehensive information
            model_save_dict = {
                'model_state_dict': model.state_dict(),
                'tokenizer': tokenizer,
                'label_encoders': label_encoders,
                'num_classes': num_classes,
                'metadata': metadata,
                'epoch': epoch + 1,
                'val_accuracy': avg_val_accuracy,
                'train_loss': train_total_loss,
                'device': str(device),
                'batch_size': batch_size,
                'learning_rate': base_lr,
                'model_params': sum(p.numel() for p in model.parameters()),
                'training_config': {
                    'use_amp': use_amp,
                    'vocab_size': len(tokenizer.word_to_idx),
                    'embed_dim': 100,
                    'hidden_dim': 128,                    'max_sentences': 10,
                    'max_words': 50
                }
            }
            
            torch.save(model_save_dict, model_dir / 'best_model.pth')
            
            print(f"  üíæ Saved best model (accuracy: {avg_val_accuracy:.4f})")
    
    # Training completion summary
    total_training_time = time.time() - training_start_time
    print(f"\nüéâ Training completed!")
    print(f"‚è±Ô∏è  Total training time: {total_training_time/60:.1f} minutes")
    print(f"üìä Best validation accuracy: {best_val_accuracy:.4f}")
    print(f"üíæ Model saved to: {model_dir}")
    print(f"üìù Logs saved to: {log_file}")
    
    if device.type == 'cuda':
        print(f"üéÆ GPU training completed successfully")
        print(f"üìä Final GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

```

### backend\ai\__init__.py
```py

```

### backend\ai\multimodal_fusion\__init__.py
```py
"""
Multi-Modal Fusion Network for Commit Analysis
M√¥ h√¨nh k·∫øt h·ª£p th√¥ng tin vƒÉn b·∫£n v√† metadata ƒë·ªÉ ph√¢n t√≠ch commit
"""

__version__ = "1.0.0"
__author__ = "AI Team"

from .models.multimodal_fusion import MultiModalFusionNetwork
from .data_preprocessing.text_processor import TextProcessor
from .data_preprocessing.metadata_processor import MetadataProcessor
from .training.multitask_trainer import MultiTaskTrainer

__all__ = [
    "MultiModalFusionNetwork",
    "TextProcessor", 
    "MetadataProcessor",
    "MultiTaskTrainer"
]

```

### backend\ai\multimodal_fusion\data\synthetic_generator.py
```py
"""
Data Generation for Multi-Modal Fusion Network
T·∫°o synthetic GitHub commit data v·ªõi realistic patterns
"""

import random
import numpy as np
import pandas as pd
import torch
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import string
from collections import defaultdict
import re


class GitHubDataGenerator:
    """
    Generator cho synthetic GitHub commit data v·ªõi metadata patterns
    """
    
    def __init__(self, seed: int = 42):
        """
        Args:
            seed: Random seed for reproducibility
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # Pre-defined patterns
        self.commit_patterns = self._init_commit_patterns()
        self.file_types = self._init_file_types()
        self.authors = self._init_authors()
        self.programming_words = self._init_programming_words()
        
    def _init_commit_patterns(self) -> Dict[str, List[str]]:
        """Initialize commit message patterns"""
        return {
            'fix': [
                "Fix bug in {component}",
                "Fixed {issue} causing {problem}",
                "Bugfix: {description}",
                "Resolve {issue} in {component}",
                "Patch for {vulnerability}",
                "Hotfix: {critical_issue}",
                "Quick fix for {problem}",
                "Emergency fix: {description}"
            ],
            'feature': [
                "Add {feature} to {component}",
                "Implement {functionality}",
                "Feature: {new_feature}",
                "Introduce {capability}",
                "New: {feature_description}",
                "Enhance {component} with {feature}",
                "Added support for {technology}",
                "Initial implementation of {feature}"
            ],
            'refactor': [
                "Refactor {component} for better {quality}",
                "Code cleanup in {module}",
                "Restructure {component}",
                "Optimize {algorithm} implementation",
                "Improve {aspect} of {component}",
                "Reorganize {module} structure",
                "Clean up {technical_debt}",
                "Modernize {legacy_code}"
            ],
            'update': [
                "Update {dependency} to version {version}",
                "Upgrade {library} dependencies",
                "Bump {package} version",
                "Update documentation for {feature}",
                "Sync with upstream {repository}",
                "Update configuration for {environment}",
                "Refresh {cache} implementation",
                "Update {api} to latest version"
            ],
            'test': [
                "Add tests for {component}",
                "Test coverage for {feature}",
                "Unit tests for {module}",
                "Integration tests for {system}",
                "Fix failing tests in {component}",
                "Improve test stability",
                "Add regression tests for {bug}",
                "Update test data for {scenario}"
            ]
        }
    
    def _init_file_types(self) -> Dict[str, Dict[str, Any]]:
        """Initialize file type patterns"""
        return {
            'python': {
                'extensions': ['.py', '.pyx', '.pyi'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            },
            'javascript': {
                'extensions': ['.js', '.jsx', '.ts', '.tsx'],
                'risk_factor': 0.8,
                'complexity_base': 0.7
            },
            'java': {
                'extensions': ['.java', '.kt', '.scala'],
                'risk_factor': 0.6,
                'complexity_base': 0.5
            },
            'cpp': {
                'extensions': ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp'],
                'risk_factor': 0.9,
                'complexity_base': 0.8
            },
            'config': {
                'extensions': ['.json', '.yml', '.yaml', '.xml', '.toml', '.ini'],
                'risk_factor': 0.3,
                'complexity_base': 0.2
            },
            'documentation': {
                'extensions': ['.md', '.rst', '.txt', '.doc'],
                'risk_factor': 0.1,
                'complexity_base': 0.1
            },
            'web': {
                'extensions': ['.html', '.css', '.scss', '.less'],
                'risk_factor': 0.4,
                'complexity_base': 0.3
            },
            'database': {
                'extensions': ['.sql', '.psql', '.mysql'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            }
        }
    
    def _init_authors(self) -> List[Dict[str, Any]]:
        """Initialize author profiles"""
        return [
            {'name': 'senior_dev_1', 'experience': 0.9, 'reliability': 0.95, 'activity': 0.8},
            {'name': 'senior_dev_2', 'experience': 0.85, 'reliability': 0.9, 'activity': 0.7},
            {'name': 'mid_dev_1', 'experience': 0.6, 'reliability': 0.8, 'activity': 0.9},
            {'name': 'mid_dev_2', 'experience': 0.65, 'reliability': 0.75, 'activity': 0.85},
            {'name': 'mid_dev_3', 'experience': 0.7, 'reliability': 0.82, 'activity': 0.8},
            {'name': 'junior_dev_1', 'experience': 0.3, 'reliability': 0.6, 'activity': 0.95},
            {'name': 'junior_dev_2', 'experience': 0.25, 'reliability': 0.65, 'activity': 0.9},
            {'name': 'junior_dev_3', 'experience': 0.35, 'reliability': 0.7, 'activity': 0.88},
            {'name': 'intern_1', 'experience': 0.1, 'reliability': 0.5, 'activity': 0.7},
            {'name': 'intern_2', 'experience': 0.15, 'reliability': 0.55, 'activity': 0.75}
        ]
    
    def _init_programming_words(self) -> Dict[str, List[str]]:
        """Initialize programming-related words"""
        return {
            'components': [
                'API', 'database', 'frontend', 'backend', 'service', 'module', 'controller',
                'model', 'view', 'router', 'middleware', 'authentication', 'authorization',
                'cache', 'session', 'webhook', 'scheduler', 'queue', 'worker', 'parser',
                'validator', 'serializer', 'repository', 'factory', 'adapter', 'connector'
            ],
            'issues': [
                'memory leak', 'race condition', 'deadlock', 'null pointer', 'buffer overflow',
                'security vulnerability', 'performance issue', 'timeout', 'connection error',
                'validation error', 'parsing error', 'encoding issue', 'permission denied',
                'resource exhaustion', 'infinite loop', 'stack overflow', 'dependency conflict'
            ],
            'features': [
                'real-time notifications', 'user dashboard', 'data analytics', 'file upload',
                'search functionality', 'user authentication', 'payment processing',
                'email integration', 'social login', 'API rate limiting', 'data export',
                'mobile responsiveness', 'dark mode', 'internationalization', 'audit logs'
            ],
            'technologies': [
                'Docker', 'Kubernetes', 'Redis', 'PostgreSQL', 'MongoDB', 'Elasticsearch',
                'RabbitMQ', 'Kafka', 'GraphQL', 'REST API', 'gRPC', 'WebSocket', 'OAuth',
                'JWT', 'TLS', 'HTTPS', 'AWS', 'Azure', 'GCP', 'Terraform', 'Ansible'
            ]
        }
    
    def generate_commit_message(self, commit_type: str, risk_level: float) -> str:
        """
        Generate realistic commit message
        
        Args:
            commit_type: Type of commit (fix, feature, etc.)
            risk_level: Risk level (0-1) to influence message complexity
            
        Returns:
            Generated commit message
        """
        if commit_type not in self.commit_patterns:
            commit_type = random.choice(list(self.commit_patterns.keys()))
        
        template = random.choice(self.commit_patterns[commit_type])
        
        # Fill template with appropriate words
        component = random.choice(self.programming_words['components'])
        issue = random.choice(self.programming_words['issues'])
        feature = random.choice(self.programming_words['features'])
        technology = random.choice(self.programming_words['technologies'])
        
        # Replace placeholders
        message = template.format(
            component=component,
            issue=issue,
            problem=issue,
            feature=feature,
            functionality=feature,
            new_feature=feature,
            capability=feature,
            feature_description=feature,
            technology=technology,
            quality='performance' if risk_level > 0.5 else 'maintainability',
            module=component,
            aspect='security' if risk_level > 0.7 else 'performance',
            technical_debt='legacy code',
            legacy_code='deprecated functions',
            dependency=technology,
            version=f"{random.randint(1,5)}.{random.randint(0,20)}.{random.randint(0,10)}",
            library=technology,
            package=technology,
            environment='production' if risk_level > 0.6 else 'development',
            repository='main',
            cache='Redis',
            api='REST API',
            system=component,
            bug=issue,
            scenario='edge case',
            description=issue,
            vulnerability='SQL injection',
            critical_issue='server crash',
            algorithm='sorting'
        )
          # Add complexity based on risk level
        if risk_level > 0.8:
            suffixes = [
                " - critical security patch",
                " - urgent production fix",
                " - breaking change",
                " - requires migration",
                " - affects multiple services"
            ]
            message += random.choice(suffixes)
        elif risk_level > 0.6:
            suffixes = [
                " - needs testing",
                " - requires review",
                " - performance impact",
                " - config change needed"
            ]
            message += random.choice(suffixes)
        
        return message
    
    def generate_file_changes(self, risk_level: float, complexity_level: float) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        Generate realistic file changes
        
        Args:
            risk_level: Risk level (0-1)
            complexity_level: Complexity level (0-1)
            
        Returns:
            Tuple of (file_data_list, change_stats)
        """
        # Determine number of files based on complexity
        if complexity_level > 0.8:
            num_files = random.randint(8, 25)
        elif complexity_level > 0.6:
            num_files = random.randint(4, 12)
        elif complexity_level > 0.3:
            num_files = random.randint(2, 6)
        else:
            num_files = random.randint(1, 3)
        
        files_data = []
        stats = {'additions': 0, 'deletions': 0, 'modifications': 0}
        
        # Select file types based on risk level
        type_weights = []
        for file_type, info in self.file_types.items():
            weight = info['risk_factor'] if risk_level > 0.5 else (1 - info['risk_factor'])
            type_weights.append((file_type, weight, info))
        
        for i in range(num_files):
            # Select file type
            selected_type = random.choices(
                [t[0] for t in type_weights],
                weights=[t[1] for t in type_weights]
            )[0]
            
            type_info = self.file_types[selected_type]
            extension = random.choice(type_info['extensions'])
            
            # Generate file path
            components = random.choice(self.programming_words['components']).lower()
            file_name = f"{components}_{random.randint(1, 100)}{extension}"
            
            # Create realistic directory structure
            if selected_type == 'python':
                dirs = ['src', 'lib', 'api', 'models', 'views', 'utils']
            elif selected_type == 'javascript':
                dirs = ['src', 'components', 'pages', 'utils', 'services']
            elif selected_type == 'java':
                dirs = ['src/main/java', 'src/test/java']
            elif selected_type == 'config':
                dirs = ['config', 'deploy', 'scripts']
            elif selected_type == 'documentation':
                dirs = ['docs', 'README']
            else:
                dirs = ['src', 'lib', 'assets']
            
            directory = random.choice(dirs)
            file_path = f"{directory}/{file_name}"
            
            # Generate change statistics for this file
            base_changes = int(complexity_level * 200)
            additions = random.randint(1, max(1, base_changes))
            deletions = random.randint(0, max(1, int(additions * 0.7)))
            changes = additions + deletions
            
            # Create file data dict as expected by MetadataProcessor
            file_data = {
                'filename': file_path,
                'additions': additions,
                'deletions': deletions,
                'changes': changes,
                'status': random.choice(['modified', 'added', 'removed']),
                'patch': f"@@ -1,{deletions} +1,{additions} @@"  # Simple patch format
            }
            
            files_data.append(file_data)
            
            stats['additions'] += additions
            stats['deletions'] += deletions
            stats['modifications'] += 1
        
        return files_data, stats
    
    def generate_temporal_features(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate temporal features with realistic patterns
        
        Args:
            risk_level: Risk level affects timing patterns
            
        Returns:
            Dict with temporal features
        """
        # Base time
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        # Generate random timestamp
        time_diff = end_date - start_date
        random_seconds = random.randint(0, int(time_diff.total_seconds()))
        commit_time = start_date + timedelta(seconds=random_seconds)
        
        # Risky commits more likely during off-hours
        if risk_level > 0.7:
            # Late night commits (22:00 - 06:00)
            if random.random() > 0.3:
                hour = random.choice(list(range(22, 24)) + list(range(0, 7)))
                commit_time = commit_time.replace(hour=hour)
        
        weekday = commit_time.weekday()  # 0=Monday, 6=Sunday
        hour = commit_time.hour
        
        # Season encoding
        month = commit_time.month
        if month in [12, 1, 2]:
            season = 0  # Winter
        elif month in [3, 4, 5]:
            season = 1  # Spring
        elif month in [6, 7, 8]:
            season = 2  # Summer
        else:
            season = 3  # Fall
        
        return {
            'timestamp': commit_time,
            'weekday': weekday,
            'hour': hour,
            'season': season,
            'is_weekend': weekday >= 5,
            'is_business_hours': 9 <= hour <= 17,
            'unix_timestamp': int(commit_time.timestamp())
        }
    
    def generate_author_info(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate author information
        
        Args:
            risk_level: Affects author selection
            
        Returns:
            Author information dict
        """
        # Select author based on risk level
        if risk_level > 0.8:
            # High risk - more likely to be junior/intern
            author_pool = [a for a in self.authors if a['experience'] < 0.5]
        elif risk_level > 0.5:
            # Medium risk - mixed experience
            author_pool = [a for a in self.authors if 0.3 <= a['experience'] <= 0.8]
        else:
            # Low risk - more likely to be senior
            author_pool = [a for a in self.authors if a['experience'] > 0.6]
        
        if not author_pool:
            author_pool = self.authors
        
        author = random.choice(author_pool)
        
        return {
            'name': author['name'],
            'experience_level': author['experience'],
            'reliability_score': author['reliability'],
            'activity_score': author['activity'],
            'commits_last_month': int(author['activity'] * 50),
            'avg_commit_size': int((1 - author['experience']) * 100 + 20)
        }
    
    def calculate_labels(self, commit_data: Dict[str, Any]) -> Dict[str, int]:
        """
        Calculate ground truth labels based on generated features
        
        Args:
            commit_data: Generated commit data
            
        Returns:
            Dict with task labels
        """
        # Extract features for label calculation
        risk_factors = []
        complexity_factors = []
        
        # File-based factors
        file_types = commit_data['metadata']['file_types']
        for file_type, info in self.file_types.items():
            if file_type in file_types and file_types[file_type] > 0:
                risk_factors.append(info['risk_factor'] * file_types[file_type])
                complexity_factors.append(info['complexity_base'] * file_types[file_type])
        
        # Author factors
        author_info = commit_data['metadata']['author_info']
        risk_factors.append(1 - author_info['reliability_score'])
        complexity_factors.append(1 - author_info['experience_level'])
        
        # Temporal factors
        temporal = commit_data['metadata']['temporal']
        if not temporal['is_business_hours']:
            risk_factors.append(0.3)
        if temporal['is_weekend']:
            risk_factors.append(0.2)
        
        # Change size factors
        stats = commit_data['metadata']['change_stats']
        total_changes = stats['additions'] + stats['deletions']
        if total_changes > 500:
            risk_factors.append(0.4)
            complexity_factors.append(0.5)
        elif total_changes > 200:
            risk_factors.append(0.2)
            complexity_factors.append(0.3)
        
        # File count factor
        num_files = len(commit_data['metadata']['files'])
        if num_files > 15:
            risk_factors.append(0.4)
            complexity_factors.append(0.4)
        elif num_files > 8:
            risk_factors.append(0.2)
            complexity_factors.append(0.2)
        
        # Calculate final scores
        avg_risk = np.mean(risk_factors) if risk_factors else 0.5
        avg_complexity = np.mean(complexity_factors) if complexity_factors else 0.5
        
        # Generate labels
        labels = {}
        
        # Commit Risk (Binary: 0=Low, 1=High)
        labels['commit_risk'] = 1 if avg_risk > 0.6 else 0
        
        # Complexity (3 classes: 0=Low, 1=Medium, 2=High)
        if avg_complexity > 0.7:
            labels['complexity'] = 2
        elif avg_complexity > 0.4:
            labels['complexity'] = 1
        else:
            labels['complexity'] = 0
        
        # Hotspot Files (Binary: 0=No, 1=Yes)
        # Based on high-risk file types and frequency
        hotspot_score = 0
        for file_type, count in file_types.items():
            if self.file_types[file_type]['risk_factor'] > 0.7:
                hotspot_score += count
        labels['hotspot'] = 1 if hotspot_score > 3 else 0
        
        # Urgent Review (Binary: 0=No, 1=Yes)
        # High risk + (low author reliability OR critical files OR large changes)
        urgent_factors = []
        if avg_risk > 0.7:
            urgent_factors.append(1)
        if author_info['reliability_score'] < 0.7:
            urgent_factors.append(1)
        if total_changes > 300:
            urgent_factors.append(1)
        if any(self.file_types[ft]['risk_factor'] > 0.8 for ft in file_types if file_types[ft] > 0):
            urgent_factors.append(1)
        
        labels['urgent_review'] = 1 if len(urgent_factors) >= 2 else 0
        
        return labels
    
    def generate_single_commit(self, target_risk: Optional[float] = None) -> Dict[str, Any]:
        """
        Generate a single commit with all features and labels
        
        Args:
            target_risk: Target risk level (0-1), if None then random
            
        Returns:
            Complete commit data dict
        """
        # Determine risk level
        if target_risk is None:
            risk_level = random.random()
        else:
            risk_level = max(0.0, min(1.0, target_risk + random.gauss(0, 0.1)))
        
        # Generate complexity level (correlated with risk)
        complexity_level = risk_level + random.gauss(0, 0.2)
        complexity_level = max(0.0, min(1.0, complexity_level))
        
        # Select commit type based on risk
        if risk_level > 0.8:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.5, 0.2, 0.1, 0.1, 0.1]
            )[0]
        elif risk_level > 0.5:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.3, 0.3, 0.2, 0.1, 0.1]
            )[0]
        else:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.1, 0.3, 0.3, 0.2, 0.1]
            )[0]
          # Generate components
        commit_message = self.generate_commit_message(commit_type, risk_level)
        files_data, change_stats = self.generate_file_changes(risk_level, complexity_level)
        temporal_features = self.generate_temporal_features(risk_level)
        author_info = self.generate_author_info(risk_level)
        
        # Count file types
        file_types = defaultdict(int)
        for file_data in files_data:
            filename = file_data['filename']
            for file_type, info in self.file_types.items():
                for ext in info['extensions']:
                    if filename.endswith(ext):
                        file_types[file_type] += 1
                        break
        
        # Create commit data structure
        commit_data = {
            'commit_message': commit_message,
            'metadata': {
                'files': files_data,  # Now this is list of dicts as expected by MetadataProcessor
                'change_stats': change_stats,
                'file_types': dict(file_types),
                'temporal': temporal_features,
                'author_info': author_info,
                'commit_type': commit_type,
                'risk_level': risk_level,
                'complexity_level': complexity_level
            }
        }
        
        # Calculate labels
        labels = self.calculate_labels(commit_data)
        commit_data['labels'] = labels
        
        return commit_data
    
    def generate_dataset(self, num_samples: int, 
                        risk_distribution: Optional[Dict[str, float]] = None,
                        save_path: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Generate complete dataset
        
        Args:
            num_samples: Number of samples to generate
            risk_distribution: Dict with risk level distribution
            save_path: Path to save dataset
            
        Returns:
            List of commit data dicts
        """
        if risk_distribution is None:
            risk_distribution = {
                'low': 0.5,      # 0.0 - 0.3
                'medium': 0.3,   # 0.3 - 0.7
                'high': 0.2      # 0.7 - 1.0
            }
        
        dataset = []
        
        for i in range(num_samples):
            # Select risk level based on distribution
            rand_val = random.random()
            if rand_val < risk_distribution['low']:
                target_risk = random.uniform(0.0, 0.3)
            elif rand_val < risk_distribution['low'] + risk_distribution['medium']:
                target_risk = random.uniform(0.3, 0.7)
            else:
                target_risk = random.uniform(0.7, 1.0)
            
            commit_data = self.generate_single_commit(target_risk)
            commit_data['id'] = i
            dataset.append(commit_data)
            
            if (i + 1) % 1000 == 0:
                print(f"Generated {i + 1}/{num_samples} samples")
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, indent=2, default=str, ensure_ascii=False)
            print(f"Dataset saved to {save_path}")
        
        return dataset
    
    def generate_splits(self, dataset: List[Dict[str, Any]], 
                       split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15),
                       stratify_by: str = 'commit_risk') -> Tuple[List, List, List]:
        """
        Split dataset into train/val/test with stratification
        
        Args:
            dataset: Complete dataset
            split_ratios: (train, val, test) ratios
            stratify_by: Label to stratify by
            
        Returns:
            Tuple of (train, val, test) datasets
        """
        from sklearn.model_selection import train_test_split
        
        # Extract labels for stratification
        if stratify_by in dataset[0]['labels']:
            stratify_labels = [item['labels'][stratify_by] for item in dataset]
        else:
            stratify_labels = None
        
        # First split: train vs (val + test)
        train_data, temp_data = train_test_split(
            dataset, 
            test_size=(1 - split_ratios[0]),
            stratify=stratify_labels,
            random_state=42
        )
        
        # Second split: val vs test
        val_ratio = split_ratios[1] / (split_ratios[1] + split_ratios[2])
        if stratify_labels:
            temp_labels = [item['labels'][stratify_by] for item in temp_data]
        else:
            temp_labels = None
        
        val_data, test_data = train_test_split(
            temp_data,
            test_size=(1 - val_ratio),
            stratify=temp_labels,
            random_state=42
        )
        
        print(f"Dataset splits: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
        
        return train_data, val_data, test_data
    
    def analyze_dataset(self, dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze generated dataset statistics
        
        Args:
            dataset: Generated dataset
            
        Returns:
            Analysis results
        """
        analysis = {
            'total_samples': len(dataset),
            'label_distributions': {},
            'feature_statistics': {},
            'correlations': {}
        }
        
        # Label distributions
        for sample in dataset:
            for task_name, label in sample['labels'].items():
                if task_name not in analysis['label_distributions']:
                    analysis['label_distributions'][task_name] = defaultdict(int)
                analysis['label_distributions'][task_name][label] += 1
        
        # Feature statistics
        risk_levels = [sample['metadata']['risk_level'] for sample in dataset]
        complexity_levels = [sample['metadata']['complexity_level'] for sample in dataset]
        file_counts = [len(sample['metadata']['files']) for sample in dataset]
        change_sizes = [sample['metadata']['change_stats']['additions'] + 
                       sample['metadata']['change_stats']['deletions'] for sample in dataset]
        
        analysis['feature_statistics'] = {
            'risk_level': {
                'mean': np.mean(risk_levels),
                'std': np.std(risk_levels),
                'min': np.min(risk_levels),
                'max': np.max(risk_levels)
            },
            'complexity_level': {
                'mean': np.mean(complexity_levels),
                'std': np.std(complexity_levels),
                'min': np.min(complexity_levels),
                'max': np.max(complexity_levels)
            },
            'file_count': {
                'mean': np.mean(file_counts),
                'std': np.std(file_counts),
                'min': np.min(file_counts),
                'max': np.max(file_counts)
            },
            'change_size': {
                'mean': np.mean(change_sizes),
                'std': np.std(change_sizes),
                'min': np.min(change_sizes),
                'max': np.max(change_sizes)
            }
        }
        
        return analysis


def main():
    """Example usage"""
    generator = GitHubDataGenerator(seed=42)
    
    # Generate small dataset for testing
    print("Generating sample dataset...")
    dataset = generator.generate_dataset(
        num_samples=1000,
        risk_distribution={'low': 0.6, 'medium': 0.3, 'high': 0.1}
    )
    
    # Analyze dataset
    analysis = generator.analyze_dataset(dataset)
    print("\nDataset Analysis:")
    print(f"Total samples: {analysis['total_samples']}")
    print("\nLabel distributions:")
    for task, dist in analysis['label_distributions'].items():
        print(f"  {task}: {dict(dist)}")
    
    print("\nFeature statistics:")
    for feature, stats in analysis['feature_statistics'].items():
        print(f"  {feature}: mean={stats['mean']:.3f}, std={stats['std']:.3f}")
    
    # Show sample
    print(f"\nSample commit:")
    sample = dataset[0]
    print(f"Message: {sample['commit_message']}")
    print(f"Files: {len(sample['metadata']['files'])} files")
    print(f"Changes: +{sample['metadata']['change_stats']['additions']} -{sample['metadata']['change_stats']['deletions']}")
    print(f"Labels: {sample['labels']}")


if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\data_preprocessing\enhanced_text_processor.py
```py
"""
Enhanced Text Processor with NLTK Support
Provides advanced natural language processing capabilities for commit message analysis
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter, defaultdict
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available. Using simple tokenization only.")

# Enhanced NLTK import with more features
try:
    import nltk
    from nltk.corpus import stopwords, wordnet
    from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tag import pos_tag
    from nltk.chunk import ne_chunk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from nltk.corpus import opinion_lexicon
    from nltk.probability import FreqDist
    from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder
    from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures
    NLTK_AVAILABLE = True
    logger.info("NLTK library available with advanced features")
except ImportError:
    NLTK_AVAILABLE = False
    logger.warning("NLTK not available. Using simple text processing.")

# Optional TextBlob for additional sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available.")


class EnhancedTextProcessor:
    """
    Enhanced Text Processor with comprehensive NLTK support
    Provides advanced NLP features for commit message analysis
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_stemming: bool = True,
                 enable_lemmatization: bool = True,
                 enable_pos_tagging: bool = True,
                 enable_sentiment_analysis: bool = True,
                 enable_ngrams: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name for transformers
            enable_stemming: Enable word stemming
            enable_lemmatization: Enable word lemmatization
            enable_pos_tagging: Enable part-of-speech tagging
            enable_sentiment_analysis: Enable sentiment analysis
            enable_ngrams: Enable n-gram extraction
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        
        # NLTK feature flags
        self.enable_stemming = enable_stemming
        self.enable_lemmatization = enable_lemmatization
        self.enable_pos_tagging = enable_pos_tagging
        self.enable_sentiment_analysis = enable_sentiment_analysis
        self.enable_ngrams = enable_ngrams
        
        # Initialize NLTK components
        self._init_nltk_components()
        
        # Initialize model components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_nltk_components(self):
        """Initialize NLTK components and download required data"""
        if not NLTK_AVAILABLE:
            logger.warning("NLTK not available. Advanced text processing features disabled.")
            self.stop_words = self._get_basic_stopwords()
            return
        
        # Download required NLTK data
        required_data = [
            'punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger',
            'maxent_ne_chunker', 'words', 'vader_lexicon', 'opinion_lexicon',
            'omw-1.4'
        ]
        
        for data in required_data:
            try:
                nltk.data.find(f'tokenizers/{data}')
            except LookupError:
                try:
                    nltk.data.find(f'corpora/{data}')
                except LookupError:
                    try:
                        nltk.data.find(f'taggers/{data}')
                    except LookupError:
                        try:
                            nltk.data.find(f'chunkers/{data}')
                        except LookupError:
                            try:
                                nltk.download(data, quiet=True)
                                logger.info(f"Downloaded NLTK data: {data}")
                            except Exception as e:
                                logger.warning(f"Failed to download {data}: {e}")
        
        # Initialize NLTK tools
        try:
            self.stop_words = set(stopwords.words('english'))
            self.stemmer = PorterStemmer() if self.enable_stemming else None
            self.lemmatizer = WordNetLemmatizer() if self.enable_lemmatization else None
            self.sentiment_analyzer = SentimentIntensityAnalyzer() if self.enable_sentiment_analysis else None
            self.tokenizer_nltk = TreebankWordTokenizer()
            
            # Load opinion lexicon for additional sentiment analysis
            try:
                self.positive_words = set(opinion_lexicon.positive())
                self.negative_words = set(opinion_lexicon.negative())
            except Exception as e:
                logger.warning(f"Failed to load opinion lexicon: {e}")
                self.positive_words = set()
                self.negative_words = set()
                
            logger.info("NLTK components initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize NLTK components: {e}")
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Get basic stopwords if NLTK is not available"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',
            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves'
        ])
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
        
        # Additional LSTM-specific features
        self.bigram_counts = Counter()
        self.trigram_counts = Counter()
        self.pos_tag_counts = Counter()
        
    def advanced_tokenize(self, text: str) -> List[str]:
        """
        Advanced tokenization with NLTK features
        """
        if not NLTK_AVAILABLE:
            # Fallback to simple tokenization
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
        
        try:
            # Use TreebankWordTokenizer for better handling of contractions and punctuation
            tokens = self.tokenizer_nltk.tokenize(text)
            
            # Apply stemming or lemmatization
            if self.enable_lemmatization and self.lemmatizer:
                # Get POS tags for better lemmatization
                pos_tags = pos_tag(tokens) if self.enable_pos_tagging else [(token, 'NN') for token in tokens]
                tokens = [self._lemmatize_with_pos(token, pos) for token, pos in pos_tags]
            elif self.enable_stemming and self.stemmer:
                tokens = [self.stemmer.stem(token) for token in tokens]
            
            return tokens
            
        except Exception as e:
            logger.warning(f"Advanced tokenization failed: {e}. Using simple fallback.")
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def _lemmatize_with_pos(self, word: str, pos_tag: str) -> str:
        """
        Lemmatize word with POS tag context
        """
        if not self.lemmatizer:
            return word
            
        # Convert POS tag to WordNet format
        tag_dict = {
            'J': wordnet.ADJ,
            'N': wordnet.NOUN,
            'V': wordnet.VERB,
            'R': wordnet.ADV
        }
        
        wordnet_pos = tag_dict.get(pos_tag[0], wordnet.NOUN)
        return self.lemmatizer.lemmatize(word, wordnet_pos)
    
    def extract_advanced_features(self, text: str) -> Dict[str, any]:
        """
        Extract advanced features using NLTK capabilities
        """
        features = {}
        
        # Basic features
        features.update(self.extract_basic_features(text))
        
        if not NLTK_AVAILABLE:
            return features
        
        try:
            # Tokenize for advanced analysis
            tokens = self.advanced_tokenize(text.lower())
            
            # Sentiment analysis
            if self.enable_sentiment_analysis and self.sentiment_analyzer:
                sentiment_scores = self.sentiment_analyzer.polarity_scores(text)
                features.update({
                    'sentiment_positive': sentiment_scores['pos'],
                    'sentiment_negative': sentiment_scores['neg'],
                    'sentiment_neutral': sentiment_scores['neu'],
                    'sentiment_compound': sentiment_scores['compound']
                })
                
                # TextBlob sentiment as additional feature
                if TEXTBLOB_AVAILABLE:
                    blob = TextBlob(text)
                    features['textblob_polarity'] = blob.sentiment.polarity
                    features['textblob_subjectivity'] = blob.sentiment.subjectivity
                
                # Opinion lexicon features
                positive_count = sum(1 for word in tokens if word in self.positive_words)
                negative_count = sum(1 for word in tokens if word in self.negative_words)
                features['positive_word_count'] = positive_count
                features['negative_word_count'] = negative_count
                features['sentiment_ratio'] = (positive_count - negative_count) / max(len(tokens), 1)
            
            # POS tagging features
            if self.enable_pos_tagging:
                pos_tags = pos_tag(tokens)
                pos_counts = Counter(tag for _, tag in pos_tags)
                
                # Important POS categories for commit analysis
                features['noun_count'] = pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + pos_counts.get('NNP', 0)
                features['verb_count'] = pos_counts.get('VB', 0) + pos_counts.get('VBD', 0) + pos_counts.get('VBG', 0) + pos_counts.get('VBN', 0)
                features['adjective_count'] = pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + pos_counts.get('JJS', 0)
                features['adverb_count'] = pos_counts.get('RB', 0) + pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0)
                
                # POS diversity
                features['pos_diversity'] = len(pos_counts) / max(len(tokens), 1)
            
            # N-gram features
            if self.enable_ngrams and len(tokens) > 1:
                # Bigrams
                bigrams = list(nltk.bigrams(tokens))
                features['unique_bigrams'] = len(set(bigrams))
                features['bigram_ratio'] = len(set(bigrams)) / max(len(bigrams), 1)
                
                # Trigrams (if enough tokens)
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    features['unique_trigrams'] = len(set(trigrams))
                    features['trigram_ratio'] = len(set(trigrams)) / max(len(trigrams), 1)
                else:
                    features['unique_trigrams'] = 0
                    features['trigram_ratio'] = 0
            
            # Lexical diversity
            features['lexical_diversity'] = len(set(tokens)) / max(len(tokens), 1)
            
            # Average word length
            features['avg_word_length'] = np.mean([len(word) for word in tokens]) if tokens else 0
            
        except Exception as e:
            logger.warning(f"Advanced feature extraction failed: {e}")
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """
        Extract basic features (fallback when NLTK not available)
        """
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'has_commit_type', 'commit_type_prefix', 'has_bug_keywords',
                'has_feature_keywords', 'has_doc_keywords', 'positive_sentiment',
                'negative_sentiment', 'urgent_sentiment'
            ]}
        
        # Basic text statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') or text.lower().startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':') or text.lower().startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Enhanced keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve', 'correct', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce', 'enable']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'docs', 'guide', 'manual']
        refactor_keywords = ['refactor', 'restructure', 'reorganize', 'cleanup', 'optimize', 'improve']
        
        text_lower = text.lower()
        features['has_bug_keywords'] = any(keyword in text_lower for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text_lower for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text_lower for keyword in doc_keywords)
        features['has_refactor_keywords'] = any(keyword in text_lower for keyword in refactor_keywords)
        
        # Sentiment indicators (basic)
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success', 'complete', 'finish']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error', 'disable', 'revert']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediate', 'quick']
        
        features['positive_sentiment'] = any(word in text_lower for word in positive_words)
        features['negative_sentiment'] = any(word in text_lower for word in negative_words)
        features['urgent_sentiment'] = any(word in text_lower for word in urgent_words)
        
        return features
    
    def clean_commit_message(self, text: str) -> str:
        """
        Enhanced commit message cleaning
        """
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'merge .* of .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        Build vocabulary with advanced NLTK features
        """
        if self.method != "lstm":
            return
        
        logger.info("üî§ Building enhanced vocabulary with NLTK features...")
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            
            # Filter tokens
            tokens = [token for token in tokens 
                     if token not in self.stop_words 
                     and len(token) > 1 
                     and token.isalpha()]
            
            all_tokens.extend(tokens)
            self.word_counts.update(tokens)
            
            # Collect n-grams if enabled
            if self.enable_ngrams and len(tokens) > 1:
                bigrams = list(nltk.bigrams(tokens))
                self.bigram_counts.update(bigrams)
                
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    self.trigram_counts.update(trigrams)
        
        # Build vocabulary with most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"‚úÖ Enhanced vocabulary built with {len(self.word_to_idx)} words")
        
        # Log vocabulary statistics
        if NLTK_AVAILABLE:
            logger.info(f"üìä Total unique bigrams: {len(self.bigram_counts)}")
            logger.info(f"üìä Total unique trigrams: {len(self.trigram_counts)}")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Enhanced LSTM encoding with NLTK preprocessing
        """
        cleaned_text = self.clean_commit_message(text)
        tokens = self.advanced_tokenize(cleaned_text.lower())
        
        # Filter tokens
        tokens = [token for token in tokens 
                 if token not in self.stop_words 
                 and len(token) > 1 
                 and token.isalpha()]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def get_collocations(self, texts: List[str], n: int = 10) -> Dict[str, List[Tuple]]:
        """
        Extract meaningful collocations from commit messages
        """
        if not NLTK_AVAILABLE or not self.enable_ngrams:
            return {'bigrams': [], 'trigrams': []}
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            tokens = [token for token in tokens 
                     if token not in self.stop_words and len(token) > 1]
            all_tokens.extend(tokens)
        
        try:
            # Bigram collocations
            bigram_finder = BigramCollocationFinder.from_words(all_tokens)
            bigram_finder.apply_freq_filter(3)  # Only bigrams appearing 3+ times
            bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, n)
            
            # Trigram collocations
            trigram_finder = TrigramCollocationFinder.from_words(all_tokens)
            trigram_finder.apply_freq_filter(2)  # Only trigrams appearing 2+ times
            trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, n)
            
            return {'bigrams': bigrams, 'trigrams': trigrams}
            
        except Exception as e:
            logger.warning(f"Collocation extraction failed: {e}")
            return {'bigrams': [], 'trigrams': []}
    
    def fit(self, texts: List[str]) -> 'EnhancedTextProcessor':
        """
        Fit the enhanced text processor to training data
        """
        logger.info("üöÄ Fitting enhanced text processor with NLTK features...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
            
            # Extract and log collocations for insights
            collocations = self.get_collocations(texts)
            if collocations['bigrams']:
                logger.info(f"üìà Top bigrams: {collocations['bigrams'][:5]}")
            if collocations['trigrams']:
                logger.info(f"üìà Top trigrams: {collocations['trigrams'][:3]}")
        
        logger.info("‚úÖ Enhanced text processor fitted successfully")
        return self
    
    # Keep all other methods from the original TextProcessor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method (unchanged)"""
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """Get text embeddings (unchanged)"""
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    embedding = outputs.last_hidden_state[:, 0, :]
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts with enhanced features"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        # Extract enhanced features
        for text in texts:
            basic_features = self.extract_basic_features(text)
            if NLTK_AVAILABLE:
                enhanced_features = self.extract_advanced_features(text)
                results['enhanced_features'].append(enhanced_features)
            else:
                results['enhanced_features'].append(basic_features)
            
            results['text_features'].append(basic_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\metadata_processor.py
```py
"""
Metadata Processor for Multi-Modal Fusion Network
X·ª≠ l√Ω v√† chu·∫©n b·ªã metadata t·ª´ GitHub commits
"""

import numpy as np
import pandas as pd
import torch
from typing import List, Dict, Tuple, Optional, Any
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime, timezone
import re
from pathlib import Path

class MetadataProcessor:
    """
    L·ªõp x·ª≠ l√Ω metadata cho GitHub commits
    Bao g·ªìm commit stats, file info, author info, timestamp info
    """
    
    def __init__(self, normalize_features: bool = True, 
                 categorical_method: str = "embedding",  # "embedding", "onehot"
                 max_files: int = 50,
                 max_authors: int = 1000):
        """
        Args:
            normalize_features: C√≥ chu·∫©n h√≥a features s·ªë kh√¥ng
            categorical_method: Ph∆∞∆°ng ph√°p encode categorical ("embedding", "onehot")
            max_files: S·ªë file t·ªëi ƒëa ƒë·ªÉ track
            max_authors: S·ªë author t·ªëi ƒëa ƒë·ªÉ track
        """
        self.normalize_features = normalize_features
        self.categorical_method = categorical_method
        self.max_files = max_files
        self.max_authors = max_authors
        
        # Scalers for numerical features
        self.numerical_scaler = StandardScaler()
        self.ratio_scaler = MinMaxScaler()
        
        # Encoders for categorical features
        self.file_type_encoder = LabelEncoder()
        self.author_encoder = LabelEncoder()
        self.file_path_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
        
        # Feature statistics
        self.feature_stats = {}
        self.is_fitted = False
        
    def extract_file_features(self, files_data: List[Dict]) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ th√¥ng tin files
        
        Args:
            files_data: List of file info dicts v·ªõi keys: filename, status, additions, deletions, changes
        """
        if not files_data:
            return self._get_empty_file_features()
        
        features = {}
        
        # Basic file stats
        features['num_files'] = len(files_data)
        features['total_additions'] = sum(f.get('additions', 0) for f in files_data)
        features['total_deletions'] = sum(f.get('deletions', 0) for f in files_data)
        features['total_changes'] = sum(f.get('changes', 0) for f in files_data)
        
        # File types
        file_extensions = []
        file_paths = []
        for file_info in files_data:
            filename = file_info.get('filename', '')
            if filename:
                file_paths.append(filename)
                # Extract extension
                if '.' in filename:
                    ext = filename.split('.')[-1].lower()
                    file_extensions.append(ext)
        
        # File type diversity
        unique_extensions = list(set(file_extensions))
        features['num_file_types'] = len(unique_extensions)
        features['file_types'] = unique_extensions[:10]  # Top 10 types
        
        # File depth analysis
        depths = []
        for path in file_paths:
            depth = len(Path(path).parts) - 1  # Subtract 1 for filename
            depths.append(depth)
        
        if depths:
            features['avg_file_depth'] = np.mean(depths)
            features['max_file_depth'] = np.max(depths)
            features['min_file_depth'] = np.min(depths)
        else:
            features['avg_file_depth'] = 0
            features['max_file_depth'] = 0
            features['min_file_depth'] = 0
        
        # Change distribution
        if features['total_changes'] > 0:
            features['additions_ratio'] = features['total_additions'] / features['total_changes']
            features['deletions_ratio'] = features['total_deletions'] / features['total_changes']
        else:
            features['additions_ratio'] = 0
            features['deletions_ratio'] = 0
        
        # File status analysis
        status_counts = {}
        for file_info in files_data:
            status = file_info.get('status', 'modified')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        features['added_files'] = status_counts.get('added', 0)
        features['modified_files'] = status_counts.get('modified', 0)
        features['deleted_files'] = status_counts.get('removed', 0)
        features['renamed_files'] = status_counts.get('renamed', 0)
        
        # Large file changes indicator
        large_changes = sum(1 for f in files_data if f.get('changes', 0) > 100)
        features['large_change_files'] = large_changes
        features['has_large_changes'] = large_changes > 0
        
        return features
    
    def _get_empty_file_features(self) -> Dict[str, Any]:
        """Tr·∫£ v·ªÅ features m·∫∑c ƒë·ªãnh khi kh√¥ng c√≥ file data"""
        return {
            'num_files': 0,
            'total_additions': 0,
            'total_deletions': 0,
            'total_changes': 0,
            'num_file_types': 0,
            'file_types': [],
            'avg_file_depth': 0,
            'max_file_depth': 0,
            'min_file_depth': 0,
            'additions_ratio': 0,
            'deletions_ratio': 0,            'added_files': 0,
            'modified_files': 0,
            'deleted_files': 0,
            'renamed_files': 0,
            'large_change_files': 0,
            'has_large_changes': False
        }
    
    def extract_author_features(self, author_info, commit_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ th√¥ng tin author
        
        Args:
            author_info: Dict v·ªõi keys: login, name, email, etc. HO·∫∂C string author name
            commit_history: L·ªãch s·ª≠ commit g·∫ßn ƒë√¢y c·ªßa author (optional)
        """
        features = {}
        
        # Handle both dict and string input
        if isinstance(author_info, str):
            # Simple string author name
            features['author_login'] = author_info
            features['author_name'] = author_info
            features['author_email'] = ''
        else:
            # Dict author info
            features['author_login'] = author_info.get('login', 'unknown')
            features['author_name'] = author_info.get('name', '')
            features['author_email'] = author_info.get('email', '')
        
        # Author activity pattern (n·∫øu c√≥ l·ªãch s·ª≠)
        if commit_history:
            features['recent_commits_count'] = len(commit_history)
            
            # T√≠nh average commit size
            recent_changes = [c.get('stats', {}).get('total', 0) for c in commit_history]
            features['avg_recent_commit_size'] = np.mean(recent_changes) if recent_changes else 0
            
            # Frequency pattern
            if len(commit_history) >= 2:
                timestamps = [c.get('timestamp') for c in commit_history if c.get('timestamp')]
                if len(timestamps) >= 2:
                    # Calculate time between commits
                    time_diffs = []
                    for i in range(1, len(timestamps)):
                        try:
                            t1 = datetime.fromisoformat(timestamps[i-1].replace('Z', '+00:00'))
                            t2 = datetime.fromisoformat(timestamps[i].replace('Z', '+00:00'))
                            diff_hours = abs((t2 - t1).total_seconds() / 3600)
                            time_diffs.append(diff_hours)
                        except:
                            continue
                    
                    if time_diffs:
                        features['avg_commit_interval_hours'] = np.mean(time_diffs)
                        features['commit_frequency_score'] = min(24 / np.mean(time_diffs), 10) if np.mean(time_diffs) > 0 else 0
                    else:
                        features['avg_commit_interval_hours'] = 24
                        features['commit_frequency_score'] = 1
                else:
                    features['avg_commit_interval_hours'] = 24
                    features['commit_frequency_score'] = 1
            else:
                features['avg_commit_interval_hours'] = 24
                features['commit_frequency_score'] = 1
        else:
            features['recent_commits_count'] = 0
            features['avg_recent_commit_size'] = 0
            features['avg_commit_interval_hours'] = 24
            features['commit_frequency_score'] = 1
        
        return features
    
    def extract_timestamp_features(self, timestamp: str) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ timestamp
        """
        features = {}
        
        try:
            # Parse timestamp
            if timestamp.endswith('Z'):
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = datetime.fromisoformat(timestamp)
            
            # Time-based features
            features['hour_of_day'] = dt.hour
            features['day_of_week'] = dt.weekday()  # 0 = Monday
            features['day_of_month'] = dt.day
            features['month'] = dt.month
            features['year'] = dt.year
            
            # Derived features
            features['is_weekend'] = dt.weekday() >= 5
            features['is_business_hours'] = 9 <= dt.hour <= 17
            features['is_late_night'] = dt.hour >= 22 or dt.hour <= 6
            
            # Season (approximate)
            if dt.month in [12, 1, 2]:
                features['season'] = 'winter'
            elif dt.month in [3, 4, 5]:
                features['season'] = 'spring'
            elif dt.month in [6, 7, 8]:
                features['season'] = 'summer'
            else:
                features['season'] = 'fall'
                
        except Exception as e:
            # Default values if parsing fails
            features.update({
                'hour_of_day': 12,
                'day_of_week': 0,
                'day_of_month': 1,
                'month': 1,
                'year': 2024,
                'is_weekend': False,
                'is_business_hours': True,
                'is_late_night': False,
                'season': 'spring'
            })
        
        return features
    
    def create_feature_engineering(self, file_features: Dict, author_features: Dict, timestamp_features: Dict) -> Dict[str, Any]:
        """
        T·∫°o c√°c feature engineering ph·ª©c t·∫°p h∆°n
        """
        engineered = {}
        
        # Commit complexity score
        complexity_score = 0
        complexity_score += min(file_features['num_files'] / 10, 1.0) * 0.3  # File count impact
        complexity_score += min(file_features['total_changes'] / 1000, 1.0) * 0.4  # Change size impact
        complexity_score += min(file_features['num_file_types'] / 5, 1.0) * 0.2  # Diversity impact
        complexity_score += min(file_features['max_file_depth'] / 10, 1.0) * 0.1  # Depth impact
        engineered['complexity_score'] = complexity_score
        
        # Risk assessment
        risk_score = 0
        risk_score += file_features['has_large_changes'] * 0.3
        risk_score += (file_features['deleted_files'] / max(file_features['num_files'], 1)) * 0.2
        risk_score += min(author_features['commit_frequency_score'] / 5, 1.0) * 0.2  # High frequency = higher risk
        risk_score += timestamp_features['is_late_night'] * 0.1
        risk_score += (timestamp_features['is_weekend'] and not timestamp_features['is_business_hours']) * 0.2
        engineered['risk_score'] = min(risk_score, 1.0)
        
        # Urgency indicators
        urgency_score = 0
        urgency_score += timestamp_features['is_late_night'] * 0.4
        urgency_score += timestamp_features['is_weekend'] * 0.3
        urgency_score += (author_features['commit_frequency_score'] > 5) * 0.3
        engineered['urgency_score'] = min(urgency_score, 1.0)
        
        # Code churn metrics
        if file_features['total_changes'] > 0:
            churn_ratio = (file_features['total_additions'] + file_features['total_deletions']) / file_features['total_changes']
            engineered['code_churn'] = min(churn_ratio, 2.0)
        else:
            engineered['code_churn'] = 0
        
        # File type hotspots (common file types that often have issues)
        risky_extensions = ['js', 'ts', 'py', 'java', 'cpp', 'c', 'php']
        config_extensions = ['json', 'xml', 'yml', 'yaml', 'cfg', 'conf']
        doc_extensions = ['md', 'txt', 'rst', 'doc']
        
        engineered['touches_risky_files'] = any(ext in risky_extensions for ext in file_features['file_types'])
        engineered['touches_config_files'] = any(ext in config_extensions for ext in file_features['file_types'])
        engineered['touches_doc_files'] = any(ext in doc_extensions for ext in file_features['file_types'])
        
        return engineered
    
    def fit(self, metadata_samples: List[Dict]) -> None:
        """
        Fit c√°c encoders v√† scalers v·ªõi training data
        """
        print("üîß Fitting metadata processors...")
        
        # Collect all features
        all_numerical_features = []
        all_categorical_features = {
            'authors': [],
            'file_types': [],
            'seasons': []
        }
        
        for sample in metadata_samples:
            # Extract features
            file_features = self.extract_file_features(sample.get('files', []))
            author_features = self.extract_author_features(
                sample.get('author', {}), 
                sample.get('commit_history', [])
            )
            timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
            engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
            
            # Collect numerical features
            numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
            all_numerical_features.append(numerical)
            
            # Collect categorical features
            all_categorical_features['authors'].append(author_features['author_login'])
            all_categorical_features['file_types'].extend(file_features['file_types'])
            all_categorical_features['seasons'].append(timestamp_features['season'])
        
        # Fit scalers
        if self.normalize_features and all_numerical_features:
            numerical_array = np.array(all_numerical_features)
            self.numerical_scaler.fit(numerical_array)
        
        # Fit encoders
        if all_categorical_features['authors']:
            unique_authors = list(set(all_categorical_features['authors']))[:self.max_authors]
            self.author_encoder.fit(unique_authors + ['<UNK>'])
        
        if all_categorical_features['file_types']:
            unique_file_types = list(set(all_categorical_features['file_types']))
            self.file_type_encoder.fit(unique_file_types + ['<UNK>'])
        
        # Fit file path vectorizer
        all_file_paths = []
        for sample in metadata_samples:
            files = sample.get('files', [])
            paths = [f.get('filename', '') for f in files]
            all_file_paths.extend(paths)
        
        if all_file_paths:
            self.file_path_vectorizer.fit(all_file_paths)
        
        self.is_fitted = True
        print("‚úÖ Metadata processors fitted successfully")
    
    def _get_numerical_features(self, file_features: Dict, author_features: Dict, 
                               timestamp_features: Dict, engineered_features: Dict) -> List[float]:
        """
        L·∫•y t·∫•t c·∫£ numerical features th√†nh m·ªôt vector
        """
        features = []
        
        # File features
        features.extend([
            file_features['num_files'],
            file_features['total_additions'],
            file_features['total_deletions'],
            file_features['total_changes'],
            file_features['num_file_types'],
            file_features['avg_file_depth'],
            file_features['max_file_depth'],
            file_features['min_file_depth'],
            file_features['additions_ratio'],
            file_features['deletions_ratio'],
            file_features['added_files'],
            file_features['modified_files'],
            file_features['deleted_files'],
            file_features['renamed_files'],
            file_features['large_change_files'],
            float(file_features['has_large_changes'])
        ])
        
        # Author features
        features.extend([
            author_features['recent_commits_count'],
            author_features['avg_recent_commit_size'],
            author_features['avg_commit_interval_hours'],
            author_features['commit_frequency_score']
        ])
        
        # Timestamp features
        features.extend([
            timestamp_features['hour_of_day'],
            timestamp_features['day_of_week'],
            timestamp_features['day_of_month'],
            timestamp_features['month'],
            float(timestamp_features['is_weekend']),
            float(timestamp_features['is_business_hours']),
            float(timestamp_features['is_late_night'])
        ])
        
        # Engineered features
        features.extend([
            engineered_features['complexity_score'],
            engineered_features['risk_score'],
            engineered_features['urgency_score'],
            engineered_features['code_churn'],
            float(engineered_features['touches_risky_files']),
            float(engineered_features['touches_config_files']),
            float(engineered_features['touches_doc_files'])
        ])
        
        return features
    
    def process_sample(self, sample: Dict) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt sample metadata
        """
        if not self.is_fitted:
            raise ValueError("MetadataProcessor must be fitted before processing samples")
        
        # Extract features
        file_features = self.extract_file_features(sample.get('files', []))
        author_features = self.extract_author_features(
            sample.get('author', {}), 
            sample.get('commit_history', [])
        )
        timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
        engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
        
        result = {}
        
        # Numerical features
        numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
        if self.normalize_features:
            numerical = self.numerical_scaler.transform([numerical])[0]
        result['numerical_features'] = torch.tensor(numerical, dtype=torch.float32)
        
        # Categorical features
        # Author encoding
        author_login = author_features['author_login']
        try:
            author_encoded = self.author_encoder.transform([author_login])[0]
        except ValueError:
            author_encoded = self.author_encoder.transform(['<UNK>'])[0]
        result['author_encoded'] = torch.tensor(author_encoded, dtype=torch.long)
        
        # Season encoding
        season_map = {'spring': 0, 'summer': 1, 'fall': 2, 'winter': 3}
        result['season_encoded'] = torch.tensor(season_map.get(timestamp_features['season'], 0), dtype=torch.long)
          # File types encoding (one-hot or multi-hot)
        try:
            # Get number of classes from fitted encoder
            num_classes = len(self.file_type_encoder.classes_)
        except AttributeError:
            # Fallback if encoder is not fitted or doesn't have classes_ attribute
            num_classes = 10  # Default reasonable size
            
        file_type_vector = np.zeros(num_classes)
        for file_type in file_features['file_types']:
            try:
                idx = self.file_type_encoder.transform([file_type])[0]
                if idx < num_classes:  # Safety check
                    file_type_vector[idx] = 1
            except (ValueError, AttributeError):
                continue
        result['file_types_encoded'] = torch.tensor(file_type_vector, dtype=torch.float32)
        
        return result
    
    def process_batch(self, samples: List[Dict]) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt batch samples
        """
        batch_results = {
            'numerical_features': [],
            'author_encoded': [],
            'season_encoded': [],
            'file_types_encoded': []
        }
        
        for sample in samples:
            processed = self.process_sample(sample)
            for key, value in processed.items():
                batch_results[key].append(value)
        
        # Stack tensors
        for key in batch_results:
            batch_results[key] = torch.stack(batch_results[key])
        
        return batch_results
    
    def get_feature_dimensions(self) -> Dict[str, int]:
        """
        Tr·∫£ v·ªÅ dimensions c·ªßa c√°c feature types
        """
        return {
            'numerical_dim': 33,  # Total numerical features
            'author_vocab_size': len(self.author_encoder.classes_) if hasattr(self.author_encoder, 'classes_') else 1000,
            'season_vocab_size': 4,
            'file_types_dim': len(self.file_type_encoder.classes_) if hasattr(self.file_type_encoder, 'classes_') else 100
        }

```

### backend\ai\multimodal_fusion\data_preprocessing\minimal_enhanced_text_processor.py
```py
"""
Minimal Enhanced Text Processor
Provides basic NLTK functionality without complex dependencies
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Try minimal NLTK imports
try:
    # Only import basic tokenization without sklearn dependencies
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    import nltk
    
    # Download only essential data
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
    
    NLTK_BASIC = True
    logger.info("Basic NLTK functionality available")
except Exception as e:
    NLTK_BASIC = False
    logger.warning(f"NLTK basic features not available: {e}")

# Try TextBlob for sentiment
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available")

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available")


class MinimalEnhancedTextProcessor:
    """
    Minimal Enhanced Text Processor with basic NLTK support
    Focuses on essential improvements without complex dependencies
    """
    
    def __init__(self, 
                 method: str = "lstm",
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_sentiment: bool = True,
                 enable_advanced_cleaning: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name
            enable_sentiment: Enable sentiment analysis with TextBlob
            enable_advanced_cleaning: Enable advanced text cleaning
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        self.enable_sentiment = enable_sentiment
        self.enable_advanced_cleaning = enable_advanced_cleaning
        
        # Initialize stopwords
        self._init_stopwords()
        
        # Initialize model components
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            self._init_lstm_components()
    
    def _init_stopwords(self):
        """Initialize stopwords with fallback"""
        if NLTK_BASIC:
            try:
                self.stop_words = set(stopwords.words('english'))
                logger.info(f"Loaded {len(self.stop_words)} NLTK stopwords")
            except Exception as e:
                logger.warning(f"Failed to load NLTK stopwords: {e}")
                self.stop_words = self._get_basic_stopwords()
        else:
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Basic stopwords list"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours'
        ])
    
    def _init_lstm_components(self):
        """Initialize LSTM components"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128
    
    def enhanced_tokenize(self, text: str) -> List[str]:
        """Enhanced tokenization with NLTK if available"""
        if NLTK_BASIC:
            try:
                tokens = word_tokenize(text.lower())
                return [token for token in tokens if token.isalpha() and len(token) > 1]
            except Exception as e:
                logger.warning(f"NLTK tokenization failed: {e}")
        
        # Fallback to simple tokenization
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        return [word for word in text.split() if len(word) > 1]
    
    def advanced_clean_commit_message(self, text: str) -> str:
        """Advanced commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        original_text = text
        
        # Basic cleaning
        text = self.clean_commit_message(text)
        
        if not self.enable_advanced_cleaning:
            return text
        
        # Advanced cleaning patterns
        advanced_patterns = [
            # Remove version numbers
            (r'\bv?\d+\.\d+(\.\d+)?(-\w+)?\b', ''),
            # Remove file extensions in isolation
            (r'\b\w+\.(js|py|html|css|md|txt|json|xml|yml|yaml)\b', ''),
            # Remove common dev terms that add noise
            (r'\b(eslint|prettier|webpack|babel|npm|yarn|pip)\b', ''),
            # Remove brackets with single words
            (r'\[\w+\]', ''),
            # Remove parentheses with single words
            (r'\(\w+\)', ''),
            # Clean up multiple spaces and special chars
            (r'[^\w\s\.\!\?\,\:\;\-]', ' '),
            (r'\s+', ' '),
        ]
        
        for pattern, replacement in advanced_patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        text = text.strip()
        
        # If cleaning removed too much, return original cleaned version
        if len(text) < len(original_text) * 0.3:
            return self.clean_commit_message(original_text)
        
        return text
    
    def clean_commit_message(self, text: str) -> str:
        """Basic commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_enhanced_features(self, text: str) -> Dict[str, any]:
        """Extract enhanced features with sentiment analysis"""
        features = self.extract_basic_features(text)
        
        if not text or not isinstance(text, str):
            return features
        
        # Add sentiment analysis if available
        if self.enable_sentiment and TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                features['sentiment_polarity'] = blob.sentiment.polarity
                features['sentiment_subjectivity'] = blob.sentiment.subjectivity
                
                # Categorize sentiment
                polarity = blob.sentiment.polarity
                if polarity > 0.1:
                    features['sentiment_category'] = 'positive'
                elif polarity < -0.1:
                    features['sentiment_category'] = 'negative'
                else:
                    features['sentiment_category'] = 'neutral'
                    
            except Exception as e:
                logger.warning(f"Sentiment analysis failed: {e}")
                features['sentiment_polarity'] = 0.0
                features['sentiment_subjectivity'] = 0.0
                features['sentiment_category'] = 'neutral'
        
        # Enhanced text statistics
        words = text.split()
        if words:
            features['avg_word_length'] = np.mean([len(word) for word in words])
            features['max_word_length'] = max(len(word) for word in words)
            features['unique_word_ratio'] = len(set(words)) / len(words)
        else:
            features['avg_word_length'] = 0
            features['max_word_length'] = 0
            features['unique_word_ratio'] = 0
        
        # Enhanced keyword detection
        technical_keywords = ['api', 'database', 'server', 'client', 'config', 'auth', 'security', 'performance']
        ui_keywords = ['ui', 'interface', 'design', 'layout', 'style', 'theme', 'responsive']
        testing_keywords = ['test', 'spec', 'mock', 'coverage', 'unit', 'integration', 'e2e']
        
        text_lower = text.lower()
        features['has_technical_keywords'] = any(kw in text_lower for kw in technical_keywords)
        features['has_ui_keywords'] = any(kw in text_lower for kw in ui_keywords)
        features['has_testing_keywords'] = any(kw in text_lower for kw in testing_keywords)
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """Extract basic text features"""
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'punctuation_count', 'has_commit_type', 'commit_type_prefix',
                'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords'
            ]}
        
        # Basic statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        text_lower = text.lower()
        features['has_commit_type'] = any(text_lower.startswith(ct + ':') or text_lower.startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type
        for ct in commit_types:
            if text_lower.startswith(ct + ':') or text_lower.startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'guide']
        
        features['has_bug_keywords'] = any(kw in text_lower for kw in bug_keywords)
        features['has_feature_keywords'] = any(kw in text_lower for kw in feature_keywords)
        features['has_doc_keywords'] = any(kw in text_lower for kw in doc_keywords)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """Build vocabulary for LSTM method"""
        if self.method != "lstm":
            return
        
        logger.info("üî§ Building enhanced vocabulary...")
        
        for text in texts:
            cleaned_text = self.advanced_clean_commit_message(text)
            tokens = self.enhanced_tokenize(cleaned_text)
            tokens = [token for token in tokens if token not in self.stop_words]
            self.word_counts.update(tokens)
        
        # Build vocabulary
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"‚úÖ Enhanced vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """Enhanced LSTM encoding"""
        cleaned_text = self.advanced_clean_commit_message(text)
        tokens = self.enhanced_tokenize(cleaned_text)
        tokens = [token for token in tokens if token not in self.stop_words]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]
        
        # Add start and end tokens
        indices = [2] + indices + [3]
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))
        
        return torch.tensor(indices, dtype=torch.long)
    
    def fit(self, texts: List[str]) -> 'MinimalEnhancedTextProcessor':
        """Fit the processor to training data"""
        logger.info("üöÄ Fitting minimal enhanced text processor...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
        
        logger.info("‚úÖ Text processor fitted successfully")
        return self
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        for text in texts:
            basic_features = self.extract_basic_features(text)
            enhanced_features = self.extract_enhanced_features(text)
            
            results['text_features'].append(basic_features)
            results['enhanced_features'].append(enhanced_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        
        return results
    
    # Keep essential methods from original processor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method"""
        cleaned_text = self.advanced_clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\text_processor.py
```py
"""
Text Processor for Multi-Modal Fusion Network
X·ª≠ l√Ω v√† chu·∫©n b·ªã d·ªØ li·ªáu vƒÉn b·∫£n t·ª´ commit messages
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from collections import Counter

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Transformers not available. Using simple tokenization only.")

# Optional NLTK import
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("NLTK not available. Using simple text processing.")

class TextProcessor:
    """
    L·ªõp x·ª≠ l√Ω vƒÉn b·∫£n cho commit messages
    H·ªó tr·ª£ c·∫£ tokenization ƒë∆°n gi·∫£n v√† pre-trained embeddings
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased"):
        """
        Args:
            method: Ph∆∞∆°ng ph√°p x·ª≠ l√Ω ("lstm", "distilbert", "transformer")
            vocab_size: K√≠ch th∆∞·ªõc vocabulary cho LSTM
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
            pretrained_model: T√™n pre-trained model n·∫øu d√πng transformer
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
          # Initialize components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
            except Exception as e:
                print(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
            
        # Download NLTK data if needed
        if NLTK_AVAILABLE:
            try:
                nltk.data.find('tokenizers/punkt')
                nltk.data.find('corpora/stopwords')
            except LookupError:
                nltk.download('punkt')
                nltk.download('stopwords')
                
            self.stop_words = set(stopwords.words('english'))
        else:
            # Simple fallback stopwords
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'])
        
    def _tokenize_text(self, text: str) -> List[str]:
        """
        Tokenize text with fallback if NLTK not available
        """
        if NLTK_AVAILABLE:
            return word_tokenize(text)
        else:
            # Simple tokenization fallback
            import string            # Remove punctuation and split by whitespace
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def build_vocab(self, texts: List[str], vocab_size: int = None):
        """
        Build vocabulary from a list of texts
        """
        if vocab_size:
            self.vocab_size = vocab_size
            
        # Count words
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self._tokenize_text(cleaned_text.lower())
            self.word_counts.update(tokens)
        
        # Build vocabulary with most common words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # Reserve space for special tokens
        
        for word, count in most_common:
            if word not in self.word_to_idx:
                idx = len(self.word_to_idx)
                self.word_to_idx[word] = idx
                self.idx_to_word[idx] = word
        
        print(f"Built vocabulary with {len(self.word_to_idx)} words")
        
    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx
    
    def clean_commit_message(self, text: str) -> str:
        """
        L√†m s·∫°ch commit message
        """
        if not text or not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references like #123, Fixes #456
        text = re.sub(r'(closes?|fixes?|resolves?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s\.\!\?\,\:\;\-\(\)]', ' ', text)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_commit_features(self, text: str) -> Dict[str, any]:
        """
        Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ commit message
        """
        features = {}
        
        # Basic features
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':'):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
            
        # Keywords detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation']
        
        features['has_bug_keywords'] = any(keyword in text.lower() for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text.lower() for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text.lower() for keyword in doc_keywords)
        
        # Sentiment indicators
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        features['positive_sentiment'] = any(word in text.lower() for word in positive_words)
        features['negative_sentiment'] = any(word in text.lower() for word in negative_words)
        features['urgent_sentiment'] = any(word in text.lower() for word in urgent_words)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        X√¢y d·ª±ng vocabulary cho LSTM method
        """
        if self.method != "lstm":
            return
            
        print("üî§ Building vocabulary for text processing...")
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            words = self._tokenize_text(cleaned_text.lower())
            # Filter out stop words and very short words
            words = [w for w in words if w not in self.stop_words and len(w) > 1]
            self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # -4 for special tokens
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"‚úÖ Vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Encode text cho LSTM method
        """
        cleaned_text = self.clean_commit_message(text)
        words = self._tokenize_text(cleaned_text.lower())
        words = [w for w in words if w not in self.stop_words and len(w) > 1]
        
        # Convert to indices
        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """
        Encode text cho transformer method
        """
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """
        L·∫•y embeddings cho list of texts
        """
        if self.method == "lstm":
            # Return token indices for LSTM
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            # Get contextual embeddings
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    # Use [CLS] token embedding or mean pooling
                    embedding = outputs.last_hidden_state[:, 0, :]  # CLS token
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt batch texts
        """
        results = {
            'text_features': [],
            'embeddings': None,
            'metadata_features': []
        }
        
        # Extract text features
        for text in texts:
            features = self.extract_commit_features(text)
            results['text_features'].append(features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def fit(self, texts: List[str]) -> 'TextProcessor':
        """
        Fit the text processor to the training data
        This method builds vocabulary for LSTM method and prepares the processor
        """
        if self.method == "lstm":
            self.build_vocabulary(texts)
        # For transformer methods, no fitting is needed as they use pre-trained models
        return self
    
    def get_vocab_size(self) -> int:
        """
        Tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc vocabulary
        """
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """
        Tr·∫£ v·ªÅ dimension c·ªßa embeddings
        """
        return self.embed_dim

```

### backend\ai\multimodal_fusion\data_preprocessing\__init__.py
```py
"""
Data Preprocessing Module Initialization
"""

from .metadata_processor import MetadataProcessor

# Import text processors with fallback
try:
    from .minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
    ENHANCED_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Enhanced text processor not available: {e}")
    ENHANCED_PROCESSOR_AVAILABLE = False

try:
    from .text_processor import TextProcessor
    BASIC_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Basic text processor not available: {e}")
    BASIC_PROCESSOR_AVAILABLE = False
    # Use minimal processor as fallback
    if ENHANCED_PROCESSOR_AVAILABLE:
        TextProcessor = MinimalEnhancedTextProcessor

__all__ = ["MetadataProcessor"]

if ENHANCED_PROCESSOR_AVAILABLE:
    __all__.append("MinimalEnhancedTextProcessor")
if BASIC_PROCESSOR_AVAILABLE:
    __all__.append("TextProcessor")

```

### backend\ai\multimodal_fusion\evaluation\interpretability.py
```py

```

### backend\ai\multimodal_fusion\evaluation\metrics_calculator.py
```py

```

### backend\ai\multimodal_fusion\evaluation\visualization.py
```py

```

### backend\ai\multimodal_fusion\evaluation\__init__.py
```py

```

### backend\ai\multimodal_fusion\losses\multi_task_losses.py
```py

```

### backend\ai\multimodal_fusion\losses\__init__.py
```py

```

### backend\ai\multimodal_fusion\models\baselines.py
```py

```

### backend\ai\multimodal_fusion\models\multimodal_fusion.py
```py
"""
Multi-Modal Fusion Network Architecture
M√¥ h√¨nh k·∫øt h·ª£p th√¥ng tin vƒÉn b·∫£n v√† metadata v·ªõi c√°c c∆° ch·∫ø fusion ti√™n ti·∫øn
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class CrossAttentionFusion(nn.Module):
    """
    Cross-Attention mechanism cho fusion gi·ªØa text v√† metadata
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128, num_heads: int = 4):
        super(CrossAttentionFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Feed forward
        self.ff = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Project to same dimension
        text_proj = self.text_proj(text_features)  # (batch_size, hidden_dim)
        metadata_proj = self.metadata_proj(metadata_features)  # (batch_size, hidden_dim)
        
        # Add sequence dimension for attention
        text_seq = text_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        metadata_seq = metadata_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        
        # Cross attention: text attends to metadata
        text_attended, _ = self.attention(text_seq, metadata_seq, metadata_seq)
        text_attended = text_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Cross attention: metadata attends to text
        metadata_attended, _ = self.attention(metadata_seq, text_seq, text_seq)
        metadata_attended = metadata_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Combine and normalize
        combined = self.norm1(text_attended + metadata_attended)
        
        # Feed forward
        output = self.ff(combined)
        output = self.norm2(combined + output)
        
        return output

class GatedFusion(nn.Module):
    """
    Gated Multimodal Units (GMU) for fusion
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128):
        super(GatedFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Gating mechanism
        self.gate_text = nn.Linear(text_dim + metadata_dim, hidden_dim)
        self.gate_metadata = nn.Linear(text_dim + metadata_dim, hidden_dim)
        
        # Final projection
        self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Concatenate for gating
        combined = torch.cat([text_features, metadata_features], dim=1)
        
        # Compute gates
        text_gate = torch.sigmoid(self.gate_text(combined))
        metadata_gate = torch.sigmoid(self.gate_metadata(combined))
        
        # Project features
        text_proj = self.text_proj(text_features)
        metadata_proj = self.metadata_proj(metadata_features)
        
        # Apply gates
        gated_text = text_gate * text_proj
        gated_metadata = metadata_gate * metadata_proj
        
        # Combine and project
        fused = torch.cat([gated_text, gated_metadata], dim=1)
        output = self.output_proj(fused)
        
        return output

class TextBranch(nn.Module):
    """
    Nh√°nh x·ª≠ l√Ω vƒÉn b·∫£n v·ªõi LSTM/GRU v√† Attention ho·∫∑c Transformer
    """
    
    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, 
                 method: str = "lstm", pretrained_dim: Optional[int] = None):
        super(TextBranch, self).__init__()
        
        self.method = method
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        if method == "lstm":
            # LSTM-based processing
            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
            self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_dim * 2,
                num_heads=4,
                batch_first=True
            )
            self.output_dim = hidden_dim * 2
            
        elif method in ["distilbert", "transformer"]:
            # Transformer-based processing
            if pretrained_dim is None:
                raise ValueError("pretrained_dim must be provided for transformer method")
            
            self.projection = nn.Linear(pretrained_dim, hidden_dim)
            self.transformer_layer = nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=4,
                dim_feedforward=hidden_dim * 2,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)
            self.output_dim = hidden_dim
            
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, text_input: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            text_input: Token IDs (batch_size, seq_len) for LSTM or embeddings (batch_size, embed_dim) for transformer
            attention_mask: Attention mask (batch_size, seq_len) - optional
        Returns:
            text_features: (batch_size, output_dim)
        """
        if self.method == "lstm":
            # LSTM processing
            embedded = self.embedding(text_input)  # (batch_size, seq_len, embed_dim)
            lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)
            
            # Self-attention
            attended, _ = self.attention(lstm_out, lstm_out, lstm_out)
            
            # Global max pooling
            pooled = torch.max(attended, dim=1)[0]  # (batch_size, hidden_dim * 2)
            
        elif self.method in ["distilbert", "transformer"]:
            # Transformer processing
            if text_input.dim() == 2 and text_input.size(1) > 1:
                # Multiple embeddings case
                projected = self.projection(text_input)  # (batch_size, seq_len, hidden_dim)
                output = self.transformer(projected)  # (batch_size, seq_len, hidden_dim)
                pooled = torch.mean(output, dim=1)  # (batch_size, hidden_dim)
            else:
                # Single embedding case
                if text_input.dim() == 2:
                    projected = self.projection(text_input)  # (batch_size, hidden_dim)
                else:
                    projected = self.projection(text_input.unsqueeze(1))  # (batch_size, 1, hidden_dim)
                    projected = projected.squeeze(1)  # (batch_size, hidden_dim)
                pooled = projected
        
        return self.dropout(pooled)

class MetadataBranchV2(nn.Module):
    """
    Flexible metadata branch with configurable categorical and numerical features
    """
    
    def __init__(self, 
                 categorical_dims: Dict[str, int],
                 numerical_features: List[str],
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranchV2, self).__init__()
        
        self.categorical_dims = categorical_dims
        self.numerical_features = numerical_features
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.categorical_embeddings = nn.ModuleDict()
        for feature_name, vocab_size in categorical_dims.items():
            self.categorical_embeddings[feature_name] = nn.Embedding(vocab_size, embed_dim)
          # Projection for numerical features - dynamic sizing
        # We'll set this in the first forward pass when we know the actual dimension
        self.numerical_proj = None
        self.numerical_dim = None
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * len(categorical_dims)
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing features for categorical and numerical data
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        feature_list = []        # Process numerical features
        if 'numerical_features' in metadata_features:
            # Handle case where we have a single tensor with all numerical features
            numerical_tensor = metadata_features['numerical_features']
            if len(numerical_tensor.shape) == 1:
                numerical_tensor = numerical_tensor.unsqueeze(0)  # Add batch dim if missing
            
            # Initialize numerical projection layer if not already done
            if self.numerical_proj is None:
                self.numerical_dim = numerical_tensor.shape[-1]
                self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
            
            numerical_proj = self.numerical_proj(numerical_tensor)
            feature_list.append(numerical_proj)
        else:
            # Handle case where numerical features are split into individual tensors
            numerical_data = []
            for feature_name in self.numerical_features:
                if feature_name in metadata_features:
                    numerical_data.append(metadata_features[feature_name].unsqueeze(-1))
            
            if numerical_data:
                numerical_tensor = torch.cat(numerical_data, dim=1)
                
                # Initialize numerical projection layer if not already done
                if self.numerical_proj is None:
                    self.numerical_dim = numerical_tensor.shape[-1]
                    self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
                
                numerical_proj = self.numerical_proj(numerical_tensor)
                feature_list.append(numerical_proj)
        
        # Process categorical embeddings
        for feature_name, embedding_layer in self.categorical_embeddings.items():
            if feature_name in metadata_features:
                embed = embedding_layer(metadata_features[feature_name])
                feature_list.append(embed)
          # Combine all features
        if feature_list:
            combined = torch.cat(feature_list, dim=1)
        else:
            # Fallback if no features found
            batch_size = next(iter(metadata_features.values())).size(0)
            device = next(iter(metadata_features.values())).device
            combined = torch.zeros(batch_size, self.hidden_dim, device=device)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class MetadataBranch(nn.Module):
    """
    Nh√°nh x·ª≠ l√Ω metadata v·ªõi embeddings v√† dense layers
    """
    
    def __init__(self, 
                 numerical_dim: int,
                 author_vocab_size: int,
                 season_vocab_size: int,
                 file_types_dim: int,
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranch, self).__init__()
        
        self.numerical_dim = numerical_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.author_embedding = nn.Embedding(author_vocab_size, embed_dim)
        self.season_embedding = nn.Embedding(season_vocab_size, embed_dim)
        
        # Projection for numerical features
        self.numerical_proj = nn.Linear(numerical_dim, hidden_dim)
        
        # Projection for file types (multi-hot encoded)
        self.file_types_proj = nn.Linear(file_types_dim, embed_dim)
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * 3  # numerical + author + season + file_types
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing numerical_features, author_encoded, season_encoded, file_types_encoded
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        # Process numerical features
        numerical = self.numerical_proj(metadata_features['numerical_features'])
        
        # Process categorical embeddings
        author_embed = self.author_embedding(metadata_features['author_encoded'])
        season_embed = self.season_embedding(metadata_features['season_encoded'])
        file_types_embed = self.file_types_proj(metadata_features['file_types_encoded'])
        
        # Combine all features
        combined = torch.cat([numerical, author_embed, season_embed, file_types_embed], dim=1)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class TaskSpecificHead(nn.Module):
    """
    Task-specific classification head
    """
    
    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 64):
        super(TaskSpecificHead, self).__init__()
        
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        return self.classifier(features)

class MultiModalFusionNetwork(nn.Module):
    """
    Main Multi-Modal Fusion Network
    """
    
    def __init__(self, config: Dict = None, **kwargs):
        """
        Initialize MultiModalFusionNetwork with flexible configuration
        
        Args:
            config: Configuration dictionary (new format)
            **kwargs: Backward compatibility parameters (old format)
        """
        super(MultiModalFusionNetwork, self).__init__()
        
        # Handle both new config format and old parameter format
        if config is not None:
            self.config = config
            
            # Extract configurations
            text_config = config['text_encoder']
            metadata_config = config['metadata_encoder']
            fusion_config = config['fusion']
            task_configs = config['task_heads']
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=text_config['vocab_size'],
                embed_dim=text_config['embedding_dim'],
                hidden_dim=text_config['hidden_dim'],
                method=text_config.get('method', 'lstm'),
                pretrained_dim=text_config.get('pretrained_dim', None)
            )
            
            # Metadata branch - use flexible version
            self.metadata_branch = MetadataBranchV2(
                categorical_dims=metadata_config['categorical_dims'],
                numerical_features=metadata_config['numerical_features'],
                embed_dim=metadata_config['embedding_dim'],
                hidden_dim=metadata_config['hidden_dim']
            )
            
            # Fusion mechanism
            fusion_method = fusion_config.get('method', 'cross_attention')
            fusion_hidden_dim = fusion_config.get('fusion_dim', 128)
            
        else:
            # Backward compatibility - use old parameter format
            text_method = kwargs.get('text_method', 'lstm')
            vocab_size = kwargs.get('vocab_size', 10000)
            text_embed_dim = kwargs.get('text_embed_dim', 128)
            text_hidden_dim = kwargs.get('text_hidden_dim', 128)
            pretrained_text_dim = kwargs.get('pretrained_text_dim', None)
            
            numerical_dim = kwargs.get('numerical_dim', 34)
            author_vocab_size = kwargs.get('author_vocab_size', 1000)
            season_vocab_size = kwargs.get('season_vocab_size', 4)
            file_types_dim = kwargs.get('file_types_dim', 100)
            metadata_embed_dim = kwargs.get('metadata_embed_dim', 64)
            metadata_hidden_dim = kwargs.get('metadata_hidden_dim', 128)
            
            fusion_method = kwargs.get('fusion_method', 'cross_attention')
            fusion_hidden_dim = kwargs.get('fusion_hidden_dim', 128)
            task_configs = kwargs.get('task_configs', {})
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=vocab_size,
                embed_dim=text_embed_dim,
                hidden_dim=text_hidden_dim,
                method=text_method,
                pretrained_dim=pretrained_text_dim
            )
            
            # Metadata branch - use old version for compatibility
            self.metadata_branch = MetadataBranch(
                numerical_dim=numerical_dim,
                author_vocab_size=author_vocab_size,
                season_vocab_size=season_vocab_size,
                file_types_dim=file_types_dim,
                embed_dim=metadata_embed_dim,
                hidden_dim=metadata_hidden_dim
            )
        
        # Common fusion setup
        if fusion_method == "cross_attention":
            self.fusion = CrossAttentionFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        elif fusion_method == "gated":
            self.fusion = GatedFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        else:  # concat
            self.fusion = None
            fusion_output_dim = self.text_branch.output_dim + self.metadata_branch.output_dim
        
        # Task-specific heads - support both formats
        self.task_heads = nn.ModuleDict()
        for task_name, task_config in task_configs.items():
            if isinstance(task_config, dict):
                if 'num_classes' in task_config:
                    num_classes = task_config['num_classes']
                elif 'classes' in task_config:
                    num_classes = len(task_config['classes'])
                else:
                    num_classes = 2  # default
            else:
                # Old format: direct number
                num_classes = task_config
            
            self.task_heads[task_name] = TaskSpecificHead(
                input_dim=fusion_output_dim,
                num_classes=num_classes
            )
    
    def forward(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor], 
                attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            text_input: Text tokens or embeddings
            metadata_input: Dict of metadata features
            attention_mask: Optional attention mask for text
            
        Returns:
            outputs: Dict mapping task names to logits
        """
        # Process text branch
        text_features = self.text_branch(text_input, attention_mask)
        
        # Process metadata branch
        metadata_features = self.metadata_branch(metadata_input)
        
        # Fusion
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            # Simple concatenation
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        # Task-specific predictions
        outputs = {}
        for task_name, head in self.task_heads.items():
            outputs[task_name] = head(fused_features)
        
        return outputs
    
    def get_fusion_features(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor],
                           attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Get fused features for analysis/visualization
        """
        text_features = self.text_branch(text_input, attention_mask)
        metadata_features = self.metadata_branch(metadata_input)
        
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        return fused_features

```

### backend\ai\multimodal_fusion\models\shared_layers.py
```py

```

### backend\ai\multimodal_fusion\models\__init__.py
```py
"""
Models Module Initialization
"""

from .multimodal_fusion import MultiModalFusionNetwork, CrossAttentionFusion, GatedFusion

__all__ = [
    "MultiModalFusionNetwork",
    "CrossAttentionFusion", 
    "GatedFusion"
]

```

### backend\ai\multimodal_fusion\scripts\train_main.py
```py

```

### backend\ai\multimodal_fusion\scripts\train_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script for Multimodal Fusion Model
Complete training pipeline for commit analysis with text + metadata fusion
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

def custom_collate_fn(batch):
    """Custom collate function to handle metadata dict structure"""
    collated = {}
    
    # Handle text features (simple tensors)
    collated['text_features'] = torch.stack([item['text_features'] for item in batch])
    
    # Handle metadata features (dict of tensors)
    metadata_keys = batch[0]['metadata_features'].keys()
    collated['metadata_features'] = {}
    for key in metadata_keys:
        collated['metadata_features'][key] = torch.stack([item['metadata_features'][key] for item in batch])
    
    # Handle labels (list of dicts)
    collated['labels'] = [item['labels'] for item in batch]
    
    # Handle text (list of strings)
    collated['text'] = [item['text'] for item in batch]
    
    return collated

# Add project paths
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai"))

# Import multimodal components
from ai.multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
from ai.multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from ai.multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
# from ai.multimodal_fusion.training.multitask_trainer import MultiTaskTrainer
# from ai.multimodal_fusion.losses.multi_task_losses import MultiTaskLoss
# from ai.multimodal_fusion.evaluation.metrics_calculator import MetricsCalculator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MultimodalDataset(Dataset):
    """Dataset for multimodal fusion training"""
    
    def __init__(self, samples, text_processor, metadata_processor, max_length=512):
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.max_length = max_length
        
        # Define label mappings for multimodal tasks
        self.task_labels = {
            'risk_prediction': {'low': 0, 'high': 1},
            'complexity_prediction': {'simple': 0, 'medium': 1, 'complex': 2},
            'hotspot_prediction': {'very_low': 0, 'low': 1, 'medium': 2, 'high': 3, 'very_high': 4},
            'urgency_prediction': {'normal': 0, 'urgent': 1}
        }
        
        logger.info(f"Created dataset with {len(samples)} samples")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
          # Process text
        commit_text = sample.get('text', '') or sample.get('message', '')
        text_features = self.text_processor.encode_text_lstm(commit_text)
          # Process metadata
        metadata = self._extract_metadata(sample)
        metadata_features = self.metadata_processor.process_sample(sample)
        
        # Generate multimodal task labels
        labels = self._generate_multimodal_labels(sample, commit_text, metadata)
        
        return {
            'text_features': text_features,
            'metadata_features': metadata_features,
            'labels': labels,
            'text': commit_text
        }
    
    def _extract_metadata(self, sample):
        """Extract metadata features from sample"""
        return {
            'author': sample.get('author', 'unknown'),
            'files_changed': len(sample.get('files_changed', [])),
            'additions': sample.get('additions', 0),
            'deletions': sample.get('deletions', 0),
            'time_of_day': sample.get('time_of_day', 12),  # hour
            'day_of_week': sample.get('day_of_week', 1),   # 1-7
            'commit_size': sample.get('additions', 0) + sample.get('deletions', 0),
            'is_merge': 'merge' in sample.get('text', '').lower()
        }
    
    def _generate_multimodal_labels(self, sample, text, metadata):
        """Generate labels for multimodal tasks based on commit analysis"""
        labels = {}
        
        # Risk prediction (high/low) - based on commit patterns
        risk_score = self._calculate_risk_score(text, metadata)
        labels['risk_prediction'] = 1 if risk_score > 0.5 else 0
        
        # Complexity prediction (simple/medium/complex) - based on changes
        complexity = self._calculate_complexity(text, metadata)
        labels['complexity_prediction'] = complexity
        
        # Hotspot prediction (very_low to very_high) - based on file patterns
        hotspot = self._calculate_hotspot_score(text, metadata)
        labels['hotspot_prediction'] = hotspot
        
        # Urgency prediction (normal/urgent) - based on keywords
        urgency = self._calculate_urgency(text, metadata)
        labels['urgency_prediction'] = urgency
        
        return labels
    
    def _calculate_risk_score(self, text, metadata):
        """Calculate risk score from commit text and metadata"""
        risk_keywords = ['fix', 'bug', 'error', 'crash', 'security', 'vulnerability', 'critical']
        risk_score = 0.0
        
        text_lower = text.lower()
        for keyword in risk_keywords:
            if keyword in text_lower:
                risk_score += 0.2
        
        # Add metadata-based risk
        if metadata['commit_size'] > 1000:  # Large commits are risky
            risk_score += 0.2
        if metadata['files_changed'] > 10:  # Many files changed
            risk_score += 0.1
            
        return min(risk_score, 1.0)
    
    def _calculate_complexity(self, text, metadata):
        """Calculate complexity level (0=simple, 1=medium, 2=complex)"""
        commit_size = metadata['commit_size']
        files_changed = metadata['files_changed']
        
        if commit_size < 50 and files_changed <= 2:
            return 0  # simple
        elif commit_size < 500 and files_changed <= 10:
            return 1  # medium
        else:
            return 2  # complex
    
    def _calculate_hotspot_score(self, text, metadata):
        """Calculate hotspot prediction (0-4 scale)"""
        # Based on files changed and commit frequency patterns
        files_changed = metadata['files_changed']
        
        if files_changed <= 1:
            return 0  # very_low
        elif files_changed <= 3:
            return 1  # low
        elif files_changed <= 7:
            return 2  # medium
        elif files_changed <= 15:
            return 3  # high
        else:
            return 4  # very_high
    
    def _calculate_urgency(self, text, metadata):
        """Calculate urgency (0=normal, 1=urgent)"""
        urgent_keywords = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediately']
        text_lower = text.lower()
        
        for keyword in urgent_keywords:
            if keyword in text_lower:
                return 1
        
        # Large commits on weekends might be urgent
        if metadata['day_of_week'] in [6, 7] and metadata['commit_size'] > 500:
            return 1
            
        return 0

def load_training_data(data_file):
    """Load and prepare training data"""
    logger.info(f"Loading training data from {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Handle different data formats
    if 'data' in data:
        samples = data['data']
    elif isinstance(data, list):
        samples = data
    else:
        samples = [data]
    
    logger.info(f"Loaded {len(samples)} samples")
    return samples

def setup_model_and_training(device, vocab_size=10000):
    """Setup model, optimizer, and training components"""
      # Model configuration
    config = {
        'text_encoder': {
            'vocab_size': vocab_size,
            'embedding_dim': 768,
            'hidden_dim': 256,
            'num_layers': 2,
            'dropout': 0.1,
            'max_length': 512,
            'method': 'lstm'
        },        'metadata_encoder': {
            'categorical_dims': {'author_encoded': 1000, 'season_encoded': 4},  # vocab sizes to match processor output
            'numerical_features': ['numerical_features'],  # single tensor from processor
            'embedding_dim': 128,
            'hidden_dim': 128,
            'dropout': 0.1
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256,
            'dropout': 0.1
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 2},
            'complexity_prediction': {'num_classes': 3},
            'hotspot_prediction': {'num_classes': 5},
            'urgency_prediction': {'num_classes': 2}
        }
    }
    
    # Initialize model
    model = MultiModalFusionNetwork(config).to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"Model created with {total_params:,} total parameters")
    logger.info(f"Trainable parameters: {trainable_params:,}")
      # Optimizer and scheduler
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
      # Loss functions
    loss_fns = {
        'risk_prediction': nn.CrossEntropyLoss().to(device),
        'complexity_prediction': nn.CrossEntropyLoss().to(device),
        'hotspot_prediction': nn.CrossEntropyLoss().to(device),
        'urgency_prediction': nn.CrossEntropyLoss().to(device)
    }
    
    return model, optimizer, scheduler, loss_fns, config

def train_epoch(model, train_loader, optimizer, loss_fns, device, scaler=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    progress_bar = tqdm(train_loader, desc="Training")
    for batch in progress_bar:
        optimizer.zero_grad()
        
        # Move data to device
        text_features = batch['text_features'].to(device)
        
        # Handle metadata features (dict of tensors from custom collate)
        metadata_features = {}
        for key, value in batch['metadata_features'].items():
            metadata_features[key] = value.to(device)
        
        # Forward pass
        if scaler:
            with torch.cuda.amp.autocast():
                outputs = model(text_features, metadata_features)
                
                # Calculate losses
                batch_loss = 0
                for task, loss_fn in loss_fns.items():
                    labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                    task_loss = loss_fn(outputs[task], labels)
                    batch_loss += task_loss
                    task_losses[task] += task_loss.item()
                    
                    # Calculate accuracy
                    _, predicted = torch.max(outputs[task], 1)
                    task_correct[task] += (predicted == labels).sum().item()
                    task_total[task] += labels.size(0)
            
            scaler.scale(batch_loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(text_features, metadata_features)
            
            # Calculate losses
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            batch_loss.backward()
            optimizer.step()
        
        total_loss += batch_loss.item()
        
        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{batch_loss.item():.4f}",
            'avg_loss': f"{total_loss/len(progress_bar):.4f}"
        })
    
    # Calculate average metrics
    avg_loss = total_loss / len(train_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def validate_epoch(model, val_loader, loss_fns, device):
    """Validate for one epoch"""
    model.eval()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            # Move data to device
            text_features = batch['text_features'].to(device)
            
            # Handle metadata features (dict of tensors from custom collate)
            metadata_features = {}
            for key, value in batch['metadata_features'].items():
                metadata_features[key] = value.to(device)
            
            # Forward pass
            outputs = model(text_features, metadata_features)
            
            # Calculate losses and metrics
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            total_loss += batch_loss.item()
    
    # Calculate average metrics
    avg_loss = total_loss / len(val_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def main():
    """Main training function"""
    logger.info("üöÄ MULTIMODAL FUSION MODEL TRAINING")
    logger.info("=" * 60)
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üîß Device: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        torch.cuda.empty_cache()
    
    # Paths
    data_file = Path(__file__).parent.parent.parent / "training_data" / "sample_preview.json"
    output_dir = Path(__file__).parent.parent.parent / "trained_models" / "multimodal_fusion"
    log_dir = Path(__file__).parent.parent.parent / "training_logs" / "multimodal_fusion"
    
    # Create directories
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load data
    if not data_file.exists():
        logger.error(f"‚ùå Dataset not found: {data_file}")
        return
    
    samples = load_training_data(data_file)
    
    # Take a subset for training (adjust as needed)
    if len(samples) > 10000:
        samples = samples[:10000]
        logger.info(f"Using subset of {len(samples)} samples for training")
      # Initialize processors
    text_processor = TextProcessor()
    metadata_processor = MetadataProcessor()
    
    # Build vocabulary from sample texts
    texts = [sample.get('text', '') or sample.get('message', '') for sample in samples]
    text_processor.build_vocab(texts, vocab_size=10000)
    
    # Fit metadata processor with samples
    logger.info("üîß Fitting metadata processor...")
    metadata_processor.fit(samples)
    
    # Create dataset
    dataset = MultimodalDataset(samples, text_processor, metadata_processor)
    
    # Split dataset
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    logger.info(f"üìä Train samples: {len(train_dataset)}")
    logger.info(f"üìä Val samples: {len(val_dataset)}")
      # Data loaders
    batch_size = 32 if device.type == 'cuda' else 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)
    
    # Setup model and training
    model, optimizer, scheduler, loss_fns, config = setup_model_and_training(
        device, vocab_size=len(text_processor.vocab)
    )
    
    # Mixed precision setup
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    # Training loop
    num_epochs = 20
    best_val_accuracy = 0
    patience = 5
    patience_counter = 0
    
    logger.info(f"üèÉ Starting training for {num_epochs} epochs")
    
    for epoch in range(num_epochs):
        logger.info(f"\nüìÖ Epoch {epoch+1}/{num_epochs}")
        
        # Train
        train_loss, train_accuracies, train_avg_acc = train_epoch(
            model, train_loader, optimizer, loss_fns, device, scaler
        )
        
        # Validate
        val_loss, val_accuracies, val_avg_acc = validate_epoch(
            model, val_loader, loss_fns, device
        )
        
        # Scheduler step
        scheduler.step(val_loss)
        
        # Log metrics
        logger.info(f"Train Loss: {train_loss:.4f}, Train Acc: {train_avg_acc:.4f}")
        logger.info(f"Val Loss: {val_loss:.4f}, Val Acc: {val_avg_acc:.4f}")
        
        for task in loss_fns.keys():
            logger.info(f"  {task}: Train {train_accuracies[task]:.4f}, Val {val_accuracies[task]:.4f}")
        
        # Save best model
        if val_avg_acc > best_val_accuracy:
            best_val_accuracy = val_avg_acc
            patience_counter = 0
            
            # Save model
            save_dict = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': config,
                'epoch': epoch + 1,
                'best_val_accuracy': best_val_accuracy,
                'val_accuracies': val_accuracies,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }
            
            torch.save(save_dict, output_dir / 'best_multimodal_fusion_model.pth')
            logger.info(f"üíæ Saved best model (Val Acc: {best_val_accuracy:.4f})")
        else:
            patience_counter += 1
            
        # Early stopping
        if patience_counter >= patience:
            logger.info(f"‚èπÔ∏è Early stopping after {patience} epochs without improvement")
            break
    
    logger.info(f"\nüéâ Training completed!")
    logger.info(f"Best validation accuracy: {best_val_accuracy:.4f}")
    
    # Save final model
    final_save_dict = {
        'model_state_dict': model.state_dict(),
        'config': config,
        'final_epoch': epoch + 1,
        'final_val_accuracy': val_avg_acc,
        'best_val_accuracy': best_val_accuracy,
        'text_processor': text_processor,
        'metadata_processor': metadata_processor
    }
    
    torch.save(final_save_dict, output_dir / 'final_multimodal_fusion_model.pth')
    logger.info(f"üíæ Saved final model")

if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\training\multitask_trainer.py
```py
"""
Multi-Task Trainer for Multi-Modal Fusion Network
Tri·ªÉn khai Joint Multi-Task Learning v·ªõi Dynamic Loss Weighting
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
from pathlib import Path
import json
import logging
from collections import defaultdict
from sklearn.metrics import classification_report, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

class MultiModalDataset(Dataset):
    """
    Dataset cho Multi-Modal Fusion Network
    """
    
    def __init__(self, samples: List[Dict], text_processor, metadata_processor, 
                 label_encoders: Dict[str, Any]):
        """
        Args:
            samples: List of samples with text, metadata, and labels
            text_processor: TextProcessor instance
            metadata_processor: MetadataProcessor instance
            label_encoders: Dict mapping task names to label encoders
        """
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.label_encoders = label_encoders
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # Process text
        text = sample.get('text', '')
        if self.text_processor.method == "lstm":
            text_input = self.text_processor.encode_text_lstm(text)
            attention_mask = None
        else:
            text_encoding = self.text_processor.encode_text_transformer(text)
            text_input = text_encoding['input_ids']
            attention_mask = text_encoding['attention_mask']
        
        # Process metadata
        metadata_input = self.metadata_processor.process_sample(sample)
        
        # Process labels
        labels = {}
        for task_name, label_value in sample.get('labels', {}).items():
            if task_name in self.label_encoders:
                try:
                    encoded_label = self.label_encoders[task_name].transform([label_value])[0]
                    labels[task_name] = torch.tensor(encoded_label, dtype=torch.long)
                except ValueError:
                    # Handle unknown labels
                    labels[task_name] = torch.tensor(0, dtype=torch.long)
        
        result = {
            'text_input': text_input,
            'metadata_input': metadata_input,
            'labels': labels
        }
        
        if attention_mask is not None:
            result['attention_mask'] = attention_mask
        
        return result

def collate_fn(batch):
    """
    Custom collate function for DataLoader
    """
    # Stack text inputs
    text_inputs = torch.stack([item['text_input'] for item in batch])
    
    # Stack attention masks if present
    attention_masks = None
    if 'attention_mask' in batch[0]:
        attention_masks = torch.stack([item['attention_mask'] for item in batch])
    
    # Stack metadata inputs
    metadata_keys = batch[0]['metadata_input'].keys()
    metadata_inputs = {}
    for key in metadata_keys:
        metadata_inputs[key] = torch.stack([item['metadata_input'][key] for item in batch])
    
    # Collect labels
    task_names = batch[0]['labels'].keys()
    labels = {}
    for task_name in task_names:
        labels[task_name] = torch.stack([item['labels'][task_name] for item in batch])
    
    result = {
        'text_input': text_inputs,
        'metadata_input': metadata_inputs,
        'labels': labels
    }
    
    if attention_masks is not None:
        result['attention_mask'] = attention_masks
    
    return result

class DynamicLossWeighting:
    """
    Dynamic Loss Weighting cho Multi-Task Learning
    """
    
    def __init__(self, task_names: List[str], method: str = "uncertainty", alpha: float = 0.5):
        """
        Args:
            task_names: List of task names
            method: "uncertainty", "gradnorm", "equal"
            alpha: Learning rate for weight updates
        """
        self.task_names = task_names
        self.method = method
        self.alpha = alpha
        
        # Initialize weights
        self.weights = {task: 1.0 for task in task_names}
        self.loss_history = {task: [] for task in task_names}
        self.prev_losses = {task: 0.0 for task in task_names}
        
        if method == "uncertainty":
            # Learnable uncertainty parameters
            self.log_vars = nn.Parameter(torch.zeros(len(task_names)))
    
    def compute_weighted_loss(self, losses: Dict[str, torch.Tensor], 
                            model: nn.Module = None) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute weighted loss
        """
        if self.method == "equal":
            # Equal weighting
            total_loss = sum(losses.values())
            return total_loss, self.weights
        
        elif self.method == "uncertainty":
            # Uncertainty weighting (Kendall et al.)
            total_loss = 0
            for i, (task, loss) in enumerate(losses.items()):
                precision = torch.exp(-self.log_vars[i])
                total_loss += precision * loss + self.log_vars[i]
            
            return total_loss, self.weights
        
        elif self.method == "gradnorm":
            # GradNorm (Chen et al.)
            if model is None:
                # Fallback to equal weighting
                total_loss = sum(losses.values())
                return total_loss, self.weights
            
            # Compute weighted loss
            weighted_losses = []
            for task in self.task_names:
                weighted_losses.append(self.weights[task] * losses[task])
            
            total_loss = sum(weighted_losses)
            
            # Update weights based on gradient norms (simplified version)
            self._update_gradnorm_weights(losses, model)
            
            return total_loss, self.weights
    
    def _update_gradnorm_weights(self, losses: Dict[str, torch.Tensor], model: nn.Module):
        """
        Update weights using GradNorm algorithm (simplified)
        """
        # This is a simplified version - full GradNorm requires more complex implementation
        for task in self.task_names:
            current_loss = losses[task].item()
            self.loss_history[task].append(current_loss)
            
            if len(self.loss_history[task]) > 1:
                # Simple heuristic: increase weight if loss is increasing
                loss_change = current_loss - self.prev_losses[task]
                if loss_change > 0:
                    self.weights[task] = min(self.weights[task] * 1.1, 5.0)
                else:
                    self.weights[task] = max(self.weights[task] * 0.95, 0.1)
            
            self.prev_losses[task] = current_loss

class MultiTaskTrainer:
    """
    Multi-Task Trainer cho Multi-Modal Fusion Network
    """
    
    def __init__(self, 
                 model: nn.Module,
                 task_configs: Dict[str, int],
                 loss_weighting_method: str = "uncertainty",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 save_dir: str = "./models/multimodal_fusion"):
        """
        Args:
            model: MultiModalFusionNetwork instance
            task_configs: Dict mapping task names to number of classes
            loss_weighting_method: "uncertainty", "gradnorm", "equal"
            device: Training device
            save_dir: Directory to save models and logs
        """
        self.model = model.to(device)
        self.task_configs = task_configs
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Loss weighting
        self.loss_weighting = DynamicLossWeighting(
            task_names=list(task_configs.keys()),
            method=loss_weighting_method
        )
          # Loss functions
        self.criterion = nn.CrossEntropyLoss()
        
        # Training history
        self.train_history = defaultdict(list)
        self.val_history = defaultdict(list)
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup logging"""
        log_file = self.save_dir / "training.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def train_epoch(self, train_loader: DataLoader, optimizer: optim.Optimizer) -> Dict[str, float]:
        """
        Train one epoch
        """
        self.model.train()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        total_batches = len(train_loader)
        
        for batch_idx, batch in enumerate(train_loader):
            # Move to device
            text_input = batch['text_input'].to(self.device)
            metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            attention_mask = batch.get('attention_mask')
            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = self.model(text_input, metadata_input, attention_mask)
            
            # Compute losses for each task
            task_losses = {}
            task_accuracies = {}
            
            for task_name, logits in outputs.items():
                if task_name in labels:
                    task_loss = self.criterion(logits, labels[task_name])
                    task_losses[task_name] = task_loss
                    
                    # Compute accuracy
                    predictions = torch.argmax(logits, dim=1)
                    accuracy = (predictions == labels[task_name]).float().mean()
                    task_accuracies[task_name] = accuracy
                    
                    epoch_losses[task_name].append(task_loss.item())
                    epoch_accuracies[task_name].append(accuracy.item())
            
            # Compute weighted loss
            total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
            
            # Backward pass
            total_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # Log progress
            if batch_idx % 100 == 0:
                self.logger.info(f"Batch {batch_idx}/{total_batches} - Total Loss: {total_loss.item():.4f}")
                for task_name, loss in task_losses.items():
                    self.logger.info(f"  {task_name}: Loss={loss.item():.4f}, Acc={task_accuracies[task_name].item():.4f}")
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """
        Validate one epoch
        """
        self.model.eval()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        all_predictions = defaultdict(list)
        all_labels = defaultdict(list)
        
        with torch.no_grad():
            for batch in val_loader:
                # Move to device
                text_input = batch['text_input'].to(self.device)
                metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                # Forward pass
                outputs = self.model(text_input, metadata_input, attention_mask)
                
                # Compute losses and metrics
                for task_name, logits in outputs.items():
                    if task_name in labels:
                        task_loss = self.criterion(logits, labels[task_name])
                        epoch_losses[task_name].append(task_loss.item())
                        
                        # Predictions and accuracy
                        predictions = torch.argmax(logits, dim=1)
                        accuracy = (predictions == labels[task_name]).float().mean()
                        epoch_accuracies[task_name].append(accuracy.item())
                        
                        # Store for detailed metrics
                        all_predictions[task_name].extend(predictions.cpu().numpy())
                        all_labels[task_name].extend(labels[task_name].cpu().numpy())
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
                
                # Compute F1 score
                if task_name in all_predictions:
                    f1 = f1_score(all_labels[task_name], all_predictions[task_name], average='weighted')
                    epoch_results[f"{task_name}_f1"] = f1
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results, all_predictions, all_labels
    
    def train(self, 
              train_loader: DataLoader, 
              val_loader: DataLoader,
              num_epochs: int = 50,
              learning_rate: float = 1e-3,
              weight_decay: float = 1e-5,
              patience: int = 10,
              save_best: bool = True) -> Dict[str, List[float]]:
        """
        Main training loop
        """
        # Optimizer and scheduler
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience//2, factor=0.5)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        self.logger.info(f"Starting training for {num_epochs} epochs")
        self.logger.info(f"Tasks: {list(self.task_configs.keys())}")
        self.logger.info(f"Device: {self.device}")
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            # Training
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Training...")
            train_results = self.train_epoch(train_loader, optimizer)
            
            # Validation
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Validation...")
            val_results, val_predictions, val_labels = self.validate_epoch(val_loader)
            
            # Update learning rate
            scheduler.step(val_results['total_loss'])
            
            # Log results
            epoch_time = time.time() - start_time
            self.logger.info(f"Epoch {epoch+1} completed in {epoch_time:.2f}s")
            self.logger.info(f"Train Loss: {train_results['total_loss']:.4f}, Val Loss: {val_results['total_loss']:.4f}")
            
            for task_name in self.task_configs.keys():
                if f"{task_name}_accuracy" in train_results and f"{task_name}_accuracy" in val_results:
                    self.logger.info(f"  {task_name}: Train Acc={train_results[f'{task_name}_accuracy']:.4f}, "
                                   f"Val Acc={val_results[f'{task_name}_accuracy']:.4f}")
            
            # Save history
            for key, value in train_results.items():
                self.train_history[key].append(value)
            for key, value in val_results.items():
                self.val_history[key].append(value)
            
            # Early stopping and model saving
            if val_results['total_loss'] < best_val_loss:
                best_val_loss = val_results['total_loss']
                patience_counter = 0
                
                if save_best:
                    self.save_model(epoch, val_results, "best_model.pth")
                    self.logger.info(f"New best model saved with validation loss: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {patience} epochs without improvement")
                break
            
            # Save checkpoint
            if (epoch + 1) % 10 == 0:
                self.save_model(epoch, val_results, f"checkpoint_epoch_{epoch+1}.pth")
        
        # Save final model
        self.save_model(epoch, val_results, "final_model.pth")
        
        # Save training history
        self._save_training_history()
        
        # Generate training plots
        self._plot_training_history()
        
        return {
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
    
    def save_model(self, epoch: int, metrics: Dict[str, float], filename: str):
        """
        Save model checkpoint
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'task_configs': self.task_configs,
            'metrics': metrics,
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
        
        # Save loss weighting parameters if using uncertainty method
        if hasattr(self.loss_weighting, 'log_vars'):
            checkpoint['loss_weighting_log_vars'] = self.loss_weighting.log_vars.data
        
        torch.save(checkpoint, self.save_dir / filename)
    
    def _save_training_history(self):
        """
        Save training history to JSON
        """
        history = {
            'train_history': {k: [float(x) for x in v] for k, v in self.train_history.items()},
            'val_history': {k: [float(x) for x in v] for k, v in self.val_history.items()}
        }
        
        with open(self.save_dir / "training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
    
    def _plot_training_history(self):
        """
        Plot training history
        """
        plt.style.use('seaborn-v0_8')
        
        # Plot losses
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training History', fontsize=16)
        
        # Total loss
        axes[0, 0].plot(self.train_history['total_loss'], label='Train')
        axes[0, 0].plot(self.val_history['total_loss'], label='Validation')
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Task-specific losses
        task_names = list(self.task_configs.keys())
        if len(task_names) > 0:
            for i, task_name in enumerate(task_names[:3]):  # Show first 3 tasks
                row = (i + 1) // 2
                col = (i + 1) % 2
                if row < 2 and col < 2:
                    train_key = f"{task_name}_loss"
                    val_key = f"{task_name}_loss"
                    if train_key in self.train_history and val_key in self.val_history:
                        axes[row, col].plot(self.train_history[train_key], label='Train')
                        axes[row, col].plot(self.val_history[val_key], label='Validation')
                        axes[row, col].set_title(f'{task_name} Loss')
                        axes[row, col].set_xlabel('Epoch')
                        axes[row, col].set_ylabel('Loss')
                        axes[row, col].legend()
                        axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_losses.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot accuracies
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Accuracies', fontsize=16)
        
        for i, task_name in enumerate(task_names[:4]):  # Show first 4 tasks
            row = i // 2
            col = i % 2
            train_key = f"{task_name}_accuracy"
            val_key = f"{task_name}_accuracy"
            
            if train_key in self.train_history and val_key in self.val_history:
                axes[row, col].plot(self.train_history[train_key], label='Train')
                axes[row, col].plot(self.val_history[val_key], label='Validation')
                axes[row, col].set_title(f'{task_name} Accuracy')
                axes[row, col].set_xlabel('Epoch')
                axes[row, col].set_ylabel('Accuracy')
                axes[row, col].legend()
                axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_accuracies.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("Training plots saved successfully")
    
    def train_step(self, text_input, metadata_input, targets, optimizer=None):
        """
        Perform a single training step
        Args:
            text_input: Text input tensor
            metadata_input: Metadata input dict
            targets: Target labels dict
            optimizer: Optional optimizer (creates AdamW if None)
        Returns:
            float: Total loss value
        """
        # Create optimizer if not provided
        if optimizer is None:
            optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        
        self.model.train()
        
        # Forward pass
        optimizer.zero_grad()
        outputs = self.model(text_input, metadata_input)
        
        # Compute losses for each task
        task_losses = {}
        for task_name, logits in outputs.items():
            if task_name in targets:
                task_loss = self.criterion(logits, targets[task_name])
                task_losses[task_name] = task_loss
        
        # Compute weighted loss
        total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
        
        # Backward pass
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()

```

### backend\ai\testmodelAi\han_model_demo.py
```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST M√î H√åNH HAN - MINH H·ªåA KH·∫¢ NƒÇNG PH√ÇN LO·∫†I
(Kh√¥ng load model th·ª±c, ch·ªâ demo flow ho·∫°t ƒë·ªông)
"""

import os
from datetime import datetime

def simulate_han_model_prediction(commit_message):
    """
    M√¥ ph·ªèng k·∫øt qu·∫£ t·ª´ model HAN th·ª±c
    (Th·ª±c t·∫ø s·∫Ω load t·ª´ best_model.pth)
    """
    text = commit_message.lower()
    
    # M√¥ ph·ªèng logic ph√¢n lo·∫°i c·ªßa HAN model
    
    # 1. Commit Type Classification
    if any(word in text for word in ['feat:', 'feature:', 'add', 'implement', 'create']):
        commit_type = 'feat'
        type_confidence = 0.95
    elif any(word in text for word in ['fix:', 'bug:', 'resolve', 'patch']):
        commit_type = 'fix'  
        type_confidence = 0.92
    elif any(word in text for word in ['docs:', 'documentation', 'readme']):
        commit_type = 'docs'
        type_confidence = 0.89
    elif any(word in text for word in ['test:', 'testing', 'spec']):
        commit_type = 'test'
        type_confidence = 0.87
    elif any(word in text for word in ['refactor:', 'restructure', 'cleanup']):
        commit_type = 'refactor'
        type_confidence = 0.91
    elif any(word in text for word in ['chore:', 'update', 'dependency']):
        commit_type = 'chore'
        type_confidence = 0.88
    elif any(word in text for word in ['style:', 'format', 'lint']):
        commit_type = 'style'
        type_confidence = 0.86
    elif any(word in text for word in ['perf:', 'performance', 'optimize']):
        commit_type = 'perf'
        type_confidence = 0.93
    else:
        commit_type = 'other'
        type_confidence = 0.75
    
    # 2. Purpose Classification
    purpose_map = {
        'feat': 'Feature Implementation',
        'fix': 'Bug Fix',
        'docs': 'Documentation Update', 
        'test': 'Test Update',
        'refactor': 'Code Refactoring',
        'chore': 'Maintenance',
        'style': 'Code Style',
        'perf': 'Performance Improvement',
        'other': 'Other'
    }
    purpose = purpose_map.get(commit_type, 'Other')
    purpose_confidence = type_confidence - 0.03
    
    # 3. Sentiment Analysis
    if any(word in text for word in ['critical', 'urgent', 'emergency', 'severe']):
        sentiment = 'urgent'
        sentiment_confidence = 0.94
    elif any(word in text for word in ['error', 'bug', 'fail', 'problem']):
        sentiment = 'negative'
        sentiment_confidence = 0.88
    elif any(word in text for word in ['improve', 'enhance', 'optimize', 'add', 'new']):
        sentiment = 'positive'
        sentiment_confidence = 0.90
    else:
        sentiment = 'neutral'
        sentiment_confidence = 0.85
    
    # 4. Tech Tag Classification (m·ªü r·ªông)
    if any(word in text for word in ['auth', 'authentication', 'login', 'oauth']):
        tech_tag = 'authentication'
        tech_confidence = 0.92
    elif any(word in text for word in ['database', 'db', 'sql', 'query']):
        tech_tag = 'database'
        tech_confidence = 0.89
    elif any(word in text for word in ['api', 'endpoint', 'rest']):
        tech_tag = 'api'
        tech_confidence = 0.91
    elif any(word in text for word in ['ui', 'frontend', 'component']):
        tech_tag = 'frontend'
        tech_confidence = 0.87
    elif any(word in text for word in ['security', 'vulnerability', 'encryption']):
        tech_tag = 'security'
        tech_confidence = 0.95
    else:
        tech_tag = 'general'
        tech_confidence = 0.80
    
    return {
        'commit_type': {'label': commit_type, 'confidence': type_confidence},
        'purpose': {'label': purpose, 'confidence': purpose_confidence},
        'sentiment': {'label': sentiment, 'confidence': sentiment_confidence},
        'tech_tag': {'label': tech_tag, 'confidence': tech_confidence}
    }

def run_han_model_demo():
    """Demo kh·∫£ nƒÉng ph√¢n lo·∫°i c·ªßa model HAN v·ªõi ph√¢n t√≠ch chi ti·∫øt"""
    
    print("=" * 80)
    print("ü§ñ DEMO M√î H√åNH HAN - PH√ÇN T√çCH COMMIT CHI TI·∫æT")
    print("=" * 80)
    print(f"‚è∞ Th·ªùi gian demo: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("üìù L∆ØU √ù: Demo n√†y m√¥ ph·ªèng k·∫øt qu·∫£ t·ª´ model HAN th·ª±c")
    print("üîß Model th·ª±c ƒë∆∞·ª£c l∆∞u t·∫°i: models/han_github_model/best_model.pth")
    print()
    
    # Test cases ƒëa d·∫°ng v·ªõi 30 commits v√† t√°c gi·∫£
    test_commits = [
        # Developer 1: John Smith (Frontend specialist)
        ("john.smith@company.com", "feat: implement responsive navigation component"),
        ("john.smith@company.com", "feat: add dark mode toggle functionality"),
        ("john.smith@company.com", "fix: resolve mobile layout issues in header"),
        ("john.smith@company.com", "style: update CSS variables for consistent theming"),
        ("john.smith@company.com", "feat: create user profile management page"),
        
        # Developer 2: Sarah Johnson (Backend specialist)  
        ("sarah.johnson@company.com", "feat: implement user authentication API"),
        ("sarah.johnson@company.com", "fix: resolve database connection timeout issues"),
        ("sarah.johnson@company.com", "feat: add JWT token refresh mechanism"),
        ("sarah.johnson@company.com", "perf: optimize database queries for user search"),
        ("sarah.johnson@company.com", "fix: handle edge case in password validation"),
        ("sarah.johnson@company.com", "feat: implement role-based access control"),
        
        # Developer 3: Mike Chen (DevOps/Infrastructure)
        ("mike.chen@company.com", "chore: update Docker configuration for production"),
        ("mike.chen@company.com", "fix: resolve CI/CD pipeline deployment issues"),
        ("mike.chen@company.com", "chore: upgrade Node.js to version 18 LTS"),
        ("mike.chen@company.com", "perf: optimize build process with caching"),
        
        # Developer 4: Emily Davis (QA/Testing)
        ("emily.davis@company.com", "test: add unit tests for authentication service"),
        ("emily.davis@company.com", "test: implement integration tests for API endpoints"),
        ("emily.davis@company.com", "fix: correct test data setup for user scenarios"),
        ("emily.davis@company.com", "test: add end-to-end tests for login flow"),
        
        # Developer 5: Alex Rodriguez (Security specialist)
        ("alex.rodriguez@company.com", "fix(security): patch XSS vulnerability in user input"),
        ("alex.rodriguez@company.com", "feat(security): implement rate limiting for API"),
        ("alex.rodriguez@company.com", "fix(security): resolve CSRF token validation issue"),
        
        # Developer 6: Lisa Wang (Documentation)
        ("lisa.wang@company.com", "docs: update API documentation with new endpoints"),
        ("lisa.wang@company.com", "docs: add installation guide for development setup"),
        ("lisa.wang@company.com", "docs: create user manual for admin features"),
        
        # Developer 7: Tom Brown (Performance specialist)
        ("tom.brown@company.com", "perf: implement lazy loading for large datasets"),
        ("tom.brown@company.com", "perf: optimize image compression and caching"),
        ("tom.brown@company.com", "refactor: simplify complex rendering logic"),
        
        # Developer 8: Anna Kim (Junior developer - fewer commits)
        ("anna.kim@company.com", "fix: correct typo in error messages"),
        ("anna.kim@company.com", "style: fix indentation in configuration files")
    ]    
    print("üß™ B·∫ÆT ƒê·∫¶U DEMO V·ªöI 30 COMMIT MESSAGES")
    print("=" * 80)
    
    total_tests = len(test_commits)
    author_stats = {}
    commit_type_stats = {}
    purpose_stats = {}
    sentiment_stats = {}
    tech_tag_stats = {}
    
    for i, (author, commit_message) in enumerate(test_commits, 1):
        print(f"\nüîç DEMO #{i}")
        print("-" * 60)
        
        # Input
        print(f"üìù ƒê·∫¶U V√ÄO:")
        print(f"   Author: {author}")
        print(f"   Commit Message: '{commit_message}'")
        
        # Model prediction (simulated)
        predictions = simulate_han_model_prediction(commit_message)
        
        print(f"\nü§ñ K·∫æT QU·∫¢ T·ª™ MODEL HAN:")
        print(f"   üìã Commit Type: {predictions['commit_type']['label']} "
              f"(tin c·∫≠y: {predictions['commit_type']['confidence']:.0%})")
        print(f"   üéØ Purpose: {predictions['purpose']['label']} "
              f"(tin c·∫≠y: {predictions['purpose']['confidence']:.0%})")
        print(f"   üòä Sentiment: {predictions['sentiment']['label']} "
              f"(tin c·∫≠y: {predictions['sentiment']['confidence']:.0%})")
        print(f"   üè∑Ô∏è Tech Tag: {predictions['tech_tag']['label']} "
              f"(tin c·∫≠y: {predictions['tech_tag']['confidence']:.0%})")
        
        # Ph√¢n t√≠ch
        expected_type = commit_message.split(':')[0].split('(')[0]
        predicted_type = predictions['commit_type']['label']
        is_correct = expected_type.lower() == predicted_type.lower()
        
        print(f"\n‚úÖ PH√ÇN T√çCH:")
        print(f"   Expected: {expected_type}")
        print(f"   Predicted: {predicted_type}")
        print(f"   K·∫øt qu·∫£: {'‚úì CH√çNH X√ÅC' if is_correct else '‚úó SAI S√ìT'}")
        
        # Thu th·∫≠p th·ªëng k√™
        if author not in author_stats:
            author_stats[author] = {
                'total_commits': 0,
                'commit_types': {},
                'purposes': {},
                'sentiments': {},
                'tech_tags': {}
            }
        
        author_stats[author]['total_commits'] += 1
        
        # Th·ªëng k√™ theo lo·∫°i commit
        commit_type = predictions['commit_type']['label']
        author_stats[author]['commit_types'][commit_type] = author_stats[author]['commit_types'].get(commit_type, 0) + 1
        commit_type_stats[commit_type] = commit_type_stats.get(commit_type, 0) + 1
        
        # Th·ªëng k√™ theo purpose
        purpose = predictions['purpose']['label']
        author_stats[author]['purposes'][purpose] = author_stats[author]['purposes'].get(purpose, 0) + 1
        purpose_stats[purpose] = purpose_stats.get(purpose, 0) + 1
        
        # Th·ªëng k√™ theo sentiment
        sentiment = predictions['sentiment']['label']
        author_stats[author]['sentiments'][sentiment] = author_stats[author]['sentiments'].get(sentiment, 0) + 1
        sentiment_stats[sentiment] = sentiment_stats.get(sentiment, 0) + 1
        
        # Th·ªëng k√™ theo tech tag
        tech_tag = predictions['tech_tag']['label']
        author_stats[author]['tech_tags'][tech_tag] = author_stats[author]['tech_tags'].get(tech_tag, 0) + 1
        tech_tag_stats[tech_tag] = tech_tag_stats.get(tech_tag, 0) + 1
        
        print("-" * 60)
    
    # T·ªïng k·∫øt v√† ph√¢n t√≠ch chi ti·∫øt
    print(f"\nüìä T·ªîNG K·∫æT DEMO & PH√ÇN T√çCH CHI TI·∫æT")
    print("=" * 80)
    print(f"üî¢ T·ªïng s·ªë commits demo: {total_tests}")
    print(f"üë• T·ªïng s·ªë developers: {len(author_stats)}")
    print()
    
    # Ph√¢n t√≠ch theo t√°c gi·∫£
    print("üë§ PH√ÇN T√çCH THEO T√ÅC GI·∫¢:")
    print("=" * 60)
    
    # S·∫Øp x·∫øp theo s·ªë commit (t·ª´ nhi·ªÅu ƒë·∫øn √≠t)
    sorted_authors = sorted(author_stats.items(), key=lambda x: x[1]['total_commits'], reverse=True)
    
    for author, stats in sorted_authors:
        name = author.split('@')[0].replace('.', ' ').title()
        print(f"\nüßë‚Äçüíª {name} ({author})")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        
        # Top commit types
        top_commit_types = sorted(stats['commit_types'].items(), key=lambda x: x[1], reverse=True)
        print(f"   üè∑Ô∏è Commit types:")
        for commit_type, count in top_commit_types:
            percentage = (count / stats['total_commits']) * 100
            print(f"      ‚Ä¢ {commit_type}: {count} l·∫ßn ({percentage:.1f}%)")
        
        # Top purposes
        top_purposes = sorted(stats['purposes'].items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"   üéØ Top purposes:")
        for purpose, count in top_purposes:
            print(f"      ‚Ä¢ {purpose}: {count} l·∫ßn")
        
        # Dominant tech tags
        top_tech_tags = sorted(stats['tech_tags'].items(), key=lambda x: x[1], reverse=True)[:2]
        print(f"   üîß Tech focus:")
        for tech_tag, count in top_tech_tags:
            print(f"      ‚Ä¢ {tech_tag}: {count} l·∫ßn")
    
    print("\n" + "=" * 60)
    
    # Ph√¢n t√≠ch t·ªïng quan
    print("\nüìà TH·ªêNG K√ä T·ªîNG QUAN:")
    print("=" * 60)
    
    # Top commit types
    print("\nüè∑Ô∏è PH√ÇN B·ªê COMMIT TYPES:")
    sorted_commit_types = sorted(commit_type_stats.items(), key=lambda x: x[1], reverse=True)
    for commit_type, count in sorted_commit_types:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {commit_type}: {count} commits ({percentage:.1f}%)")
    
    # Top purposes
    print("\nüéØ PH√ÇN B·ªê PURPOSES:")
    sorted_purposes = sorted(purpose_stats.items(), key=lambda x: x[1], reverse=True)
    for purpose, count in sorted_purposes[:5]:  # Top 5
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {purpose}: {count} commits ({percentage:.1f}%)")
    
    # Sentiment analysis
    print("\nüòä PH√ÇN B·ªê SENTIMENT:")
    sorted_sentiments = sorted(sentiment_stats.items(), key=lambda x: x[1], reverse=True)
    for sentiment, count in sorted_sentiments:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {sentiment}: {count} commits ({percentage:.1f}%)")
    
    # Tech tags
    print("\nüîß PH√ÇN B·ªê TECH TAGS:")
    sorted_tech_tags = sorted(tech_tag_stats.items(), key=lambda x: x[1], reverse=True)
    for tech_tag, count in sorted_tech_tags:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {tech_tag}: {count} commits ({percentage:.1f}%)")
    
    # Insights v√† recommendations
    print("\nüí° INSIGHTS & NH·∫¨N X√âT:")
    print("=" * 60)
    
    # Developer v·ªõi nhi·ªÅu commits nh·∫•t
    most_active = sorted_authors[0]
    least_active = sorted_authors[-1]
    
    print(f"üèÜ Developer ho·∫°t ƒë·ªông nh·∫•t: {most_active[0].split('@')[0].replace('.', ' ').title()}")
    print(f"   ‚Ä¢ {most_active[1]['total_commits']} commits ({(most_active[1]['total_commits']/total_tests)*100:.1f}% t·ªïng commits)")
    
    print(f"\nüìâ Developer √≠t commits nh·∫•t: {least_active[0].split('@')[0].replace('.', ' ').title()}")
    print(f"   ‚Ä¢ {least_active[1]['total_commits']} commits ({(least_active[1]['total_commits']/total_tests)*100:.1f}% t·ªïng commits)")
    
    # Ph√¢n t√≠ch xu h∆∞·ªõng
    feat_count = commit_type_stats.get('feat', 0)
    fix_count = commit_type_stats.get('fix', 0)
    
    print(f"\nüîç Ph√¢n t√≠ch xu h∆∞·ªõng:")
    print(f"   ‚Ä¢ T·ª∑ l·ªá feat/fix: {feat_count}:{fix_count}")
    if feat_count > fix_count:
        print("   ‚Ä¢ Team ƒëang focus v√†o ph√°t tri·ªÉn t√≠nh nƒÉng m·ªõi")
    elif fix_count > feat_count:
        print("   ‚Ä¢ Team ƒëang focus v√†o s·ª≠a l·ªói v√† ·ªïn ƒë·ªãnh h·ªá th·ªëng")
    else:
        print("   ‚Ä¢ Team c√≥ s·ª± c√¢n b·∫±ng gi·ªØa ph√°t tri·ªÉn v√† maintenance")
    
    print(f"\nüìà Model HAN c√≥ th·ªÉ ph√¢n lo·∫°i: 4 tasks ƒë·ªìng th·ªùi")
    print(f"üéØ C√°c tasks:")
    print(f"   ‚Ä¢ Commit Type (feat, fix, docs, test, refactor, etc.)")
    print(f"   ‚Ä¢ Purpose (Feature Implementation, Bug Fix, etc.)")
    print(f"   ‚Ä¢ Sentiment (positive, negative, neutral, urgent)")
    print(f"   ‚Ä¢ Tech Tag (authentication, database, api, etc.)")
    print()
    print(f"‚ö° ∆Øu ƒëi·ªÉm c·ªßa Model HAN:")
    print(f"   ‚úì Multi-task learning (4 tasks c√πng l√∫c)")
    print(f"   ‚úì Hierarchical attention (word-level + sentence-level)")
    print(f"   ‚úì High accuracy tr√™n training data (~99%)")
    print(f"   ‚úì H·ªó tr·ª£ conventional commit format")
    print(f"   ‚úì Ph√¢n t√≠ch ƒë∆∞·ª£c patterns c·ªßa t·ª´ng developer")
    print()
    print(f"üîß S·ª≠ d·ª•ng Model th·ª±c:")
    print(f"   1. Load t·ª´: models/han_github_model/best_model.pth")
    print(f"   2. Thay th·∫ø simulate_han_model_prediction() b·∫±ng model th·ª±c")
    print(f"   3. S·ª≠ d·ª•ng tokenizer v√† label_encoders t·ª´ checkpoint")
    print(f"\nüéâ DEMO HO√ÄN TH√ÄNH!")
    print("=" * 80)
    
    # T·∫°o v√† l∆∞u b√°o c√°o chi ti·∫øt
    print(f"\nüìÑ T·∫†O B√ÅO C√ÅO CHI TI·∫æT...")
    detailed_report = generate_detailed_report(
        author_stats, commit_type_stats, purpose_stats, 
        sentiment_stats, tech_tag_stats, total_tests
    )
    
    # L∆∞u b√°o c√°o
    report_saved = save_analysis_report(detailed_report)
    
    if report_saved:
        print(f"‚úÖ B√°o c√°o ph√¢n t√≠ch ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
        print(f"üìä C√≥ th·ªÉ s·ª≠ d·ª•ng b√°o c√°o n√†y ƒë·ªÉ:")
        print(f"   ‚Ä¢ ƒê√°nh gi√° hi·ªáu su·∫•t team")
        print(f"   ‚Ä¢ Ph√¢n t√≠ch xu h∆∞·ªõng ph√°t tri·ªÉn")
        print(f"   ‚Ä¢ L·∫≠p k·∫ø ho·∫°ch ph√¢n c√¥ng c√¥ng vi·ªác")
        print(f"   ‚Ä¢ Training v√† mentoring developers")
    
    return detailed_report

def generate_detailed_report(author_stats, commit_type_stats, purpose_stats, sentiment_stats, tech_tag_stats, total_commits):
    """T·∫°o b√°o c√°o chi ti·∫øt v·ªÅ ph√¢n t√≠ch commits"""
    
    report = {
        'summary': {
            'total_commits': total_commits,
            'total_developers': len(author_stats),
            'analysis_date': datetime.now().isoformat()
        },
        'developer_analysis': {},
        'overall_statistics': {
            'commit_types': commit_type_stats,
            'purposes': purpose_stats,
            'sentiments': sentiment_stats,
            'tech_tags': tech_tag_stats
        },
        'insights': {}
    }
    
    # Ph√¢n t√≠ch chi ti·∫øt t·ª´ng developer
    sorted_authors = sorted(author_stats.items(), key=lambda x: x[1]['total_commits'], reverse=True)
    
    for author, stats in sorted_authors:
        name = author.split('@')[0].replace('.', ' ').title()
        
        # T√¨m commit type ch·ªß ƒë·∫°o
        main_commit_type = max(stats['commit_types'].items(), key=lambda x: x[1])
        
        # T√≠nh productivity score (commits per category diversity)
        diversity_score = len(stats['commit_types']) / len(commit_type_stats) * 100
        
        report['developer_analysis'][author] = {
            'name': name,
            'total_commits': stats['total_commits'],
            'commit_percentage': (stats['total_commits'] / total_commits) * 100,
            'main_commit_type': main_commit_type[0],
            'main_commit_type_count': main_commit_type[1],
            'diversity_score': diversity_score,
            'specialization': 'Specialist' if diversity_score < 40 else 'Generalist',
            'detailed_stats': stats
        }
    
    # Insights t·ªïng quan
    most_active = sorted_authors[0]
    least_active = sorted_authors[-1]
    feat_count = commit_type_stats.get('feat', 0)
    fix_count = commit_type_stats.get('fix', 0)
    
    report['insights'] = {
        'most_active_developer': {
            'email': most_active[0],
            'name': most_active[0].split('@')[0].replace('.', ' ').title(),
            'commits': most_active[1]['total_commits']
        },
        'least_active_developer': {
            'email': least_active[0],
            'name': least_active[0].split('@')[0].replace('.', ' ').title(),
            'commits': least_active[1]['total_commits']
        },
        'team_focus': 'Feature Development' if feat_count > fix_count else 'Bug Fixing' if fix_count > feat_count else 'Balanced',
        'feat_fix_ratio': f"{feat_count}:{fix_count}",
        'productivity_distribution': 'Balanced' if max(author_stats.values(), key=lambda x: x['total_commits'])['total_commits'] <= total_commits * 0.4 else 'Concentrated'
    }
    
    return report

def save_analysis_report(report, filename="commit_analysis_detailed_report.json"):
    """L∆∞u b√°o c√°o ph√¢n t√≠ch ra file JSON"""
    import json
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"üìÑ B√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u: {filename}")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u b√°o c√°o: {e}")
        return False

def main():
    """H√†m ch√≠nh"""
    try:
        detailed_report = run_han_model_demo()
        
        # Hi·ªÉn th·ªã m·ªôt s·ªë insights quan tr·ªçng
        print(f"\nüéØ INSIGHTS QUAN TR·ªåNG:")
        print(f"=" * 50)
        
        insights = detailed_report['insights']
        print(f"üëë Developer t√≠ch c·ª±c nh·∫•t: {insights['most_active_developer']['name']}")
        print(f"   ({insights['most_active_developer']['commits']} commits)")
        
        print(f"üìâ Developer √≠t commit nh·∫•t: {insights['least_active_developer']['name']}")
        print(f"   ({insights['least_active_developer']['commits']} commits)")
        
        print(f"üéØ Focus c·ªßa team: {insights['team_focus']}")
        print(f"‚öñÔ∏è T·ª∑ l·ªá feat/fix: {insights['feat_fix_ratio']}")
        print(f"üìä Ph√¢n b·ªë productivity: {insights['productivity_distribution']}")
        
        print(f"\nüíº G·ª¢I √ù QU·∫¢N L√ù TEAM:")
        print(f"=" * 50)
        
        # Ph√¢n t√≠ch v√† ƒë∆∞a ra g·ª£i √Ω
        dev_analysis = detailed_report['developer_analysis']
        specialists = [dev for dev in dev_analysis.values() if dev['specialization'] == 'Specialist']
        generalists = [dev for dev in dev_analysis.values() if dev['specialization'] == 'Generalist']
        
        print(f"üîß Specialists ({len(specialists)} ng∆∞·ªùi): Focus s√¢u v√†o 1-2 lƒ©nh v·ª±c")
        for dev in specialists[:3]:  # Top 3
            print(f"   ‚Ä¢ {dev['name']}: chuy√™n {dev['main_commit_type']} ({dev['main_commit_type_count']} commits)")
        
        print(f"üåê Generalists ({len(generalists)} ng∆∞·ªùi): ƒêa d·∫°ng nhi·ªÅu lƒ©nh v·ª±c")
        for dev in generalists[:3]:  # Top 3
            print(f"   ‚Ä¢ {dev['name']}: diversity score {dev['diversity_score']:.1f}%")
        
        return detailed_report
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y demo: {e}")
        return None

if __name__ == "__main__":
    main()

```

### backend\ai\testmodelAi\han_model_real_test_fixed.py
```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST M√î H√åNH HAN TH·ª∞C - S·ª¨ D·ª§NG MODEL ƒê√É TRAIN
"""

import os
import sys
import torch
import json
import numpy as np
from datetime import datetime
from pathlib import Path

# Add backend directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

# Import c√°c class c·∫ßn thi·∫øt t·ª´ train script
sys.path.insert(0, os.path.join(current_dir, '..', '..'))
from ai import train_han_github
from ai.train_han_github import SimpleHANModel, SimpleTokenizer

def load_han_model():
    """Load model HAN th·ª±c ƒë√£ train v·ªõi 100k+ commits"""
    
    model_path = Path(current_dir).parent / "models" / "han_github_model" / "best_model.pth"
    
    if not model_path.exists():
        print(f"‚ùå Model kh√¥ng t·ªìn t·∫°i: {model_path}")
        print("   C·∫ßn ch·∫°y script train tr∆∞·ªõc: python train_han_github.py")
        return None, None, None
    
    print(f"üì• Loading model t·ª´: {model_path}")
    
    try:
        # Load checkpoint v·ªõi weights_only=False ƒë·ªÉ c√≥ th·ªÉ load tokenizer
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # Extract model components
        tokenizer = checkpoint['tokenizer']
        label_encoders = checkpoint['label_encoders']
        model_state = checkpoint['model_state_dict']
        num_classes = checkpoint['num_classes']
        metadata = checkpoint['metadata']
        
        print(f"‚úÖ Model metadata:")
        print(f"   üìä Validation Accuracy: {checkpoint.get('val_accuracy', 'N/A'):.4f}")
        print(f"   üìà Training Loss: {checkpoint.get('train_loss', 'N/A'):.4f}")
        print(f"   üè∑Ô∏è Tasks: {list(num_classes.keys())}")
        print(f"   üìè Vocab Size: {len(tokenizer.word_to_idx)}")
        print(f"   üî¢ Model Parameters: {checkpoint.get('model_params', 'N/A'):,}")
        
        # Load model architecture
        model = SimpleHANModel(
            vocab_size=len(tokenizer.word_to_idx),
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        )
        
        # Load trained weights
        model.load_state_dict(model_state)
        model.eval()
        
        print(f"üéØ Model loaded th√†nh c√¥ng!")
        return model, tokenizer, label_encoders
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def preprocess_commit_message(message, tokenizer, max_sentences=10, max_words=50):
    """Preprocess commit message theo format HAN"""
    # S·ª≠ d·ª•ng encode_text c·ªßa SimpleTokenizer ƒë·ªÉ l·∫•y token ids
    tokenized_sentences = tokenizer.encode_text(message, max_sentences, max_words)
    return torch.tensor([tokenized_sentences], dtype=torch.long)

def predict_with_real_model(model, tokenizer, label_encoders, commit_message):
    """D·ª± ƒëo√°n v·ªõi model HAN th·ª±c"""
    
    try:
        # Preprocess
        input_tensor = preprocess_commit_message(commit_message, tokenizer)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
        
        # Decode predictions
        predictions = {}
        
        for task, output in outputs.items():
            # Get prediction probabilities
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_idx = torch.max(probabilities, 1)
            
            # Decode label
            encoder_keys = list(label_encoders[task].keys())
            predicted_label = encoder_keys[predicted_idx.item()]
            confidence_score = confidence.item()
            
            predictions[task] = {
                'label': predicted_label,
                'confidence': confidence_score
            }
        
        return predictions
        
    except Exception as e:
        print(f"‚ùå L·ªói prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_real_han_test():
    """Test model HAN th·ª±c v·ªõi commits ƒëa d·∫°ng"""
    
    print("=" * 80)
    print("ü§ñ TEST M√î H√åNH HAN TH·ª∞C - MODEL ƒê√É TRAIN V·ªöI 100K+ COMMITS")
    print("=" * 80)
    print(f"‚è∞ Th·ªùi gian test: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Load model
    model, tokenizer, label_encoders = load_han_model()
    
    if model is None:
        return None
    
    print(f"üîç Available tasks: {list(label_encoders.keys())}")
    print(f"üìã Available labels per task:")
    for task, encoder in label_encoders.items():
        print(f"   {task}: {list(encoder.keys())}")
    print()
    
    # Test cases th·ª±c t·∫ø t·ª´ GitHub - gi·∫£m xu·ªëng 10 ƒë·ªÉ test nhanh
    test_commits = [
        ("real_user1@gmail.com", "feat: add user authentication with JWT tokens"),
        ("real_user2@github.com", "fix: resolve memory leak in image processing module"),
        ("dev.team@company.com", "docs: update API documentation for v2.0 endpoints"),
        ("qa.engineer@startup.io", "test: add unit tests for payment processing service"),
        ("senior.dev@bigtech.com", "refactor: simplify database connection pooling logic"),
        ("intern@company.com", "chore: update dependencies to latest versions"),
        ("designer@agency.com", "style: improve CSS styling for mobile responsiveness"),
        ("performance.eng@scale.com", "perf: optimize query performance for large datasets"),
        ("vn.dev@company.vn", "feat: th√™m t√≠nh nƒÉng ƒëƒÉng nh·∫≠p b·∫±ng Google OAuth"),
        ("security.expert@bank.com", "fix: patch critical XSS vulnerability in user input"),
    ]
    
    print("üß™ B·∫ÆT ƒê·∫¶U TEST V·ªöI MODEL TH·ª∞C")
    print("=" * 80)
    
    total_tests = len(test_commits)
    successful_predictions = 0
    
    # Statistics tracking
    prediction_stats = {task: {} for task in label_encoders.keys()}
    confidence_stats = {task: [] for task in label_encoders.keys()}
    
    for i, (author, commit_message) in enumerate(test_commits, 1):
        print(f"\nüîç TEST #{i}/{total_tests}")
        print("-" * 60)
        
        # Input
        print(f"üìù ƒê·∫¶U V√ÄO:")
        print(f"   Author: {author}")
        print(f"   Commit Message: '{commit_message}'")
        
        # Real model prediction
        predictions = predict_with_real_model(model, tokenizer, label_encoders, commit_message)
        
        if predictions:
            successful_predictions += 1
            
            print(f"\nü§ñ K·∫æT QU·∫¢ T·ª™ MODEL HAN TH·ª∞C:")
            
            for task, result in predictions.items():
                print(f"   üè∑Ô∏è {task.upper()}: {result['label']} "
                      f"(confidence: {result['confidence']:.3f})")
                
                # Collect statistics
                label = result['label']
                confidence = result['confidence']
                
                if label not in prediction_stats[task]:
                    prediction_stats[task][label] = 0
                prediction_stats[task][label] += 1
                confidence_stats[task].append(confidence)
            
            print(f"   ‚úÖ Prediction th√†nh c√¥ng")
        else:
            print(f"   ‚ùå Prediction th·∫•t b·∫°i")
        
        print("-" * 60)
    
    # Summary statistics
    print(f"\nüìä T·ªîNG K·∫æT TEST MODEL TH·ª∞C")
    print("=" * 80)
    print(f"üî¢ T·ªïng s·ªë test: {total_tests}")
    print(f"‚úÖ Predictions th√†nh c√¥ng: {successful_predictions}")
    print(f"üìà Success rate: {successful_predictions/total_tests*100:.1f}%")
    print()
    
    # Task-wise statistics
    print("üìã TH·ªêNG K√ä THEO TASK:")
    print("=" * 60)
    
    for task in label_encoders.keys():
        print(f"\nüè∑Ô∏è {task.upper()}:")
        
        # Label distribution
        if prediction_stats[task]:
            sorted_labels = sorted(prediction_stats[task].items(), 
                                 key=lambda x: x[1], reverse=True)
            print(f"   üìä Label distribution:")
            for label, count in sorted_labels:
                percentage = (count / successful_predictions) * 100
                print(f"      ‚Ä¢ {label}: {count} ({percentage:.1f}%)")
        
        # Confidence statistics
        if confidence_stats[task]:
            confidences = confidence_stats[task]
            avg_confidence = np.mean(confidences)
            min_confidence = np.min(confidences)
            max_confidence = np.max(confidences)
            
            print(f"   üéØ Confidence statistics:")
            print(f"      ‚Ä¢ Average: {avg_confidence:.3f}")
            print(f"      ‚Ä¢ Range: {min_confidence:.3f} - {max_confidence:.3f}")
            print(f"      ‚Ä¢ High confidence (>0.9): {len([c for c in confidences if c > 0.9])}")
            print(f"      ‚Ä¢ Low confidence (<0.7): {len([c for c in confidences if c < 0.7])}")
    
    # Model insights
    print(f"\nüí° INSIGHTS V·ªÄ MODEL:")
    print("=" * 60)
    
    # Task performance analysis
    for task in label_encoders.keys():
        if confidence_stats[task]:
            avg_conf = np.mean(confidence_stats[task])
            if avg_conf > 0.85:
                print(f"üéØ {task}: Hi·ªáu su·∫•t t·ªët (avg confidence: {avg_conf:.3f})")
            elif avg_conf > 0.7:
                print(f"‚ö†Ô∏è {task}: Hi·ªáu su·∫•t trung b√¨nh (avg confidence: {avg_conf:.3f})")
            else:
                print(f"‚ùå {task}: C·∫ßn c·∫£i thi·ªán (avg confidence: {avg_conf:.3f})")
    
    print(f"\n‚úÖ Model evaluation:")
    print(f"   ‚Ä¢ Model ƒë√£ ƒë∆∞·ª£c train v·ªõi dataset th·ª±c t·ª´ GitHub")
    print(f"   ‚Ä¢ C√≥ th·ªÉ ph√¢n lo·∫°i ƒë∆∞·ª£c {len(label_encoders)} tasks ƒë·ªìng th·ªùi")
    print(f"   ‚Ä¢ Ho·∫°t ƒë·ªông t·ªët v·ªõi conventional commit format")
    print(f"   ‚Ä¢ H·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát")
    
    # Save detailed results
    results = {
        'test_summary': {
            'total_tests': total_tests,
            'successful_predictions': successful_predictions,
            'success_rate': successful_predictions/total_tests,
            'test_date': datetime.now().isoformat()
        },
        'prediction_statistics': prediction_stats,
        'confidence_statistics': {
            task: {
                'average': float(np.mean(confidences)) if confidences else 0,
                'min': float(np.min(confidences)) if confidences else 0,
                'max': float(np.max(confidences)) if confidences else 0,
                'count': len(confidences)
            } for task, confidences in confidence_stats.items()
        },
        'model_info': {
            'vocab_size': len(tokenizer.word_to_idx) if tokenizer else 0,
            'tasks': list(label_encoders.keys()) if label_encoders else [],
            'available_labels': {
                task: list(encoder.keys()) 
                for task, encoder in label_encoders.items()
            } if label_encoders else {}
        }
    }
    
    print(f"\nüéâ TEST MODEL TH·ª∞C HO√ÄN TH√ÄNH!")
    print("=" * 80)
    
    return results

def main():
    """H√†m ch√≠nh"""
    try:
        results = run_real_han_test()
        
        if results:
            print(f"\nüéØ K·∫æT LU·∫¨N:")
            print(f"=" * 50)
            
            success_rate = results['test_summary']['success_rate']
            
            if success_rate >= 0.9:
                print(f"üåü Model ho·∫°t ƒë·ªông xu·∫•t s·∫Øc ({success_rate:.1%} success rate)")
            elif success_rate >= 0.7:
                print(f"‚úÖ Model ho·∫°t ƒë·ªông t·ªët ({success_rate:.1%} success rate)")
            else:
                print(f"‚ö†Ô∏è Model c·∫ßn c·∫£i thi·ªán ({success_rate:.1%} success rate)")
            
            print(f"\nüíº KHUY·∫æN NGH·ªä:")
            print(f"   ‚Ä¢ C√≥ th·ªÉ s·ª≠ d·ª•ng model n√†y cho production")
            print(f"   ‚Ä¢ Model support t·ªët conventional commits")
            print(f"   ‚Ä¢ Ph√π h·ª£p cho automated commit analysis")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\api\deps.py
```py
# KLTN04\backend\api\deps.py
# File ch·ª©a c√°c dependencies (ph·ª• thu·ªôc) chung c·ªßa ·ª©ng d·ª•ng

# Import AsyncSession t·ª´ SQLAlchemy ƒë·ªÉ l√†m vi·ªác v·ªõi database async
from sqlalchemy.ext.asyncio import AsyncSession

# Import k·∫øt n·ªëi database t·ª´ module database
from db.database import database

# Dependency (ph·ª• thu·ªôc) ƒë·ªÉ l·∫•y database session
async def get_db() -> AsyncSession:
    """
    Dependency t·∫°o v√† qu·∫£n l√Ω database session
    
    C√°ch ho·∫°t ƒë·ªông:
    - T·∫°o m·ªôt async session m·ªõi t·ª´ connection pool
    - Yield session ƒë·ªÉ s·ª≠ d·ª•ng trong request
    - ƒê·∫£m b·∫£o session ƒë∆∞·ª£c ƒë√≥ng sau khi request ho√†n th√†nh
    
    Returns:
        AsyncSession: Session database async ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi DB
    """
    # T·∫°o v√† qu·∫£n l√Ω session th√¥ng qua context manager
    async with database.session() as session:
        # Yield session ƒë·ªÉ s·ª≠ d·ª•ng trong route
        yield session
        # Session s·∫Ω t·ª± ƒë·ªông ƒë√≥ng khi ra kh·ªèi block with
```

### backend\api\__init__.py
```py

```

### backend\api\auth\middleware.py
```py

```

### backend\api\routes\ai.py
```py

```

### backend\api\routes\ai_suggestions.py
```py

```

### backend\api\routes\auth.py
```py
# KLTN04\backend\api\routes\auth.py
from fastapi import APIRouter, Request, HTTPException, Depends
from core.oauth import oauth
from fastapi.responses import RedirectResponse
from services.user_service import save_user  # Import h√†m l∆∞u ng∆∞·ªùi d√πng
from core.security import get_current_user, get_current_user_optional, CurrentUser
import os

auth_router = APIRouter()

# Endpoint /login ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh x√°c th·ª±c v·ªõi GitHub
@auth_router.get("/login")
async def login(request: Request):
    # L·∫•y callback URL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    redirect_uri = os.getenv("GITHUB_CALLBACK_URL")
    
    # Chuy·ªÉn h∆∞·ªõng ng∆∞·ªùi d√πng ƒë·∫øn trang x√°c th·ª±c GitHub
    return await oauth.github.authorize_redirect(request, redirect_uri)

# Endpoint /auth/callback - GitHub s·∫Ω g·ªçi l·∫°i endpoint n√†y sau khi x√°c th·ª±c th√†nh c√¥ng
@auth_router.get("/auth/callback")
async def auth_callback(request: Request):
    try:
        code = request.query_params.get("code")
        if not code:
            raise HTTPException(status_code=400, detail="Missing code")        # L·∫•y access token t·ª´ GitHub
        try:
            token = await oauth.github.authorize_access_token(request)
        except Exception as token_error:
            print(f"Failed to get access token: {token_error}")
            raise HTTPException(status_code=400, detail="Invalid authorization code")
        
        # G·ªçi API GitHub ƒë·ªÉ l·∫•y th√¥ng tin user c∆° b·∫£n
        resp = await oauth.github.get("user", token=token)
        profile = resp.json()  # Chuy·ªÉn response th√†nh dictionary

        # L·∫•y email n·∫øu kh√¥ng c√≥ trong profile
        if not profile.get("email"):
            # G·ªçi API ri√™ng ƒë·ªÉ l·∫•y danh s√°ch email
            emails_resp = await oauth.github.get("user/emails", token=token)
            emails = emails_resp.json()
            
            # T√¨m email ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† primary (ch√≠nh)
            primary_email = next((e["email"] for e in emails if e["primary"]), None)
            
            # G√°n email ch√≠nh v√†o profile
            profile["email"] = primary_email

        # Ki·ªÉm tra th√¥ng tin b·∫Øt bu·ªôc
        if not profile.get("email") or not profile.get("login"):
            raise HTTPException(status_code=400, detail="Missing required user information")
          # L∆∞u th√¥ng tin ng∆∞·ªùi d√πng v√†o c∆° s·ªü d·ªØ li·ªáu
        # Parse github_created_at to handle timezone properly
        github_created_at = None
        if profile.get("created_at"):
            try:
                from datetime import datetime
                import dateutil.parser
                github_created_at = dateutil.parser.parse(profile["created_at"]).replace(tzinfo=None)
            except Exception as date_error:
                print(f"Error parsing github_created_at: {date_error}")
                github_created_at = None
        
        user_data = {
            "github_id": profile["id"],
            "github_username": profile["login"],
            "email": profile["email"],
            "display_name": profile.get("name"),  # GitHub display name
            "full_name": profile.get("name"),     # Same as display name
            "avatar_url": profile.get("avatar_url"),
            "bio": profile.get("bio"),
            "location": profile.get("location"),
            "company": profile.get("company"),
            "blog": profile.get("blog"),
            "twitter_username": profile.get("twitter_username"),
            "github_profile_url": profile.get("html_url"),
            "repos_url": profile.get("repos_url"),
            "github_created_at": github_created_at,
            # Set default active status
            "is_active": True,
            "is_verified": False
        }
        await save_user(user_data)

        # Redirect v·ªÅ frontend v·ªõi token v√† th√¥ng tin ng∆∞·ªùi d√πng
        redirect_url = (
            f"http://localhost:5173/auth-success"
            f"?token={token['access_token']}"
            f"&username={profile['login']}"
            f"&email={profile['email']}"
            f"&avatar_url={profile['avatar_url']}"        )

        # Th·ª±c hi·ªán chuy·ªÉn h∆∞·ªõng v·ªÅ frontend
        return RedirectResponse(redirect_url)
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"Auth callback error: {e}")
        print(f"Full traceback: {error_details}")
        raise HTTPException(status_code=500, detail=f"Authentication failed: {str(e)}")

# Th√™m endpoint ƒë·ªÉ ki·ªÉm tra th√¥ng tin user hi·ªán t·∫°i
@auth_router.get("/me")
async def get_current_user_info(current_user: CurrentUser = Depends(get_current_user)):
    """
    Get current authenticated user information
    """
    return {
        "success": True,
        "user": current_user.to_dict(),
        "message": "User authenticated successfully"
    }

@auth_router.get("/me/optional")
async def get_current_user_optional_info(current_user: CurrentUser = Depends(get_current_user_optional)):
    """
    Get current user info (optional authentication)
    """
    if current_user:
        return {
            "authenticated": True,
            "user": current_user.to_dict()
        }
    else:
        return {
            "authenticated": False,
            "user": None
        }
```

### backend\api\routes\branch.py
```py
# backend/api/routes/branch.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.branch_service import save_branch
from services.repo_service import get_repo_id_by_owner_and_name

branch_router = APIRouter()

@branch_router.get("/github/{owner}/{repo}/branches")
async def get_branches(owner: str, repo: str, request: Request):
    # L·∫•y token t·ª´ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # G·ªçi GitHub API l·∫•y danh s√°ch branch
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

```

### backend\api\routes\commit.py
```py
# backend/api/routes/commit.py
"""
Commit API Routes - Comprehensive commit management system

ENDPOINT CATEGORIES:
1. DATABASE QUERIES (Fast, stored data):
   - /commits/{owner}/{repo}/branches/{branch_name}/commits - Get commits by branch from DB
   - /commits/{owner}/{repo}/commits - Get all repo commits from DB with filters
   - /commits/{owner}/{repo}/branches - Get all branches with commit stats
   - /commits/{owner}/{repo}/compare/{base}...{head} - Compare commits between branches
   - /commits/{sha} - Get specific commit details

2. GITHUB DIRECT FETCH (Real-time, live data):
   - /github/{owner}/{repo}/branches/{branch_name}/commits - Fetch branch commits from GitHub API
   - /github/{owner}/{repo}/commits - Fetch repo commits from GitHub API with full filters

3. SYNC & MANAGEMENT:
   - /github/{owner}/{repo}/sync-commits - Sync commits from GitHub to database
   - /github/{owner}/{repo}/sync-all-branches-commits - Sync all branches' commits
   - /commits/{owner}/{repo}/validate-commit-consistency - Validate & fix data consistency

4. ANALYTICS & STATS:
   - /github/{owner}/{repo}/commit-stats - Get comprehensive commit statistics

USAGE GUIDELINES:
- Use DATABASE endpoints for fast queries on stored data
- Use GITHUB DIRECT endpoints for real-time, up-to-date data
- Use SYNC endpoints to populate/update database from GitHub
- Use ANALYTICS endpoints for insights and statistics
"""
from fastapi import APIRouter, Request, HTTPException, Query
import httpx
import asyncio
import logging
from typing import Optional, List
from services.commit_service import (
    save_commit, save_multiple_commits, get_commits_by_repo_id, 
    get_commit_by_sha, get_commit_statistics
)
from services.repo_service import get_repo_id_by_owner_and_name, get_repository
from services.branch_service import get_branches_by_repo_id
from services.github_service import fetch_commits, fetch_commit_details
from datetime import datetime

commit_router = APIRouter()
logger = logging.getLogger(__name__)

async def github_api_call(url: str, token: str, params: dict = None):
    """Helper function for GitHub API calls with error handling"""
    headers = {
        "Authorization": token,
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28"
    }
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.get(url, headers=headers, params=params or {})
        
        if response.status_code == 429:
            raise HTTPException(status_code=429, detail="GitHub API rate limit exceeded")
        elif response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"GitHub API error: {response.text}"
            )
        
        return response.json()

# ==================== NEW BRANCH-SPECIFIC COMMIT ENDPOINTS ====================

@commit_router.get("/commits/{owner}/{repo}/branches/{branch_name}/commits")
async def get_branch_commits(
    owner: str,
    repo: str,
    branch_name: str,
    limit: int = Query(50, ge=1, le=500, description="Number of commits to return"),
    offset: int = Query(0, ge=0, description="Offset for pagination"),
    request: Request = None
):
    """
    L·∫•y commits c·ªßa m·ªôt branch c·ª• th·ªÉ v·ªõi validation ƒë·∫ßy ƒë·ªß
    """
    try:
        from services.commit_service import get_repo_id_by_owner_and_name, get_commits_by_branch_safe, get_commits_by_branch_fallback
        
        # Get repo_id
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        # Try safe method first (with branch_id validation)
        commits_data = await get_commits_by_branch_safe(repo_id, branch_name, limit, offset)
        
        # Fallback to branch_name only if safe method returns empty
        if not commits_data:
            logger.warning(f"Safe method returned empty, trying fallback for {owner}/{repo}:{branch_name}")
            commits_data = await get_commits_by_branch_fallback(repo_id, branch_name, limit, offset)
        
        # Convert to dict format for JSON response
        commits_list = []
        for commit in commits_data:
            commit_dict = {
                "id": commit.id,
                "sha": commit.sha,
                "message": commit.message,
                "author_name": commit.author_name,
                "author_email": commit.author_email,
                "committer_name": commit.committer_name,
                "committer_email": commit.committer_email,
                "date": commit.date.isoformat() if commit.date else None,
                "committer_date": commit.committer_date.isoformat() if commit.committer_date else None,
                "insertions": commit.insertions,
                "deletions": commit.deletions,
                "files_changed": commit.files_changed,
                "is_merge": commit.is_merge,
                "merge_from_branch": commit.merge_from_branch,
                "branch_name": commit.branch_name,
                "author_role_at_commit": commit.author_role_at_commit
            }
            commits_list.append(commit_dict)
        
        return {
            "repository": f"{owner}/{repo}",
            "branch": branch_name,
            "commits": commits_list,
            "count": len(commits_list),
            "limit": limit,
            "offset": offset,
            "total_found": len(commits_list)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting commits for branch {branch_name}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@commit_router.get("/commits/{owner}/{repo}/branches")
async def get_repository_branches_with_commits(
    owner: str,
    repo: str,
    request: Request = None
):
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ branches v·ªõi th·ªëng k√™ commits
    """
    try:
        from services.commit_service import get_repo_id_by_owner_and_name, get_all_branches_with_commit_stats
        
        # Get repo_id
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        # Get branches with commit stats
        branches_data = await get_all_branches_with_commit_stats(repo_id)
        
        # Format response
        branches_list = []
        for branch in branches_data:
            branch_dict = {
                "id": branch.id,
                "name": branch.name,
                "is_default": branch.is_default,
                "is_protected": branch.is_protected,
                "stored_commit_count": branch.commits_count,
                "actual_commit_count": branch.actual_commit_count,
                "latest_commit_date": branch.latest_commit_date.isoformat() if branch.latest_commit_date else None,
                "last_synced_commit_date": branch.last_commit_date.isoformat() if branch.last_commit_date else None
            }
            branches_list.append(branch_dict)
        
        return {
            "repository": f"{owner}/{repo}",
            "branches": branches_list,
            "total_branches": len(branches_list)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting branches for repo {owner}/{repo}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@commit_router.get("/commits/{owner}/{repo}/compare/{base_branch}...{head_branch}")
async def compare_branch_commits(
    owner: str,
    repo: str,
    base_branch: str,
    head_branch: str,
    limit: int = Query(100, ge=1, le=500, description="Number of commits to return"),
    request: Request = None
):
    """
    So s√°nh commits gi·ªØa 2 branches (commits in head_branch but not in base_branch)
    """
    try:
        from services.commit_service import get_repo_id_by_owner_and_name, compare_commits_between_branches
        
        # Get repo_id
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        # Get diff commits
        diff_commits = await compare_commits_between_branches(repo_id, base_branch, head_branch, limit)
        
        # Format response
        commits_list = []
        for commit in diff_commits:
            commit_dict = {
                "sha": commit.sha,
                "message": commit.message,
                "author_name": commit.author_name,
                "author_email": commit.author_email,
                "date": commit.date.isoformat() if commit.date else None,
                "insertions": commit.insertions,
                "deletions": commit.deletions,
                "files_changed": commit.files_changed,
                "is_merge": commit.is_merge
            }
            commits_list.append(commit_dict)
        
        return {
            "repository": f"{owner}/{repo}",
            "comparison": f"{base_branch}...{head_branch}",
            "commits_ahead": commits_list,
            "commits_ahead_count": len(commits_list),
            "base_branch": base_branch,
            "head_branch": head_branch
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error comparing branches {base_branch}...{head_branch}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@commit_router.post("/commits/{owner}/{repo}/validate-commit-consistency")
async def validate_commit_branch_consistency(
    owner: str,
    repo: str,
    request: Request = None
):
    """
    Ki·ªÉm tra v√† s·ª≠a inconsistency gi·ªØa branch_id v√† branch_name trong commits
    """
    try:
        from services.commit_service import get_repo_id_by_owner_and_name, validate_and_fix_commit_branch_consistency
        
        # Get repo_id
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        # Validate and fix consistency
        fixed_count = await validate_and_fix_commit_branch_consistency(repo_id)
        
        return {
            "repository": f"{owner}/{repo}",
            "message": "Commit-branch consistency validation completed",
            "inconsistencies_fixed": fixed_count,
            "status": "success" if fixed_count >= 0 else "error"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error validating commit consistency for {owner}/{repo}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# Endpoint ƒë·ªìng b·ªô commit t·ª´ GitHub v·ªÅ database - Optimized version
@commit_router.post("/github/{owner}/{repo}/sync-commits")
async def sync_commits(
    owner: str,
    repo: str,
    request: Request,
    branch: str = Query("main", description="Branch name to sync commits from"),
    since: Optional[str] = Query(None, description="Only commits after this date (ISO format)"),
    until: Optional[str] = Query(None, description="Only commits before this date (ISO format)"),
    per_page: int = Query(100, ge=1, le=100, description="Number of commits per page"),
    max_pages: int = Query(10, ge=1, le=50, description="Maximum pages to fetch"),
    include_stats: bool = Query(False, description="Include commit statistics (slower)")
):
    """
    ƒê·ªìng b·ªô commits t·ª´ GitHub v·ªõi full model support
    
    H·ªó tr·ª£ t·∫•t c·∫£ c√°c fields trong commit model:
    - Basic info (sha, message, author, committer, dates)
    - Statistics (insertions, deletions, files_changed)
    - Relationships (repo_id, branch_id, parent_sha)
    - Metadata (is_merge, merge_from_branch)
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid GitHub token")
    
    try:
        logger.info(f"Starting commit sync for {owner}/{repo}:{branch}")
        
        # 1. Validate repository exists
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found in database")
        
        # 2. Get branch info
        branches = await get_branches_by_repo_id(repo_id)
        branch_id = None
        for b in branches:
            if b['name'] == branch:
                branch_id = b['id']
                break
        
        if not branch_id:
            logger.warning(f"Branch {branch} not found in database, using branch_name only")
        
        # 3. Fetch commits with enhanced data
        all_commits = []
        page = 1
        
        while page <= max_pages:
            logger.info(f"Fetching commits page {page}/{max_pages}")
            
            params = {
                "sha": branch,
                "per_page": per_page,
                "page": page
            }
            
            if since:
                params["since"] = since
            if until:
                params["until"] = until
                
            url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            commits_data = await github_api_call(url, token, params)
            
            if not commits_data:
                break
            
            # Optionally enhance commits with detailed stats
            if include_stats:
                logger.info(f"Enhancing {len(commits_data)} commits with detailed stats...")
                for commit in commits_data:
                    sha = commit.get("sha")
                    if sha:
                        try:
                            # Fetch detailed commit info including stats
                            detail_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{sha}"
                            detailed_commit = await github_api_call(detail_url, token)
                            
                            # Merge detailed info
                            if detailed_commit:
                                commit["detailed_stats"] = detailed_commit.get("stats", {})
                                commit["files"] = detailed_commit.get("files", [])
                                
                        except Exception as e:
                            logger.warning(f"Could not fetch details for commit {sha}: {e}")
                        
                        # Small delay to avoid rate limiting
                        await asyncio.sleep(0.05)
            
            all_commits.extend(commits_data)
            
            if len(commits_data) < per_page:
                break
                
            page += 1
            await asyncio.sleep(0.1)
        
        logger.info(f"Fetched {len(all_commits)} commits from GitHub")
        
        # 4. Process and save commits with full model data
        saved_count = await save_multiple_commits(
            commits_data=all_commits,
            repo_id=repo_id,
            branch_name=branch,
            branch_id=branch_id        )
        
        logger.info(f"Successfully saved {saved_count} new commits")
        
        return {
            "message": f"Successfully synced {saved_count} commits",
            "repository": f"{owner}/{repo}",
            "branch": branch,
            "total_fetched": len(all_commits),
            "new_commits_saved": saved_count,
            "pages_processed": min(page, max_pages),
            "enhanced_with_stats": include_stats,
            "branch_id": branch_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error syncing commits for {owner}/{repo}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Commit sync failed: {str(e)}")

# Endpoint ƒë·ªìng b·ªô commits cho t·∫•t c·∫£ branches
@commit_router.post("/github/{owner}/{repo}/sync-all-branches-commits")
async def sync_all_branches_commits(
    owner: str,
    repo: str,
    request: Request,
    since: Optional[str] = Query(None, description="Only commits after this date"),
    per_page: int = Query(50, ge=1, le=100),
    max_pages_per_branch: int = Query(5, ge=1, le=20)
):
    """
    ƒê·ªìng b·ªô commits cho t·∫•t c·∫£ branches c·ªßa repository
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid GitHub token")
    
    try:
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
        
        # Get all branches
        branches = await get_branches_by_repo_id(repo_id)
        
        if not branches:
            raise HTTPException(status_code=404, detail="No branches found for repository")
        
        total_saved = 0
        branch_results = []
        
        for branch in branches:
            branch_name = branch['name']
            logger.info(f"Processing commits for branch: {branch_name}")
            
            try:
                # Fetch commits for this branch
                params = {
                    "sha": branch_name,
                    "per_page": per_page
                }
                if since:
                    params["since"] = since
                
                all_commits = []
                page = 1
                
                while page <= max_pages_per_branch:
                    params["page"] = page
                    url = f"https://api.github.com/repos/{owner}/{repo}/commits"
                    commits_data = await github_api_call(url, token, params)
                    
                    if not commits_data:
                        break
                    
                    all_commits.extend(commits_data)
                    
                    if len(commits_data) < per_page:
                        break
                    
                    page += 1
                    await asyncio.sleep(0.1)
                
                # Save commits for this branch
                saved_count = await save_multiple_commits(
                    commits_data=all_commits,
                    repo_id=repo_id,
                    branch_name=branch_name,
                    branch_id=branch['id']
                )
                
                total_saved += saved_count
                branch_results.append({
                    "branch": branch_name,
                    "commits_fetched": len(all_commits),
                    "commits_saved": saved_count
                })
                
            except Exception as e:
                logger.warning(f"Failed to sync commits for branch {branch_name}: {e}")
                branch_results.append({
                    "branch": branch_name,
                    "error": str(e)
                })
        
        return {
            "message": f"Synced commits for {len(branches)} branches",
            "repository": f"{owner}/{repo}",
            "total_commits_saved": total_saved,
            "branch_results": branch_results
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Multi-branch sync failed: {str(e)}")

# REDUNDANT ENDPOINT REMOVED - Use /commits/{owner}/{repo}/branches/{branch_name}/commits instead

# Endpoint l·∫•y commit chi ti·∫øt
@commit_router.get("/commits/{sha}")
async def get_commit_details(sha: str):
    """Get detailed information about a specific commit"""
    try:
        commit = await get_commit_by_sha(sha)
        if not commit:
            raise HTTPException(status_code=404, detail="Commit not found")
        
        return commit
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get commit: {str(e)}")

# Endpoint th·ªëng k√™ commits
@commit_router.get("/github/{owner}/{repo}/commit-stats")
async def get_repository_commit_statistics(owner: str, repo: str):
    """Get comprehensive commit statistics for a repository"""
    try:
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
        
        stats = await get_commit_statistics(repo_id)
        
        return {
            "repository": f"{owner}/{repo}",
            "statistics": stats
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get commit statistics: {str(e)}")

# CONSOLIDATED ENDPOINT - This replaces the removed /github/{owner}/{repo}/commits
@commit_router.get("/commits/{owner}/{repo}/commits")
async def get_repository_commits_from_database(
    owner: str,
    repo: str,
    branch: Optional[str] = Query(None, description="Filter commits by branch name (redirects to branch-specific endpoint)"),
    limit: int = Query(50, ge=1, le=500, description="Number of commits to return"),
    offset: int = Query(0, ge=0, description="Offset for pagination"),
    since: Optional[str] = Query(None, description="Only commits after this date (ISO format)"),
    until: Optional[str] = Query(None, description="Only commits before this date (ISO format)"),
    request: Request = None
):
    """
    L·∫•y commits c·ªßa repository t·ª´ database v·ªõi filtering n√¢ng cao
    Note: N·∫øu ch·ªâ ƒë·ªãnh branch, s·∫Ω redirect ƒë·∫øn endpoint branch-specific cho hi·ªáu su·∫•t t·ªët h∆°n
    """
    try:
        # If branch is specified, redirect to branch-specific endpoint
        if branch:
            return await get_branch_commits(owner, repo, branch, limit, offset, request)
        
        # Otherwise, get all commits (existing logic)
        from services.commit_service import get_repo_id_by_owner_and_name
        from db.models.commits import commits
        from db.database import database
        from sqlalchemy import select, and_
        
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        # Build query with filters
        query = select(commits).where(commits.c.repo_id == repo_id)
        
        # Add date filters if provided
        if since:
            try:
                since_date = datetime.fromisoformat(since.replace('Z', '+00:00'))
                query = query.where(commits.c.date >= since_date)
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid 'since' date format")
        
        if until:
            try:
                until_date = datetime.fromisoformat(until.replace('Z', '+00:00'))
                query = query.where(commits.c.date <= until_date)
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid 'until' date format")
        
        # Apply pagination and ordering
        query = query.order_by(commits.c.date.desc()).limit(limit).offset(offset)
        
        commits_data = await database.fetch_all(query)
        
        # Format response
        commits_list = []
        for commit in commits_data:
            commit_dict = {
                "id": commit.id,
                "sha": commit.sha,
                "message": commit.message,
                "author_name": commit.author_name,
                "author_email": commit.author_email,
                "date": commit.date.isoformat() if commit.date else None,
                "branch_name": commit.branch_name,
                "insertions": commit.insertions,
                "deletions": commit.deletions,
                "files_changed": commit.files_changed,
                "is_merge": commit.is_merge
            }
            commits_list.append(commit_dict)
        
        return {
            "repository": f"{owner}/{repo}",
            "commits": commits_list,
            "count": len(commits_list),
            "limit": limit,
            "offset": offset,
            "filters": {
                "since": since,
                "until": until,
                "branch": branch
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting commits for repo {owner}/{repo}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# ==================== GITHUB DIRECT FETCH ENDPOINTS ====================

@commit_router.get("/github/{owner}/{repo}/branches/{branch_name}/commits")
async def get_branch_commits_from_github(
    owner: str,
    repo: str,
    branch_name: str,
    request: Request,
    per_page: int = Query(30, ge=1, le=100, description="Number of commits per page"),
    page: int = Query(1, ge=1, le=100, description="Page number"),
    since: Optional[str] = Query(None, description="Only commits after this date (ISO format)"),
    until: Optional[str] = Query(None, description="Only commits before this date (ISO format)")
):
    """
    Fetch commits directly from GitHub for a specific branch (real-time data)
    
    This endpoint fetches commits directly from GitHub API without storing in database.
    Use this when you need real-time, up-to-date commit data.
    For faster access to stored data, use /commits/{owner}/{repo}/branches/{branch_name}/commits instead.
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid GitHub token")
    
    try:
        logger.info(f"Fetching commits from GitHub for {owner}/{repo}:{branch_name}")
        
        # Build parameters for GitHub API
        params = {
            "sha": branch_name,
            "per_page": per_page,
            "page": page
        }
        
        if since:
            params["since"] = since
        if until:
            params["until"] = until
        
        # Fetch commits from GitHub
        url = f"https://api.github.com/repos/{owner}/{repo}/commits"
        commits_data = await github_api_call(url, token, params)
        
        if not commits_data:
            return {
                "repository": f"{owner}/{repo}",
                "branch": branch_name,
                "commits": [],
                "count": 0,
                "page": page,
                "per_page": per_page,
                "source": "github_api",
                "message": "No commits found"
            }
        
        # Format commits to match our standard response format
        formatted_commits = []
        for commit in commits_data:
            commit_info = commit.get("commit", {})
            author_info = commit_info.get("author", {})
            committer_info = commit_info.get("committer", {})
            
            formatted_commit = {
                "sha": commit.get("sha"),
                "message": commit_info.get("message"),
                "author_name": author_info.get("name"),
                "author_email": author_info.get("email"),
                "author_date": author_info.get("date"),
                "committer_name": committer_info.get("name"),
                "committer_email": committer_info.get("email"),
                "committer_date": committer_info.get("date"),
                "url": commit.get("html_url"),
                "api_url": commit.get("url"),
                "comment_count": commit_info.get("comment_count", 0),
                "verification": commit_info.get("verification", {}),
                "author": commit.get("author"),  # GitHub user info
                "committer": commit.get("committer"),  # GitHub user info
                "parents": [{"sha": p.get("sha"), "url": p.get("url")} for p in commit.get("parents", [])]
            }
            formatted_commits.append(formatted_commit)
        
        return {
            "repository": f"{owner}/{repo}",
            "branch": branch_name,
            "commits": formatted_commits,
            "count": len(formatted_commits),
            "page": page,
            "per_page": per_page,
            "source": "github_api",
            "filters": {
                "since": since,
                "until": until
            },
            "note": "This data is fetched directly from GitHub API. For stored data, use /commits/{owner}/{repo}/branches/{branch_name}/commits"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching commits from GitHub for {owner}/{repo}:{branch_name}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch commits from GitHub: {str(e)}")

@commit_router.get("/github/{owner}/{repo}/commits")
async def get_repository_commits_from_github(
    owner: str,
    repo: str,
    request: Request,
    sha: Optional[str] = Query(None, description="SHA or branch to start listing commits from"),
    path: Optional[str] = Query(None, description="Only commits containing this file path will be returned"),
    author: Optional[str] = Query(None, description="GitHub username or email address"),
    committer: Optional[str] = Query(None, description="GitHub username or email address"),
    since: Optional[str] = Query(None, description="Only commits after this date (ISO format)"),
    until: Optional[str] = Query(None, description="Only commits before this date (ISO format)"),
    per_page: int = Query(30, ge=1, le=100, description="Number of commits per page"),
    page: int = Query(1, ge=1, le=100, description="Page number")
):
    """
    Fetch commits directly from GitHub for a repository (real-time data)
    
    This endpoint provides comprehensive filtering options available in GitHub API.
    For branch-specific queries, consider using /github/{owner}/{repo}/branches/{branch_name}/commits.
    For stored data, use /commits/{owner}/{repo}/commits.
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid GitHub token")
    
    try:
        logger.info(f"Fetching commits from GitHub for {owner}/{repo}")
        
        # Build parameters for GitHub API with all available filters
        params = {
            "per_page": per_page,
            "page": page
        }
        
        # Add optional filters
        if sha:
            params["sha"] = sha
        if path:
            params["path"] = path
        if author:
            params["author"] = author
        if committer:
            params["committer"] = committer
        if since:
            params["since"] = since
        if until:
            params["until"] = until
        
        # Fetch commits from GitHub
        url = f"https://api.github.com/repos/{owner}/{repo}/commits"
        commits_data = await github_api_call(url, token, params)
        
        if not commits_data:
            return {
                "repository": f"{owner}/{repo}",
                "commits": [],
                "count": 0,
                "page": page,
                "per_page": per_page,
                "source": "github_api",
                "message": "No commits found"
            }
        
        # Format commits (same as branch-specific endpoint)
        formatted_commits = []
        for commit in commits_data:
            commit_info = commit.get("commit", {})
            author_info = commit_info.get("author", {})
            committer_info = commit_info.get("committer", {})
            
            formatted_commit = {
                "sha": commit.get("sha"),
                "message": commit_info.get("message"),
                "author_name": author_info.get("name"),
                "author_email": author_info.get("email"),
                "author_date": author_info.get("date"),
                "committer_name": committer_info.get("name"),
                "committer_email": committer_info.get("email"),
                "committer_date": committer_info.get("date"),
                "url": commit.get("html_url"),
                "api_url": commit.get("url"),
                "comment_count": commit_info.get("comment_count", 0),
                "verification": commit_info.get("verification", {}),
                "author": commit.get("author"),
                "committer": commit.get("committer"),
                "parents": [{"sha": p.get("sha"), "url": p.get("url")} for p in commit.get("parents", [])]
            }
            formatted_commits.append(formatted_commit)
        
        return {
            "repository": f"{owner}/{repo}",
            "commits": formatted_commits,
            "count": len(formatted_commits),
            "page": page,
            "per_page": per_page,
            "source": "github_api",
            "filters": {
                "sha": sha,
                "path": path,
                "author": author,
                "committer": committer,
                "since": since,
                "until": until
            },
            "note": "This data is fetched directly from GitHub API. For stored data, use /commits/{owner}/{repo}/commits"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching commits from GitHub for {owner}/{repo}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch commits from GitHub: {str(e)}")

# ==================== END GITHUB DIRECT FETCH ENDPOINTS ====================

# ==================== BRANCH-SPECIFIC SYNC ENDPOINT ====================

@commit_router.post("/github/{owner}/{repo}/branches/{branch_name}/sync-commits")
async def sync_branch_commits_enhanced(
    owner: str,
    repo: str,
    branch_name: str,
    request: Request,
    force_refresh: bool = Query(False, description="Force refresh all commits even if they exist"),
    per_page: int = Query(100, ge=1, le=100, description="Number of commits per page"),
    max_pages: int = Query(10, ge=1, le=50, description="Maximum pages to fetch"),
    include_stats: bool = Query(True, description="Include detailed commit statistics")
):
    """
    ƒê·ªìng b·ªô commits cho m·ªôt branch c·ª• th·ªÉ v·ªõi enhanced features
    
    Endpoint n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho BranchSelector component:
    - Sync commits cho branch ƒë∆∞·ª£c ch·ªçn
    - H·ªó tr·ª£ force refresh ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu
    - Bao g·ªìm th·ªëng k√™ chi ti·∫øt cho commit analysis
    - T·ªëi ∆∞u h√≥a cho UI responsiveness
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid GitHub token")
    
    try:
        logger.info(f"Starting enhanced commit sync for {owner}/{repo}:{branch_name}")
        
        # 1. Validate repository exists
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found in database")
        
        # 2. Get or create branch info
        from services.branch_service import get_branches_by_repo_id, save_branch
        branches = await get_branches_by_repo_id(repo_id)
        branch_id = None
        
        for branch in branches:
            if branch['name'] == branch_name:
                branch_id = branch['id']
                break
        
        # Create branch if not exists
        if not branch_id:
            logger.info(f"Branch {branch_name} not found, creating new branch record")
            try:
                # Fetch branch info from GitHub
                branch_url = f"https://api.github.com/repos/{owner}/{repo}/branches/{branch_name}"
                branch_data = await github_api_call(branch_url, token)
                
                new_branch = {
                    "name": branch_name,
                    "repo_id": repo_id,
                    "sha": branch_data.get("commit", {}).get("sha"),
                    "is_protected": branch_data.get("protected", False)
                }
                await save_branch(new_branch)
                
                # Re-fetch to get branch_id
                branches = await get_branches_by_repo_id(repo_id)
                for branch in branches:
                    if branch['name'] == branch_name:
                        branch_id = branch['id']
                        break
                        
            except Exception as e:
                logger.warning(f"Could not create branch record: {e}")
        
        # 3. Check existing commits if not force refresh
        existing_count = 0
        if not force_refresh:
            from services.commit_service import get_commits_by_branch_safe
            existing_commits = await get_commits_by_branch_safe(repo_id, branch_name, 1, 0)
            existing_count = len(existing_commits) if existing_commits else 0
        
        # 4. Fetch commits from GitHub with enhanced data
        all_commits = []
        page = 1
        
        while page <= max_pages:
            logger.info(f"Fetching commits page {page}/{max_pages} for branch {branch_name}")
            
            params = {
                "sha": branch_name,
                "per_page": per_page,
                "page": page
            }
            
            url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            commits_data = await github_api_call(url, token, params)
            
            if not commits_data:
                break
            
            # Enhance commits with detailed stats if requested
            if include_stats:
                logger.info(f"Enhancing {len(commits_data)} commits with detailed stats...")
                for commit in commits_data:
                    sha = commit.get("sha")
                    if sha:
                        try:
                            # Fetch detailed commit info including stats
                            detail_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{sha}"
                            detailed_commit = await github_api_call(detail_url, token)
                            
                            if detailed_commit:
                                commit["stats"] = detailed_commit.get("stats", {})
                                commit["files"] = detailed_commit.get("files", [])
                                
                        except Exception as e:
                            logger.warning(f"Could not fetch details for commit {sha}: {e}")
                        
                        # Small delay to avoid rate limiting
                        await asyncio.sleep(0.05)
            
            all_commits.extend(commits_data)
            
            if len(commits_data) < per_page:
                break
                
            page += 1
            await asyncio.sleep(0.1)
        
        logger.info(f"Fetched {len(all_commits)} commits from GitHub for branch {branch_name}")
        
        # 5. Process and save commits with full model data
        saved_count = await save_multiple_commits(
            commits_data=all_commits,
            repo_id=repo_id,
            branch_name=branch_name,
            branch_id=branch_id
        )
        
        logger.info(f"Successfully saved {saved_count} new commits for branch {branch_name}")
        
        # 6. Get final stats
        total_commits_in_db = existing_count + saved_count
        
        return {
            "success": True,
            "message": f"Successfully synced commits for branch '{branch_name}'",
            "repository": f"{owner}/{repo}",
            "branch": branch_name,
            "branch_id": branch_id,
            "stats": {
                "total_fetched_from_github": len(all_commits),
                "new_commits_saved": saved_count,
                "existing_commits_before_sync": existing_count,
                "total_commits_in_database": total_commits_in_db,
                "pages_processed": min(page, max_pages),
                "enhanced_with_stats": include_stats,
                "force_refresh_enabled": force_refresh
            },
            "next_actions": {
                "view_commits": f"/api/commits/{owner}/{repo}/branches/{branch_name}/commits",
                "analyze_commits": f"/api/ai/analyze-repo/{owner}/{repo}?branch={branch_name}"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error syncing commits for branch {branch_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Branch commit sync failed: {str(e)}")

# ==================== END BRANCH-SPECIFIC SYNC ENDPOINT ====================

```

### backend\api\routes\commit_routes.py
```py
# File: backend/api/routes/commit_routes.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Header
from fastapi.responses import JSONResponse
from typing import List, Optional
import pandas as pd
from services.model_loader import predict_commit
from pathlib import Path
import tempfile
import httpx

router = APIRouter(prefix="/api/commits", tags=["Commit Analysis"])

@router.get("/analyze-github/{owner}/{repo}")
async def analyze_github_commits(
    owner: str,
    repo: str,
    authorization: str = Header(..., alias="Authorization"),
    per_page: int = 30,
    since: Optional[str] = None,
    until: Optional[str] = None
):
    """
    Ph√¢n t√≠ch commit t·ª´ repository GitHub
    
    Args:
        owner: T√™n ch·ªß repo
        repo: T√™n repository
        authorization: Token GitHub (Format: Bearer <token>)
        per_page: S·ªë commit t·ªëi ƒëa c·∫ßn ph√¢n t√≠ch (1-100)
        since: L·ªçc commit t·ª´ ng√†y (YYYY-MM-DDTHH:MM:SSZ)
        until: L·ªçc commit ƒë·∫øn ng√†y (YYYY-MM-DDTHH:MM:SSZ)
    
    Returns:
        {
            "repo": f"{owner}/{repo}",
            "total": int,
            "critical": int,
            "critical_percentage": float,
            "details": List[dict],
            "analysis_date": str
        }
    """
    try:
        # Validate input
        if per_page < 1 or per_page > 100:
            raise HTTPException(
                status_code=400,
                detail="per_page must be between 1 and 100"
            )

        # Configure GitHub API request
        headers = {
            "Authorization": authorization,
            "Accept": "application/vnd.github.v3+json"
        }
        params = {
            "per_page": per_page,
            "since": since,
            "until": until
        }
        
        # Fetch commits from GitHub
        async with httpx.AsyncClient() as client:
            # Get first page to check repo accessibility
            initial_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(initial_url, headers=headers, params={**params, "per_page": 1})
            
            if response.status_code == 404:
                raise HTTPException(
                    status_code=404,
                    detail="Repository not found or access denied"
                )
            response.raise_for_status()

            # Get all requested commits
            full_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(full_url, headers=headers, params=params)
            response.raise_for_status()
            commits_data = response.json()

        # Prepare analysis data
        commits_for_analysis = [
            {
                "id": commit["sha"],
                "message": commit["commit"]["message"],
                "date": commit["commit"]["committer"]["date"] if commit["commit"]["committer"] else None
            }
            for commit in commits_data
            if commit.get("sha") and commit.get("commit", {}).get("message")
        ]

        # Analyze commits
        results = {
            "repo": f"{owner}/{repo}",
            "total": len(commits_for_analysis),
            "critical": 0,
            "critical_percentage": 0.0,
            "details": [],
            "analysis_date": datetime.utcnow().isoformat()
        }

        for commit in commits_for_analysis:
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
            
            results["details"].append({
                "id": commit["id"],
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message'],
                "date": commit["date"]
            })

        # Calculate percentage
        if results["total"] > 0:
            results["critical_percentage"] = round(
                (results["critical"] / results["total"]) * 100, 2
            )

        return results

    except httpx.HTTPStatusError as e:
        error_detail = "GitHub API error"
        if e.response.status_code == 403:
            error_detail = "API rate limit exceeded" if "rate limit" in str(e.response.content) else "Forbidden"
        elif e.response.status_code == 401:
            error_detail = "Invalid GitHub token"
        
        raise HTTPException(
            status_code=e.response.status_code,
            detail=f"{error_detail}: {e.response.text}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing GitHub commits: {str(e)}"
        )
@router.post("/analyze-text")
async def analyze_commit_text(message: str):
    """
    Ph√¢n t√≠ch m·ªôt commit message d·∫°ng text
    
    Args:
        message: N·ªôi dung commit message
    
    Returns:
        {"is_critical": 0|1, "message": string}
    """
    try:
        is_critical = predict_commit(message)
        return {
            "is_critical": is_critical,
            "message": "Ph√¢n t√≠ch th√†nh c√¥ng",
            "input_sample": message[:100] + "..." if len(message) > 100 else message
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch: {str(e)}")

@router.post("/analyze-json")
async def analyze_commits_json(commits: List[dict]):
    """
    Ph√¢n t√≠ch nhi·ªÅu commit t·ª´ JSON
    
    Args:
        commits: List[{"id": string, "message": string}]
    
    Returns:
        {"total": int, "critical": int, "details": List[dict]}
    """
    try:
        results = {
            "total": len(commits),
            "critical": 0,
            "details": []
        }
        
        for commit in commits:
            if not isinstance(commit, dict) or 'message' not in commit:
                continue
                
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
                
            results["details"].append({
                "id": commit.get("id", ""),
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message']
            })
            
        return JSONResponse(content=results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch h√†ng lo·∫°t: {str(e)}")

@router.post("/analyze-csv", response_model=dict)
async def analyze_commits_csv(file: UploadFile = File(...)):
    """
    Ph√¢n t√≠ch commit t·ª´ file CSV
    
    Args:
        file: File CSV c√≥ c·ªôt 'message' ho·∫∑c 'commit_message'
    
    Returns:
        {"filename": string, "total": int, "critical": int}
    """
    try:
        # L∆∞u file t·∫°m
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(await file.read())
            tmp_path = Path(tmp.name)
        
        # ƒê·ªçc file CSV
        df = pd.read_csv(tmp_path)
        tmp_path.unlink()  # X√≥a file t·∫°m
        
        # Ki·ªÉm tra c·ªôt message
        message_col = 'message' if 'message' in df.columns else 'commit_message'
        if message_col not in df.columns:
            raise HTTPException(status_code=400, detail="File thi·∫øu c·ªôt 'message' ho·∫∑c 'commit_message'")
        
        # Ph√¢n t√≠ch
        results = {
            "filename": file.filename,
            "total": len(df),
            "critical": 0,
            "sample_results": []
        }
        
        df['is_critical'] = df[message_col].apply(predict_commit)
        results["critical"] = int(df['is_critical'].sum())
        
        # L·∫•y 5 k·∫øt qu·∫£ m·∫´u
        sample = df.head(5).to_dict('records')
        results["sample_results"] = [{
            "message": row[message_col][:100] + "..." if len(row[message_col]) > 100 else row[message_col],
            "is_critical": bool(row['is_critical'])
        } for row in sample]
        
        return results
        
    except Exception as e:
        if tmp_path.exists():
            tmp_path.unlink()
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}")
```

### backend\api\routes\contributors.py
```py
# backend/api/routes/contributors.py
from fastapi import APIRouter, Depends, HTTPException, status, Header
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime

from core.security import get_current_user
from services.collaborator_service import (
    get_collaborators_with_fallback,
    sync_repository_collaborators,
    get_collaborators_by_repo
)
from db.models.repositories import repositories
from db.database import database
from sqlalchemy import select

logger = logging.getLogger(__name__)
router = APIRouter()

@router.get("/{owner}/{repo}")
async def get_repository_collaborators(
    owner: str,
    repo: str,
    authorization: Optional[str] = Header(None),
    current_user: dict = Depends(get_current_user)
) -> Dict[str, Any]:
    """Get collaborators for a specific repository"""
    try:
        logger.info(f"Getting collaborators for repository {owner}/{repo}")
          # Get repository ID first
        repo_query = select(repositories.c.id).where(
            (repositories.c.owner == owner) &
            (repositories.c.name == repo)
        )
        repo_result = await database.fetch_one(repo_query)
        
        if not repo_result:
            logger.warning(f"Repository {owner}/{repo} not found in database")
            # Return empty but valid response
            return {
                "repository": f"{owner}/{repo}",
                "collaborators": [],
                "count": 0,
                "has_synced_data": False,
                "message": "Repository not found in database. Please sync first."
            }
        
        # Only get from database - NO automatic fallback
        collaborators = await get_collaborators_by_repo(repo_result.id)
        has_synced_data = len(collaborators) > 0
          # Create response in expected format
        response = {
            "repository": f"{owner}/{repo}",
            "collaborators": collaborators,
            "count": len(collaborators),
            "has_synced_data": has_synced_data,
            "message": (
                f"Loaded {len(collaborators)} synced collaborators from database" if has_synced_data 
                else "No collaborators found in database. Click 'Sync' to import from GitHub."
            )
        }
        
        logger.info(f"Retrieved {len(collaborators)} collaborators for {owner}/{repo}")
        return response
        
    except Exception as e:
        logger.error(f"Error getting collaborators for {owner}/{repo}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get collaborators: {str(e)}"
        )

@router.post("/{owner}/{repo}/sync")
async def sync_repository_collaborators_endpoint(
    owner: str,
    repo: str,
    authorization: Optional[str] = Header(None),
    current_user: dict = Depends(get_current_user)
) -> Dict[str, Any]:
    """Sync collaborators from GitHub to database"""
    try:
        logger.info(f"Syncing collaborators for repository {owner}/{repo}")
        
        # Extract GitHub token from Authorization header
        github_token = None
        if authorization:
            # Handle both "Bearer token" and "token token" formats
            if authorization.startswith("Bearer "):
                github_token = authorization[7:]
            elif authorization.startswith("token "):
                github_token = authorization[6:]
            else:
                github_token = authorization
        
        if not github_token:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="GitHub access token required for syncing collaborators"
            )
        
        # Ensure repository exists in our database
        repo_query = select(repositories.c.id).where(
            (repositories.c.owner == owner) &
            (repositories.c.name == repo)
        )
        repo_result = await database.fetch_one(repo_query)
        
        if not repo_result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Repository {owner}/{repo} not found in database"
            )
        
        # Sync collaborators using the service
        sync_result = await sync_repository_collaborators(
            owner=owner,
            repo=repo,
            github_token=github_token
        )
        
        if sync_result.get("status") == "error":
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Sync failed: {sync_result.get('error', 'Unknown error')}"
            )
        
        logger.info(f"Successfully synced collaborators for {owner}/{repo}")
        return sync_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error syncing collaborators for {owner}/{repo}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync collaborators: {str(e)}"
        )

@router.get("/repository/{repo_id}")
async def get_collaborators_by_repository_id(
    repo_id: int,
    current_user: dict = Depends(get_current_user)
) -> List[Dict[str, Any]]:
    """Get collaborators by repository ID"""
    try:
        logger.info(f"Getting collaborators for repository ID {repo_id}")
        
        collaborators = await get_collaborators_by_repo(repo_id)
        
        logger.info(f"Retrieved {len(collaborators)} collaborators for repository ID {repo_id}")
        return collaborators
        
    except Exception as e:
        logger.error(f"Error getting collaborators for repository ID {repo_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get collaborators: {str(e)}"
        )

@router.get("/health")
async def collaborators_health_check():
    """Health check endpoint for collaborators API"""
    return {
        "status": "healthy",
        "service": "collaborators",
        "timestamp": datetime.now().isoformat()
    }
```

### backend\api\routes\github.py
```py
# backend/api/routes/github.py
# File t·ªïng h·ª£p c√°c router GitHub APIs

from fastapi import APIRouter
from .repo import repo_router
from .commit import commit_router
from .branch import branch_router
from .issue import issue_router
from .sync import sync_router

# Router ch√≠nh cho GitHub APIs
github_router = APIRouter()

# Include c√°c sub-routers
github_router.include_router(repo_router, tags=["repositories"])
github_router.include_router(commit_router, tags=["commits"])
github_router.include_router(branch_router, tags=["branches"])
github_router.include_router(issue_router, tags=["issues"])
github_router.include_router(sync_router, tags=["synchronization"])
```

### backend\api\routes\gitlab.py
```py

```

### backend\api\routes\issue.py
```py
# backend/api/routes/issue.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.issue_service import save_issue
from services.repo_service import get_repo_id_by_owner_and_name

issue_router = APIRouter()

# L∆∞u issues v√†o database
@issue_router.post("/github/{owner}/{repo}/save-issues")
async def save_issues(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # L·∫•y danh s√°ch issue t·ª´ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        issues = resp.json()

    # L∆∞u issue v√†o database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    saved_count = 0
    for issue in issues:
        try:
            issue_data = {
                "title": issue["title"],
                "body": issue["body"],
                "state": issue["state"],
                "created_at": issue["created_at"],
                "updated_at": issue["updated_at"],
                "repo_id": repo_id,
            }
            await save_issue(issue_data)
            saved_count += 1
        except Exception as e:
            print(f"L·ªói khi l∆∞u issue {issue['title']}: {e}")
            continue

    return {"message": f"ƒê√£ l∆∞u {saved_count}/{len(issues)} issues!"}

```

### backend\api\routes\member_analysis.py
```py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List, Dict, Any
from db.database import get_db
from services.member_analysis_service import MemberAnalysisService

router = APIRouter(prefix="/api/repositories", tags=["member-analysis"])

@router.get("/{repo_id}/members")
async def get_repository_members(
    repo_id: int,
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch members c·ªßa repository"""
    try:
        service = MemberAnalysisService(db)
        members = service.get_repository_members(repo_id)
        
        return {
            "success": True,
            "repository_id": repo_id,
            "members": members,
            "total": len(members)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching members: {str(e)}")

@router.get("/{repo_id}/members/{member_login}/commits")
async def get_member_commits_analysis(
    repo_id: int,
    member_login: str,
    branch_name: str = None,  # NEW: Optional branch filter
    limit: int = 50,
    use_ai: bool = True,
    db: Session = Depends(get_db)
):
    """L·∫•y commits c·ªßa member v·ªõi AI analysis v√† branch filter"""
    try:
        service = MemberAnalysisService(db)
        
        if use_ai:
            # Use AI-powered analysis
            analysis = await service.get_member_commits_with_ai_analysis(
                repo_id, member_login, limit, branch_name
            )
        else:
            # Use pattern-based analysis
            analysis = service.get_member_commits_with_analysis(
                repo_id, member_login, limit, branch_name
            )
        
        return {
            "success": True,
            "data": analysis,
            "branch_filter": branch_name
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error analyzing member commits: {str(e)}")

@router.get("/{repo_id}/ai-features")
async def get_ai_features_status(repo_id: int):
    """L·∫•y status c·ªßa c√°c t√≠nh nƒÉng AI available"""
    return {
        "success": True,
        "repository_id": repo_id,
        "features": {
            "commit_analysis": True,
            "member_insights": True,
            "productivity_tracking": True,
            "code_pattern_detection": True,
            "han_model_analysis": True
        },
        "ai_model": {
            "name": "HAN Commit Analyzer",
            "version": "1.0",
            "type": "Hierarchical Attention Network",
            "capabilities": [
                "Deep commit message understanding",
                "Semantic commit classification",
                "Developer behavior analysis", 
                "Technology area detection",
                "Impact and urgency assessment",
                "Code quality insights"
            ]
        },
        "endpoints": {
            "commit_analysis": f"/api/repositories/{repo_id}/members/{{member_login}}/commits?use_ai=true",
            "batch_analysis": f"/api/repositories/{repo_id}/ai/analyze-batch",
            "developer_insights": f"/api/repositories/{repo_id}/ai/developer-insights"
        }
    }

@router.post("/{repo_id}/ai/analyze-batch")
async def analyze_commits_batch(
    repo_id: int,
    commit_messages: List[str],
    db: Session = Depends(get_db)
):
    """Batch analysis cho nhi·ªÅu commit messages"""
    try:
        service = MemberAnalysisService(db)
        results = await service.ai_service.analyze_commits_batch(commit_messages)
        
        return {
            "success": True,
            "repository_id": repo_id,
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error in batch analysis: {str(e)}")

@router.get("/{repo_id}/ai/developer-insights")
async def get_developer_insights(
    repo_id: int,
    db: Session = Depends(get_db)
):
    """L·∫•y insights v·ªÅ t·∫•t c·∫£ developers trong repo"""
    try:
        service = MemberAnalysisService(db)
        
        # Get all members
        members = service.get_repository_members(repo_id)
        
        # Get commits for each member and analyze
        developer_commits = {}
        for member in members[:5]:  # Limit to first 5 for demo
            member_login = member['github_username']
            commits_data = service._get_member_commits_raw(repo_id, member_login, 20)
            if commits_data:
                developer_commits[member_login] = [row[2] for row in commits_data]  # messages
        
        # Analyze patterns
        insights = await service.ai_service.analyze_developer_patterns(developer_commits)
        
        return {
            "success": True,
            "repository_id": repo_id,
            "insights": insights
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting developer insights: {str(e)}")

@router.get("/{repo_id}/ai/model-status")
async def get_ai_model_status(repo_id: int):
    """Ki·ªÉm tra tr·∫°ng th√°i AI model"""
    try:
        from services.han_ai_service import HANAIService
        ai_service = HANAIService()
        
        return {
            "success": True,
            "repository_id": repo_id,
            "model_loaded": ai_service.is_model_loaded,
            "model_info": {
                "type": "HAN (Hierarchical Attention Network)",
                "purpose": "Commit message analysis and classification",
                "features": [
                    "Semantic understanding",
                    "Multi-level attention",
                    "Context-aware classification"
                ]
            }
        }
    except Exception as e:
        return {
            "success": False,
            "repository_id": repo_id,
            "model_loaded": False,
            "error": str(e)
        }

@router.get("/{repo_id}/branches")
async def get_repository_branches(
    repo_id: int,
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch branches c·ªßa repository"""
    try:
        service = MemberAnalysisService(db)
        branches = service.get_repository_branches(repo_id)
        
        return {
            "success": True,
            "repository_id": repo_id,
            "branches": branches,
            "total": len(branches)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching branches: {str(e)}")

```

### backend\api\routes\projects.py
```py
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import select, insert, update, delete, and_, func, or_
from typing import List, Optional
from datetime import datetime
from pydantic import BaseModel

# from core.security import get_current_user  # Temporarily disabled
from core.security import get_current_user, CurrentUser
from db.database import get_db, engine
from db.models.project_tasks import project_tasks, TaskStatus, TaskPriority

router = APIRouter()

# Temporary mock user dependency - REMOVED, using real auth now
# async def get_current_user():
#     return {"username": "test_user", "id": 1}

# Pydantic models cho Task
class TaskBase(BaseModel):
    title: str
    description: Optional[str] = None
    assignee: str
    priority: str = "MEDIUM"  # LOW, MEDIUM, HIGH, URGENT
    status: str = "TODO"  # TODO, IN_PROGRESS, DONE, CANCELLED
    due_date: Optional[str] = None

class TaskCreate(TaskBase):
    # repo_owner v√† repo_name s·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ URL path, kh√¥ng c·∫ßn trong request body
    pass

class TaskUpdate(TaskBase):
    pass

class TaskResponse(TaskBase):
    id: int
    repo_owner: str
    repo_name: str
    created_at: datetime
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True

@router.get("/projects/{owner}/{repo}/tasks", response_model=List[TaskResponse])
async def get_project_tasks(
    owner: str,
    repo: str,
    current_user: CurrentUser = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch tasks c·ªßa repository"""
    try:
        # Query tasks from database
        with engine.connect() as conn:
            query = select(project_tasks).where(
                and_(
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )            ).order_by(project_tasks.c.created_at.desc())
            
            result = conn.execute(query)
            tasks = []
            
            for row in result:
                task_dict = {
                    "id": row.id,
                    "title": row.title,
                    "description": row.description,
                    "assignee": row.assignee_github_username,  # Use correct field name
                    "priority": row.priority if row.priority else "MEDIUM",  # Already string
                    "status": row.status if row.status else "TODO",  # Already string
                    "due_date": row.due_date,
                    "repo_owner": row.repo_owner,
                    "repo_name": row.repo_name,
                    "created_at": row.created_at,
                    "updated_at": row.updated_at
                }
                tasks.append(task_dict)
            
            return tasks
    except Exception as e:
        print(f"Database error: {e}")
        # Fallback to empty list if database error
        return []

@router.post("/projects/{owner}/{repo}/tasks", response_model=TaskResponse)
async def create_project_task(
    owner: str,
    repo: str,
    task: TaskCreate,
    current_user: CurrentUser = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """T·∫°o task m·ªõi cho repository"""
    try:
        # Insert task into database
        with engine.connect() as conn:            # Validate priority and status
            priority_enum = TaskPriority.MEDIUM
            if task.priority == "LOW":
                priority_enum = TaskPriority.LOW            
            elif task.priority == "HIGH":
                priority_enum = TaskPriority.HIGH
            
            status_enum = TaskStatus.TODO
            if task.status == "IN_PROGRESS":
                status_enum = TaskStatus.IN_PROGRESS
            elif task.status == "DONE":
                status_enum = TaskStatus.DONE
              # Handle due_date conversion
            due_date_value = None
            if task.due_date:
                try:
                    from datetime import datetime
                    # Try to parse the date string
                    if isinstance(task.due_date, str):
                        due_date_value = datetime.strptime(task.due_date, '%Y-%m-%d').date()
                    else:
                        due_date_value = task.due_date
                except (ValueError, TypeError) as e:
                    print(f"Date parsing error: {e}")
                    due_date_value = None
            
            # Resolve IDs
            assignee_user_id = get_user_id_by_github_username(conn, task.assignee)
            repository_id = get_repository_id(conn, owner, repo)
            
            insert_stmt = insert(project_tasks).values(
                title=task.title,
                description=task.description,
                assignee_github_username=task.assignee,  # Use correct field name
                assignee_user_id=assignee_user_id,  # Resolved user ID
                priority=priority_enum.value,  # Convert enum to string
                status=status_enum.value,  # Convert enum to string
                due_date=str(due_date_value) if due_date_value else None,  # Store as string
                repository_id=repository_id,  # Resolved repository ID
                repo_owner=owner,
                repo_name=repo,                is_completed=False,  # Default to False for new tasks
                created_by=current_user.github_username,
                created_by_user_id=current_user.id  # Use user ID if available
            )
            
            result = conn.execute(insert_stmt)
            conn.commit()
            
            # Get the created task
            task_id = result.inserted_primary_key[0]
            query = select(project_tasks).where(project_tasks.c.id == task_id)
            created_task = conn.execute(query).fetchone()
            
            return {
                "id": created_task.id,
                "title": created_task.title,
                "description": created_task.description,
                "assignee": created_task.assignee_github_username,  # Use correct field
                "priority": created_task.priority,  # Should be string already
                "status": created_task.status,  # Should be string already  
                "due_date": created_task.due_date,
                "repo_owner": created_task.repo_owner,
                "repo_name": created_task.repo_name,
                "created_at": created_task.created_at,
                "updated_at": created_task.updated_at
            }
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.put("/projects/{owner}/{repo}/tasks/{task_id}", response_model=TaskResponse)
async def update_project_task(
    owner: str,
    repo: str,
    task_id: int,
    task_update: TaskUpdate,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """C·∫≠p nh·∫≠t task"""
    try:
        with engine.connect() as conn:
            # Check if task exists
            check_query = select(project_tasks).where(
                and_(
                    project_tasks.c.id == task_id,
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )
            )
            existing_task = conn.execute(check_query).fetchone()
            
            if not existing_task:
                raise HTTPException(status_code=404, detail="Task not found")
              # Validate priority and status
            priority_enum = TaskPriority.MEDIUM
            if task_update.priority == "LOW":
                priority_enum = TaskPriority.LOW
            elif task_update.priority == "HIGH":
                priority_enum = TaskPriority.HIGH
                
            status_enum = TaskStatus.TODO
            if task_update.status == "IN_PROGRESS":
                status_enum = TaskStatus.IN_PROGRESS
            elif task_update.status == "DONE":
                status_enum = TaskStatus.DONE            # Resolve assignee user ID if assignee changed
            assignee_user_id = get_user_id_by_github_username(conn, task_update.assignee)
            
            # Update task
            update_stmt = update(project_tasks).where(
                project_tasks.c.id == task_id
            ).values(
                title=task_update.title,
                description=task_update.description,
                assignee_github_username=task_update.assignee,  # Use correct field name
                assignee_user_id=assignee_user_id,  # Resolved user ID
                priority=priority_enum.value,  # Convert enum to string
                status=status_enum.value,  # Convert enum to string
                due_date=task_update.due_date,
                is_completed=(status_enum == TaskStatus.DONE)  # Set is_completed based on status
            )
            
            conn.execute(update_stmt)
            conn.commit()
              # Get updated task
            updated_task = conn.execute(check_query).fetchone()
            
            return {
                "id": updated_task.id,
                "title": updated_task.title,
                "description": updated_task.description,
                "assignee": updated_task.assignee_github_username,  # Use correct field
                "priority": updated_task.priority,  # Already string
                "status": updated_task.status,  # Already string
                "due_date": updated_task.due_date,
                "repo_owner": updated_task.repo_owner,
                "repo_name": updated_task.repo_name,
                "created_at": updated_task.created_at,
                "updated_at": updated_task.updated_at
            }
    except HTTPException:
        raise
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.delete("/projects/{owner}/{repo}/tasks/{task_id}")
async def delete_project_task(
    owner: str,
    repo: str,
    task_id: int,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """X√≥a task"""
    try:
        with engine.connect() as conn:
            # Check if task exists
            check_query = select(project_tasks).where(
                and_(
                    project_tasks.c.id == task_id,
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )
            )
            existing_task = conn.execute(check_query).fetchone()
            
            if not existing_task:
                raise HTTPException(status_code=404, detail="Task not found")
            
            # Delete task
            delete_stmt = delete(project_tasks).where(
                project_tasks.c.id == task_id
            )
            
            conn.execute(delete_stmt)
            conn.commit()
            
            return {"message": "Task deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.get("/projects/{owner}/{repo}/collaborators")
async def get_project_collaborators(
    owner: str,
    repo: str,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch collaborators c·ªßa repository"""
    try:
        # Mock data - trong th·ª±c t·∫ø s·∫Ω g·ªçi GitHub API
        collaborators = [
            {
                "login": "john_doe",
                "avatar_url": "https://via.placeholder.com/32",
                "type": "User"
            },
            {
                "login": "jane_smith", 
                "avatar_url": "https://via.placeholder.com/32",
                "type": "User"
            },
            {
                "login": owner,
                "avatar_url": "https://via.placeholder.com/32",
                "type": "Owner"
            }
        ]
        return collaborators
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ================= TASK MANAGEMENT APIs - Direct Database Access =================

# Pydantic models b·ªï sung cho c√°c API m·ªõi
class TaskStats(BaseModel):
    total_tasks: int
    todo_tasks: int
    in_progress_tasks: int
    done_tasks: int
    high_priority_tasks: int
    medium_priority_tasks: int
    low_priority_tasks: int
    overdue_tasks: int

class BulkTaskCreate(BaseModel):
    tasks: List[TaskCreate]

class BulkTaskUpdate(BaseModel):
    task_ids: List[int]
    updates: TaskUpdate

# Helper functions for resolving IDs
def get_user_id_by_github_username(conn, github_username: str) -> Optional[int]:
    """Get user ID from github username"""
    try:
        from db.models.users import users
        query = select(users.c.id).where(users.c.github_username == github_username)
        result = conn.execute(query).fetchone()
        print(f"Debug: Looking for user '{github_username}', found result: {result}")
        return result[0] if result else None
    except Exception as e:
        print(f"Error getting user ID: {e}")
        return None

def get_repository_id(conn, owner: str, repo_name: str) -> Optional[int]:
    """Get repository ID from owner and name"""
    try:
        from db.models.repositories import repositories
        query = select(repositories.c.id).where(
            and_(
                repositories.c.owner == owner,
                repositories.c.name == repo_name
            )
        )
        result = conn.execute(query).fetchone()
        print(f"Debug: Looking for repository '{owner}/{repo_name}', found result: {result}")
        return result[0] if result else None
    except Exception as e:
        print(f"Error getting repository ID: {e}")
        return None

@router.get("/tasks", response_model=List[TaskResponse])
async def get_all_tasks(
    limit: Optional[int] = Query(100, description="Limit number of results"),
    offset: Optional[int] = Query(0, description="Offset for pagination"),
    status: Optional[str] = Query(None, description="Filter by status"),
    priority: Optional[str] = Query(None, description="Filter by priority"),
    assignee: Optional[str] = Query(None, description="Filter by assignee"),
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y t·∫•t c·∫£ tasks t·ª´ database v·ªõi filtering v√† pagination"""
    try:
        with engine.connect() as conn:
            # Base query
            query = select(project_tasks)
            
            # Apply filters
            conditions = []
            if status:
                if status == "TODO":
                    conditions.append(project_tasks.c.status == "TODO")
                elif status == "IN_PROGRESS":
                    conditions.append(project_tasks.c.status == "IN_PROGRESS")
                elif status == "DONE":
                    conditions.append(project_tasks.c.status == "DONE")
            
            if priority:
                if priority == "LOW":
                    conditions.append(project_tasks.c.priority == "LOW")
                elif priority == "MEDIUM":
                    conditions.append(project_tasks.c.priority == "MEDIUM")
                elif priority == "HIGH":
                    conditions.append(project_tasks.c.priority == "HIGH")
            
            if assignee:
                conditions.append(project_tasks.c.assignee_github_username == assignee)
            
            if conditions:
                query = query.where(and_(*conditions))
            
            # Apply pagination and ordering
            query = query.order_by(project_tasks.c.created_at.desc()).limit(limit).offset(offset)
            
            result = conn.execute(query)
            tasks = []
            
            for row in result:
                task_dict = {
                    "id": row.id,
                    "title": row.title,
                    "description": row.description,
                    "assignee": row.assignee_github_username,  # Use correct field name
                    "priority": row.priority if row.priority else "MEDIUM",  # Already string
                    "status": row.status if row.status else "TODO",  # Already string
                    "due_date": row.due_date,
                    "repo_owner": row.repo_owner,
                    "repo_name": row.repo_name,
                    "created_at": row.created_at,
                    "updated_at": row.updated_at
                }
                tasks.append(task_dict)
            
            return tasks
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.get("/tasks/stats", response_model=TaskStats)
async def get_task_statistics(
    repo_owner: Optional[str] = Query(None, description="Filter by repository owner"),
    repo_name: Optional[str] = Query(None, description="Filter by repository name"),
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y th·ªëng k√™ tasks t·ª´ database"""
    try:
        with engine.connect() as conn:
            # Base query with optional repo filtering
            base_conditions = []
            if repo_owner:
                base_conditions.append(project_tasks.c.repo_owner == repo_owner)
            if repo_name:
                base_conditions.append(project_tasks.c.repo_name == repo_name)
            
            # Total tasks
            total_query = select([func.count(project_tasks.c.id)])
            if base_conditions:
                total_query = total_query.where(and_(*base_conditions))
            total_tasks = conn.execute(total_query).scalar() or 0
            
            # Tasks by status
            todo_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.status == TaskStatus.TODO
            )
            if base_conditions:
                todo_query = todo_query.where(and_(*base_conditions))
            todo_tasks = conn.execute(todo_query).scalar() or 0
            
            in_progress_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.status == TaskStatus.IN_PROGRESS
            )
            if base_conditions:
                in_progress_query = in_progress_query.where(and_(*base_conditions))
            in_progress_tasks = conn.execute(in_progress_query).scalar() or 0
            
            done_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.status == TaskStatus.DONE
            )
            if base_conditions:
                done_query = done_query.where(and_(*base_conditions))
            done_tasks = conn.execute(done_query).scalar() or 0
            
            # Tasks by priority
            high_priority_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.priority == TaskPriority.HIGH
            )
            if base_conditions:
                high_priority_query = high_priority_query.where(and_(*base_conditions))
            high_priority_tasks = conn.execute(high_priority_query).scalar() or 0
            
            medium_priority_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.priority == TaskPriority.MEDIUM
            )
            if base_conditions:
                medium_priority_query = medium_priority_query.where(and_(*base_conditions))
            medium_priority_tasks = conn.execute(medium_priority_query).scalar() or 0
            
            low_priority_query = select([func.count(project_tasks.c.id)]).where(
                project_tasks.c.priority == TaskPriority.LOW
            )
            if base_conditions:
                low_priority_query = low_priority_query.where(and_(*base_conditions))
            low_priority_tasks = conn.execute(low_priority_query).scalar() or 0
            
            # Overdue tasks (tasks with due_date < today and status != done)
            from datetime import date
            today = date.today()
            overdue_conditions = [
                project_tasks.c.due_date < today,
                project_tasks.c.status != TaskStatus.DONE
            ]
            if base_conditions:
                overdue_conditions.extend(base_conditions)
            
            overdue_query = select([func.count(project_tasks.c.id)]).where(
                and_(*overdue_conditions)
            )
            overdue_tasks = conn.execute(overdue_query).scalar() or 0
            
            return {
                "total_tasks": total_tasks,
                "todo_tasks": todo_tasks,
                "in_progress_tasks": in_progress_tasks,
                "done_tasks": done_tasks,
                "high_priority_tasks": high_priority_tasks,
                "medium_priority_tasks": medium_priority_tasks,
                "low_priority_tasks": low_priority_tasks,
                "overdue_tasks": overdue_tasks
            }
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.get("/tasks/by-assignee/{assignee}", response_model=List[TaskResponse])
async def get_tasks_by_assignee(
    assignee: str,
    status: Optional[str] = Query(None, description="Filter by status"),
    limit: Optional[int] = Query(50, description="Limit number of results"),
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y t·∫•t c·∫£ tasks ƒë∆∞·ª£c giao cho m·ªôt ng∆∞·ªùi c·ª• th·ªÉ"""
    try:
        with engine.connect() as conn:
            conditions = [project_tasks.c.assignee == assignee]
            
            if status:
                if status == "todo":
                    conditions.append(project_tasks.c.status == TaskStatus.TODO)
                elif status == "in_progress":
                    conditions.append(project_tasks.c.status == TaskStatus.IN_PROGRESS)
                elif status == "done":
                    conditions.append(project_tasks.c.status == TaskStatus.DONE)
            
            query = select(project_tasks).where(
                and_(*conditions)
            ).order_by(project_tasks.c.created_at.desc()).limit(limit)
            
            result = conn.execute(query)
            tasks = []
            
            for row in result:
                task_dict = {
                    "id": row.id,
                    "title": row.title,
                    "description": row.description,
                    "assignee": row.assignee,
                    "priority": row.priority.value if row.priority else "medium",
                    "status": row.status.value if row.status else "todo",
                    "due_date": row.due_date,
                    "repo_owner": row.repo_owner,
                    "repo_name": row.repo_name,
                    "created_at": row.created_at,
                    "updated_at": row.updated_at
                }
                tasks.append(task_dict)
            
            return tasks
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.post("/tasks/bulk", response_model=List[TaskResponse])
async def create_bulk_tasks(
    bulk_data: BulkTaskCreate,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """T·∫°o nhi·ªÅu tasks c√πng l√∫c"""
    try:
        created_tasks = []
        
        with engine.connect() as conn:
            for task in bulk_data.tasks:
                # Validate priority and status
                priority_enum = TaskPriority.MEDIUM
                if task.priority == "low":
                    priority_enum = TaskPriority.LOW
                elif task.priority == "high":
                    priority_enum = TaskPriority.HIGH
                    
                status_enum = TaskStatus.TODO
                if task.status == "in_progress":
                    status_enum = TaskStatus.IN_PROGRESS
                elif task.status == "done":
                    status_enum = TaskStatus.DONE
                
                insert_stmt = insert(project_tasks).values(
                    title=task.title,
                    description=task.description,
                    assignee=task.assignee,
                    priority=priority_enum,
                    status=status_enum,
                    due_date=task.due_date,
                    repo_owner=task.repo_owner,
                    repo_name=task.repo_name,
                    created_by=current_user["username"]
                )
                
                result = conn.execute(insert_stmt)
                task_id = result.inserted_primary_key[0]
                
                # Get the created task
                query = select(project_tasks).where(project_tasks.c.id == task_id)
                created_task = conn.execute(query).fetchone()
                
                created_tasks.append({
                    "id": created_task.id,
                    "title": created_task.title,
                    "description": created_task.description,
                    "assignee": created_task.assignee,
                    "priority": created_task.priority.value,
                    "status": created_task.status.value,
                    "due_date": created_task.due_date,
                    "repo_owner": created_task.repo_owner,
                    "repo_name": created_task.repo_name,
                    "created_at": created_task.created_at,
                    "updated_at": created_task.updated_at
                })
            
            conn.commit()
            
        return created_tasks
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.put("/tasks/bulk-update")
async def bulk_update_tasks(
    bulk_update: BulkTaskUpdate,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """C·∫≠p nh·∫≠t nhi·ªÅu tasks c√πng l√∫c"""
    try:
        with engine.connect() as conn:
            # Validate priority and status
            updates = {}
            
            if bulk_update.updates.title:
                updates["title"] = bulk_update.updates.title
            if bulk_update.updates.description:
                updates["description"] = bulk_update.updates.description
            if bulk_update.updates.assignee:
                updates["assignee"] = bulk_update.updates.assignee
            
            if bulk_update.updates.priority:
                priority_enum = TaskPriority.MEDIUM
                if bulk_update.updates.priority == "low":
                    priority_enum = TaskPriority.LOW
                elif bulk_update.updates.priority == "high":
                    priority_enum = TaskPriority.HIGH
                updates["priority"] = priority_enum
                
            if bulk_update.updates.status:
                status_enum = TaskStatus.TODO
                if bulk_update.updates.status == "in_progress":
                    status_enum = TaskStatus.IN_PROGRESS
                elif bulk_update.updates.status == "done":
                    status_enum = TaskStatus.DONE
                updates["status"] = status_enum
            
            if bulk_update.updates.due_date:
                updates["due_date"] = bulk_update.updates.due_date
            
            # Update tasks
            update_stmt = update(project_tasks).where(
                project_tasks.c.id.in_(bulk_update.task_ids)
            ).values(**updates)
            
            result = conn.execute(update_stmt)
            conn.commit()
            
            return {
                "message": f"Successfully updated {result.rowcount} tasks",
                "updated_count": result.rowcount,
                "task_ids": bulk_update.task_ids
            }
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.get("/tasks/search")
async def search_tasks(
    q: str = Query(..., description="Search query"),
    limit: Optional[int] = Query(50, description="Limit number of results"),
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """T√¨m ki·∫øm tasks theo t·ª´ kh√≥a"""
    try:
        with engine.connect() as conn:
            # Search in title, description, and assignee
            search_term = f"%{q}%"
            query = select(project_tasks).where(
                or_(
                    project_tasks.c.title.ilike(search_term),
                    project_tasks.c.description.ilike(search_term),
                    project_tasks.c.assignee.ilike(search_term)
                )
            ).order_by(project_tasks.c.created_at.desc()).limit(limit)
            
            result = conn.execute(query)
            tasks = []
            
            for row in result:
                task_dict = {
                    "id": row.id,
                    "title": row.title,
                    "description": row.description,
                    "assignee": row.assignee,
                    "priority": row.priority.value if row.priority else "medium",
                    "status": row.status.value if row.status else "todo",
                    "due_date": row.due_date,
                    "repo_owner": row.repo_owner,
                    "repo_name": row.repo_name,
                    "created_at": row.created_at,
                    "updated_at": row.updated_at
                }
                tasks.append(task_dict)
            
            return {
                "query": q,
                "results": tasks,
                "count": len(tasks)
            }
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
```

### backend\api\routes\repo.py
```py
# backend/api/routes/repo.py
from fastapi import APIRouter, Request, HTTPException, Query
import httpx
from typing import Optional, List
from services.repo_service import (
    save_repository, fetch_repo_from_github, fetch_repo_from_database,
    get_user_repos_from_database, get_repositories_by_owner, get_repository_stats,
    get_repo_id_by_owner_and_name
)
from services.collaborator_service import get_collaborators_by_repo

repo_router = APIRouter()

# Endpoint l·∫•y th√¥ng tin repository c·ª• th·ªÉ t·ª´ GitHub
@repo_router.get("/github/{owner}/{repo}")
async def fetch_repo(owner: str, repo: str):
    return await fetch_repo_from_github(owner, repo)

@repo_router.get("/github/repos")
async def get_user_repos(request: Request):
    # L·∫•y token t·ª´ header Authorization
    token = request.headers.get("Authorization")
    
    # Ki·ªÉm tra token h·ª£p l·ªá (ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng "token ")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    # G·ªçi GitHub API ƒë·ªÉ l·∫•y danh s√°ch repo
    async with httpx.AsyncClient() as client:
        resp = await client.get( 
            "https://api.github.com/user/repos",
            headers={"Authorization": token}
        )
        # N·∫øu l·ªói th√¨ raise exception
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)
    
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON
    return resp.json()

# Endpoint l·∫•y th√¥ng tin repository t·ª´ database
@repo_router.get("/repodb/{owner}/{repo}")
async def get_repo_from_database(owner: str, repo: str):
    """Fetch repository information from database"""
    try:
        repo_data = await fetch_repo_from_database(owner, repo)
        if not repo_data:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found in database")
        return repo_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching repository from database: {str(e)}")

#Endpoint l·∫•y danh s√°ch repositories t·ª´ database
@repo_router.get("/repodb/repos")
async def get_repos_from_database(
    user_id: Optional[int] = Query(None, description="Filter by user ID"),
    owner: Optional[str] = Query(None, description="Filter by owner"),
    limit: Optional[int] = Query(50, description="Limit number of results"),
    offset: Optional[int] = Query(0, description="Offset for pagination")
):
    """Fetch repositories from database with optional filtering"""
    try:
        if owner:
            # L·∫•y repositories theo owner
            repos = await get_repositories_by_owner(owner, limit, offset)
        elif user_id:
            # L·∫•y repositories theo user_id
            repos = await get_user_repos_from_database(user_id, limit, offset)
        else:
            # L·∫•y t·∫•t c·∫£ repositories
            repos = await get_user_repos_from_database(None, limit, offset)
        
        return {
            "repositories": repos,
            "count": len(repos),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching repositories from database: {str(e)}")

# Save repo v√†o database
@repo_router.post("/github/{owner}/{repo}/save")
async def save_repo(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        repo_data = resp.json()

    repo_entry = {
        "github_id": repo_data["id"],
        "name": repo_data["name"],
        "owner": repo_data["owner"]["login"],
        "description": repo_data["description"],
        "stars": repo_data["stargazers_count"],
        "forks": repo_data["forks_count"],
        "language": repo_data["language"],
        "open_issues": repo_data["open_issues_count"],
        "url": repo_data["html_url"],
    }

    try:
        await save_repository(repo_entry)
        return {"message": f"Repository {owner}/{repo} saved successfully!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving repository: {str(e)}")

# Endpoint l·∫•y collaborators t·ª´ database
@repo_router.get("/github/{owner}/{repo}/collaborators")
async def get_repo_collaborators(owner: str, repo: str):
    """Fetch collaborators for a repository from database"""
    try:
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found in database")
        
        collaborators = await get_collaborators_by_repo(repo_id)
        
        return {
            "repository": f"{owner}/{repo}",
            "collaborators": collaborators,
            "count": len(collaborators)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching collaborators: {str(e)}")
```

### backend\api\routes\repositories.py
```py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import text
from typing import List, Dict, Any
from db.database import get_db
from core.security import get_current_user, CurrentUser

router = APIRouter(prefix="/api", tags=["repositories"])

@router.get("/repositories")
async def get_repositories(
    db: Session = Depends(get_db),
    current_user: CurrentUser = Depends(get_current_user)
):
    """L·∫•y danh s√°ch repositories c·ªßa user - REQUIRE AUTHENTICATION"""
    try:
        # Ch·ªâ user ƒë√£ ƒëƒÉng nh·∫≠p m·ªõi ƒë∆∞·ª£c l·∫•y repos v·ªõi avatar c·ªßa owner
        query = text("""
            SELECT DISTINCT
                r.id,
                r.name,
                r.owner,
                r.full_name,
                r.description,
                r.stars,
                r.forks,
                r.language,
                r.is_private,
                r.url,
                r.created_at,
                u.avatar_url as owner_avatar_url
            FROM repositories r
            LEFT JOIN repository_collaborators rc ON r.id = rc.repository_id
            LEFT JOIN collaborators c ON rc.collaborator_id = c.id
            LEFT JOIN users u ON r.owner = u.github_username
            WHERE 
                r.owner = :github_username  -- Repos owned by user
                OR c.github_username = :github_username  -- Repos where user is collaborator
            ORDER BY r.name
        """)
        
        result = db.execute(query, {"github_username": current_user.github_username}).fetchall()
        
        repositories = []
        for row in result:
            repo = {
                "id": row[0],
                "name": row[1],
                "owner": {
                    "login": row[2],
                    "avatar_url": row[11]  # owner_avatar_url from JOIN with users table
                },
                "full_name": row[3] or f"{row[2]}/{row[1]}",
                "description": row[4],
                "stargazers_count": row[5] or 0,
                "forks_count": row[6] or 0,
                "language": row[7],
                "private": row[8] or False,
                "html_url": row[9],
                "created_at": row[10].isoformat() if row[10] else None
            }
            repositories.append(repo)
        
        return repositories
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching repositories: {str(e)}")

@router.get("/github/repositories")  
async def get_github_repositories(
    db: Session = Depends(get_db),
    current_user: CurrentUser = Depends(get_current_user)
):
    """Alias cho compatibility v·ªõi frontend"""
    return await get_repositories(db, current_user)

@router.get("/{owner}/{repo}/branches")
async def get_repository_branches(
    owner: str,
    repo: str,
    db: Session = Depends(get_db),
    current_user: CurrentUser = Depends(get_current_user)
):
    """L·∫•y danh s√°ch branches c·ªßa repository theo owner/repo"""
    try:
        # Get repository first
        repo_query = text("""
            SELECT id FROM repositories 
            WHERE owner = :owner AND name = :repo
        """)
        repo_result = db.execute(repo_query, {"owner": owner, "repo": repo}).fetchone()
        
        if not repo_result:
            raise HTTPException(status_code=404, detail=f"Repository {owner}/{repo} not found")
        
        repo_id = repo_result[0]
        
        # Get branches for this repository
        branches_query = text("""
            SELECT 
                id, name, repo_id, creator_name, last_committer_name,
                sha, is_default, is_protected, created_at, last_commit_date,
                commits_count, contributors_count
            FROM branches 
            WHERE repo_id = :repo_id
            ORDER BY is_default DESC, name ASC
        """)
        
        results = db.execute(branches_query, {"repo_id": repo_id}).fetchall()
        
        branches = []
        for row in results:
            branch = {
                "id": row[0],
                "name": row[1],
                "repo_id": row[2],
                "creator_name": row[3],
                "last_committer_name": row[4],
                "sha": row[5],
                "is_default": row[6],
                "is_protected": row[7],
                "created_at": row[8].isoformat() if row[8] else None,
                "last_commit_date": row[9].isoformat() if row[9] else None,
                "commits_count": row[10] or 0,
                "contributors_count": row[11] or 0
            }
            branches.append(branch)
        
        return {
            "repository": f"{owner}/{repo}",
            "branches": branches,
            "total": len(branches)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching branches: {str(e)}")

```

### backend\api\routes\sync.py
```py
# backend/api/routes/sync.py
from fastapi import APIRouter, Request, HTTPException
import httpx
import asyncio
import logging
from typing import Dict, Any, Optional
from services.repo_service import save_repository, get_repo_id_by_owner_and_name
from services.branch_service import sync_branches_for_repo
from services.commit_service import save_commit
from services.issue_service import save_issue
from services.github_service import fetch_commit_details, fetch_branch_stats

sync_router = APIRouter()
logger = logging.getLogger(__name__)

# Constants
GITHUB_API_BASE = "https://api.github.com"

async def github_api_call(url: str, token: str, retries: int = 3) -> Dict[str, Any]:
    """
    G·ªçi GitHub API v·ªõi error handling v√† retry logic
    
    Args:
        url: GitHub API URL
        token: Authorization token
        retries: S·ªë l·∫ßn retry n·∫øu rate limit
    
    Returns:
        Response JSON data
    
    Raises:
        HTTPException: Khi API call th·∫•t b·∫°i
    """
    headers = {
        "Authorization": token,
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28"
    }
    
    for attempt in range(retries + 1):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.get(url, headers=headers)
                
                # Handle rate limiting
                if resp.status_code == 429:
                    if attempt < retries:
                        reset_time = int(resp.headers.get("X-RateLimit-Reset", "0"))
                        wait_time = min(reset_time - int(asyncio.get_event_loop().time()), 60)
                        logger.warning(f"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}")
                        await asyncio.sleep(max(wait_time, 1))
                        continue
                    else:
                        raise HTTPException(
                            status_code=429, 
                            detail="GitHub API rate limit exceeded. Please try again later."
                        )
                
                # Handle other HTTP errors
                if resp.status_code != 200:
                    error_detail = f"GitHub API error: {resp.status_code}"
                    try:
                        error_data = resp.json()
                        error_detail += f" - {error_data.get('message', resp.text)}"
                    except:
                        error_detail += f" - {resp.text}"
                    
                    raise HTTPException(status_code=resp.status_code, detail=error_detail)
                
                return resp.json()
                
            except httpx.TimeoutException:
                if attempt < retries:
                    logger.warning(f"Request timeout, retrying... (attempt {attempt + 1})")
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    raise HTTPException(status_code=408, detail="GitHub API request timeout")
            
            except httpx.RequestError as e:
                if attempt < retries:
                    logger.warning(f"Request error: {e}, retrying... (attempt {attempt + 1})")
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    raise HTTPException(status_code=500, detail=f"GitHub API request failed: {str(e)}")
    
    raise HTTPException(status_code=500, detail="All retry attempts failed")

# ƒê·ªìng b·ªô to√†n b·ªô d·ªØ li·ªáu
@sync_router.post("/github/{owner}/{repo}/sync-all")
async def sync_all(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    try:
        # 1. Sync repository
        repo_data = await github_api_call(f"https://api.github.com/repos/{owner}/{repo}", token)
        repo_entry = {
            "github_id": repo_data["id"],
            "name": repo_data["name"],
            "owner": repo_data["owner"]["login"],
            "description": repo_data["description"],
            "stars": repo_data["stargazers_count"],
            "forks": repo_data["forks_count"],
            "language": repo_data["language"],
            "open_issues": repo_data["open_issues_count"],
            "url": repo_data["html_url"],
            # B·ªï sung c√°c fields t·ª´ database model
            "full_name": repo_data.get("full_name"),
            "clone_url": repo_data.get("clone_url"),
            "is_private": repo_data.get("private", False),
            "is_fork": repo_data.get("fork", False),
            "default_branch": repo_data.get("default_branch", "main"),
            "sync_status": "completed",        }
        await save_repository(repo_entry)
        
        # 2. Sync branches
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
            
        branches_data = await github_api_call(f"https://api.github.com/repos/{owner}/{repo}/branches", token)
          # Chu·∫©n h√≥a d·ªØ li·ªáu branch v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
        default_branch = repo_data.get("default_branch", "main")
        branches_to_save = []
        
        for branch in branches_data:
            branch_info = {
                "name": branch["name"],
                "sha": branch.get("commit", {}).get("sha"),
                "is_default": branch["name"] == default_branch,
                "is_protected": branch.get("protected", False),
            }
            
            # T√πy ch·ªçn: L·∫•y th√™m th√¥ng tin commit chi ti·∫øt (c√≥ th·ªÉ l√†m ch·∫≠m API)
            # Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën l·∫•y th√™m th√¥ng tin
            # if branch_info["sha"]:
            #     commit_details = await fetch_commit_details(branch_info["sha"], owner, repo, token)
            #     if commit_details:
            #         branch_info["last_commit_date"] = commit_details["date"]
            #         branch_info["last_committer_name"] = commit_details["committer_name"]
            
            branches_to_save.append(branch_info)
        
        # ƒê·ªìng b·ªô h√≥a h√†ng lo·∫°t v·ªõi d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß
        branches_synced = await sync_branches_for_repo(
            repo_id, 
            branches_to_save, 
            default_branch=default_branch,
            replace_existing=True
        )
        return {"message": f"ƒê·ªìng b·ªô repository {owner}/{repo} th√†nh c√¥ng!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªìng b·ªô {owner}/{repo}: {str(e)}")

# Endpoint ƒë·ªìng b·ªô nhanh - ch·ªâ th√¥ng tin c∆° b·∫£n
@sync_router.post("/github/{owner}/{repo}/sync-basic")
async def sync_basic(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    try:
        # Ch·ªâ ƒë·ªìng b·ªô repository
        repo_data = await github_api_call(f"https://api.github.com/repos/{owner}/{repo}", token)
        repo_entry = {
            "github_id": repo_data["id"],
            "name": repo_data["name"],
            "owner": repo_data["owner"]["login"],
            "description": repo_data["description"],
            "stars": repo_data["stargazers_count"],
            "forks": repo_data["forks_count"],
            "language": repo_data["language"],
            "open_issues": repo_data["open_issues_count"],
            "url": repo_data["html_url"],
            "full_name": repo_data.get("full_name"),
            "clone_url": repo_data.get("clone_url"),
            "is_private": repo_data.get("private", False),
            "is_fork": repo_data.get("fork", False),
            "default_branch": repo_data.get("default_branch", "main"),
            "sync_status": "completed",
        }
        await save_repository(repo_entry)
        
        return {"message": f"ƒê·ªìng b·ªô c∆° b·∫£n {owner}/{repo} th√†nh c√¥ng!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªìng b·ªô c∆° b·∫£n {owner}/{repo}: {str(e)}")

# Endpoint ƒë·ªìng b·ªô n√¢ng cao - bao g·ªìm th√¥ng tin commit chi ti·∫øt
@sync_router.post("/github/{owner}/{repo}/sync-enhanced")
async def sync_enhanced(owner: str, repo: str, request: Request):
    """
    ƒê·ªìng b·ªô repository v·ªõi th√¥ng tin chi ti·∫øt bao g·ªìm:
    - Th√¥ng tin repository ƒë·∫ßy ƒë·ªß
    - Th√¥ng tin branch ƒë·∫ßy ƒë·ªß
    - Th√¥ng tin commit cu·ªëi c√πng cho m·ªói branch
    - Th·ªëng k√™ branch (n·∫øu c√≥)
    
    L∆∞u √Ω: Endpoint n√†y s·∫Ω ch·∫≠m h∆°n do ph·∫£i g·ªçi nhi·ªÅu API calls
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    try:
        logger.info(f"Starting enhanced sync for {owner}/{repo}")
        
        # 1. Sync repository
        repo_data = await github_api_call(f"https://api.github.com/repos/{owner}/{repo}", token)
        repo_entry = {
            "github_id": repo_data["id"],
            "name": repo_data["name"],
            "owner": repo_data["owner"]["login"],
            "description": repo_data["description"],
            "stars": repo_data["stargazers_count"],
            "forks": repo_data["forks_count"],
            "language": repo_data["language"],
            "open_issues": repo_data["open_issues_count"],
            "url": repo_data["html_url"],
            "full_name": repo_data.get("full_name"),
            "clone_url": repo_data.get("clone_url"),
            "is_private": repo_data.get("private", False),
            "is_fork": repo_data.get("fork", False),
            "default_branch": repo_data.get("default_branch", "main"),
            "sync_status": "enhanced_completed",
        }
        await save_repository(repo_entry)
        
        # 2. Sync branches v·ªõi th√¥ng tin chi ti·∫øt
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
        
        branches_data = await github_api_call(f"https://api.github.com/repos/{owner}/{repo}/branches", token)
        
        default_branch = repo_data.get("default_branch", "main")
        branches_to_save = []
        
        # Process branches with enhanced data
        for i, branch in enumerate(branches_data):
            logger.info(f"Processing branch {i+1}/{len(branches_data)}: {branch['name']}")
            
            branch_info = {
                "name": branch["name"],
                "sha": branch.get("commit", {}).get("sha"),
                "is_default": branch["name"] == default_branch,
                "is_protected": branch.get("protected", False),
            }
            
            # L·∫•y th√¥ng tin commit chi ti·∫øt cho branch
            if branch_info["sha"]:
                try:
                    commit_details = await fetch_commit_details(
                        branch_info["sha"], owner, repo, token
                    )
                    if commit_details:
                        branch_info.update({
                            "last_commit_date": commit_details.get("date"),
                            "last_committer_name": commit_details.get("committer_name"),
                            "creator_name": commit_details.get("author_name"),  # Assuming first commit author as creator
                        })
                except Exception as e:
                    logger.warning(f"Failed to fetch commit details for branch {branch['name']}: {e}")
            
            # L·∫•y th·ªëng k√™ branch (optional)
            try:
                branch_stats = await fetch_branch_stats(owner, repo, branch["name"], token)
                if branch_stats:
                    branch_info.update({
                        "commits_count": branch_stats.get("commits_count"),
                        "contributors_count": branch_stats.get("contributors_count"),
                    })
            except Exception as e:
                logger.warning(f"Failed to fetch branch stats for {branch['name']}: {e}")
            
            branches_to_save.append(branch_info)
            
            # Add small delay to avoid hitting rate limits too hard
            if i < len(branches_data) - 1:  # Don't sleep after last branch
                await asyncio.sleep(0.1)
        
        # ƒê·ªìng b·ªô h√≥a h√†ng lo·∫°t v·ªõi d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß
        branches_synced = await sync_branches_for_repo(
            repo_id, 
            branches_to_save, 
            default_branch=default_branch,
            replace_existing=True
        )
        
        logger.info(f"Enhanced sync completed for {owner}/{repo}: {branches_synced} branches synced")
        
        return {
            "message": f"ƒê·ªìng b·ªô n√¢ng cao {owner}/{repo} th√†nh c√¥ng!",
            "repository_synced": True,
            "branches_synced": branches_synced,
            "enhanced_data": True
        }
        
    except HTTPException:
        raise  # Re-raise HTTP exceptions as-is
    except Exception as e:
        logger.error(f"Enhanced sync error for {owner}/{repo}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªìng b·ªô n√¢ng cao {owner}/{repo}: {str(e)}")

# Endpoint ki·ªÉm tra tr·∫°ng th√°i GitHub API v√† token
@sync_router.get("/github/status")
async def github_status(request: Request):
    """
    Ki·ªÉm tra tr·∫°ng th√°i k·∫øt n·ªëi GitHub API v√† th√¥ng tin rate limit
    
    Returns:
        dict: Th√¥ng tin v·ªÅ token, rate limit, v√† tr·∫°ng th√°i API
    """
    from services.github_service import validate_github_token, get_rate_limit_info
    
    token = request.headers.get("Authorization", "").replace("token ", "")
    
    result = {
        "github_api_accessible": False,
        "token_valid": False,
        "rate_limit": None,
        "token_provided": bool(token)
    }
    
    try:
        # Ki·ªÉm tra token n·∫øu ƒë∆∞·ª£c cung c·∫•p
        if token:
            result["token_valid"] = await validate_github_token(token)
        
        # L·∫•y th√¥ng tin rate limit
        rate_limit_info = await get_rate_limit_info(token if token else None)
        result["rate_limit"] = rate_limit_info.get("resources", {}).get("core", {})
        result["github_api_accessible"] = True
        
    except Exception as e:
        result["error"] = str(e)
    
    return result

# Endpoint l·∫•y danh s√°ch repositories c√≥ s·∫µn cho user
@sync_router.get("/github/repositories")
async def list_user_repositories(request: Request, per_page: int = 30, page: int = 1):
    """
    L·∫•y danh s√°ch repositories c·ªßa user hi·ªán t·∫°i
    
    Args:
        per_page: S·ªë repo tr√™n m·ªói trang (max 100)
        page: S·ªë trang
    
    Returns:
        list: Danh s√°ch repositories
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    try:
        # Gi·ªõi h·∫°n per_page ƒë·ªÉ tr√°nh qu√° t·∫£i
        per_page = min(max(per_page, 1), 100)
        page = max(page, 1)
        
        url = f"https://api.github.com/user/repos?per_page={per_page}&page={page}&sort=updated"
        repos_data = await github_api_call(url, token)
        
        # Tr·∫£ v·ªÅ th√¥ng tin c∆° b·∫£n c·ªßa c√°c repos
        simplified_repos = []
        for repo in repos_data:
            simplified_repos.append({
                "id": repo["id"],
                "name": repo["name"],
                "full_name": repo["full_name"],
                "owner": repo["owner"]["login"],
                "description": repo.get("description"),
                "language": repo.get("language"),
                "stars": repo["stargazers_count"],
                "forks": repo["forks_count"],
                "updated_at": repo["updated_at"],
                "is_private": repo["private"],
                "is_fork": repo["fork"],
                "default_branch": repo.get("default_branch", "main")
            })
        
        return {
            "repositories": simplified_repos,
            "page": page,
            "per_page": per_page,
            "total_returned": len(simplified_repos)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y danh s√°ch repositories: {str(e)}")

# Endpoint th·ªëng k√™ repository v√† branches
@sync_router.get("/github/{owner}/{repo}/stats")
async def get_repository_stats(owner: str, repo: str):
    """
    L·∫•y th·ªëng k√™ chi ti·∫øt v·ªÅ repository v√† branches
    
    Returns:
        dict: Th·ªëng k√™ repository v√† branches
    """
    from services.branch_service import get_branch_statistics, find_stale_branches, get_most_active_branches
    from services.repo_service import get_repository_by_owner_and_name
    
    try:
        # L·∫•y th√¥ng tin repository
        repo_info = await get_repository_by_owner_and_name(owner, repo)
        if not repo_info:
            raise HTTPException(status_code=404, detail="Repository not found in database")
        
        repo_id = repo_info['id']
        
        # L·∫•y th·ªëng k√™ branches
        branch_stats = await get_branch_statistics(repo_id)
        
        # L·∫•y branches c≈© (90 ng√†y)
        stale_branches = await find_stale_branches(repo_id, days_threshold=90)
        
        # L·∫•y branches ho·∫°t ƒë·ªông nh·∫•t
        active_branches = await get_most_active_branches(repo_id, limit=5)
        
        return {
            "repository": {
                "name": repo_info['name'],
                "owner": repo_info['owner'],
                "stars": repo_info.get('stars', 0),
                "forks": repo_info.get('forks', 0),
                "language": repo_info.get('language'),
                "last_synced": repo_info.get('updated_at'),
                "sync_status": repo_info.get('sync_status', 'unknown')
            },
            "branch_statistics": branch_stats,
            "stale_branches": {
                "count": len(stale_branches),
                "branches": [{"name": b["name"], "last_commit_date": b["last_commit_date"]} for b in stale_branches[:10]]
            },
            "most_active_branches": [
                {
                    "name": b["name"], 
                    "commits_count": b["commits_count"],
                    "is_default": b["is_default"],
                    "is_protected": b["is_protected"]
                } 
                for b in active_branches
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y th·ªëng k√™ {owner}/{repo}: {str(e)}")

# Endpoint ƒë·ªÉ c·∫≠p nh·∫≠t branch metadata
@sync_router.patch("/github/{owner}/{repo}/branches/{branch_name}")
async def update_branch_info(owner: str, repo: str, branch_name: str, metadata: dict, request: Request):
    """
    C·∫≠p nh·∫≠t th√¥ng tin metadata c·ªßa m·ªôt branch
    
    Args:
        owner: Ch·ªß s·ªü h·ªØu repository
        repo: T√™n repository
        branch_name: T√™n branch
        metadata: D·ªØ li·ªáu c·∫ßn c·∫≠p nh·∫≠t
    
    Returns:
        dict: K·∫øt qu·∫£ c·∫≠p nh·∫≠t
    """
    from services.branch_service import update_branch_metadata
    from services.repo_service import get_repo_id_by_owner_and_name
    
    # Optional: Ki·ªÉm tra token n·∫øu c·∫ßn authorization
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    try:
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
        
        success = await update_branch_metadata(repo_id, branch_name, metadata)
        
        if success:
            return {
                "message": f"Branch {branch_name} updated successfully",
                "repository": f"{owner}/{repo}",
                "branch": branch_name,
                "updated_fields": list(metadata.keys())
            }
        else:
            raise HTTPException(status_code=404, detail="Branch not found or no valid fields to update")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói c·∫≠p nh·∫≠t branch {branch_name}: {str(e)}")

# ==================== AUTO-SYNC ENDPOINTS FOR REPOSITORY SELECTION ====================

@sync_router.post("/github/{owner}/{repo}/sync-branches")
async def sync_repository_branches(owner: str, repo: str, request: Request):
    """
    Sync branches for specific repository when user selects it
    Auto-creates repository if not exists
    """
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    try:
        # Get repository ID, create if not exists
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            # Auto-create repository
            repo_data = await github_api_call(f"{GITHUB_API_BASE}/repos/{owner}/{repo}", token)
            repo_entry = {
                "github_id": repo_data["id"],
                "name": repo_data["name"],
                "owner": repo_data["owner"]["login"],
                "description": repo_data.get("description"),
                "full_name": repo_data.get("full_name"),
                "default_branch": repo_data.get("default_branch", "main"),
                "stars": repo_data.get("stargazers_count", 0),
                "forks": repo_data.get("forks_count", 0),
                "language": repo_data.get("language"),
                "is_private": repo_data.get("private", False),
                "sync_status": "auto_created"
            }
            await save_repository(repo_entry)
            repo_id = await get_repo_id_by_owner_and_name(owner, repo)

        # Sync branches
        branches_data = await github_api_call(f"{GITHUB_API_BASE}/repos/{owner}/{repo}/branches", token)
        
        # Enhanced branch data with commit info
        enhanced_branches = []
        default_branch = None
        
        # Get repository info for default branch
        try:
            repo_info = await github_api_call(f"{GITHUB_API_BASE}/repos/{owner}/{repo}", token)
            default_branch = repo_info.get("default_branch", "main")
        except:
            default_branch = "main"
        
        for branch in branches_data:
            try:
                # Get additional commit info for each branch
                commit_data = await github_api_call(
                    f"{GITHUB_API_BASE}/repos/{owner}/{repo}/commits/{branch['commit']['sha']}", 
                    token
                )
                
                enhanced_branch = {
                    "name": branch["name"],
                    "sha": branch["commit"]["sha"],
                    "is_protected": branch.get("protected", False),
                    "is_default": branch["name"] == default_branch,
                    "repo_id": repo_id,
                    "creator_user_id": None,  # Could enhance later
                    "last_committer_user_id": None,  # Could enhance later
                    "commits_count": 1,  # Basic count, could enhance
                    "contributors_count": 1,  # Basic count, could enhance
                    "last_commit_date": commit_data["commit"]["committer"]["date"]
                }
                enhanced_branches.append(enhanced_branch)
                
            except Exception as e:
                logger.warning(f"Error getting commit info for branch {branch['name']}: {e}")
                # Fallback to basic branch info
                enhanced_branches.append({
                    "name": branch["name"],
                    "sha": branch["commit"]["sha"],
                    "is_protected": branch.get("protected", False),
                    "is_default": branch["name"] == default_branch,
                    "repo_id": repo_id,
                    "commits_count": 1,
                    "contributors_count": 1
                })        # Save branches to database
        saved_count = await save_multiple_branches(repo_id, enhanced_branches)
        
        return {
            "status": "success",
            "repository": f"{owner}/{repo}",
            "branches": enhanced_branches,
            "saved_count": saved_count,
            "message": f"Successfully synced {saved_count} branches"
        }
        
    except Exception as e:
        logger.error(f"Error syncing branches for {owner}/{repo}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to sync branches: {str(e)}")

# ==================== HELPER FUNCTIONS ====================

async def save_multiple_branches(repo_id: int, branches_list: list):
    """Save multiple branches efficiently"""
    if not branches_list:
        return 0
    
    try:
        from db.models.branches import branches
        from db.database import engine
        from sqlalchemy import insert, delete
        
        with engine.connect() as conn:
            # Clear existing branches for this repository
            delete_query = delete(branches).where(branches.c.repo_id == repo_id)
            conn.execute(delete_query)
            
            # Insert new branches
            if branches_list:
                insert_query = insert(branches)
                conn.execute(insert_query, branches_list)
            
            conn.commit()
        
        return len(branches_list)
        
    except Exception as e:
        logger.error(f"Error saving branches: {e}")
        return 0

```

### backend\api\routes\users.py
```py

```

### backend\api\routes\__init__.py
```py

```

### backend\core\config.py
```py
# backend/core/config.py
# File c·∫•u h√¨nh ch√≠nh cho ·ª©ng d·ª•ng FastAPI

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os  # L√†m vi·ªác v·ªõi bi·∫øn m√¥i tr∆∞·ªùng
from fastapi.middleware.cors import CORSMiddleware  # Middleware CORS
from starlette.middleware.sessions import SessionMiddleware  # Middleware qu·∫£n l√Ω session
from fastapi import FastAPI  # Framework ch√≠nh
from api.routes.sync import sync_router  # Router cho GitHub Sync API
from api.routes.repo import repo_router  # Router cho Repository API
from api.routes.auth import auth_router  # Router cho x√°c th·ª±c
from api.routes.commit import commit_router  # Router cho Commit API
from dotenv import load_dotenv  # ƒê·ªçc file .env

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# H√†m c·∫•u h√¨nh c√°c middleware cho ·ª©ng d·ª•ng
def setup_middlewares(app: FastAPI):
    """
    Thi·∫øt l·∫≠p c√°c middleware c·∫ßn thi·∫øt cho ·ª©ng d·ª•ng
    
    Args:
        app (FastAPI): Instance c·ªßa FastAPI app
    """
      # Th√™m middleware CORS (Cross-Origin Resource Sharing)
    app.add_middleware(
        CORSMiddleware,
        # Danh s√°ch domain ƒë∆∞·ª£c ph√©p truy c·∫≠p
        allow_origins=[
            "http://localhost:5173",  # Frontend dev (Vite th∆∞·ªùng ch·∫°y ·ªü port 5173)
            "http://localhost:3000",  # Frontend dev (React c√≥ th·ªÉ ch·∫°y ·ªü port 3000)
            "http://127.0.0.1:5173",  # Alternative localhost
            "http://127.0.0.1:3000",  # Alternative localhost
            "*"  # Allow all origins for development (remove in production)
        ],
        allow_credentials=True,  # Cho ph√©p g·ª≠i credential (cookies, auth headers)
        allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ HTTP methods
        allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers (bao g·ªìm Authorization)
    )

    # Th√™m middleware qu·∫£n l√Ω session
    app.add_middleware(
        SessionMiddleware,
        secret_key=os.getenv('SECRET_KEY')  # Kh√≥a b√≠ m·∫≠t t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    )


# H√†m c·∫•u h√¨nh c√°c router cho ·ª©ng d·ª•ng
def setup_routers(app: FastAPI):
    """
    ƒêƒÉng k√Ω c√°c router ch√≠nh c·ªßa ·ª©ng d·ª•ng
    
    Args:
        app (FastAPI): Instance c·ªßa FastAPI app
    """
    # ƒêƒÉng k√Ω auth router v·ªõi prefix /auth
    app.include_router(auth_router, prefix="/auth")
    
    # ƒêƒÉng k√Ω GitHub sync router v·ªõi prefix /api
    app.include_router(sync_router, prefix="/api")
    
    # ƒêƒÉng k√Ω repository router v·ªõi prefix /api
    app.include_router(repo_router, prefix="/api")
    
    # ƒêƒÉng k√Ω commit router v·ªõi prefix /api/commits
    app.include_router(commit_router, prefix="/api")
```

### backend\core\lifespan.py
```py
from fastapi import FastAPI
from contextlib import asynccontextmanager
from db.database import database
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        await database.connect()
        logger.info("‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi database th√†nh c√¥ng.")
        yield  # Ch·ªâ yield n·∫øu connect th√†nh c√¥ng
    except Exception as e:
        logger.error(f"‚ùå K·∫øt n·ªëi database th·∫•t b·∫°i: {e}")
        raise e  # D·ª´ng app n·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c DB
    finally:
        try:
            await database.disconnect()
            logger.info("üõë ƒê√£ ng·∫Øt k·∫øt n·ªëi database.")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ng·∫Øt k·∫øt n·ªëi database: {e}")

```

### backend\core\logger.py
```py
# core/logger.py

import logging

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,  # Hi·ªán log t·ª´ c·∫•p INFO tr·ªü l√™n
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

```

### backend\core\oauth.py
```py
# backend/core/oauth.py
# File c·∫•u h√¨nh OAuth cho ·ª©ng d·ª•ng, ch·ªß y·∫øu d√πng cho GitHub OAuth

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os  # ƒê·ªÉ l√†m vi·ªác v·ªõi bi·∫øn m√¥i tr∆∞·ªùng
from dotenv import load_dotenv  # ƒê·ªÉ ƒë·ªçc file .env
from authlib.integrations.starlette_client import OAuth  # Th∆∞ vi·ªán OAuth cho Starlette/FastAPI

# Load c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# Kh·ªüi t·∫°o instance OAuth
oauth = OAuth()

# ƒêƒÉng k√Ω provider GitHub cho OAuth
oauth.register(
    name='github',  # T√™n provider
    
    # Client ID t·ª´ ·ª©ng d·ª•ng GitHub OAuth App
    client_id=os.getenv('GITHUB_CLIENT_ID'),
    
    # Client Secret t·ª´ ·ª©ng d·ª•ng GitHub OAuth App
    client_secret=os.getenv('GITHUB_CLIENT_SECRET'),
    
    # URL ƒë·ªÉ l·∫•y access token
    access_token_url='https://github.com/login/oauth/access_token',
    
    # C√°c params th√™m khi l·∫•y access token (None n·∫øu kh√¥ng c√≥)
    access_token_params=None,
    
    # URL ƒë·ªÉ x√°c th·ª±c
    authorize_url='https://github.com/login/oauth/authorize',
    
    # C√°c params th√™m khi x√°c th·ª±c (None n·∫øu kh√¥ng c√≥)
    authorize_params=None,
    
    # Base URL cho API GitHub
    api_base_url='https://api.github.com/',
    
    # C√°c tham s·ªë b·ªï sung cho client
    client_kwargs={
        'scope': 'read:user user:email repo'  # C√°c quy·ªÅn y√™u c·∫ßu
        # read:user - ƒê·ªçc th√¥ng tin user
        # user:email - ƒê·ªçc email user
        # repo - Truy c·∫≠p repository
    }
)
```

### backend\core\security.py
```py
# backend/core/security.py
"""
Security module for authentication and authorization
Handles GitHub OAuth tokens and user session management
"""

from fastapi import Depends, HTTPException, status, Header, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.security.utils import get_authorization_scheme_param
from fastapi.security.base import SecurityBase
from fastapi.openapi.models import HTTPBearer as HTTPBearerModel
from starlette.requests import Request
from typing import Optional, Dict, Any
import httpx
import logging
from functools import lru_cache

from services.user_service import get_user_by_github_id
from db.models.users import users
from db.database import database, engine
from sqlalchemy import select

logger = logging.getLogger(__name__)

class GitHubTokenBearer(SecurityBase):
    """
    Custom security scheme that accepts both 'Bearer' and 'token' schemes
    """
    def __init__(self, auto_error: bool = True):
        self.auto_error = auto_error

    async def __call__(self, request: Request) -> Optional[str]:
        authorization = request.headers.get("Authorization")
        if not authorization:
            if self.auto_error:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Not authenticated",
                    headers={"WWW-Authenticate": "Bearer"},
                )
            return None

        try:
            scheme, credentials = authorization.split(' ', 1)
            if scheme.lower() not in ["bearer", "token"]:
                if self.auto_error:
                    raise HTTPException(
                        status_code=status.HTTP_401_UNAUTHORIZED,
                        detail="Invalid authentication credentials",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
                return None
        except ValueError:
            if self.auto_error:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid authorization header format",
                    headers={"WWW-Authenticate": "Bearer"},
                )
            return None

        return credentials  # Return just the token string

# Security scheme for both Bearer and token formats
security = GitHubTokenBearer(auto_error=False)

class CurrentUser:
    """Current user data structure"""
    def __init__(self, user_data: dict):
        self.id = user_data.get("id")
        self.github_id = user_data.get("github_id")
        self.github_username = user_data.get("github_username")
        self.email = user_data.get("email")
        self.display_name = user_data.get("display_name")
        self.full_name = user_data.get("full_name")
        self.avatar_url = user_data.get("avatar_url")
        self.is_active = user_data.get("is_active", True)
        self.is_verified = user_data.get("is_verified", False)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "github_id": self.github_id,
            "github_username": self.github_username,
            "username": self.github_username,  # Alias for backward compatibility
            "email": self.email,
            "display_name": self.display_name,
            "full_name": self.full_name,
            "avatar_url": self.avatar_url,
            "is_active": self.is_active,
            "is_verified": self.is_verified
        }

async def verify_github_token(token: str) -> Optional[Dict[str, Any]]:
    """
    Verify GitHub token and get user info
    Note: LRU cache removed as it doesn't work with async functions
    """
    try:
        async with httpx.AsyncClient() as client:
            headers = {
                "Authorization": f"token {token}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            # Get user info from GitHub API
            response = await client.get("https://api.github.com/user", headers=headers)
            
            if response.status_code == 200:
                github_user = response.json()
                return github_user
            elif response.status_code == 401:
                logger.warning("Invalid GitHub token provided")
                return None
            else:
                logger.error(f"GitHub API error: {response.status_code}")
                return None
                
    except Exception as e:
        logger.error(f"Error verifying GitHub token: {e}")
        return None

async def get_current_user_from_token(token: str) -> Optional[CurrentUser]:
    """
    Get current user from GitHub token
    """
    try:
        # Verify token with GitHub
        github_user = await verify_github_token(token)
        if not github_user:
            return None
        
        # Get user from our database using engine connection
        with engine.connect() as conn:
            query = select(users).where(users.c.github_id == github_user["id"])
            db_user = conn.execute(query).fetchone()
            
            if not db_user:
                logger.warning(f"User {github_user['login']} not found in database")
                return None
            
            # Check if user is active (handle None values properly)
            if db_user.is_active is False:  # Only reject if explicitly False
                logger.warning(f"User {github_user['login']} is inactive")
                return None
            
            # Convert row to dict using _mapping
            user_dict = dict(db_user._mapping)
            return CurrentUser(user_dict)
        
    except Exception as e:
        logger.error(f"Error getting current user: {e}")
        return None

async def get_current_user(
    token: Optional[str] = Depends(security)
) -> CurrentUser:
    """
    FastAPI dependency to get current authenticated user
    """
    if not token:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authentication required",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    user = await get_current_user_from_token(token)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    return user

async def get_current_user_optional(
    token: Optional[str] = Depends(security)
) -> Optional[CurrentUser]:
    """
    FastAPI dependency to get current user (optional)
    Returns None if not authenticated instead of raising error
    """
    if not token:
        return None
    
    return await get_current_user_from_token(token)

# Alternative dependency that accepts token from header (supports both Bearer and token formats)
async def get_current_user_from_header(
    authorization: Optional[str] = Header(None)
) -> CurrentUser:
    """
    FastAPI dependency to get current user from Authorization header
    Supports both "Bearer <token>" and "token <token>" formats
    """
    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authorization header required",
        )
    
    # Extract token from "Bearer <token>" or "token <token>" format
    try:
        scheme, token = authorization.split(' ', 1)  # Split only on first space
        if scheme.lower() not in ["bearer", "token"]:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication scheme. Use 'Bearer' or 'token'",
            )
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authorization header format",
        )
    
    user = await get_current_user_from_token(token)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired token",
        )
    
    return user

# th√™m m·ªôt h√†m ƒë·ªÉ l·∫•y ng∆∞·ªùi d√πng hi·ªán t·∫°i v·ªõi t√πy ch·ªçn tr·∫£ v·ªÅ None n·∫øu kh√¥ng c√≥ token
async def get_current_user_strict_optional(
    request: Request
) -> Optional[CurrentUser]:
    """
    FastAPI dependency to get current user with strict validation
    - If no Authorization header: returns None (allowed)
    - If Authorization header exists but invalid: raises 401 error
    - If Authorization header exists and valid: returns CurrentUser
    """
    authorization = request.headers.get("Authorization")
    if not authorization:
        return None  # No token provided - this is OK
    
    try:
        scheme, credentials = authorization.split(' ', 1)
        if scheme.lower() not in ["bearer", "token"]:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication scheme. Use 'Bearer' or 'token'",
                headers={"WWW-Authenticate": "Bearer"},
            )
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authorization header format",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Token provided - must be valid
    user = await get_current_user_from_token(credentials)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    return user

# Convenience function for backward compatibility
async def get_user_from_token(token: str) -> Optional[Dict[str, Any]]:
    """
    Legacy function for backward compatibility
    Returns user dict instead of CurrentUser object
    """
    user = await get_current_user_from_token(token)
    return user.to_dict() if user else None

```

### backend\migrations\env.py
```py
import os
from dotenv import load_dotenv
from sqlalchemy import engine_from_config, pool
from alembic import context
from db.metadata import metadata  # Import metadata t·ª´ metadata.py

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# L·∫•y DATABASE_URL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
config = context.config
database_url = os.getenv("DATABASE_URL").replace("asyncpg", "psycopg2")
config.set_main_option("sqlalchemy.url", database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    from logging.config import fileConfig
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```

### backend\migrations\versions\a989fa2a380c_initial_migration_with_all_models.py
```py
"""Initial migration with all models

Revision ID: a989fa2a380c
Revises: 
Create Date: 2025-06-20 21:03:16.707542

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'a989fa2a380c'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('repository_collaborators')
    op.drop_table('assignments')
    op.drop_table('collaborators')
    op.drop_table('project_tasks')
    op.drop_table('issues')
    op.drop_table('user_repositories')
    op.drop_table('users')
    op.drop_table('repositories')
    op.drop_table('pull_requests')
    op.drop_table('commits')
    op.drop_table('branches')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('branches',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('branches_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('repo_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('creator_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('creator_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('last_committer_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('last_committer_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('sha', sa.VARCHAR(length=40), autoincrement=False, nullable=True),
    sa.Column('is_default', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('is_protected', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_commit_date', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_synced', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('commits_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('contributors_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['creator_user_id'], ['users.id'], name='branches_creator_user_id_fkey'),
    sa.ForeignKeyConstraint(['last_committer_user_id'], ['users.id'], name='branches_last_committer_user_id_fkey'),
    sa.ForeignKeyConstraint(['repo_id'], ['repositories.id'], name='branches_repo_id_fkey'),
    sa.PrimaryKeyConstraint('id', name='branches_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_table('commits',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('sha', sa.VARCHAR(length=40), autoincrement=False, nullable=False),
    sa.Column('message', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('author_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('author_name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('author_email', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('committer_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('committer_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('committer_email', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('repo_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('branch_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('branch_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('author_role_at_commit', sa.VARCHAR(length=20), autoincrement=False, nullable=True),
    sa.Column('author_permissions_at_commit', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('date', postgresql.TIMESTAMP(), autoincrement=False, nullable=False),
    sa.Column('committer_date', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('insertions', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('deletions', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('files_changed', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('parent_sha', sa.VARCHAR(length=40), autoincrement=False, nullable=True),
    sa.Column('is_merge', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('merge_from_branch', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('last_synced', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['author_user_id'], ['users.id'], name=op.f('commits_author_user_id_fkey')),
    sa.ForeignKeyConstraint(['branch_id'], ['branches.id'], name=op.f('commits_branch_id_fkey')),
    sa.ForeignKeyConstraint(['committer_user_id'], ['users.id'], name=op.f('commits_committer_user_id_fkey')),
    sa.ForeignKeyConstraint(['repo_id'], ['repositories.id'], name=op.f('commits_repo_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('commits_pkey'))
    )
    op.create_table('pull_requests',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('github_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('title', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('description', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('state', sa.VARCHAR(length=50), autoincrement=False, nullable=True),
    sa.Column('repo_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['repo_id'], ['repositories.id'], name=op.f('pull_requests_repo_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('pull_requests_pkey'))
    )
    op.create_table('repositories',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('repositories_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('github_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('owner', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('full_name', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('stars', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('forks', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('language', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('open_issues', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('url', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('clone_url', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('is_private', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('is_fork', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('default_branch', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('last_synced', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('sync_status', sa.VARCHAR(length=20), autoincrement=False, nullable=True),
    sa.Column('user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name='repositories_user_id_fkey'),
    sa.PrimaryKeyConstraint('id', name='repositories_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_table('users',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('users_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('github_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('github_username', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('email', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('display_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('full_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('avatar_url', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('bio', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('location', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('company', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('blog', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('twitter_username', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('github_profile_url', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('repos_url', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('is_active', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('is_verified', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('github_created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_synced', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name='users_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_table('user_repositories',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('user_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('repository_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('role', sa.VARCHAR(length=12), autoincrement=False, nullable=False),
    sa.Column('permissions', sa.VARCHAR(length=5), autoincrement=False, nullable=False),
    sa.Column('is_primary_owner', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('joined_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('last_accessed', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['repository_id'], ['repositories.id'], name=op.f('user_repositories_repository_id_fkey')),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('user_repositories_user_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('user_repositories_pkey'))
    )
    op.create_table('issues',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('github_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('title', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('body', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('state', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('repo_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['repo_id'], ['repositories.id'], name=op.f('issues_repo_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('issues_pkey'))
    )
    op.create_table('project_tasks',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('title', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('assignee_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('assignee_github_username', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('status', sa.VARCHAR(length=11), autoincrement=False, nullable=False),
    sa.Column('priority', sa.VARCHAR(length=6), autoincrement=False, nullable=False),
    sa.Column('due_date', sa.VARCHAR(length=10), autoincrement=False, nullable=True),
    sa.Column('repository_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('repo_owner', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('repo_name', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('is_completed', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('created_by_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('created_by', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['assignee_user_id'], ['users.id'], name=op.f('project_tasks_assignee_user_id_fkey')),
    sa.ForeignKeyConstraint(['created_by_user_id'], ['users.id'], name=op.f('project_tasks_created_by_user_id_fkey')),
    sa.ForeignKeyConstraint(['repository_id'], ['repositories.id'], name=op.f('project_tasks_repository_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('project_tasks_pkey'))
    )
    op.create_table('collaborators',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('user_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('github_user_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('github_username', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('collaborators_user_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('collaborators_pkey'))
    )
    op.create_table('assignments',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('task_name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('description', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('is_completed', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('user_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('assignments_user_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('assignments_pkey'))
    )
    op.create_table('repository_collaborators',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('repository_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('user_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('role', sa.VARCHAR(length=8), autoincrement=False, nullable=False),
    sa.Column('permissions', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('is_owner', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('joined_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('invited_by', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('invitation_status', sa.VARCHAR(length=20), autoincrement=False, nullable=True),
    sa.Column('commits_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('issues_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('prs_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('last_activity', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_synced', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['repository_id'], ['repositories.id'], name=op.f('repository_collaborators_repository_id_fkey')),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('repository_collaborators_user_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('repository_collaborators_pkey'))
    )
    # ### end Alembic commands ###

```

### backend\models\commit_model.py
```py
# KLTN04\backend\models\commit_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
import joblib
import os

class CommitClassifier:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.model = RandomForestClassifier()
        self.labels = ['normal', 'critical']  # 0: normal, 1: critical/bugfix

    def train(self, df: pd.DataFrame):
        """Hu·∫•n luy·ªán model t·ª´ dataframe"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical']  # C·ªôt nh√£n (0/1)
        self.model.fit(X, y)
        
    def predict(self, new_messages: list):
        """D·ª± ƒëo√°n commit quan tr·ªçng c·∫ßn review"""
        X_new = self.vectorizer.transform(new_messages)
        return self.model.predict(X_new)
    
    def save(self, path=None):
        """L∆∞u model"""
        if path is None:
            # L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa file model
            current_dir = os.path.dirname(os.path.abspath(__file__))
            path = os.path.join(current_dir, 'commit_classifier.joblib')
        
        joblib.dump({
            'vectorizer': self.vectorizer,
            'model': self.model
        }, path)
    
    @classmethod
    def load(cls, path=None):
        """Load model ƒë√£ l∆∞u"""
        if path is None:
            # L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa file model
            current_dir = os.path.dirname(os.path.abspath(__file__))
            path = os.path.join(current_dir, 'commit_classifier.joblib')
        
        try:
            data = joblib.load(path)
            classifier = cls()
            classifier.vectorizer = data['vectorizer']
            classifier.model = data['model']
            return classifier
        except FileNotFoundError:
            print(f"Warning: Model file not found at {path}. Creating new classifier.")
            return cls()
```

### backend\schemas\commit.py
```py
from pydantic import BaseModel
from datetime import datetime


class CommitCreate(BaseModel):
    commit_id: str
    message: str
    author_name: str
    author_email: str
    committed_date: datetime
    repository_id: int


class CommitOut(CommitCreate):
    id: int

    class Config:
        from_attributes = True  # D√†nh cho Pydantic V2 thay cho orm_mode

```

### backend\scripts\commit_analysis_system.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import warnings
warnings.filterwarnings('ignore')

class CommitAnalysisSystem:
    def __init__(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
        self.vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            ngram_range=(1, 1)
        )
        self.model = RandomForestClassifier(
            n_estimators=30,
            max_depth=8,
            n_jobs=1,
            class_weight='balanced'
        )
        self.client = None

    def init_dask_client(self):
        """Kh·ªüi t·∫°o Dask client"""
        self.client = Client(n_workers=2, threads_per_worker=1, memory_limit='2GB')

    @staticmethod
    def lightweight_heuristic(msg):
        """H√†m heuristic tƒ©nh ƒë·ªÉ x·ª≠ l√Ω song song"""
        if not isinstance(msg, str) or not msg.strip():
            return 0
        msg = msg.lower()[:150]
        return int(any(kw in msg for kw in ['fix', 'bug', 'error', 'fail']))

    def process_large_file(self, input_path, output_dir):
        """X·ª≠ l√Ω file l·ªõn v·ªõi Dask """
        try:
            if self.client:
                self.client.close()
            self.init_dask_client()

            # ƒê·ªçc file v·ªõi Dask
            ddf = dd.read_csv(
                str(input_path),
                blocksize="20MB",
                dtype={'message': 'string'},
                usecols=['commit', 'message'],
                na_values=['', 'NA', 'N/A', 'nan']
            )
            
            # S·ª≠a l·ªói: Thay .notna() b·∫±ng .notnull() cho Dask
            ddf = ddf[ddf['message'].notnull()]
            
            # G√°n nh√£n
            ddf['is_critical'] = ddf['message'].map(
                self.lightweight_heuristic,
                meta=('is_critical', 'int8')
            )
            
           # L∆∞u k·∫øt qu·∫£ (ƒë√£ s·ª≠a ph·∫ßn compute)
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # S·ª≠a l·ªói: G·ªçi compute() tr·ª±c ti·∫øp tr√™n to_csv()
            ddf.to_csv(
                str(output_dir / "part_*.csv"),
                index=False
            )
            
            return True
        except Exception as e:
            print(f"üö® L·ªói x·ª≠ l√Ω file: {str(e)}")
            return False
        finally:
            if self.client:
                self.client.close()

    def clean_data(self, df):
        """L√†m s·∫°ch d·ªØ li·ªáu"""
        if 'message' not in df.columns:
            raise ValueError("Thi·∫øu c·ªôt 'message' trong d·ªØ li·ªáu")
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df):
        """G√°n nh√£n t·ª± ƒë·ªông"""
        df = self.clean_data(df)
        df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
        return df

    def train_model(self, df):
        """Hu·∫•n luy·ªán m√¥ h√¨nh"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical'].values
        self.model.fit(X, y)

    def evaluate(self, test_df):
        """ƒê√°nh gi√° m√¥ h√¨nh"""
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        print(classification_report(y_test, self.model.predict(X_test)))

    def save_model(self, path):
        """L∆∞u m√¥ h√¨nh"""
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer
        }, str(path))

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch commit...")
    system = CommitAnalysisSystem()
    
    input_path = Path("D:/Project/KLTN04/data/oneline.csv")
    output_dir = Path("D:/Project/KLTN04/data/processed")
    
    if system.process_large_file(input_path, output_dir):
        print("‚úÖ ƒê√£ x·ª≠ l√Ω file th√†nh c√¥ng")
        
        # N·∫°p v√† x·ª≠ l√Ω d·ªØ li·ªáu
        df = pd.concat([pd.read_csv(f) for f in output_dir.glob("part_*.csv")])
        df = system.auto_label(df)
        
        # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
        system.train_model(df)
        test_df = df.sample(frac=0.2, random_state=42)
        system.evaluate(test_df)
        
        # L∆∞u m√¥ h√¨nh
        model_path = "backend/models/commit_classifier.joblib"
        system.save_model(model_path)
        print(f"üíæ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {model_path}")

if __name__ == "__main__":
    main()
```

### backend\scripts\commit_analysis_system_v1.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import logging
from typing import Optional, Union
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CommitAnalysisSystem:
    """H·ªá th·ªëng ph√¢n t√≠ch commit t·ª± ƒë·ªông v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn"""
    
    VERSION = "1.0.0"
    
    def __init__(self, model_params: Optional[dict] = None, 
                 vectorizer_params: Optional[dict] = None):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng ph√¢n t√≠ch commit
        
        Args:
            model_params: Tham s·ªë cho RandomForestClassifier
            vectorizer_params: Tham s·ªë cho TfidfVectorizer
        """
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        default_vectorizer_params = {
            'max_features': 1000,
            'stop_words': 'english',
            'ngram_range': (1, 2),  # Th√™m bigram
            'min_df': 5,
            'max_df': 0.8
        }
        
        default_model_params = {
            'n_estimators': 100,
            'max_depth': 15,
            'class_weight': 'balanced',
            'random_state': 42
        }
        
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or default_vectorizer_params))
        self.model = RandomForestClassifier(**(model_params or default_model_params))
        self.client = None
        self._is_trained = False

    def init_dask_client(self, **kwargs):
        """Kh·ªüi t·∫°o Dask client v·ªõi c·∫•u h√¨nh t√πy ch·ªçn"""
        default_config = {
            'n_workers': 2,
            'threads_per_worker': 1,
            'memory_limit': '2GB',
            'silence_logs': logging.ERROR
        }
        config = {**default_config, **kwargs}
        
        try:
            self.client = Client(**config)
            logger.info(f"Dask client initialized with config: {config}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Dask client: {str(e)}")
            return False

    @staticmethod
    def lightweight_heuristic(msg: str) -> int:
        """Ph√¢n lo·∫°i commit s·ª≠ d·ª•ng heuristic ƒë∆°n gi·∫£n
        
        Args:
            msg: N·ªôi dung commit message
            
        Returns:
            1 n·∫øu l√† commit quan tr·ªçng (bugfix), 0 n·∫øu kh√¥ng
        """
        if not isinstance(msg, str) or not msg.strip():
            return 0
            
        msg = msg.lower()[:200]  # Gi·ªõi h·∫°n ƒë·ªô d√†i x·ª≠ l√Ω
        keywords = {
            'fix', 'bug', 'error', 'fail', 'patch', 
            'resolve', 'crash', 'defect', 'issue'
        }
        return int(any(kw in msg for kw in keywords))

    def process_large_file(self, input_path: Union[str, Path], output_dir: Union[str, Path]) -> bool:
        """X·ª≠ l√Ω file d·ªØ li·ªáu l·ªõn b·∫±ng Dask"""
        try:
            input_path = Path(input_path)
            output_dir = Path(output_dir)

            if not input_path.exists():
                logger.error(f"Input file not found: {input_path}")
                return False

            logger.info(f"Starting processing large file: {input_path}")
            start_time = datetime.now()

            # Kh·ªüi t·∫°o Dask client
            if not self.init_dask_client():
                return False

            try:
                # ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu
                ddf = dd.read_csv(
                    str(input_path),
                    blocksize="10MB",  # Gi·∫£m k√≠ch th∆∞·ªõc block ƒë·ªÉ an to√†n
                    dtype={'message': 'string'},
                    usecols=['commit', 'message'],
                    na_values=['', 'NA', 'N/A', 'nan']
                )

                # L·ªçc v√† g√°n nh√£n
                ddf = ddf[ddf['message'].notnull()]
                ddf['is_critical'] = ddf['message'].map(
                    self.lightweight_heuristic,
                    meta=('is_critical', 'int8')
                )

                # L∆∞u k·∫øt qu·∫£
                output_dir.mkdir(exist_ok=True, parents=True)
                output_path = str(output_dir / f"processed_{input_path.stem}.csv")

                # S·ª≠ d·ª•ng dask.dataframe.to_csv v·ªõi single_file=True
                ddf.to_csv(
                    output_path,
                    index=False,
                    single_file=True
                )

                logger.info(f"Processing completed in {datetime.now() - start_time}")
                logger.info(f"Results saved to: {output_path}")
                return True

            except Exception as e:
                logger.exception(f"Error during processing: {str(e)}")
                return False

        except Exception as e:
            logger.exception(f"System error: {str(e)}")
            return False

        finally:
            if self.client:
                self.client.close()
                self.client = None

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """L√†m s·∫°ch d·ªØ li·ªáu ƒë·∫ßu v√†o
        
        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu commit
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        if 'message' not in df.columns:
            raise ValueError("Input data must contain 'message' column")
            
        df = df.copy()
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df: pd.DataFrame) -> pd.DataFrame:
        """T·ª± ƒë·ªông g√°n nh√£n cho d·ªØ li·ªáu commit
        
        Args:
            df: DataFrame ch·ª©a c√°c commit message
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
        """
        try:
            df = self.clean_data(df)
            df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
            logger.info(f"Label distribution:\n{df['is_critical'].value_counts()}")
            return df
        except Exception as e:
            logger.error(f"Auto-labeling failed: {str(e)}")
            raise

    def train_model(self, df: pd.DataFrame) -> bool:
        """Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i commit
        
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
            
        Returns:
            True n·∫øu hu·∫•n luy·ªán th√†nh c√¥ng
        """
        try:
            logger.info("Starting model training...")
            
            X = self.vectorizer.fit_transform(df['message'])
            y = df['is_critical'].values
            
            self.model.fit(X, y)
            self._is_trained = True
            
            logger.info("Model training completed successfully")
            return True
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return False

    def evaluate(self, test_df: pd.DataFrame) -> None:
        """ƒê√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh
        
        Args:
            test_df: DataFrame ch·ª©a d·ªØ li·ªáu test
        """
        if not self._is_trained:
            logger.warning("Model has not been trained yet")
            return
            
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        y_pred = self.model.predict(X_test)
        
        report = classification_report(
            y_test, 
            y_pred, 
            target_names=['normal', 'critical']
        )
        logger.info(f"\nModel evaluation:\n{report}")

    def save_model(self, path: Union[str, Path]) -> bool:
        """L∆∞u m√¥ h√¨nh v√† vectorizer
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n l∆∞u model
            
        Returns:
            True n·∫øu l∆∞u th√†nh c√¥ng
        """
        try:
            path = Path(path)
            path.parent.mkdir(exist_ok=True, parents=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'version': self.VERSION,
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, str(path))
            logger.info(f"Model saved to {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            return False

    @classmethod
    def load_model(cls, path: Union[str, Path]):
        """T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model
            
        Returns:
            Instance c·ªßa CommitAnalysisSystem v·ªõi model ƒë√£ t·∫£i
        """
        try:
            path = Path(path)
            model_data = joblib.load(str(path))
            
            system = cls()
            system.model = model_data['model']
            system.vectorizer = model_data['vectorizer']
            system._is_trained = True
            
            logger.info(f"Loaded model (v{model_data.get('version', 'unknown')} "
                       f"created at {model_data.get('timestamp', 'unknown')}")
            return system
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

def main():
    """Entry point cho ·ª©ng d·ª•ng"""
    try:
        logger.info("üöÄ Starting commit analysis system")
        
        # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
        input_path = Path("D:/Project/KLTN04/data/oneline.csv")
        output_dir = Path("D:/Project/KLTN04/data/processed")
        model_path = Path("backend/models/commit_classifier_v1.joblib")
        
        # Kh·ªüi t·∫°o h·ªá th·ªëng
        system = CommitAnalysisSystem()
        
        # X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn
        if system.process_large_file(input_path, output_dir):
            # T·ªïng h·ª£p k·∫øt qu·∫£
            df = pd.concat([
                pd.read_csv(f) 
                for f in output_dir.glob("processed_*.csv")
            ])
            
            # G√°n nh√£n v√† hu·∫•n luy·ªán
            labeled_data = system.auto_label(df)
            system.train_model(labeled_data)
            
            # ƒê√°nh gi√° tr√™n t·∫≠p test
            test_df = labeled_data.sample(frac=0.2, random_state=42)
            system.evaluate(test_df)
            
            # L∆∞u model
            if system.save_model(model_path):
                logger.info(f"‚úÖ Pipeline completed successfully. Model saved to {model_path}")
        
    except Exception as e:
        logger.exception("‚ùå Critical error in main pipeline")
    finally:
        logger.info("üèÅ System shutdown")

if __name__ == "__main__":
    main()
```

### backend\scripts\fix_commit_branch_consistency.py
```py
#!/usr/bin/env python3
"""
Script ƒë·ªÉ fix commit-branch consistency issues
Ch·∫°y script n√†y ƒë·ªÉ ki·ªÉm tra v√† s·ª≠a c√°c v·∫•n ƒë·ªÅ inconsistency trong database
"""

import asyncio
import sys
import os

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from db.database import database
from services.commit_service import validate_and_fix_commit_branch_consistency, get_repo_id_by_owner_and_name
from db.models.repositories import repositories
from sqlalchemy import select
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def fix_all_repositories():
    """Fix commit-branch consistency for all repositories"""
    try:
        await database.connect()
        
        # Get all repositories
        query = select(repositories.c.id, repositories.c.owner, repositories.c.name)
        repos = await database.fetch_all(query)
        
        logger.info(f"üîç Found {len(repos)} repositories to check")
        
        total_fixed = 0
        
        for repo in repos:
            repo_name = f"{repo.owner}/{repo.name}"
            logger.info(f"üìù Checking {repo_name}...")
            
            try:
                fixed_count = await validate_and_fix_commit_branch_consistency(repo.id)
                total_fixed += fixed_count
                
                if fixed_count > 0:
                    logger.info(f"‚úÖ Fixed {fixed_count} inconsistencies in {repo_name}")
                else:
                    logger.info(f"‚úÖ No issues found in {repo_name}")
                    
            except Exception as e:
                logger.error(f"‚ùå Error checking {repo_name}: {e}")
        
        logger.info(f"üéØ SUMMARY: Fixed {total_fixed} total inconsistencies across {len(repos)} repositories")
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}")
    finally:
        await database.disconnect()

async def fix_specific_repository(owner: str, name: str):
    """Fix commit-branch consistency for a specific repository"""
    try:
        await database.connect()
        
        repo_id = await get_repo_id_by_owner_and_name(owner, name)
        if not repo_id:
            logger.error(f"‚ùå Repository {owner}/{name} not found")
            return
        
        logger.info(f"üîç Checking repository {owner}/{name}...")
        
        fixed_count = await validate_and_fix_commit_branch_consistency(repo_id)
        
        if fixed_count > 0:
            logger.info(f"‚úÖ Fixed {fixed_count} inconsistencies in {owner}/{name}")
        else:
            logger.info(f"‚úÖ No issues found in {owner}/{name}")
            
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
    finally:
        await database.disconnect()

if __name__ == "__main__":
    if len(sys.argv) == 3:
        # Fix specific repository
        owner, name = sys.argv[1], sys.argv[2]
        asyncio.run(fix_specific_repository(owner, name))
    else:
        # Fix all repositories
        print("üöÄ Fixing commit-branch consistency for all repositories...")
        print("üìù To fix a specific repository, run: python fix_commit_branch_consistency.py OWNER REPO_NAME")
        asyncio.run(fix_all_repositories())

```

### backend\scripts\migrate_collaborators.py
```py
# backend/scripts/migrate_collaborators.py
"""
Migration script to refactor collaborators model and migrate existing data
"""

import asyncio
import logging
from sqlalchemy import select, insert, update, text, MetaData, Table, Column, Integer, String, DateTime, Boolean, Text, ForeignKey, Index
from db.database import database, engine
from db.models.repository_collaborators import repository_collaborators
from db.models.users import users
from db.models.repositories import repositories
from datetime import datetime

logger = logging.getLogger(__name__)

async def create_new_collaborators_table():
    """Create the new collaborators table with enhanced schema"""
    try:
        # Create the new collaborators table
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS collaborators_new (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            github_user_id INTEGER UNIQUE NOT NULL,
            github_username VARCHAR(255) NOT NULL,
            display_name VARCHAR(255),
            email VARCHAR(255),
            avatar_url VARCHAR(500),
            bio TEXT,
            company VARCHAR(255),
            location VARCHAR(255),
            blog VARCHAR(500),
            is_site_admin BOOLEAN DEFAULT FALSE,
            node_id VARCHAR(255),
            gravatar_id VARCHAR(255),
            type VARCHAR(50) DEFAULT 'User',
            user_id INTEGER,
            created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users(id)
        );
        """
        
        await database.execute(text(create_table_sql))
        
        # Create indexes
        await database.execute(text("CREATE INDEX IF NOT EXISTS idx_collaborators_github_user_id ON collaborators_new(github_user_id);"))
        await database.execute(text("CREATE INDEX IF NOT EXISTS idx_collaborators_github_username ON collaborators_new(github_username);"))
        
        logger.info("Created new collaborators table")
        
    except Exception as e:
        logger.error(f"Error creating new collaborators table: {e}")
        raise

async def migrate_existing_data():
    """Migrate existing data from users and repository_collaborators to new schema"""
    try:
        # First, migrate unique GitHub users from repository_collaborators to collaborators_new
        migrate_sql = """
        INSERT OR IGNORE INTO collaborators_new (
            github_user_id, github_username, display_name, email, avatar_url, 
            bio, company, location, blog, user_id, created_at, updated_at
        )
        SELECT DISTINCT 
            u.github_id,
            u.github_username,
            u.display_name,
            u.email,
            u.avatar_url,
            u.bio,
            u.company,
            u.location,
            u.blog,
            u.id,
            COALESCE(u.created_at, CURRENT_TIMESTAMP),
            CURRENT_TIMESTAMP
        FROM repository_collaborators rc
        JOIN users u ON rc.user_id = u.id
        WHERE u.github_id IS NOT NULL;
        """
        
        result = await database.execute(text(migrate_sql))
        logger.info(f"Migrated {result} unique collaborators from existing data")
        
        return result
        
    except Exception as e:
        logger.error(f"Error migrating existing data: {e}")
        raise

async def create_new_repository_collaborators_table():
    """Create new repository_collaborators table with collaborator_id reference"""
    try:
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS repository_collaborators_new (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            repository_id INTEGER NOT NULL,
            collaborator_id INTEGER NOT NULL,
            role VARCHAR(50) NOT NULL,
            permissions VARCHAR(100),
            is_owner BOOLEAN NOT NULL DEFAULT FALSE,
            joined_at DATETIME,
            invited_by VARCHAR(255),
            invitation_status VARCHAR(20),
            commits_count INTEGER DEFAULT 0,
            issues_count INTEGER DEFAULT 0,
            prs_count INTEGER DEFAULT 0,
            last_activity DATETIME,
            created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            last_synced DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (repository_id) REFERENCES repositories(id),
            FOREIGN KEY (collaborator_id) REFERENCES collaborators_new(id)
        );
        """
        
        await database.execute(text(create_table_sql))
        
        # Create indexes
        await database.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_collaborators_repo_new ON repository_collaborators_new(repository_id);"))
        await database.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_collaborators_collaborator_new ON repository_collaborators_new(collaborator_id);"))
        await database.execute(text("CREATE UNIQUE INDEX IF NOT EXISTS idx_repo_collaborators_unique_new ON repository_collaborators_new(repository_id, collaborator_id);"))
        
        logger.info("Created new repository_collaborators table")
        
    except Exception as e:
        logger.error(f"Error creating new repository_collaborators table: {e}")
        raise

async def migrate_repository_collaborators():
    """Migrate repository_collaborators to use collaborator_id instead of user_id"""
    try:
        migrate_sql = """
        INSERT OR IGNORE INTO repository_collaborators_new (
            repository_id, collaborator_id, role, permissions, is_owner,
            joined_at, invited_by, invitation_status, commits_count, 
            issues_count, prs_count, last_activity, created_at, updated_at, last_synced
        )
        SELECT 
            rc.repository_id,
            c.id as collaborator_id,
            rc.role,
            rc.permissions,
            COALESCE(rc.is_owner, FALSE),
            rc.joined_at,
            rc.invited_by,
            rc.invitation_status,
            COALESCE(rc.commits_count, 0),
            COALESCE(rc.issues_count, 0),
            COALESCE(rc.prs_count, 0),
            rc.last_activity,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP,
            COALESCE(rc.last_synced, CURRENT_TIMESTAMP)
        FROM repository_collaborators rc
        JOIN users u ON rc.user_id = u.id
        JOIN collaborators_new c ON u.github_id = c.github_user_id;
        """
        
        result = await database.execute(text(migrate_sql))
        logger.info(f"Migrated {result} repository collaborator relationships")
        
        return result
        
    except Exception as e:
        logger.error(f"Error migrating repository collaborators: {e}")
        raise

async def backup_old_tables():
    """Backup old tables before dropping them"""
    try:
        # Backup old tables
        await database.execute(text("CREATE TABLE IF NOT EXISTS collaborators_backup AS SELECT * FROM collaborators;"))
        await database.execute(text("CREATE TABLE IF NOT EXISTS repository_collaborators_backup AS SELECT * FROM repository_collaborators;"))
        
        logger.info("Created backup tables")
        
    except Exception as e:
        logger.error(f"Error creating backup tables: {e}")
        raise

async def replace_tables():
    """Replace old tables with new ones"""
    try:
        # Drop old tables
        await database.execute(text("DROP TABLE IF EXISTS collaborators;"))
        await database.execute(text("DROP TABLE IF EXISTS repository_collaborators;"))
        
        # Rename new tables
        await database.execute(text("ALTER TABLE collaborators_new RENAME TO collaborators;"))
        await database.execute(text("ALTER TABLE repository_collaborators_new RENAME TO repository_collaborators;"))
        
        logger.info("Replaced old tables with new ones")
        
    except Exception as e:
        logger.error(f"Error replacing tables: {e}")
        raise

async def verify_migration():
    """Verify the migration was successful"""
    try:
        # Count records in new tables
        collaborators_count = await database.fetch_val(text("SELECT COUNT(*) FROM collaborators;"))
        repo_collabs_count = await database.fetch_val(text("SELECT COUNT(*) FROM repository_collaborators;"))
        
        logger.info(f"Migration verification:")
        logger.info(f"  - Collaborators: {collaborators_count} records")
        logger.info(f"  - Repository collaborators: {repo_collabs_count} records")
        
        # Test a join query
        test_query = text("""
            SELECT COUNT(*) FROM repository_collaborators rc
            JOIN collaborators c ON rc.collaborator_id = c.id
            JOIN repositories r ON rc.repository_id = r.id;
        """)
        join_count = await database.fetch_val(test_query)
        logger.info(f"  - Successful joins: {join_count} records")
        
        return {
            'collaborators_count': collaborators_count,
            'repo_collabs_count': repo_collabs_count,
            'join_count': join_count
        }
        
    except Exception as e:
        logger.error(f"Error verifying migration: {e}")
        raise

async def run_migration():
    """Run the complete migration process"""
    try:
        logger.info("Starting collaborators migration...")
        
        # Connect to database
        await database.connect()
        
        # Step 1: Backup old tables
        logger.info("Step 1: Creating backups...")
        await backup_old_tables()
        
        # Step 2: Create new collaborators table
        logger.info("Step 2: Creating new collaborators table...")
        await create_new_collaborators_table()
        
        # Step 3: Migrate existing data to new collaborators table
        logger.info("Step 3: Migrating collaborator data...")
        collaborators_migrated = await migrate_existing_data()
        
        # Step 4: Create new repository_collaborators table
        logger.info("Step 4: Creating new repository_collaborators table...")
        await create_new_repository_collaborators_table()
        
        # Step 5: Migrate repository_collaborators relationships
        logger.info("Step 5: Migrating repository collaborator relationships...")
        relationships_migrated = await migrate_repository_collaborators()
        
        # Step 6: Replace old tables
        logger.info("Step 6: Replacing old tables...")
        await replace_tables()
        
        # Step 7: Verify migration
        logger.info("Step 7: Verifying migration...")
        verification = await verify_migration()
        
        logger.info("Migration completed successfully!")
        logger.info(f"Summary: {collaborators_migrated} collaborators, {relationships_migrated} relationships migrated")
        logger.info(f"Verification: {verification}")
        
        return {
            'success': True,
            'collaborators_migrated': collaborators_migrated,
            'relationships_migrated': relationships_migrated,
            'verification': verification
        }
        
    except Exception as e:
        logger.error(f"Migration failed: {e}")
        return {
            'success': False,
            'error': str(e)
        }
    finally:
        await database.disconnect()

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Run migration
    result = asyncio.run(run_migration())
    
    if result['success']:
        print("‚úÖ Migration completed successfully!")
        print(f"üìä Summary: {result}")
    else:
        print("‚ùå Migration failed!")
        print(f"üö® Error: {result['error']}")

```

### backend\services\ai_model.py
```py

```

### backend\services\ai_service.py
```py

```

### backend\services\branch_service.py
```py
from db.models.branches import branches
from db.database import database
from services.repo_service import get_repo_id_by_owner_and_name
from sqlalchemy import select, delete
from fastapi import HTTPException
import logging

logger = logging.getLogger(__name__)

async def save_branch(branch_data):
    """Save single branch to database with full data"""
    query = branches.insert().values(
        name=branch_data["name"],
        repo_id=branch_data["repo_id"],
        # Th√™m c√°c fields m·ªõi
        sha=branch_data.get("sha"),
        is_default=branch_data.get("is_default", False),
        is_protected=branch_data.get("is_protected", False),
        creator_name=branch_data.get("creator_name"),
        last_committer_name=branch_data.get("last_committer_name"),
        last_commit_date=branch_data.get("last_commit_date"),
        commits_count=branch_data.get("commits_count"),
        contributors_count=branch_data.get("contributors_count"),
        created_at=branch_data.get("created_at"),
    )
    await database.execute(query)

async def save_multiple_branches(repo_id: int, branches_list: list, default_branch: str = "main"):
    """Save multiple branches efficiently with full data"""
    if not branches_list:
        return 0
    
    # Prepare batch data v·ªõi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a t·ª´ API layer
    batch_data = []
    for branch in branches_list:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
            # D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a t·ª´ sync.py
            "sha": branch.get("sha"),
            "is_default": branch.get("is_default", branch["name"] == default_branch),
            "is_protected": branch.get("is_protected", False),
            
            # C√°c tr∆∞·ªùng s·∫Ω ƒë∆∞·ª£c th√™m sau khi c√≥ th√™m API calls
            "last_commit_date": branch.get("last_commit_date"),
            "creator_name": branch.get("creator_name"),
            "last_committer_name": branch.get("last_committer_name"),
            "commits_count": branch.get("commits_count"),
            "contributors_count": branch.get("contributors_count"),
            "created_at": branch.get("created_at"),
        }
        batch_data.append(branch_data)
    
    # Batch insert
    if batch_data:
        query = branches.insert()
        await database.execute_many(query, batch_data)
    
    return len(batch_data)

async def get_branches_by_repo_id(repo_id: int):
    """Get all branches for a repository"""
    query = select(branches).where(branches.c.repo_id == repo_id)
    result = await database.fetch_all(query)
    return [dict(row) for row in result]

async def get_branches_by_owner_repo(owner: str, repo_name: str):
    """Get branches by owner/repo name"""
    repo_id = await get_repo_id_by_owner_and_name(owner, repo_name)
    if not repo_id:
        return []
    
    return await get_branches_by_repo_id(repo_id)

async def delete_branches_by_repo_id(repo_id: int):
    """Delete all branches for a repository (for cleanup/re-sync)"""
    query = delete(branches).where(branches.c.repo_id == repo_id)
    result = await database.execute(query)
    return result

async def sync_branches_for_repo(repo_id: int, branches_data: list, default_branch: str = "main", replace_existing: bool = False):
    """
    Sync branches for a repository with full branch data
    Args:
        repo_id: Repository ID
        branches_data: List of branch data from GitHub API
        default_branch: Name of default branch (from repo data)
        replace_existing: If True, delete existing branches first
    """
    if replace_existing:
        await delete_branches_by_repo_id(repo_id)
    
    saved_count = await save_multiple_branches(repo_id, branches_data, default_branch)
    logger.info(f"Synced {saved_count} branches for repo_id {repo_id}")
    
    return saved_count

async def get_branch_statistics(repo_id: int):
    """
    L·∫•y th·ªëng k√™ v·ªÅ c√°c branches c·ªßa repository
    
    Args:
        repo_id: ID c·ªßa repository
    
    Returns:
        dict: Th·ªëng k√™ branches
    """
    from sqlalchemy import func, case
    
    query = select([
        func.count().label('total_branches'),
        func.count(case([(branches.c.is_default == True, 1)])).label('default_branches'),
        func.count(case([(branches.c.is_protected == True, 1)])).label('protected_branches'),
        func.count(case([(branches.c.last_commit_date.isnot(None), 1)])).label('branches_with_commits'),
        func.avg(branches.c.commits_count).label('avg_commits_per_branch'),
        func.max(branches.c.last_commit_date).label('last_activity')
    ]).where(branches.c.repo_id == repo_id)
    
    result = await database.fetch_one(query)
    
    if result:
        return {
            'total_branches': result['total_branches'] or 0,
            'default_branches': result['default_branches'] or 0,
            'protected_branches': result['protected_branches'] or 0,
            'branches_with_commits': result['branches_with_commits'] or 0,
            'avg_commits_per_branch': float(result['avg_commits_per_branch'] or 0),
            'last_activity': result['last_activity']
        }
    
    return {
        'total_branches': 0,
        'default_branches': 0,
        'protected_branches': 0,
        'branches_with_commits': 0,
        'avg_commits_per_branch': 0,
        'last_activity': None
    }

async def find_stale_branches(repo_id: int, days_threshold: int = 90):
    """
    T√¨m c√°c branches c≈© (kh√¥ng c√≥ commit trong X ng√†y)
    
    Args:
        repo_id: ID c·ªßa repository
        days_threshold: S·ªë ng√†y ƒë·ªÉ coi l√† c≈©
    
    Returns:
        list: Danh s√°ch branches c≈©
    """
    from datetime import datetime, timedelta
    
    cutoff_date = datetime.utcnow() - timedelta(days=days_threshold)
    
    query = select(branches).where(
        (branches.c.repo_id == repo_id) &
        (branches.c.last_commit_date < cutoff_date) &
        (branches.c.is_default == False)  # Kh√¥ng bao g·ªìm default branch
    ).order_by(branches.c.last_commit_date.asc())
    
    result = await database.fetch_all(query)
    return [dict(row) for row in result]

async def get_most_active_branches(repo_id: int, limit: int = 10):
    """
    L·∫•y c√°c branches ho·∫°t ƒë·ªông nhi·ªÅu nh·∫•t (theo s·ªë commits)
    
    Args:
        repo_id: ID c·ªßa repository
        limit: S·ªë l∆∞·ª£ng branches tr·∫£ v·ªÅ
    
    Returns:
        list: Danh s√°ch branches ho·∫°t ƒë·ªông nh·∫•t
    """
    query = select(branches).where(
        (branches.c.repo_id == repo_id) &
        (branches.c.commits_count.isnot(None))
    ).order_by(branches.c.commits_count.desc()).limit(limit)
    
    result = await database.fetch_all(query)
    return [dict(row) for row in result]

async def update_branch_metadata(repo_id: int, branch_name: str, metadata: dict):
    """
    C·∫≠p nh·∫≠t metadata c·ªßa m·ªôt branch c·ª• th·ªÉ
    
    Args:
        repo_id: ID c·ªßa repository
        branch_name: T√™n branch
        metadata: Dictionary ch·ª©a d·ªØ li·ªáu c·∫ßn c·∫≠p nh·∫≠t
    
    Returns:
        bool: True n·∫øu c·∫≠p nh·∫≠t th√†nh c√¥ng
    """
    from sqlalchemy import update
    
    # Ch·ªâ cho ph√©p c·∫≠p nh·∫≠t m·ªôt s·ªë fields nh·∫•t ƒë·ªãnh
    allowed_fields = {
        'sha', 'is_protected', 'last_commit_date', 'last_committer_name',
        'commits_count', 'contributors_count', 'creator_name'
    }
    
    update_data = {k: v for k, v in metadata.items() if k in allowed_fields}
    
    if not update_data:
        return False
    
    query = update(branches).where(
        (branches.c.repo_id == repo_id) & 
        (branches.c.name == branch_name)
    ).values(**update_data)
    
    result = await database.execute(query)
    return result > 0
```

### backend\services\collaborator_service.py
```py
# backend/services/collaborator_service.py
from db.models.repository_collaborators import repository_collaborators
from db.models.users import users
from db.models.repositories import repositories
from db.models.collaborators import collaborators
from sqlalchemy import select, join, func, insert, update
from db.database import database
import logging
from typing import List, Dict, Any, Optional
import httpx
from datetime import datetime

logger = logging.getLogger(__name__)

async def get_collaborators_by_repo(repo_id: int) -> List[Dict[str, Any]]:
    """
    Get all collaborators for a specific repository using the new schema
    
    Args:
        repo_id: Repository ID
        
    Returns:
        List of collaborator data with user information
    """
    try:
        # Join repository_collaborators with collaborators table to get full user info
        query = (
            select(
                repository_collaborators.c.id,
                repository_collaborators.c.repository_id,
                repository_collaborators.c.collaborator_id,
                repository_collaborators.c.role,
                repository_collaborators.c.permissions,
                repository_collaborators.c.is_owner,
                repository_collaborators.c.commits_count,
                repository_collaborators.c.issues_count,
                repository_collaborators.c.prs_count,
                repository_collaborators.c.joined_at,
                repository_collaborators.c.last_synced.label('collab_last_synced'),
                # Collaborator info from collaborators table
                collaborators.c.github_user_id,
                collaborators.c.github_username,
                collaborators.c.display_name,
                collaborators.c.email,
                collaborators.c.avatar_url,
                collaborators.c.bio,
                collaborators.c.company,
                collaborators.c.location,
                collaborators.c.blog,
                collaborators.c.is_site_admin,
                collaborators.c.type.label('collaborator_type'),
                collaborators.c.user_id  # Link to users table if they've logged in
            )
            .select_from(
                repository_collaborators.join(
                    collaborators,
                    repository_collaborators.c.collaborator_id == collaborators.c.id,
                    isouter=True
                )
            )
            .where(repository_collaborators.c.repository_id == repo_id)
            .order_by(repository_collaborators.c.is_owner.desc(), repository_collaborators.c.role.desc())
        )
        
        results = await database.fetch_all(query)
        
        collaborators_list = []
        for row in results:
            collab_data = {
                "id": row.id,
                "repository_id": row.repository_id,
                "collaborator_id": row.collaborator_id,
                "github_user_id": row.github_user_id,
                "github_username": row.github_username,
                "login": row.github_username,  # For frontend compatibility
                "role": row.role,
                "permissions": row.permissions,
                "is_owner": row.is_owner,
                "commits_count": row.commits_count or 0,
                "issues_count": row.issues_count or 0,
                "prs_count": row.prs_count or 0,
                "contributions": row.commits_count or 0,  # For frontend compatibility
                "type": "Owner" if row.is_owner else row.role.capitalize() if row.role else "Collaborator",
                "joined_at": row.joined_at,
                "last_synced": row.collab_last_synced,
                # Collaborator info
                "display_name": row.display_name,
                "email": row.email,
                "avatar_url": row.avatar_url,
                "bio": row.bio,
                "location": row.location,
                "company": row.company,
                "blog": row.blog,
                "is_site_admin": row.is_site_admin,                "collaborator_type": row.collaborator_type,
                "user_id": row.user_id  # If they have logged into our system
            }
            collaborators_list.append(collab_data)
        
        logger.info(f"‚úÖ Found {len(collaborators_list)} synced collaborators for repo {repo_id}")
        return collaborators_list
          
    except Exception as e:
        logger.error(f"Error getting collaborators for repo {repo_id}: {e}")
        return []

async def get_collaborator_by_repo_and_username(repo_id: int, github_username: str):
    """
    Get specific collaborator by repository and GitHub username
    
    Args:
        repo_id: Repository ID
        github_username: GitHub username
        
    Returns:
        Collaborator data or None
    """
    try:
        query = (
            select(repository_collaborators)
            .where(
                (repository_collaborators.c.repository_id == repo_id) &
                (repository_collaborators.c.github_username == github_username)
            )
        )
        
        result = await database.fetch_one(query)
        return dict(result) if result else None
        
    except Exception as e:
        logger.error(f"Error getting collaborator {github_username} for repo {repo_id}: {e}")
        return None

async def update_collaborator_stats(repo_id: int, github_username: str, stats: Dict[str, int]):
    """
    Update collaborator statistics (commits, issues, PRs count)
    
    Args:
        repo_id: Repository ID
        github_username: GitHub username
        stats: Dictionary with commits_count, issues_count, prs_count
    """
    try:
        query = (
            update(repository_collaborators)
            .where(
                (repository_collaborators.c.repository_id == repo_id) &
                (repository_collaborators.c.github_username == github_username)
            )
            .values(
                commits_count=stats.get('commits_count', 0),
                issues_count=stats.get('issues_count', 0),
                prs_count=stats.get('prs_count', 0),
                updated_at=func.now()
            )
        )
        
        await database.execute(query)
        logger.info(f"Updated stats for collaborator {github_username} in repo {repo_id}")
        
    except Exception as e:
        logger.error(f"Error updating collaborator stats: {e}")
        raise e

async def get_collaborators_with_user_info(owner: str, repo: str) -> List[Dict[str, Any]]:
    """
    Get collaborators for a repository by owner/repo names with full user info
    Legacy function - now redirects to get_collaborators_with_fallback
    
    Args:
        owner: Repository owner (GitHub username)
        repo: Repository name
        
    Returns:
        List of collaborator data with user information
    """
    try:
        logger.info(f"üîç Legacy function called for repository: {owner}/{repo}")
        
        # Redirect to new function
        return await get_collaborators_with_fallback(owner, repo, None)
        
    except Exception as e:
        logger.error(f"üí• Error in legacy function for {owner}/{repo}: {e}")
        return []

def create_fallback_collaborator(user_record, repo_id: int, is_owner: bool = False) -> Dict[str, Any]:
    """Create a fallback collaborator object from user record"""
    return {
        "id": 0,  # Special ID for fallback
        "repository_id": repo_id,
        "user_id": user_record.id,
        "github_id": user_record.github_id,
        "github_username": user_record.github_username,
        "login": user_record.github_username,
        "role": "ADMIN" if is_owner else "PUSH",
        "permissions": '{"admin": true, "push": true, "pull": true}' if is_owner else '{"push": true, "pull": true}',
        "is_owner": is_owner,
        "commits_count": 0,
        "issues_count": 0,
        "prs_count": 0,
        "contributions": 0,
        "type": "Owner (Auto-detected)" if is_owner else "User (Auto-detected)",
        "joined_at": None,
        "last_synced": None,
        "sync_status": "fallback",
        # User info
        "display_name": user_record.display_name,
        "full_name": user_record.full_name,
        "email": user_record.email,
        "avatar_url": user_record.avatar_url,
        "bio": user_record.bio,
        "location": user_record.location,
        "company": user_record.company,
        "blog": user_record.blog,
        "twitter_username": user_record.twitter_username,
        "github_profile_url": user_record.github_profile_url,
        "github_created_at": user_record.github_created_at
    }

async def upsert_collaborator(github_user_data: Dict[str, Any]) -> int:
    """
    Insert or update a collaborator in the collaborators table
    
    Args:
        github_user_data: GitHub user data from API
        
    Returns:
        collaborator_id: ID of the collaborator in our database
    """
    try:
        github_user_id = github_user_data.get("id")
        github_username = github_user_data.get("login")
        
        if not github_user_id or not github_username:
            raise ValueError("Missing required GitHub user data")
        
        # Check if collaborator already exists
        existing_query = select(collaborators.c.id).where(
            collaborators.c.github_user_id == github_user_id
        )
        existing = await database.fetch_one(existing_query)
        
        collaborator_data = {
            "github_user_id": github_user_id,
            "github_username": github_username,
            "display_name": github_user_data.get("name"),
            "email": github_user_data.get("email"),
            "avatar_url": github_user_data.get("avatar_url"),
            "bio": github_user_data.get("bio"),
            "company": github_user_data.get("company"),
            "location": github_user_data.get("location"),
            "blog": github_user_data.get("blog"),
            "is_site_admin": github_user_data.get("site_admin", False),
            "node_id": github_user_data.get("node_id"),
            "gravatar_id": github_user_data.get("gravatar_id"),
            "type": github_user_data.get("type", "User"),
            "updated_at": func.now()
        }
        
        if existing:
            # Update existing collaborator
            update_query = (
                update(collaborators)
                .where(collaborators.c.github_user_id == github_user_id)
                .values(**collaborator_data)
            )
            await database.execute(update_query)
            collaborator_id = existing.id
            logger.info(f"Updated collaborator {github_username} (ID: {collaborator_id})")
        else:
            # Insert new collaborator
            insert_query = insert(collaborators).values(**collaborator_data)
            collaborator_id = await database.execute(insert_query)
            logger.info(f"Created new collaborator {github_username} (ID: {collaborator_id})")
        
        return collaborator_id
        
    except Exception as e:
        logger.error(f"Error upserting collaborator {github_user_data.get('login', 'unknown')}: {e}")
        raise e

async def link_collaborator_to_repository(
    repository_id: int, 
    collaborator_id: int, 
    github_collab_data: Dict[str, Any]
) -> bool:
    """
    Link a collaborator to a repository with permissions and role
    
    Args:
        repository_id: Repository ID
        collaborator_id: Collaborator ID
        github_collab_data: GitHub collaborator data with permissions
        
    Returns:
        bool: True if successful
    """
    try:
        # Check if link already exists
        existing_query = select(repository_collaborators.c.id).where(
            (repository_collaborators.c.repository_id == repository_id) &
            (repository_collaborators.c.collaborator_id == collaborator_id)
        )
        existing = await database.fetch_one(existing_query)
        
        # Extract permissions and role from GitHub data
        permissions = github_collab_data.get("permissions", {})
        role = github_collab_data.get("role_name", "pull")  # Default to pull
        
        # Determine if this is the owner
        is_owner = (
            permissions.get("admin", False) and 
            github_collab_data.get("login") == github_collab_data.get("repository_owner")
        )
        
        link_data = {
            "repository_id": repository_id,
            "collaborator_id": collaborator_id,
            "role": role,
            "permissions": str(permissions),  # Store as JSON string
            "is_owner": is_owner,
            "joined_at": datetime.now(),
            "last_synced": func.now(),
            "updated_at": func.now()
        }
        
        if existing:
            # Update existing link
            update_query = (
                update(repository_collaborators)
                .where(repository_collaborators.c.id == existing.id)
                .values(**link_data)
            )
            await database.execute(update_query)
            logger.info(f"Updated repository-collaborator link (repo: {repository_id}, collaborator: {collaborator_id})")
        else:
            # Create new link
            insert_query = insert(repository_collaborators).values(**link_data)
            await database.execute(insert_query)
            logger.info(f"Created repository-collaborator link (repo: {repository_id}, collaborator: {collaborator_id})")
        
        return True
        
    except Exception as e:
        logger.error(f"Error linking collaborator {collaborator_id} to repository {repository_id}: {e}")
        raise e

async def sync_repository_collaborators(owner: str, repo: str, github_token: str) -> Dict[str, Any]:
    """
    Sync collaborators from GitHub API to our database
    
    Args:
        owner: Repository owner
        repo: Repository name  
        github_token: GitHub API token
        
    Returns:
        Sync result with counts and status
    """
    try:
        logger.info(f"üîÑ Starting collaborator sync for {owner}/{repo}")
        
        # Get repository ID
        repo_query = select(repositories.c.id).where(
            (repositories.c.owner == owner) &
            (repositories.c.name == repo)
        )
        repo_result = await database.fetch_one(repo_query)
        
        if not repo_result:
            raise ValueError(f"Repository {owner}/{repo} not found in database")
        
        repository_id = repo_result.id
        
        # Fetch collaborators from GitHub API
        headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                f"https://api.github.com/repos/{owner}/{repo}/collaborators",
                headers=headers
            )
            
            if response.status_code != 200:
                raise ValueError(f"GitHub API error: {response.status_code} - {response.text}")
            
            github_collaborators = response.json()
        
        logger.info(f"üì• Fetched {len(github_collaborators)} collaborators from GitHub")
        
        synced_count = 0
        errors = []
        
        for github_user in github_collaborators:
            try:
                # 1. Upsert collaborator
                collaborator_id = await upsert_collaborator(github_user)
                
                # 2. Link to repository
                await link_collaborator_to_repository(
                    repository_id, 
                    collaborator_id, 
                    {**github_user, "repository_owner": owner}
                )
                
                synced_count += 1
                
            except Exception as e:
                error_msg = f"Failed to sync collaborator {github_user.get('login', 'unknown')}: {e}"
                logger.error(error_msg)
                errors.append(error_msg)
        
        result = {
            "status": "success" if synced_count > 0 else "partial",
            "repository": f"{owner}/{repo}",
            "total_fetched": len(github_collaborators),
            "synced_count": synced_count,
            "errors": errors
        }
        
        logger.info(f"‚úÖ Sync completed for {owner}/{repo}: {synced_count}/{len(github_collaborators)} successful")
        return result
        
    except Exception as e:
        logger.error(f"üí• Error syncing collaborators for {owner}/{repo}: {e}")
        return {
            "status": "error",
            "repository": f"{owner}/{repo}",
            "error": str(e)
        }

async def get_collaborators_with_fallback(owner: str, repo: str, github_token: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Get collaborators for a repository with fallback to GitHub API if none synced
    
    Args:
        owner: Repository owner
        repo: Repository name
        github_token: Optional GitHub token for fallback
        
    Returns:
        List of collaborator data
    """
    try:
        logger.info(f"üîç Getting collaborators for {owner}/{repo}")
        
        # Get repository ID
        repo_query = select(repositories.c.id).where(
            (repositories.c.owner == owner) &
            (repositories.c.name == repo)
        )
        repo_result = await database.fetch_one(repo_query)
        if not repo_result:
            logger.warning(f"Repository {owner}/{repo} not found in database - returning empty list")
            return []
        
        repo_id = repo_result.id
        
        # Try to get synced collaborators first
        collaborators_list = await get_collaborators_by_repo(repo_id)
        
        if collaborators_list:
            logger.info(f"‚úÖ Found {len(collaborators_list)} synced collaborators")
            return collaborators_list
        
        # Fallback: try to fetch from GitHub directly if token provided
        if github_token:
            logger.info(f"üîÑ No synced collaborators found, trying GitHub API fallback")
            
            try:
                headers = {
                    "Authorization": f"Bearer {github_token}",
                    "Accept": "application/vnd.github+json",
                    "X-GitHub-Api-Version": "2022-11-28"
                }
                
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(
                        f"https://api.github.com/repos/{owner}/{repo}/collaborators",
                        headers=headers
                    )
                    
                    if response.status_code == 200:
                        github_collaborators = response.json()
                        
                        # Convert to our format for frontend compatibility
                        fallback_collaborators = []
                        for i, github_user in enumerate(github_collaborators):
                            fallback_collab = {
                                "id": f"fallback_{i}",
                                "repository_id": repo_id,
                                "github_user_id": github_user.get("id"),
                                "github_username": github_user.get("login"),
                                "login": github_user.get("login"),
                                "role": "unknown",
                                "is_owner": False,  # We can't determine this easily
                                "type": "Collaborator (Live)",
                                "display_name": github_user.get("name"),
                                "avatar_url": github_user.get("avatar_url"),
                                "github_profile_url": github_user.get("html_url"),
                                "sync_status": "live_fallback"
                            }
                            fallback_collaborators.append(fallback_collab)
                        
                        logger.info(f"üì° Retrieved {len(fallback_collaborators)} collaborators via GitHub API fallback")
                        return fallback_collaborators
                        
            except Exception as e:
                logger.error(f"GitHub API fallback failed: {e}")
        
        logger.warning(f"No collaborators found for {owner}/{repo} (synced or fallback)")
        return []
        
    except Exception as e:
        logger.error(f"Error getting collaborators with fallback for {owner}/{repo}: {e}")
        return []

```

### backend\services\commit_service.py
```py
from db.models.commits import commits
from db.database import database
from sqlalchemy import select, insert, update, and_
from datetime import datetime, timezone
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object (timezone-naive UTC)"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        # Parse as timezone-aware datetime first
        dt_aware = datetime.fromisoformat(date_str)
        
        # Convert to UTC and make timezone-naive for database storage
        dt_utc = dt_aware.astimezone(timezone.utc)
        return dt_utc.replace(tzinfo=None)
        
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

def normalize_datetime(dt):
    """Normalize datetime to timezone-naive UTC for consistent database storage"""
    if dt is None:
        return None
    
    if dt.tzinfo is not None:
        # Convert timezone-aware to UTC and make timezone-naive
        return dt.astimezone(timezone.utc).replace(tzinfo=None)
    else:
        # Already timezone-naive, assume it's UTC
        return dt

async def save_commit(commit_data):
    """Save commit with full data model support including new fields"""
    try:
        # Ki·ªÉm tra commit ƒë√£ t·ªìn t·∫°i ch∆∞a
        query = select(commits).where(commits.c.sha == commit_data["sha"])
        existing_commit = await database.fetch_one(query)

        if existing_commit:
            logger.info(f"Commit {commit_data['sha']} already exists, skipping")
            return existing_commit.id        # Prepare full commit entry with all model fields
        commit_entry = {
            "sha": commit_data["sha"],
            "message": commit_data.get("message", ""),
            "author_name": commit_data.get("author_name", ""),
            "author_email": commit_data.get("author_email", ""),
            "committer_name": commit_data.get("committer_name"),
            "committer_email": commit_data.get("committer_email"),
            "repo_id": commit_data["repo_id"],
            "branch_id": commit_data.get("branch_id"),
            "branch_name": commit_data.get("branch_name"),
            "author_role_at_commit": commit_data.get("author_role_at_commit"),
            "author_permissions_at_commit": commit_data.get("author_permissions_at_commit"),
            "date": normalize_datetime(parse_github_datetime(commit_data.get("date"))),
            "committer_date": normalize_datetime(parse_github_datetime(commit_data.get("committer_date"))),
            "insertions": commit_data.get("insertions"),
            "deletions": commit_data.get("deletions"),
            "files_changed": commit_data.get("files_changed"),
            "parent_sha": commit_data.get("parent_sha"),
            "is_merge": commit_data.get("is_merge", False),
            "merge_from_branch": commit_data.get("merge_from_branch"),
            # author_user_id v√† committer_user_id s·∫Ω ƒë∆∞·ª£c resolve sau n·∫øu c√≥ user mapping
            "author_user_id": commit_data.get("author_user_id"),
            "committer_user_id": commit_data.get("committer_user_id"),
        }

        # Ch√®n commit m·ªõi
        query = insert(commits).values(commit_entry)
        result = await database.execute(query)
        logger.info(f"Created new commit: {commit_data['sha']}")
        return result
        
    except Exception as e:
        logger.error(f"Error saving commit {commit_data.get('sha')}: {e}")
        raise e

async def save_multiple_commits(commits_data: list, repo_id: int, branch_name: str = None, branch_id: int = None):
    """
    Batch save multiple commits efficiently
    
    Args:
        commits_data: List of commit data from GitHub API
        repo_id: Repository ID
        branch_name: Branch name (optional)
        branch_id: Branch ID (optional)
    
    Returns:
        int: Number of commits saved
    """
    if not commits_data:
        return 0
    
    # Prepare batch data
    batch_data = []
    existing_shas = set()
    
    # Check existing commits to avoid duplicates
    shas = [commit.get("sha") for commit in commits_data if commit.get("sha")]
    if shas:
        query = select(commits.c.sha).where(commits.c.sha.in_(shas))
        existing_results = await database.fetch_all(query)
        existing_shas = {row.sha for row in existing_results}
    
    # Process commits data
    for commit_data in commits_data:
        sha = commit_data.get("sha")
        if not sha or sha in existing_shas:
            continue
        
        # Extract commit info from GitHub API response
        commit_info = commit_data.get("commit", {})
        author_info = commit_info.get("author", {})
        committer_info = commit_info.get("committer", {})
        stats = commit_data.get("stats", {})
          # Check if this is a merge commit
        parents = commit_data.get("parents", [])
        is_merge = len(parents) > 1
        parent_sha = parents[0].get("sha") if parents else None
        
        commit_entry = {
            "sha": sha,
            "message": commit_info.get("message", ""),
            "author_name": author_info.get("name", ""),
            "author_email": author_info.get("email", ""),
            "committer_name": committer_info.get("name"),
            "committer_email": committer_info.get("email"),
            "repo_id": repo_id,
            "branch_id": branch_id,
            "branch_name": branch_name,
            "date": normalize_datetime(parse_github_datetime(author_info.get("date"))),
            "committer_date": normalize_datetime(parse_github_datetime(committer_info.get("date"))),
            "insertions": stats.get("additions"),
            "deletions": stats.get("deletions"),
            "files_changed": len(commit_data.get("files", [])) if commit_data.get("files") else None,
            "parent_sha": parent_sha,
            "is_merge": is_merge,
            # User IDs v√† permissions s·∫Ω ƒë∆∞·ª£c resolve sau n·∫øu c√≥ user service
        }
        
        batch_data.append(commit_entry)
    
    # Batch insert
    if batch_data:
        query = commits.insert()
        await database.execute_many(query, batch_data)
        logger.info(f"Batch saved {len(batch_data)} commits for repo_id {repo_id}")
    
    return len(batch_data)

async def get_commits_by_repo_id(repo_id: int, limit: int = 100, offset: int = 0):
    """Get commits by repository ID with pagination"""
    query = select(commits).where(
        commits.c.repo_id == repo_id
    ).order_by(commits.c.date.desc()).limit(limit).offset(offset)
    
    result = await database.fetch_all(query)
    return [dict(row) for row in result]

async def get_commit_by_sha(sha: str):
    """Get single commit by SHA"""
    query = select(commits).where(commits.c.sha == sha)
    result = await database.fetch_one(query)
    return dict(result) if result else None

async def get_commit_statistics(repo_id: int):
    """Get commit statistics for a repository"""
    from sqlalchemy import func
    
    try:
        # Basic stats query
        query = select(
            func.count().label('total_commits'),
            func.sum(commits.c.insertions).label('total_insertions'),
            func.sum(commits.c.deletions).label('total_deletions'),
            func.sum(commits.c.files_changed).label('total_files_changed'),
            func.count(func.distinct(commits.c.author_email)).label('unique_authors'),
            func.max(commits.c.date).label('latest_commit_date'),
            func.min(commits.c.date).label('first_commit_date')
        ).where(commits.c.repo_id == repo_id)
        
        result = await database.fetch_one(query)
        
        # Count merge commits separately
        merge_query = select(func.count()).where(
            (commits.c.repo_id == repo_id) & (commits.c.is_merge == True)
        )
        merge_count = await database.fetch_val(merge_query)
        
        if result:
            return {
                'total_commits': result['total_commits'] or 0,
                'merge_commits': merge_count or 0,
                'total_insertions': result['total_insertions'] or 0,
                'total_deletions': result['total_deletions'] or 0,
                'total_files_changed': result['total_files_changed'] or 0,
                'unique_authors': result['unique_authors'] or 0,
                'latest_commit_date': result['latest_commit_date'],
                'first_commit_date': result['first_commit_date']
            }
        
    except Exception as e:
        print(f"Error in get_commit_statistics: {e}")
        
    return {
        'total_commits': 0,
        'merge_commits': 0,
        'total_insertions': 0,
        'total_deletions': 0,
        'total_files_changed': 0,
        'unique_authors': 0,
        'latest_commit_date': None,
        'first_commit_date': None
    }

async def update_commit_user_mapping(sha: str, author_user_id: int = None, committer_user_id: int = None):
    """Update commit with user ID mappings"""
    update_data = {}
    if author_user_id:
        update_data['author_user_id'] = author_user_id
    if committer_user_id:
        update_data['committer_user_id'] = committer_user_id
    
    if update_data:
        query = update(commits).where(commits.c.sha == sha).values(**update_data)
        result = await database.execute(query)
        return result > 0
    
    return False

# ==================== NEW BRANCH-SPECIFIC COMMIT FUNCTIONS ====================

async def get_repo_id_by_owner_and_name(owner: str, name: str):
    """Get repository ID by owner and name"""
    from db.models.repositories import repositories
    
    query = select(repositories.c.id).where(
        repositories.c.owner == owner,
        repositories.c.name == name
    )
    result = await database.fetch_one(query)
    return result.id if result else None

async def get_branch_id_by_repo_and_name(repo_id: int, branch_name: str):
    """Get branch ID and validate it exists in the repository"""
    from db.models.branches import branches
    
    query = select(branches).where(
        branches.c.repo_id == repo_id,
        branches.c.name == branch_name
    )
    result = await database.fetch_one(query)
    return result if result else None

async def get_commits_by_branch_safe(repo_id: int, branch_name: str, limit: int = 100, offset: int = 0):
    """
    L·∫•y commits theo branch v·ªõi validation ƒë·∫ßy ƒë·ªß
    """
    try:
        # 1. Verify branch exists and get branch info
        branch = await get_branch_id_by_repo_and_name(repo_id, branch_name)
        
        if not branch:
            logger.warning(f"Branch {branch_name} not found in repo_id {repo_id}")
            return []
        
        # 2. Get commits using both branch_id AND branch_name for data consistency
        query = select(commits).where(
            commits.c.repo_id == repo_id,
            commits.c.branch_id == branch.id,
            commits.c.branch_name == branch_name  # Double validation
        ).order_by(commits.c.date.desc()).limit(limit).offset(offset)
        
        commits_data = await database.fetch_all(query)
        
        logger.info(f"‚úÖ Found {len(commits_data)} commits for branch {branch_name} in repo {repo_id}")
        return commits_data
        
    except Exception as e:
        logger.error(f"‚ùå Error getting commits for branch {branch_name}: {e}")
        return []

async def get_commits_by_branch_fallback(repo_id: int, branch_name: str, limit: int = 100, offset: int = 0):
    """
    Fallback method: l·∫•y commits ch·ªâ b·∫±ng branch_name n·∫øu branch_id kh√¥ng c√≥
    """
    try:
        query = select(commits).where(
            commits.c.repo_id == repo_id,
            commits.c.branch_name == branch_name
        ).order_by(commits.c.date.desc()).limit(limit).offset(offset)
        
        commits_data = await database.fetch_all(query)
        
        logger.info(f"‚ö†Ô∏è Fallback: Found {len(commits_data)} commits for branch {branch_name} using branch_name only")
        return commits_data
        
    except Exception as e:
        logger.error(f"‚ùå Fallback failed for branch {branch_name}: {e}")
        return []

async def get_all_branches_with_commit_stats(repo_id: int):
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ branches v·ªõi th·ªëng k√™ commits
    """
    try:
        from db.models.branches import branches
        from sqlalchemy import func
        
        # Query t·∫•t c·∫£ branches v·ªõi commit count
        query = select(
            branches.c.id,
            branches.c.name,
            branches.c.is_default,
            branches.c.is_protected,
            branches.c.commits_count,
            branches.c.last_commit_date,
            func.count(commits.c.id).label('actual_commit_count'),
            func.max(commits.c.date).label('latest_commit_date')
        ).select_from(
            branches.outerjoin(
                commits, 
                branches.c.id == commits.c.branch_id
            )
        ).where(
            branches.c.repo_id == repo_id
        ).group_by(
            branches.c.id,
            branches.c.name,
            branches.c.is_default,
            branches.c.is_protected,
            branches.c.commits_count,
            branches.c.last_commit_date
        ).order_by(branches.c.is_default.desc(), branches.c.name)
        
        branches_data = await database.fetch_all(query)
        
        logger.info(f"‚úÖ Found {len(branches_data)} branches with commit stats for repo {repo_id}")
        return branches_data
        
    except Exception as e:
        logger.error(f"‚ùå Error getting branches with commit stats: {e}")
        return []

async def compare_commits_between_branches(repo_id: int, base_branch: str, head_branch: str, limit: int = 100):
    """
    So s√°nh commits gi·ªØa 2 branches
    """
    try:
        # Get commits from head branch that are not in base branch
        from sqlalchemy import and_, not_, exists
        
        # Subquery ƒë·ªÉ l·∫•y SHAs c·ªßa base branch
        base_commits_subquery = select(commits.c.sha).where(
            and_(
                commits.c.repo_id == repo_id,
                commits.c.branch_name == base_branch
            )
        )
        
        # Main query: commits in head but not in base
        query = select(commits).where(
            and_(
                commits.c.repo_id == repo_id,
                commits.c.branch_name == head_branch,
                not_(commits.c.sha.in_(base_commits_subquery))
            )
        ).order_by(commits.c.date.desc()).limit(limit)
        
        diff_commits = await database.fetch_all(query)
        
        logger.info(f"‚úÖ Found {len(diff_commits)} commits in {head_branch} but not in {base_branch}")
        return diff_commits
        
    except Exception as e:
        logger.error(f"‚ùå Error comparing branches {base_branch}...{head_branch}: {e}")
        return []

async def validate_and_fix_commit_branch_consistency(repo_id: int):
    """
    Ki·ªÉm tra v√† s·ª≠a inconsistency gi·ªØa branch_id v√† branch_name
    """
    try:
        from db.models.branches import branches
        
        # Find commits with inconsistent branch data
        inconsistent_query = select(
            commits.c.id,
            commits.c.sha,
            commits.c.branch_id,
            commits.c.branch_name,
            branches.c.name.label('actual_branch_name')
        ).select_from(
            commits.join(branches, commits.c.branch_id == branches.c.id)
        ).where(
            and_(
                commits.c.repo_id == repo_id,
                commits.c.branch_name != branches.c.name
            )
        )
        
        inconsistent_commits = await database.fetch_all(inconsistent_query)
        
        if inconsistent_commits:
            logger.warning(f"‚ö†Ô∏è Found {len(inconsistent_commits)} commits with inconsistent branch data")
            
            # Fix inconsistencies
            for commit in inconsistent_commits:
                update_query = update(commits).where(
                    commits.c.id == commit.id
                ).values(
                    branch_name=commit.actual_branch_name
                )
                await database.execute(update_query)
                
            logger.info(f"‚úÖ Fixed {len(inconsistent_commits)} commit branch inconsistencies")
        
        return len(inconsistent_commits)
        
    except Exception as e:
        logger.error(f"‚ùå Error validating commit-branch consistency: {e}")
        return 0
```

### backend\services\github_service.py
```py
# backend/services/github_service.py
# Service x·ª≠ l√Ω c√°c t∆∞∆°ng t√°c v·ªõi GitHub API

import httpx
import os
import re
from typing import Optional, Dict, List, Any
from dotenv import load_dotenv

load_dotenv()

# Configuration
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
BASE_URL = "https://api.github.com"

# Default headers for GitHub API requests
def get_headers(token: str = None) -> Dict[str, str]:
    """T·∫°o headers chu·∫©n cho GitHub API request"""
    auth_token = token or GITHUB_TOKEN
    return {
        "Authorization": f"Bearer {auth_token}",
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28"
    }

async def fetch_from_github(url: str, token: str = None) -> Dict[str, Any]:
    """
    H√†m t·ªïng qu√°t ƒë·ªÉ fetch d·ªØ li·ªáu t·ª´ GitHub API
    
    Args:
        url (str): Ph·∫ßn cu·ªëi c·ªßa URL (sau BASE_URL)
        token (str): GitHub token (optional)
    
    Returns:
        dict: D·ªØ li·ªáu JSON tr·∫£ v·ªÅ t·ª´ GitHub API
    
    Raises:
        HTTPError: N·∫øu request l·ªói
    """
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL}{url}", headers=get_headers(token))
        response.raise_for_status()
        return response.json()

async def fetch_commits(
    token: str, 
    owner: str, 
    name: str, 
    branch: str, 
    since: Optional[str] = None, 
    until: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    L·∫•y danh s√°ch commit t·ª´ repository GitHub
    
    Args:
        token (str): GitHub access token
        owner (str): Ch·ªß repository
        name (str): T√™n repository
        branch (str): T√™n branch
        since (Optional[str]): L·ªçc commit t·ª´ th·ªùi gian n√†y (ISO format)
        until (Optional[str]): L·ªçc commit ƒë·∫øn th·ªùi gian n√†y (ISO format)
    
    Returns:
        list: Danh s√°ch commit
    
    Raises:
        HTTPError: N·∫øu request l·ªói
    """
    url = f"/repos/{owner}/{name}/commits"
    
    # Parameters cho request
    params = {"sha": branch}
    
    # Th√™m tham s·ªë l·ªçc th·ªùi gian n·∫øu c√≥
    if since:
        params["since"] = since
    if until:
        params["until"] = until

    # G·ªçi API GitHub
    async with httpx.AsyncClient() as client:
        full_url = f"{BASE_URL}{url}"
        response = await client.get(full_url, headers=get_headers(token), params=params)
        response.raise_for_status()
        return response.json()

async def fetch_commit_details(commit_sha: str, owner: str, repo: str, token: str = None) -> Optional[Dict[str, Any]]:
    """
    L·∫•y th√¥ng tin chi ti·∫øt c·ªßa m·ªôt commit
    
    Args:
        commit_sha (str): SHA hash c·ªßa commit
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        token (str): GitHub token (optional)
    
    Returns:
        dict: Th√¥ng tin chi ti·∫øt commit ho·∫∑c None n·∫øu l·ªói
    """
    try:
        url = f"/repos/{owner}/{repo}/commits/{commit_sha}"
        
        async with httpx.AsyncClient() as client:
            full_url = f"{BASE_URL}{url}"
            response = await client.get(full_url, headers=get_headers(token))
            
            if response.status_code == 200:
                commit_data = response.json()
                return {
                    "sha": commit_data.get("sha"),
                    "date": commit_data.get("commit", {}).get("committer", {}).get("date"),
                    "author_name": commit_data.get("commit", {}).get("author", {}).get("name"),
                    "author_email": commit_data.get("commit", {}).get("author", {}).get("email"),
                    "committer_name": commit_data.get("commit", {}).get("committer", {}).get("name"),
                    "committer_email": commit_data.get("commit", {}).get("committer", {}).get("email"),
                    "message": commit_data.get("commit", {}).get("message"),
                    "url": commit_data.get("html_url"),
                    "stats": {
                        "additions": commit_data.get("stats", {}).get("additions", 0),
                        "deletions": commit_data.get("stats", {}).get("deletions", 0),
                        "total": commit_data.get("stats", {}).get("total", 0)
                    }
                }
            return None
            
    except Exception as e:
        print(f"Error fetching commit details for {commit_sha}: {e}")
        return None

async def fetch_branch_stats(owner: str, repo: str, branch_name: str, token: str = None):
    """
    L·∫•y th·ªëng k√™ c·ªßa branch (s·ªë commits, contributors)
    """
    try:
        headers = {}
        if token:
            headers["Authorization"] = f"token {token}"
        elif GITHUB_TOKEN:
            headers["Authorization"] = f"token {GITHUB_TOKEN}"
        
        async with httpx.AsyncClient() as client:
            # L·∫•y s·ªë commits cho branch c·ª• th·ªÉ
            commits_url = f"{BASE_URL}/repos/{owner}/{repo}/commits?sha={branch_name}&per_page=1"
            commits_response = await client.get(commits_url, headers=headers)
            commits_count = 0
            if commits_response.status_code == 200:
                # ƒê∆°n gi·∫£n h√≥a: l·∫•y t·ª´ response headers n·∫øu c√≥
                link_header = commits_response.headers.get("Link", "")
                if "rel=\"last\"" in link_header:
                    import re
                    last_page_match = re.search(r'page=(\d+)>; rel="last"', link_header)
                    if last_page_match:
                        commits_count = int(last_page_match.group(1))
            
            return {
                "commits_count": commits_count,
                "contributors_count": None  # S·∫Ω implement sau
            }
            
    except Exception as e:
        print(f"Error fetching branch stats: {e}")
        return {"commits_count": None, "contributors_count": None}

async def validate_github_token(token: str) -> bool:
    """
    Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa GitHub token
    
    Args:
        token (str): GitHub token ƒë·ªÉ ki·ªÉm tra
    
    Returns:
        bool: True n·∫øu token h·ª£p l·ªá
    """
    try:
        url = f"{BASE_URL}/user"
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=get_headers(token))
            return response.status_code == 200
    except Exception:
        return False

async def get_rate_limit_info(token: str = None) -> Dict[str, Any]:
    """
    L·∫•y th√¥ng tin rate limit c·ªßa GitHub API
    
    Args:
        token (str): GitHub token (optional)
    
    Returns:
        dict: Th√¥ng tin rate limit
    """
    try:
        url = f"{BASE_URL}/rate_limit"
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=get_headers(token))
            if response.status_code == 200:
                return response.json()
    except Exception as e:
        print(f"Error fetching rate limit: {e}")
    
    return {"resources": {"core": {"remaining": 0, "limit": 5000}}}

async def fetch_repository_languages(owner: str, repo: str, token: str = None) -> Dict[str, int]:
    """
    L·∫•y th√¥ng tin ng√¥n ng·ªØ l·∫≠p tr√¨nh c·ªßa repository
    
    Args:
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository  
        token (str): GitHub token (optional)
    
    Returns:
        dict: Dictionary v·ªõi key l√† ng√¥n ng·ªØ, value l√† s·ªë bytes
    """
    try:
        url = f"/repos/{owner}/{repo}/languages"
        return await fetch_from_github(url, token)
    except Exception as e:
        print(f"Error fetching repository languages: {e}")
        return {}

async def fetch_repository_topics(owner: str, repo: str, token: str = None) -> List[str]:
    """
    L·∫•y danh s√°ch topics c·ªßa repository
    
    Args:
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        token (str): GitHub token (optional)
    
    Returns:
        list: Danh s√°ch topics
    """
    try:
        url = f"/repos/{owner}/{repo}/topics"
        headers = get_headers(token)
        headers["Accept"] = "application/vnd.github.mercy-preview+json"  # For topics API
        
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BASE_URL}{url}", headers=headers)
            if response.status_code == 200:
                data = response.json()
                return data.get("names", [])
    except Exception as e:
        print(f"Error fetching repository topics: {e}")
    
    return []

async def fetch_branch_protection_rules(owner: str, repo: str, branch: str, token: str = None) -> Optional[Dict[str, Any]]:
    """
    L·∫•y th√¥ng tin protection rules c·ªßa branch
    
    Args:
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        branch (str): T√™n branch
        token (str): GitHub token (optional)
    
    Returns:
        dict: Th√¥ng tin protection rules ho·∫∑c None n·∫øu kh√¥ng c√≥
    """
    try:
        url = f"/repos/{owner}/{repo}/branches/{branch}/protection"
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BASE_URL}{url}", headers=get_headers(token))
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                return None  # Branch not protected
    except Exception as e:
        print(f"Error fetching branch protection for {branch}: {e}")
    
    return None

async def fetch_enhanced_commits(
    owner: str, 
    repo: str, 
    branch: str = "main",
    token: str = None,
    since: str = None,
    until: str = None,
    per_page: int = 100
) -> List[Dict[str, Any]]:
    """
    L·∫•y commits v·ªõi th√¥ng tin chi ti·∫øt bao g·ªìm stats v√† files
    
    Args:
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        branch (str): T√™n branch
        token (str): GitHub token
        since (str): ISO datetime string
        until (str): ISO datetime string
        per_page (int): S·ªë commits per page
    
    Returns:
        list: Danh s√°ch commits v·ªõi enhanced data
    """
    try:
        params = {
            "sha": branch,
            "per_page": min(per_page, 100)
        }
        
        if since:
            params["since"] = since
        if until:
            params["until"] = until
        
        url = f"/repos/{owner}/{repo}/commits"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{BASE_URL}{url}", 
                headers=get_headers(token),
                params=params
            )
            response.raise_for_status()
            commits_data = response.json()
        
        # Enrich each commit with detailed information
        enhanced_commits = []
        for commit in commits_data:
            sha = commit.get("sha")
            if not sha:
                continue
            
            # Get detailed commit info if needed
            enhanced_commit = {
                **commit,
                "enhanced_stats": None,
                "file_details": None
            }
            
            # Optionally fetch detailed stats for each commit
            # (This can be expensive for large repos)
            try:
                detailed_commit = await fetch_commit_details(sha, owner, repo, token)
                if detailed_commit:
                    enhanced_commit["enhanced_stats"] = detailed_commit.get("stats", {})
                    
            except Exception as e:
                print(f"Warning: Could not fetch enhanced data for commit {sha}: {e}")
            
            enhanced_commits.append(enhanced_commit)
        
        return enhanced_commits
        
    except Exception as e:
        print(f"Error fetching enhanced commits: {e}")
        return []

async def fetch_commit_files(commit_sha: str, owner: str, repo: str, token: str = None) -> List[Dict[str, Any]]:
    """
    L·∫•y danh s√°ch files thay ƒë·ªïi trong m·ªôt commit
    
    Args:
        commit_sha (str): SHA c·ªßa commit
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        token (str): GitHub token
    
    Returns:
        list: Danh s√°ch files v·ªõi th√¥ng tin thay ƒë·ªïi
    """
    try:
        url = f"/repos/{owner}/{repo}/commits/{commit_sha}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BASE_URL}{url}", headers=get_headers(token))
            
            if response.status_code == 200:
                commit_data = response.json()
                files = commit_data.get("files", [])
                
                # Process file information
                processed_files = []
                for file in files:
                    processed_files.append({
                        "filename": file.get("filename"),
                        "status": file.get("status"),  # added, modified, removed, renamed
                        "additions": file.get("additions", 0),
                        "deletions": file.get("deletions", 0),
                        "changes": file.get("changes", 0),
                        "patch": file.get("patch"),  # Actual diff content
                        "previous_filename": file.get("previous_filename"),  # For renamed files
                        "blob_url": file.get("blob_url"),
                        "raw_url": file.get("raw_url")
                    })
                
                return processed_files
            
            return []
            
    except Exception as e:
        print(f"Error fetching commit files for {commit_sha}: {e}")
        return []

async def fetch_commit_author_info(commit_sha: str, owner: str, repo: str, token: str = None) -> Optional[Dict[str, Any]]:
    """
    L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ author c·ªßa commit (bao g·ªìm GitHub user info n·∫øu c√≥)
    
    Args:
        commit_sha (str): SHA c·ªßa commit
        owner (str): Ch·ªß s·ªü h·ªØu repo
        repo (str): T√™n repository
        token (str): GitHub token
    
    Returns:
        dict: Th√¥ng tin author chi ti·∫øt
    """
    try:
        url = f"/repos/{owner}/{repo}/commits/{commit_sha}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BASE_URL}{url}", headers=get_headers(token))
            
            if response.status_code == 200:
                commit_data = response.json()
                
                # Extract author information
                author_info = {
                    "git_author": commit_data.get("commit", {}).get("author", {}),
                    "git_committer": commit_data.get("commit", {}).get("committer", {}),
                    "github_author": commit_data.get("author"),  # GitHub user info
                    "github_committer": commit_data.get("committer")  # GitHub user info
                }
                
                return author_info
            
            return None
            
    except Exception as e:
        print(f"Error fetching author info for {commit_sha}: {e}")
        return None

async def analyze_commit_type(message: str, files: List[Dict] = None) -> Dict[str, Any]:
    """
    Ph√¢n t√≠ch lo·∫°i commit d·ª±a tr√™n message v√† files thay ƒë·ªïi
    
    Args:
        message (str): Commit message
        files (list): Danh s√°ch files thay ƒë·ªïi
    
    Returns:
        dict: Th√¥ng tin ph√¢n lo·∫°i commit
    """
    import re
    
    analysis = {
        "type": "other",
        "is_feature": False,
        "is_bugfix": False,
        "is_refactor": False,
        "is_documentation": False,
        "is_test": False,
        "is_merge": False,
        "conventional_commit": False,
        "breaking_change": False,
        "scope": None
    }
    
    message_lower = message.lower()
    
    # Check for merge commits
    if message.startswith("Merge") or "merge" in message_lower:
        analysis["is_merge"] = True
        analysis["type"] = "merge"
    
    # Check for conventional commits (feat:, fix:, docs:, etc.)
    conventional_pattern = r"^(feat|fix|docs|style|refactor|test|chore|perf|ci|build|revert)(\(.+\))?\!?:"
    match = re.match(conventional_pattern, message)
    if match:
        analysis["conventional_commit"] = True
        commit_type = match.group(1)
        scope = match.group(2)
        
        analysis["type"] = commit_type
        if scope:
            analysis["scope"] = scope.strip("()")
        
        # Check for breaking changes
        if "!" in match.group(0) or "BREAKING CHANGE" in message:
            analysis["breaking_change"] = True
        
        # Set specific flags
        analysis["is_feature"] = commit_type == "feat"
        analysis["is_bugfix"] = commit_type == "fix"
        analysis["is_refactor"] = commit_type == "refactor"
        analysis["is_documentation"] = commit_type == "docs"
        analysis["is_test"] = commit_type == "test"
    
    else:
        # Fallback analysis based on keywords
        if any(keyword in message_lower for keyword in ["add", "implement", "feature", "new"]):
            analysis["is_feature"] = True
            analysis["type"] = "feature"
        elif any(keyword in message_lower for keyword in ["fix", "bug", "issue", "error"]):
            analysis["is_bugfix"] = True
            analysis["type"] = "bugfix"
        elif any(keyword in message_lower for keyword in ["refactor", "restructure", "improve"]):
            analysis["is_refactor"] = True
            analysis["type"] = "refactor"
        elif any(keyword in message_lower for keyword in ["doc", "readme", "comment"]):
            analysis["is_documentation"] = True
            analysis["type"] = "documentation"
        elif any(keyword in message_lower for keyword in ["test", "spec", "coverage"]):
            analysis["is_test"] = True
            analysis["type"] = "test"
    
    # Analyze files if provided
    if files:
        file_types = []
        for file in files:
            filename = file.get("filename", "")
            if filename.endswith((".md", ".txt", ".rst")):
                file_types.append("documentation")
            elif filename.endswith((".test.", ".spec.", "_test.", "_spec.")):
                file_types.append("test")
            elif filename.endswith((".py", ".js", ".ts", ".java", ".cpp", ".c")):
                file_types.append("code")
            elif filename.endswith((".css", ".scss", ".less")):
                file_types.append("style")
            elif filename.endswith((".json", ".yml", ".yaml", ".toml", ".ini")):
                file_types.append("config")
        
        # Update analysis based on file types
        if "documentation" in file_types and len(set(file_types)) == 1:
            analysis["is_documentation"] = True
            if analysis["type"] == "other":
                analysis["type"] = "documentation"
        
        if "test" in file_types and len(set(file_types)) == 1:
            analysis["is_test"] = True
            if analysis["type"] == "other":
                analysis["type"] = "test"
    
    return analysis
```

### backend\services\gitlab_service.py
```py

```

### backend\services\han_ai_service.py
```py
# backend/services/han_ai_service.py
"""
HAN AI Service - Service layer for HAN model integration
Provides high-level API for commit analysis and project management AI features
"""

import os
import sys
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
import asyncio
from functools import lru_cache

# Add AI directory to path
ai_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'ai')
sys.path.insert(0, ai_dir)

try:
    from ai.han_commit_analyzer import HANCommitAnalyzer
except ImportError as e:
    logging.warning(f"HAN model not available: {e}")
    HANCommitAnalyzer = None

logger = logging.getLogger(__name__)

class HANAIService:
    """
    Service class for HAN-based AI analysis in project management
    """
    
    def __init__(self):
        self.analyzer = None
        self.is_model_loaded = False
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize HAN model with error handling"""
        try:
            if HANCommitAnalyzer is None:
                logger.warning("HAN analyzer not available - using mock responses")
                return
                
            self.analyzer = HANCommitAnalyzer()
            self.analyzer.load_model()
            self.is_model_loaded = True
            logger.info("HAN model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load HAN model: {e}")
            self.is_model_loaded = False
    
    async def analyze_commit_message(self, message: str) -> Dict[str, Any]:
        """
        Analyze a single commit message
        
        Args:
            message: Commit message text
            
        Returns:
            Analysis results including category, impact, urgency
        """
        try:
            if not self.is_model_loaded:
                return self._mock_commit_analysis(message)
            
            # Run prediction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.analyzer.predict_commit_analysis, 
                message
            )
            
            return {
                'success': True,
                'message': message,
                'analysis': result,
                'model_version': 'HAN-v1',
                'confidence': result.get('confidence', {})
            }
            
        except Exception as e:
            logger.error(f"Error analyzing commit: {e}")
            return {
                'success': False,
                'message': message,
                'error': str(e),
                'analysis': self._mock_commit_analysis(message)['analysis']
            }
    
    async def analyze_commits_batch(self, messages: List[str]) -> Dict[str, Any]:
        """
        Analyze multiple commit messages in batch
        
        Args:
            messages: List of commit message texts
            
        Returns:
            Batch analysis results
        """
        results = []
        
        for message in messages:
            analysis = await self.analyze_commit_message(message)
            results.append(analysis)
        
        # Generate batch statistics
        stats = self._calculate_batch_statistics(results)
        
        return {
            'success': True,
            'total_commits': len(messages),
            'results': results,
            'statistics': stats
        }
    
    async def analyze_developer_patterns(self, developer_commits: Dict[str, List[str]]) -> Dict[str, Any]:
        """
        Analyze commit patterns for each developer
        
        Args:
            developer_commits: Dict mapping developer name to list of commits
            
        Returns:
            Developer pattern analysis
        """
        developer_profiles = {}
        
        for developer, commits in developer_commits.items():
            if not commits:
                continue
                
            # Analyze all commits for this developer
            batch_result = await self.analyze_commits_batch(commits)
            
            # Create developer profile
            profile = self._create_developer_profile(commits, batch_result)
            developer_profiles[developer] = profile
        
        return {
            'success': True,
            'developer_profiles': developer_profiles,
            'total_developers': len(developer_profiles)
        }
    
    async def suggest_task_assignment(self, tasks: List[Dict], developers: List[Dict]) -> Dict[str, Any]:
        """
        Suggest task assignments based on commit analysis and developer profiles
        
        Args:
            tasks: List of task dictionaries
            developers: List of developer dictionaries with commit history
            
        Returns:
            Task assignment suggestions
        """
        try:
            # Analyze developer patterns first
            developer_commits = {}
            for dev in developers:
                developer_commits[dev['login']] = dev.get('recent_commits', [])
            
            developer_analysis = await self.analyze_developer_patterns(developer_commits)
            
            # Generate task assignments
            assignments = []
            for task in tasks:
                assignment = self._match_task_to_developer(
                    task, 
                    developer_analysis['developer_profiles']
                )
                assignments.append(assignment)
            
            return {
                'success': True,
                'assignments': assignments,
                'developer_analysis': developer_analysis
            }
            
        except Exception as e:
            logger.error(f"Error in task assignment: {e}")
            return {
                'success': False,
                'error': str(e),
                'assignments': self._mock_task_assignments(tasks, developers)
            }
    
    async def generate_project_insights(self, project_data: Dict) -> Dict[str, Any]:
        """
        Generate comprehensive project insights based on commit analysis
        
        Args:
            project_data: Project data including commits, contributors, etc.
            
        Returns:
            Project insights and recommendations
        """
        try:
            all_commits = project_data.get('commits', [])
            contributors = project_data.get('contributors', [])
            
            # Analyze all commits
            commit_messages = [commit.get('message', '') for commit in all_commits]
            batch_analysis = await self.analyze_commits_batch(commit_messages)
            
            # Generate insights
            insights = {
                'commit_analysis': batch_analysis,
                'code_quality_trends': self._analyze_quality_trends(batch_analysis),
                'team_collaboration': self._analyze_team_collaboration(all_commits, contributors),
                'project_health': self._assess_project_health(batch_analysis),
                'recommendations': self._generate_recommendations(batch_analysis)
            }
            
            return {
                'success': True,
                'project_name': project_data.get('name', 'Unknown'),
                'insights': insights
            }
            
        except Exception as e:
            logger.error(f"Error generating project insights: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _mock_commit_analysis(self, message: str) -> Dict[str, Any]:
        """Mock analysis when model is not available"""
        # Simple keyword-based mock analysis
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            category = 'bug'
            impact = 'medium'
            urgency = 'high'
        elif any(word in message_lower for word in ['feat', 'feature', 'add', 'new']):
            category = 'feature'
            impact = 'high'
            urgency = 'medium'
        elif any(word in message_lower for word in ['docs', 'doc', 'readme']):
            category = 'docs'
            impact = 'low'
            urgency = 'low'
        elif any(word in message_lower for word in ['test', 'spec']):
            category = 'test'
            impact = 'medium'
            urgency = 'low'
        else:
            category = 'chore'
            impact = 'low'
            urgency = 'medium'
        
        return {
            'success': True,
            'analysis': {
                'category': category,
                'impact': impact,
                'urgency': urgency,
                'is_mock': True
            }
        }
    
    def _calculate_batch_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """Calculate statistics from batch analysis results"""
        if not results:
            return {}
        
        categories = {}
        impacts = {}
        urgencies = {}
        successful = 0
        
        for result in results:
            if result.get('success', False):
                successful += 1
                analysis = result.get('analysis', {})
                
                # Count categories
                category = analysis.get('category', 'unknown')
                categories[category] = categories.get(category, 0) + 1
                
                # Count impacts
                impact = analysis.get('impact', 'unknown')
                impacts[impact] = impacts.get(impact, 0) + 1
                
                # Count urgencies
                urgency = analysis.get('urgency', 'unknown')
                urgencies[urgency] = urgencies.get(urgency, 0) + 1
        
        return {
            'total_analyzed': len(results),
            'successful_analyses': successful,
            'success_rate': successful / len(results) if results else 0,
            'category_distribution': categories,
            'impact_distribution': impacts,
            'urgency_distribution': urgencies
        }
    
    def _create_developer_profile(self, commits: List[str], analysis_result: Dict) -> Dict[str, Any]:
        """Create developer profile from commit analysis"""
        stats = analysis_result.get('statistics', {})
        
        return {
            'total_commits': len(commits),
            'preferred_categories': self._get_top_categories(stats.get('category_distribution', {})),
            'impact_pattern': self._get_top_categories(stats.get('impact_distribution', {})),
            'urgency_pattern': self._get_top_categories(stats.get('urgency_distribution', {})),
            'activity_score': min(len(commits) / 10, 10),  # Scale 0-10
            'specialization': self._determine_specialization(stats)
        }
    
    def _get_top_categories(self, distribution: Dict[str, int], top_n: int = 3) -> List[str]:
        """Get top N categories from distribution"""
        if not distribution:
            return []
        
        sorted_items = sorted(distribution.items(), key=lambda x: x[1], reverse=True)
        return [item[0] for item in sorted_items[:top_n]]
    
    def _determine_specialization(self, stats: Dict) -> str:
        """Determine developer specialization based on commit patterns"""
        categories = stats.get('category_distribution', {})
        
        if not categories:
            return 'generalist'
        
        top_category = max(categories.items(), key=lambda x: x[1])
        total_commits = sum(categories.values())
        
        if top_category[1] / total_commits > 0.5:
            return f"{top_category[0]}_specialist"
        else:
            return 'generalist'
    
    def _match_task_to_developer(self, task: Dict, developer_profiles: Dict) -> Dict[str, Any]:
        """Match a task to the best developer based on profiles"""
        task_type = task.get('type', 'feature').lower()
        task_priority = task.get('priority', 'medium').lower()
        
        best_match = None
        best_score = 0
        
        for dev_name, profile in developer_profiles.items():
            score = self._calculate_match_score(task_type, task_priority, profile)
            if score > best_score:
                best_score = score
                best_match = dev_name
        
        return {
            'task_title': task.get('title', 'Untitled'),
            'task_type': task_type,
            'recommended_developer': best_match or 'No suitable match',
            'confidence_score': best_score,
            'reasoning': self._generate_assignment_reasoning(task, best_match, developer_profiles.get(best_match, {}))
        }
    
    def _calculate_match_score(self, task_type: str, task_priority: str, profile: Dict) -> float:
        """Calculate match score between task and developer"""
        score = 0.0
        
        # Check category preference
        preferred_categories = profile.get('preferred_categories', [])
        if task_type in preferred_categories:
            score += 0.4
        
        # Check specialization
        specialization = profile.get('specialization', '')
        if task_type in specialization:
            score += 0.3
        
        # Activity score
        activity_score = profile.get('activity_score', 0)
        score += (activity_score / 10) * 0.3
        
        return score
    
    def _generate_assignment_reasoning(self, task: Dict, developer: str, profile: Dict) -> str:
        """Generate reasoning for task assignment"""
        if not developer or not profile:
            return "No suitable developer found based on commit analysis"
        
        specialization = profile.get('specialization', 'generalist')
        activity_score = profile.get('activity_score', 0)
        
        return f"Recommended {developer} based on {specialization} specialization and activity score of {activity_score:.1f}/10"
    
    def _mock_task_assignments(self, tasks: List[Dict], developers: List[Dict]) -> List[Dict]:
        """Generate mock task assignments when model is unavailable"""
        assignments = []
        
        for i, task in enumerate(tasks):
            dev_index = i % len(developers) if developers else 0
            developer = developers[dev_index]['login'] if developers else 'Unknown'
            
            assignments.append({
                'task_title': task.get('title', 'Untitled'),
                'recommended_developer': developer,
                'confidence_score': 0.5,
                'reasoning': 'Mock assignment - HAN model not available'
            })
        
        return assignments
    
    def _analyze_quality_trends(self, analysis: Dict) -> Dict[str, Any]:
        """Analyze code quality trends from commit analysis"""
        stats = analysis.get('statistics', {})
        categories = stats.get('category_distribution', {})
        
        bug_ratio = categories.get('bug', 0) / max(stats.get('total_analyzed', 1), 1)
        test_ratio = categories.get('test', 0) / max(stats.get('total_analyzed', 1), 1)
        
        quality_score = max(0, 1 - bug_ratio + test_ratio * 0.5)
        
        return {
            'quality_score': round(quality_score, 2),
            'bug_fix_ratio': round(bug_ratio, 2),
            'test_coverage_indicator': round(test_ratio, 2),
            'trend': 'improving' if quality_score > 0.7 else 'needs_attention'
        }
    
    def _analyze_team_collaboration(self, commits: List[Dict], contributors: List[Dict]) -> Dict[str, Any]:
        """Analyze team collaboration patterns"""
        return {
            'total_contributors': len(contributors),
            'commit_distribution': 'balanced',  # Simplified
            'collaboration_score': 0.8  # Mock score
        }
    
    def _assess_project_health(self, analysis: Dict) -> Dict[str, Any]:
        """Assess overall project health"""
        stats = analysis.get('statistics', {})
        success_rate = stats.get('success_rate', 0)
        
        health_score = success_rate * 0.8 + 0.2  # Base score
        
        return {
            'health_score': round(health_score, 2),
            'status': 'healthy' if health_score > 0.7 else 'needs_attention',
            'analysis_coverage': f"{stats.get('successful_analyses', 0)}/{stats.get('total_analyzed', 0)}"
        }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Generate project recommendations based on analysis"""
        recommendations = []
        stats = analysis.get('statistics', {})
        categories = stats.get('category_distribution', {})
        
        total = sum(categories.values()) if categories else 1
        
        if categories.get('bug', 0) / total > 0.3:
            recommendations.append("Consider increasing code review practices to reduce bug fixes")
        
        if categories.get('test', 0) / total < 0.1:
            recommendations.append("Increase test coverage to improve code quality")
        
        if categories.get('docs', 0) / total < 0.05:
            recommendations.append("Improve documentation practices")
        
        if not recommendations:
            recommendations.append("Project shows good development practices")
        
        return recommendations

# Singleton instance
_han_ai_service = None

def get_han_ai_service() -> HANAIService:
    """Get singleton instance of HAN AI Service"""
    global _han_ai_service
    if _han_ai_service is None:
        _han_ai_service = HANAIService()
    return _han_ai_service

```

### backend\services\issue_service.py
```py
from db.database import database
from db.models.issues import issues
from sqlalchemy import select, insert
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_issue_by_github_id(github_id: int):
    """Get issue by GitHub ID"""
    query = select(issues).where(issues.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

# L∆∞u m·ªôt issue duy nh·∫•t
async def save_issue(issue_data):
    """Save issue with proper datetime conversion"""
    try:
        # Ki·ªÉm tra issue ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_issue = await get_issue_by_github_id(issue_data["github_id"])
        
        if existing_issue:
            logger.info(f"Issue {issue_data['github_id']} already exists, skipping")
            return existing_issue.id

        # Convert datetime strings
        issue_entry = {
            **issue_data,
            "created_at": parse_github_datetime(issue_data.get("created_at")),
            "updated_at": parse_github_datetime(issue_data.get("updated_at"))
        }

        query = insert(issues).values(issue_entry)
        result = await database.execute(query)
        logger.info(f"Created new issue: {issue_data['title']}")
        return result
        
    except Exception as e:
        logger.error(f"Error saving issue {issue_data.get('title')}: {e}")
        raise e

# L∆∞u danh s√°ch nhi·ªÅu issue
async def save_issues(issue_list):
    """Save multiple issues"""
    for issue in issue_list:
        await save_issue(issue)

```

### backend\services\member_analysis_service.py
```py
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
from datetime import datetime, timedelta
from collections import defaultdict
import re
import asyncio
from services.han_ai_service import HANAIService

class MemberAnalysisService:
    def __init__(self, db: Session):        
        self.db = db
        self.ai_service = HANAIService()
    
    def get_repository_members(self, repository_id: int) -> List[Dict[str, Any]]:
        """L·∫•y danh s√°ch members c·ªßa repository t·ª´ collaborators table v√† commit authors"""
        # First get formal collaborators with their commit counts
        query = text("""
            SELECT 
                c.id,
                c.github_username,
                c.display_name,
                c.avatar_url,
                COUNT(co.id) as total_commits
            FROM collaborators c
            JOIN repository_collaborators rc ON c.id = rc.collaborator_id
            LEFT JOIN commits co ON (
                (LOWER(co.author_name) = LOWER(c.github_username)) OR 
                (LOWER(co.author_name) = LOWER(c.display_name))
            ) AND co.repo_id = :repo_id
            WHERE rc.repository_id = :repo_id
            GROUP BY c.id, c.github_username, c.display_name, c.avatar_url
            ORDER BY total_commits DESC
        """)
        
        result = self.db.execute(query, {"repo_id": repository_id}).fetchall()
        
        members = []
        processed_authors = set()
          # Helper function to normalize names for comparison
        def normalize_name(name):
            if not name:
                return ""
            # Remove accents and normalize to lowercase
            import unicodedata
            normalized = unicodedata.normalize('NFD', name)
            without_accents = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')
            return without_accents.lower().strip()
        
        # Helper function to extract name parts for better matching
        def extract_name_parts(name):
            if not name:
                return set()
            # Split by common separators and extract meaningful parts
            parts = re.split(r'[\s\-_\.@]+', name.lower().strip())
            # Remove empty parts and very short parts (< 2 chars)
            meaningful_parts = {part for part in parts if len(part) >= 2}
            return meaningful_parts        # Helper function to check if names are similar
        def are_names_similar(name1, name2):
            if not name1 or not name2:
                return False
                
            n1, n2 = normalize_name(name1), normalize_name(name2)
            
            # Exact match
            if n1 == n2:
                return True
              # For very short names (like "San", "SAN"), be very strict
            # Only match if they are exact (case-insensitive) matches
            if len(n1) <= 3 or len(n2) <= 3:
                return n1 == n2
            
            # For longer names, check substring matches but be very careful
            # Require the shorter name to be at least 70% of the longer name
            if len(n1) > 4 and len(n2) > 4:
                shorter = min(n1, n2, key=len)
                longer = max(n1, n2, key=len)
                
                if shorter in longer:
                    # Check if the shorter name is substantial part of the longer name
                    ratio = len(shorter) / len(longer)
                    if ratio >= 0.7:  # At least 70% overlap
                        return True
            
            # Check name parts intersection (for complex names)
            # But be very strict about part matching
            parts1 = extract_name_parts(name1)
            parts2 = extract_name_parts(name2)
            
            if parts1 and parts2:
                # For single part names (like "San"), be very strict
                if len(parts1) == 1 and len(parts2) == 1:
                    # Only match if exactly the same
                    return parts1 == parts2
                
                # For multi-part names, require substantial overlap
                common_parts = parts1.intersection(parts2)
                if common_parts:
                    # Require multiple common parts OR one very long common part
                    if len(common_parts) >= 2:  # Multiple common parts
                        return True
                    elif len(common_parts) == 1:  # Single common part
                        part = list(common_parts)[0]
                        if len(part) > 4:  # Very long common part
                            part_ratio1 = len(part) / len(n1) if n1 else 0
                            part_ratio2 = len(part) / len(n2) if n2 else 0
                            if part_ratio1 > 0.7 or part_ratio2 > 0.7:
                                return True
            
            return False
        
        for row in result:
            github_username = row[1]
            display_name = row[2] or row[1]
            commit_count = row[4]
            
            members.append({
                "id": row[0],
                "login": github_username,  # Use github_username as primary login
                "display_name": display_name,
                "avatar_url": row[3],
                "total_commits": commit_count
            })
            
            # Track processed authors (case-insensitive and similar names)
            processed_authors.add(normalize_name(github_username))
            if display_name:
                processed_authors.add(normalize_name(display_name))
          # Get commit authors who aren't formal collaborators
        unmatched_query = text("""
            SELECT 
                author_name,
                author_email,
                COUNT(*) as total_commits
            FROM commits 
            WHERE repo_id = :repo_id
            GROUP BY author_name, author_email
            HAVING COUNT(*) > 0
            ORDER BY total_commits DESC
        """)
        
        unmatched_result = self.db.execute(unmatched_query, {"repo_id": repository_id}).fetchall()
        
        # Group authors by email first (same email = same person)
        email_to_authors = {}
        for row in unmatched_result:
            author_name = row[0]
            author_email = row[1]
            commit_count = row[2]
            
            if author_email not in email_to_authors:
                email_to_authors[author_email] = []
            email_to_authors[author_email].append((author_name, commit_count))
          # Process each email group
        for author_email, authors_data in email_to_authors.items():
            # Consolidate authors with same email
            total_commits_for_email = sum(count for _, count in authors_data)
            # Use the author name with most commits as primary
            primary_author = max(authors_data, key=lambda x: x[1])[0]
              # CONSERVATIVE: Only check for exact email matches with GitHub noreply
            email_matches_collaborator = False
            collaborator_member = None
            
            # Only merge if it's a GitHub noreply email with EXACT username match
            if author_email and '@users.noreply.github.com' in author_email:
                # Extract username from GitHub noreply email
                email_parts = author_email.split('@')[0]
                if '+' in email_parts:
                    github_username = email_parts.split('+')[1]
                    # Only merge if there's an EXACT collaborator with this username
                    for member in members:
                        if (member['id'] != f"author_{member['login']}" and  # This is a formal collaborator
                            github_username.lower() == member['login'].lower()):
                            email_matches_collaborator = True
                            collaborator_member = member
                            break
            
            if email_matches_collaborator and collaborator_member:
                # Merge with existing collaborator (add commit counts)
                collaborator_member['total_commits'] += total_commits_for_email
                # Mark all author names as processed
                for author_name, _ in authors_data:
                    processed_authors.add(normalize_name(author_name))
                continue
              # Check if this email/author is already covered by a collaborator (name-based)
            is_already_processed = False
            for processed_name in processed_authors:
                for author_name, _ in authors_data:
                    if are_names_similar(author_name, processed_name):
                        is_already_processed = True
                        break
                if is_already_processed:
                    break
              # CONSERVATIVE: Only check for exact name matches with collaborators
            if not is_already_processed:
                collaborator_match = None
                for member in members:
                    if member['id'] != f"author_{member['login']}":  # This is a formal collaborator
                        for author_name, _ in authors_data:
                            # Only exact matches (case-insensitive)
                            if (normalize_name(author_name) == normalize_name(member['login']) or 
                                normalize_name(author_name) == normalize_name(member['display_name'] or '')):
                                collaborator_match = member
                                break
                    if collaborator_match:
                        break
                
                if collaborator_match:
                    # Merge with existing collaborator (add commit counts)
                    collaborator_match['total_commits'] += total_commits_for_email
                    # Mark all author names as processed
                    for author_name, _ in authors_data:
                        processed_authors.add(normalize_name(author_name))
                    continue
            
            if not is_already_processed:
                # Check if we already have a similar author in our members list (name-based)
                similar_member = None
                for member in members:
                    for author_name, _ in authors_data:
                        if (are_names_similar(author_name, member['login']) or 
                            are_names_similar(author_name, member['display_name'])):
                            similar_member = member
                            break
                    if similar_member:
                        break
                
                if similar_member:
                    # Merge with existing member (add commit counts)
                    similar_member['total_commits'] += total_commits_for_email
                    # Use the name with more commits as primary
                    if total_commits_for_email > similar_member['total_commits'] - total_commits_for_email:
                        similar_member['login'] = primary_author
                        similar_member['display_name'] = primary_author
                else:
                    # Add as new informal member (consolidated by email)
                    members.append({
                        "id": f"author_{primary_author}",  # Special ID for non-collaborator authors
                        "login": primary_author,
                        "display_name": primary_author,
                        "avatar_url": None,
                        "total_commits": total_commits_for_email
                    })
                
                # Mark all author names as processed
                for author_name, _ in authors_data:
                    processed_authors.add(normalize_name(author_name))
        
        # Sort by commit count
        members.sort(key=lambda x: x['total_commits'], reverse=True)
        
        return members
    
    def get_repository_branches(self, repository_id: int) -> List[Dict[str, Any]]:
        """L·∫•y danh s√°ch branches c·ªßa repository"""
        query = text("""
            SELECT 
                b.id,
                b.name,
                b.is_default,
                b.commits_count,
                b.last_commit_date,
                COUNT(c.id) as actual_commits_count
            FROM branches b
            LEFT JOIN commits c ON c.branch_name = b.name AND c.repo_id = :repo_id
            WHERE b.repo_id = :repo_id
            GROUP BY b.id, b.name, b.is_default, b.commits_count, b.last_commit_date
            ORDER BY b.is_default DESC, b.commits_count DESC, b.name ASC
        """)
        
        result = self.db.execute(query, {"repo_id": repository_id}).fetchall()
        
        branches = []
        for row in result:
            branches.append({
                "id": row[0],
                "name": row[1],
                "is_default": row[2] or False,
                "commits_count": row[5],  # actual count from commits table
                "last_commit_date": row[4].isoformat() if row[4] else None
            })
        return branches
    # l·∫•y commits c·ªßa member v·ªõi ph√¢n t√≠ch ƒë∆°n gi·∫£n v√† branch filter
    def get_member_commits_with_analysis(
        self, 
        repository_id: int, 
        member_login: str, 
        limit: int = 50,
        branch_name: str = None  # NEW: Optional branch filter
    ) -> Dict[str, Any]:
        """L·∫•y commits c·ªßa member v·ªõi analysis ƒë∆°n gi·∫£n v√† branch filter"""
        
        # ENHANCED: Get all author names associated with this member
        all_author_names = self._get_all_author_names_for_member(repository_id, member_login)
        
        if not all_author_names:
            all_author_names = [member_login]  # Fallback
        
        # Create IN clause for multiple author names
        author_placeholders = ', '.join([f':author_{i}' for i in range(len(all_author_names))])
        
        # Query commits c·ªßa member v·ªõi multiple author names v√† branch filter
        base_query = f"""
            SELECT 
                id, sha, message, author_name, author_email,
                date, branch_name, insertions, deletions, files_changed
            FROM commits 
            WHERE repo_id = :repo_id 
                AND LOWER(author_name) IN ({author_placeholders})
        """
        
        params = {
            "repo_id": repository_id,
            "limit": limit
        }
        
        # th√™m t√™n t√°c gi·∫£ v√†o params
        for i, author_name in enumerate(all_author_names):
            params[f"author_{i}"] = author_name.lower()
        
        # Add branch filter if specified
        if branch_name:
            base_query += " AND branch_name = :branch_name"
            params["branch_name"] = branch_name
            
        base_query += " ORDER BY committer_date DESC LIMIT :limit"
        
        query = text(base_query)
        
        commits_data = self.db.execute(query, params).fetchall()
        
        if not commits_data:
            return {
                "member": {"login": member_login, "display_name": member_login},
                "summary": {
                    "total_commits": 0, 
                    "message": f"No commits found{' on branch ' + branch_name if branch_name else ''}",
                    "branch_filter": branch_name,
                    "ai_powered": False,
                    "analysis_date": datetime.now().isoformat()
                },
                "commits": [],
                "statistics": {"commit_types": {}, "tech_analysis": {}, "productivity": {"total_additions": 0, "total_deletions": 0}}
            }
          # ph√¢n t√≠ch commits
        commits_with_analysis = []
        commit_type_stats = defaultdict(int)
        tech_stats = defaultdict(int)
        total_additions = 0
        total_deletions = 0
        
        for commit in commits_data:
            # Simple pattern-based analysis
            analysis = self._analyze_commit_message(commit[2])  # commit.message
            
            commit_info = {
                "id": commit[0],
                "sha": commit[1][:8] if commit[1] else "N/A",
                "message": commit[2],
                "author": commit[3],
                "date": commit[5].isoformat() if commit[5] else None,  # date is index 5
                "branch": commit[6] or "main",
                "stats": {
                    "insertions": commit[7] or 0,
                    "deletions": commit[8] or 0,
                    "files_changed": commit[9] or 0
                },
                "analysis": {
                    "type": analysis["type"],
                    "type_icon": analysis["icon"],
                    "tech_area": analysis["tech_area"],
                    "ai_powered": False
                }
            }
            
            commits_with_analysis.append(commit_info)
            commit_type_stats[analysis["type"]] += 1
            tech_stats[analysis["tech_area"]] += 1
            total_additions += commit[7] or 0
            total_deletions += commit[8] or 0
        
        return {
            "member": {"login": member_login, "display_name": member_login},
            "summary": {
                "total_commits": len(commits_with_analysis),
                "branch_filter": branch_name,
                "ai_powered": False,
                "analysis_date": datetime.now().isoformat()
            },
            "commits": commits_with_analysis,
            "statistics": {
                "commit_types": dict(commit_type_stats),
                "tech_analysis": dict(tech_stats),
                "productivity": {
                    "total_additions": total_additions,
                    "total_deletions": total_deletions
                }
            }        }

    async def get_member_commits_with_ai_analysis(
        self, 
        repository_id: int, 
        member_login: str, 
        limit: int = 50,
        branch_name: str = None  # NEW: Optional branch filter
    ) -> Dict[str, Any]:
        """L·∫•y commits c·ªßa member v·ªõi AI analysis v√† branch filter"""
        
        # Get all author names associated with this member (including merged names)
        all_author_names = self._get_all_author_names_for_member(repository_id, member_login)
        
        # Build query to match any of the associated author names
        author_conditions = " OR ".join([f"LOWER(author_name) = LOWER(:author_name_{i})" for i in range(len(all_author_names))])
        
        base_query = f"""
            SELECT 
                id, sha, message, author_name, author_email,
                committer_date, branch_name, insertions, deletions, files_changed
            FROM commits 
            WHERE repo_id = :repo_id 
                AND ({author_conditions})
        """
        
        params = {
            "repo_id": repository_id,
            "limit": limit
        }
        
        # Add all author names as parameters
        for i, author_name in enumerate(all_author_names):
            params[f"author_name_{i}"] = author_name
        
        # Add branch filter if specified
        if branch_name:
            base_query += " AND branch_name = :branch_name"
            params["branch_name"] = branch_name
            
        base_query += " ORDER BY committer_date DESC LIMIT :limit"
        
        query = text(base_query)
        
        commits_data = self.db.execute(query, params).fetchall()
        
        if not commits_data:
            return {
                "member": {"login": member_login, "display_name": member_login},
                "summary": {
                    "total_commits": 0, 
                    "message": f"No commits found{' on branch ' + branch_name if branch_name else ''}",
                    "branch_filter": branch_name,
                    "ai_powered": True,
                    "analysis_date": datetime.now().isoformat()
                },
                "commits": [],
                "statistics": {"commit_types": {}, "tech_analysis": {}, "productivity": {"total_additions": 0, "total_deletions": 0}}
            }
        
        # Prepare data for AI analysis
        commit_messages = [commit[2] for commit in commits_data]  # Extract messages
        
        # Get AI analysis
        try:
            ai_analysis = await self.ai_service.analyze_commits(commit_messages)
        except Exception as e:
            print(f"AI analysis failed, falling back to pattern analysis: {e}")
            # Fallback to pattern-based analysis
            return self.get_member_commits_with_analysis(repository_id, member_login, limit, branch_name)
        
        # Combine commits with AI analysis
        commits_with_analysis = []
        commit_type_stats = defaultdict(int)
        tech_stats = defaultdict(int)
        total_additions = 0
        total_deletions = 0
        
        for i, commit in enumerate(commits_data):
            ai_result = ai_analysis.get(i, {}) if ai_analysis else {}
            
            commit_info = {
                "id": commit[0],
                "sha": commit[1][:8] if commit[1] else "N/A",
                "message": commit[2],
                "author": commit[3],
                "date": commit[5].isoformat() if commit[5] else None,
                "branch": commit[6] or "main",
                "stats": {
                    "insertions": commit[7] or 0,
                    "deletions": commit[8] or 0,
                    "files_changed": commit[9] or 0
                },
                "analysis": {
                    "type": ai_result.get("type", "other"),
                    "type_icon": self._get_type_icon(ai_result.get("type", "other")),
                    "tech_area": ai_result.get("tech_area", "general"),
                    "impact": ai_result.get("impact", "medium"),
                    "urgency": ai_result.get("urgency", "normal"),
                    "ai_powered": True
                }
            }
            
            commits_with_analysis.append(commit_info)
            commit_type_stats[ai_result.get("type", "other")] += 1
            tech_stats[ai_result.get("tech_area", "general")] += 1
            total_additions += commit[7] or 0
            total_deletions += commit[8] or 0
        
        return {
            "member": {"login": member_login, "display_name": member_login},
            "summary": {
                "total_commits": len(commits_with_analysis),
                "branch_filter": branch_name,
                "ai_powered": True,
                "analysis_date": datetime.now().isoformat()
            },
            "commits": commits_with_analysis,
            "statistics": {
                "commit_types": dict(commit_type_stats),
                "tech_analysis": dict(tech_stats),
                "productivity": {
                    "total_additions": total_additions,
                    "total_deletions": total_deletions
                }
            }
        }
    
    def _analyze_commit_message(self, message: str) -> Dict[str, str]:
        """Simple pattern-based commit analysis"""
        message_lower = message.lower()
        
        # Determine type
        if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement']):
            commit_type = "feat"
            icon = "üöÄ"
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            commit_type = "fix"
            icon = "üêõ"
        elif any(word in message_lower for word in ['docs', 'documentation', 'readme']):
            commit_type = "docs"
            icon = "üìù"
        elif any(word in message_lower for word in ['chore', 'cleanup', 'refactor']):
            commit_type = "chore"
            icon = "üîß"
        else:
            commit_type = "other"
            icon = "üì¶"
        
        # Determine tech area
        if any(word in message_lower for word in ['api', 'endpoint', 'rest', 'graphql']):
            tech_area = "API"
        elif any(word in message_lower for word in ['ui', 'frontend', 'react', 'vue', 'component']):
            tech_area = "Frontend"
        elif any(word in message_lower for word in ['database', 'db', 'sql', 'migration']):
            tech_area = "Database"
        elif any(word in message_lower for word in ['test', 'testing', 'spec', 'unittest']):
            tech_area = "Testing"
        else:
            tech_area = "General"
        
        return {
            "type": commit_type,
            "icon": icon,
            "tech_area": tech_area
        }
    
    def _get_type_icon(self, commit_type: str) -> str:
        """Get icon for commit type"""
        icons = {
            "feat": "üöÄ",
            "fix": "üêõ", 
            "docs": "üìù",
            "chore": "üîß",
            "refactor": "‚ôªÔ∏è",
            "test": "‚úÖ",
            "style": "üíÑ",
            "other": "üì¶"        }
        return icons.get(commit_type, "üì¶")
    
    def _get_all_author_names_for_member(self, repository_id: int, member_login: str) -> List[str]:
        """Get all author names associated with a member - CONSERVATIVE approach based on email evidence only"""
        author_names = []
        
        # 1. Add the member login itself
        author_names.append(member_login)
        
        # 2. Only add names that have CLEAR email evidence of being the same person
        query = text("""
            SELECT DISTINCT author_name, author_email
            FROM commits 
            WHERE repo_id = :repo_id
        """)
        
        all_authors = self.db.execute(query, {"repo_id": repository_id}).fetchall()
        
        for author_name, author_email in all_authors:
            # Only check GitHub noreply email pattern (most reliable)
            if author_email and '@users.noreply.github.com' in author_email:
                email_parts = author_email.split('@')[0]
                if '+' in email_parts:
                    github_username = email_parts.split('+')[1]
                    # Only match if GitHub username exactly matches member login
                    if github_username.lower() == member_login.lower():
                        if author_name not in author_names:
                            author_names.append(author_name)
            
            # Exact name match (case-insensitive) - very conservative
            if author_name.lower() == member_login.lower():
                if author_name not in author_names:
                    author_names.append(author_name)
        
        return author_names
    
```

### backend\services\model_loader.py
```py
# KLTN04\backend\services\model_loader.py
import joblib
from pathlib import Path
from typing import Optional, Union
import logging
from functools import lru_cache
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoader:
    _instance = None
    
    def __init__(self):
        try:
            model_path = self._get_model_path()
            logger.info(f"Loading model from {model_path}")
            
            self.model_data = joblib.load(model_path)
            self.model = self.model_data['model']
            self.vectorizer = self.model_data['vectorizer']
            
            # Warm-up predict
            self._warm_up()
            logger.info("Model loaded successfully")
            
        except Exception as e:
            logger.exception("Failed to load model")
            raise

    @staticmethod
    def _get_model_path() -> Path:
        """Validate and return model path"""
        model_path = Path(__file__).parent.parent / "models" / "commit_classifier_v1.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        return model_path

    def _warm_up(self):
        """Warm-up model with sample input"""
        sample = "fix: critical security vulnerability"
        self.predict(sample)
        
    @lru_cache(maxsize=1000)
    def vectorize(self, message: str) -> np.ndarray:
        """Cache vectorized results for frequent messages"""
        return self.vectorizer.transform([message])

    def predict(self, message: str) -> int:
        """Predict if commit is critical (with input validation)"""
        if not message or not isinstance(message, str):
            raise ValueError("Input must be non-empty string")
            
        X = self.vectorize(message.strip())
        return int(self.model.predict(X)[0])

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

def predict_commit(message: str) -> dict:
    """Public API for commit prediction
    
    Returns:
        {
            "prediction": 0|1,
            "confidence": float,
            "error": str|None
        }
    """
    try:
        loader = ModelLoader.get_instance()
        proba = loader.model.predict_proba(loader.vectorize(message))[0]
        return {
            "prediction": loader.predict(message),
            "confidence": float(np.max(proba)),
            "error": None
        }
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        return {
            "prediction": -1,
            "confidence": 0.0,
            "error": str(e)
        }
```

### backend\services\multimodal_ai_service.py
```py

```

### backend\services\pull_request_service.py
```py
from db.models.pull_requests import pull_requests
from sqlalchemy import select, insert, update, func
from db.database import database
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_pull_request_by_github_id(github_id: int):
    """L·∫•y pull request t·ª´ github_id"""
    query = select(pull_requests).where(pull_requests.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

async def save_pull_request(pr_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin pull request
    
    Args:
        pr_data (dict): Th√¥ng tin pull request t·ª´ GitHub API
    
    Returns:
        int: pull_request_id
    """
    try:
        # Ki·ªÉm tra xem pull request ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_pr = await get_pull_request_by_github_id(pr_data["github_id"])

        if existing_pr:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(pull_requests)
                .where(pull_requests.c.github_id == pr_data["github_id"])
                .values(
                    title=pr_data.get("title"),
                    description=pr_data.get("description"),
                    state=pr_data.get("state"),
                    updated_at=parse_github_datetime(pr_data.get("updated_at"))
                )
            )
            await database.execute(query)
            logger.info(f"Updated pull request: {pr_data.get('title')}")
            return existing_pr.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(pull_requests).values(
                github_id=pr_data["github_id"],
                title=pr_data.get("title"),
                description=pr_data.get("description"),
                state=pr_data.get("state"),
                repo_id=pr_data["repo_id"],
                created_at=parse_github_datetime(pr_data.get("created_at")),
                updated_at=parse_github_datetime(pr_data.get("updated_at"))
            )
            
            result = await database.execute(query)
            logger.info(f"Created new pull request: {pr_data.get('title')}")
            return result
            
    except Exception as e:
        logger.error(f"Error saving pull request {pr_data.get('title')}: {e}")
        raise e

async def get_pull_requests_by_repo_id(repo_id: int):
    """L·∫•y danh s√°ch pull requests c·ªßa repository"""
    query = select(pull_requests).where(pull_requests.c.repo_id == repo_id)
    results = await database.fetch_all(query)
    return results

```

### backend\services\report_generator.py
```py

```

### backend\services\repository_collaborator_service.py
```py
from db.models.repository_collaborators import repository_collaborators
from sqlalchemy import select, insert, update, func
from db.database import database
import logging

logger = logging.getLogger(__name__)

async def get_repository_collaborator(repository_id: int, user_id: int):
    """L·∫•y th√¥ng tin collaborator c·ªßa repository"""
    query = select(repository_collaborators).where(
        repository_collaborators.c.repository_id == repository_id,
        repository_collaborators.c.user_id == user_id
    )
    result = await database.fetch_one(query)
    return result

async def save_repository_collaborator(collaborator_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin repository collaborator
    
    Args:
        collaborator_data (dict): Th√¥ng tin collaborator
            - repository_id: ID c·ªßa repository
            - user_id: ID c·ªßa user
            - role: vai tr√≤ (admin, write, read, etc.)
            - permissions: quy·ªÅn h·∫°n
            - is_owner: c√≥ ph·∫£i owner kh√¥ng
    
    Returns:
        int: collaborator_id
    """
    try:
        # Ki·ªÉm tra xem collaborator ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_collaborator = await get_repository_collaborator(
            collaborator_data["repository_id"], 
            collaborator_data["user_id"]
        )

        if existing_collaborator:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(repository_collaborators)
                .where(
                    repository_collaborators.c.repository_id == collaborator_data["repository_id"],
                    repository_collaborators.c.user_id == collaborator_data["user_id"]
                )
                .values(
                    role=collaborator_data.get("role", "read"),
                    permissions=collaborator_data.get("permissions"),
                    is_owner=collaborator_data.get("is_owner", False),
                    invitation_status=collaborator_data.get("invitation_status", "accepted"),
                    last_synced=func.now()
                )
            )
            await database.execute(query)
            logger.info(f"Updated repository collaborator: repo_id={collaborator_data['repository_id']}, user_id={collaborator_data['user_id']}")
            return existing_collaborator.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(repository_collaborators).values(
                repository_id=collaborator_data["repository_id"],
                user_id=collaborator_data["user_id"],
                role=collaborator_data.get("role", "read"),
                permissions=collaborator_data.get("permissions"),
                is_owner=collaborator_data.get("is_owner", False),
                joined_at=collaborator_data.get("joined_at"),
                invited_by=collaborator_data.get("invited_by"),
                invitation_status=collaborator_data.get("invitation_status", "accepted"),
                commits_count=collaborator_data.get("commits_count", 0),
                issues_count=collaborator_data.get("issues_count", 0),
                prs_count=collaborator_data.get("prs_count", 0),
                last_activity=collaborator_data.get("last_activity"),
                last_synced=func.now()
            )
            
            result = await database.execute(query)
            logger.info(f"Created new repository collaborator: repo_id={collaborator_data['repository_id']}, user_id={collaborator_data['user_id']}")
            return result
            
    except Exception as e:
        logger.error(f"Error saving repository collaborator: {e}")
        raise e

async def get_collaborators_by_repository_id(repository_id: int):
    """L·∫•y danh s√°ch collaborators c·ªßa repository"""
    query = select(repository_collaborators).where(
        repository_collaborators.c.repository_id == repository_id
    )
    results = await database.fetch_all(query)
    return results

```

### backend\services\repo_service.py
```py
# backend/services/repo_service.py
from .github_service import fetch_from_github
from db.models.repositories import repositories
from sqlalchemy import select, update
from sqlalchemy.sql import func
from db.database import database

async def get_repository(owner: str, repo_name: str, id_only: bool = False):
    """Get repository from DB - flexible return"""
    query = select(repositories).where(
        repositories.c.owner == owner,
        repositories.c.name == repo_name
    )
    result = await database.fetch_one(query)
    if result:
        return result.id if id_only else dict(result)
    return None

async def get_repo_id_by_owner_and_name(owner: str, repo_name: str):
    """Backward compatibility wrapper"""
    return await get_repository(owner, repo_name, id_only=True)

async def get_repo_by_owner_and_name(owner: str, repo_name: str):
    """Backward compatibility wrapper"""
    return await get_repository(owner, repo_name, id_only=False)

async def save_repository(repo_entry):
    """Save or update repository"""    # Check if exists
    query = select(repositories).where(repositories.c.github_id == repo_entry["github_id"])
    existing_repo = await database.fetch_one(query)
    
    if existing_repo:
        # Update existing
        update_query = (
            update(repositories)
            .where(repositories.c.github_id == repo_entry["github_id"])
            .values(
                name=repo_entry["name"],
                owner=repo_entry["owner"],
                description=repo_entry["description"],
                stars=repo_entry["stars"],
                forks=repo_entry["forks"],
                language=repo_entry["language"],
                open_issues=repo_entry["open_issues"],
                url=repo_entry["url"],
                # B·ªï sung c√°c fields m·ªõi
                full_name=repo_entry.get("full_name"),
                clone_url=repo_entry.get("clone_url"),
                is_private=repo_entry.get("is_private", False),
                is_fork=repo_entry.get("is_fork", False),
                default_branch=repo_entry.get("default_branch", "main"),
                sync_status=repo_entry.get("sync_status", "completed"),
                updated_at=func.now(),
            )
        )
        await database.execute(update_query)
    else:
        # Insert new
        query = repositories.insert().values(repo_entry)
        await database.execute(query)

async def fetch_repo_from_github(owner: str, repo: str):
    """Fetch repository data from GitHub API"""
    url = f"/repos/{owner}/{repo}"
    data = await fetch_from_github(url)
    
    return {
        "github_id": data.get("id"),
        "name": data.get("name"),
        "owner": data.get("owner", {}).get("login"),
        "description": data.get("description"),
        "stars": data.get("stargazers_count"),
        "forks": data.get("forks_count"),
        "language": data.get("language"),
        "open_issues": data.get("open_issues_count"),
        "url": data.get("html_url"),
        # B·ªï sung c√°c fields m·ªõi
        "full_name": data.get("full_name"),
        "clone_url": data.get("clone_url"),
        "is_private": data.get("private", False),
        "is_fork": data.get("fork", False),
        "default_branch": data.get("default_branch", "main"),
        "sync_status": "completed",
    }

async def fetch_repo_from_database(owner: str, repo_name: str):
    """Fetch repository data from database (similar to GitHub API format)"""
    repo_data = await get_repository(owner, repo_name, id_only=False)
    
    if not repo_data:
        return None
    
    # Format similar to GitHub API response
    return {
        "id": repo_data.get("github_id"),
        "name": repo_data.get("name"),
        "full_name": repo_data.get("full_name") or f"{repo_data.get('owner')}/{repo_data.get('name')}",
        "owner": {
            "login": repo_data.get("owner")
        },
        "description": repo_data.get("description"),
        "stargazers_count": repo_data.get("stars", 0),
        "forks_count": repo_data.get("forks", 0),
        "language": repo_data.get("language"),
        "open_issues_count": repo_data.get("open_issues", 0),
        "html_url": repo_data.get("url"),
        "clone_url": repo_data.get("clone_url"),
        "private": repo_data.get("is_private", False),
        "fork": repo_data.get("is_fork", False),
        "default_branch": repo_data.get("default_branch", "main"),
        "sync_status": repo_data.get("sync_status"),
        "last_synced": repo_data.get("last_synced"),
        "created_at": repo_data.get("created_at"),
        "updated_at": repo_data.get("updated_at"),
    }

async def get_user_repos_from_database(user_id: int = None, limit: int = 100, offset: int = 0):
    """Get user repositories from database (similar to GitHub API but from DB)"""
    query = select(repositories)
    
    # Filter by user if provided
    if user_id:
        query = query.where(repositories.c.user_id == user_id)
    
    # Add pagination
    query = query.limit(limit).offset(offset).order_by(repositories.c.updated_at.desc())
    
    results = await database.fetch_all(query)
    
    # Format results similar to GitHub API response
    repos = []
    for repo in results:
        repos.append({
            "id": repo.github_id,
            "name": repo.name,
            "full_name": repo.full_name or f"{repo.owner}/{repo.name}",
            "owner": {
                "login": repo.owner
            },
            "description": repo.description,
            "stargazers_count": repo.stars or 0,
            "forks_count": repo.forks or 0,
            "language": repo.language,
            "open_issues_count": repo.open_issues or 0,
            "html_url": repo.url,
            "clone_url": repo.clone_url,
            "private": repo.is_private or False,
            "fork": repo.is_fork or False,
            "default_branch": repo.default_branch or "main",
            "sync_status": repo.sync_status,
            "last_synced": repo.last_synced,
            "created_at": repo.created_at,
            "updated_at": repo.updated_at
        })
    
    return repos

async def get_repositories_by_owner(owner: str, limit: int = 100, offset: int = 0):
    """Get all repositories by owner from database"""
    query = select(repositories).where(
        repositories.c.owner == owner
    ).limit(limit).offset(offset).order_by(repositories.c.updated_at.desc())
    
    results = await database.fetch_all(query)
    
    # Format results similar to GitHub API response
    repos = []
    for repo in results:
        repos.append({
            "id": repo.github_id,
            "name": repo.name,
            "full_name": repo.full_name or f"{repo.owner}/{repo.name}",
            "owner": {
                "login": repo.owner
            },
            "description": repo.description,
            "stargazers_count": repo.stars or 0,
            "forks_count": repo.forks or 0,
            "language": repo.language,
            "open_issues_count": repo.open_issues or 0,
            "html_url": repo.url,
            "clone_url": repo.clone_url,
            "private": repo.is_private or False,
            "fork": repo.is_fork or False,
            "default_branch": repo.default_branch or "main",
            "sync_status": repo.sync_status,
            "last_synced": repo.last_synced,
            "created_at": repo.created_at,
            "updated_at": repo.updated_at
        })
    
    return repos

async def get_repository_stats():
    """Get repository statistics from database"""
    query = select([
        func.count().label('total_repos'),
        func.count(func.distinct(repositories.c.owner)).label('unique_owners'),
        func.count(func.distinct(repositories.c.language)).label('unique_languages'),
        func.sum(repositories.c.stars).label('total_stars'),
        func.sum(repositories.c.forks).label('total_forks'),
        func.avg(repositories.c.stars).label('avg_stars'),
        func.max(repositories.c.updated_at).label('last_updated')
    ])
    
    result = await database.fetch_one(query)
    
    return {
        'total_repositories': result['total_repos'] or 0,
        'unique_owners': result['unique_owners'] or 0,
        'unique_languages': result['unique_languages'] or 0,
        'total_stars': result['total_stars'] or 0,
        'total_forks': result['total_forks'] or 0,
        'average_stars': float(result['avg_stars'] or 0),
        'last_updated': result['last_updated']
    }
```

### backend\services\user_service.py
```py
from db.models.users import users
from sqlalchemy import select, insert, update, func
from db.database import database
import logging
from datetime import datetime
from typing import Optional

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str: Optional[str]) -> Optional[datetime]:
    """
    Convert GitHub API datetime string to Python datetime object
    
    Args:
        date_str: GitHub datetime string in ISO format (e.g., '2021-03-06T14:28:54Z')
    
    Returns:
        datetime object or None if parsing fails
    """
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_user_id_by_github_username(username: str):
    """L·∫•y user_id t·ª´ github_username"""
    query = select(users).where(users.c.github_username == username)
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None

async def get_user_by_github_id(github_id: int):
    """L·∫•y user t·ª´ github_id"""
    query = select(users).where(users.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

async def save_user(user_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin user
    
    Args:
        user_data (dict): Th√¥ng tin user t·ª´ GitHub API
    
    Returns:
        int: user_id
    """
    try:
        # Convert datetime strings to datetime objects
        github_created_at = parse_github_datetime(user_data.get("github_created_at"))
        
        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_user = await get_user_by_github_id(user_data["github_id"])

        if existing_user:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(users)
                .where(users.c.github_id == user_data["github_id"])
                .values(
                    github_username=user_data.get("github_username"),
                    email=user_data.get("email"),
                    display_name=user_data.get("display_name"),
                    full_name=user_data.get("full_name"),
                    avatar_url=user_data.get("avatar_url"),
                    bio=user_data.get("bio"),
                    location=user_data.get("location"),
                    company=user_data.get("company"),
                    blog=user_data.get("blog"),
                    twitter_username=user_data.get("twitter_username"),
                    github_profile_url=user_data.get("github_profile_url"),
                    repos_url=user_data.get("repos_url"),
                    github_created_at=github_created_at,
                    last_synced=func.now(),
                    updated_at=func.now()
                )
            )
            await database.execute(query)
            logger.info(f"Updated user: {user_data.get('github_username')}")
            return existing_user.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(users).values(
                github_id=user_data["github_id"],
                github_username=user_data.get("github_username"),
                email=user_data.get("email"),
                display_name=user_data.get("display_name"),
                full_name=user_data.get("full_name"),
                avatar_url=user_data.get("avatar_url"),
                bio=user_data.get("bio"),
                location=user_data.get("location"),
                company=user_data.get("company"),
                blog=user_data.get("blog"),
                twitter_username=user_data.get("twitter_username"),
                github_profile_url=user_data.get("github_profile_url"),
                repos_url=user_data.get("repos_url"),
                is_active=True,
                is_verified=False,
                github_created_at=github_created_at,
                last_synced=func.now(),
                created_at=func.now(),
                updated_at=func.now()
            )
            
            result = await database.execute(query)
            user_id = result
            logger.info(f"Created new user: {user_data.get('github_username')}")
            return user_id
            
    except Exception as e:
        logger.error(f"Error saving user {user_data.get('github_username')}: {e}")
        raise e

```

### backend\services\__init__.py
```py

```

### backend\utils\formatter.py
```py

```

### backend\utils\scheduler.py
```py

```

### backend\utils\__init__.py
```py

```

### frontend\eslint.config.js
```js
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]

```

### frontend\vite.config.js
```js
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import { fileURLToPath, URL } from 'node:url'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url)),
      '@components': fileURLToPath(new URL('./src/components', import.meta.url)),
      '@dashboard': fileURLToPath(new URL('./src/components/Dashboard', import.meta.url)),
      '@taskmanager': fileURLToPath(new URL('./src/components/Dashboard/ProjectTaskManager', import.meta.url)),
    }
  }
})

```

### frontend\src\App.jsx
```jsx
import { BrowserRouter as Router, Routes, Route, Navigate } from "react-router-dom";
import Login from "./pages/Login";
import AuthSuccess from "./pages/AuthSuccess";
import Dashboard from "./pages/Dashboard"; 
import RepoDetails from "./pages/RepoDetails";
import CommitTable from './components/commits/CommitTable';
import TestPage from './pages/TestPage';
import ErrorBoundary from './components/ErrorBoundary';

function App() {
  return (
    <ErrorBoundary>
      <Router>
        <Routes>
          {/* ‚úÖ Test route */}
          <Route path="/test" element={<TestPage />} />
          
          {/* ‚úÖ Trang m·∫∑c ƒë·ªãnh l√† Login */}
          <Route path="/" element={<Navigate to="/login" />} />

          {/* C√°c route ch√≠nh */}
          <Route path="/login" element={<Login />} />
          <Route path="/auth-success" element={<AuthSuccess />} />
          <Route path="/dashboard" element={<Dashboard />} />
          <Route path="/repo/:owner/:repo" element={<RepoDetails />} />
          <Route path="/commits" element={<CommitTable />} />

        </Routes>
      </Router>
    </ErrorBoundary>
  );
}

export default App;

```

### frontend\src\config.js
```js

```

### frontend\src\main.jsx
```jsx
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import 'antd/dist/reset.css'
import './index.css';
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)

```

### frontend\src\api\github.js
```js

```

### frontend\src\components\AliasTest.jsx
```jsx
// Test component ƒë·ªÉ ki·ªÉm tra alias
import React from 'react';
// Test named imports from index
import { 
  RepoSelector, 
  StatisticsPanel, 
  FiltersPanel 
} from '@taskmanager/index';

const AliasTest = () => {
  console.log('‚úÖ Alias @taskmanager ho·∫°t ƒë·ªông!');
  console.log('Components imported:', { RepoSelector, StatisticsPanel, FiltersPanel });
  
  return (
    <div style={{ padding: '20px', border: '2px solid green', borderRadius: '8px' }}>
      <h3>‚úÖ Alias Test th√†nh c√¥ng!</h3>
      <p>ƒê√£ import th√†nh c√¥ng t·ª´ @taskmanager:</p>
      <ul>
        <li>RepoSelector: {RepoSelector ? '‚úÖ' : '‚ùå'}</li>
        <li>StatisticsPanel: {StatisticsPanel ? '‚úÖ' : '‚ùå'}</li>
        <li>FiltersPanel: {FiltersPanel ? '‚úÖ' : '‚ùå'}</li>
      </ul>
    </div>
  );
};

export default AliasTest;

```

### frontend\src\components\ErrorBoundary.jsx
```jsx
// frontend/src/components/ErrorBoundary.jsx
import React from 'react';
import { Alert, Button } from 'antd';

class ErrorBoundary extends React.Component {
  constructor(props) {
    super(props);
    this.state = { hasError: false, error: null };
  }

  static getDerivedStateFromError(error) {
    return { hasError: true, error };
  }

  componentDidCatch(error, errorInfo) {
    console.error('ErrorBoundary caught an error:', error, errorInfo);
  }

  render() {
    if (this.state.hasError) {
      return (
        <div style={{ 
          padding: '40px', 
          display: 'flex', 
          justifyContent: 'center', 
          alignItems: 'center',
          minHeight: '100vh',
          backgroundColor: '#f5f5f5'
        }}>
          <div style={{ maxWidth: '600px', width: '100%' }}>
            <Alert
              message="ƒê√£ x·∫£y ra l·ªói!"
              description={
                <div>
                  <p>·ª®ng d·ª•ng ƒë√£ g·∫∑p l·ªói kh√¥ng mong mu·ªën. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c li√™n h·ªá h·ªó tr·ª£.</p>
                  <details style={{ marginTop: '16px' }}>
                    <summary>Chi ti·∫øt l·ªói (cho developer)</summary>
                    <pre style={{ 
                      marginTop: '8px', 
                      padding: '8px', 
                      backgroundColor: '#f8f8f8',
                      borderRadius: '4px',
                      fontSize: '12px',
                      overflow: 'auto'
                    }}>
                      {this.state.error?.toString()}
                    </pre>
                  </details>
                </div>
              }
              type="error"
              showIcon
              action={
                <Button 
                  size="small" 
                  danger 
                  onClick={() => window.location.reload()}
                >
                  T·∫£i l·∫°i trang
                </Button>
              }
            />
          </div>
        </div>
      );
    }

    return this.props.children;
  }
}

export default ErrorBoundary;

```

### frontend\src\components\SimpleAliasTest.jsx
```jsx
// Simple test for alias
import React from 'react';

// Test individual imports
import RepoSelector from '@taskmanager/RepoSelector';
import StatisticsPanel from '@taskmanager/StatisticsPanel';

const SimpleAliasTest = () => {
  console.log('Testing individual imports:', { RepoSelector, StatisticsPanel });
  
  return (
    <div style={{ padding: '10px', background: '#f0f0f0', margin: '10px' }}>
      <h4>Simple Alias Test</h4>
      <p>RepoSelector: {RepoSelector ? '‚úÖ Loaded' : '‚ùå Failed'}</p>
      <p>StatisticsPanel: {StatisticsPanel ? '‚úÖ Loaded' : '‚ùå Failed'}</p>
    </div>
  );
};

export default SimpleAliasTest;

```

### frontend\src\components\Branchs\BranchCommitList.jsx
```jsx
import React, { useState, useEffect } from 'react';
import { Card, List, Tag, Typography, Button, Space, message, Spin, Empty, Modal } from 'antd';
import { 
  GitlabOutlined, 
  UserOutlined, 
  CalendarOutlined, 
  FileTextOutlined,
  EyeOutlined,
  BranchesOutlined,
  PlusOutlined,
  MinusOutlined,
  FileOutlined
} from '@ant-design/icons';
import axios from 'axios';
import styled from 'styled-components';

const { Text, Paragraph } = Typography;

const CommitCard = styled(Card)`
  margin-bottom: 12px;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;

  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
  }

  .ant-card-body {
    padding: 16px;
  }
`;

const CommitHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  margin-bottom: 8px;
`;

const CommitMessage = styled(Paragraph)`
  font-weight: 500;
  color: #262626;
  margin-bottom: 8px;
  
  &.ant-typography {
    margin-bottom: 8px;
  }
`;

const CommitMeta = styled.div`
  display: flex;
  align-items: center;
  gap: 16px;
  color: #8c8c8c;
  font-size: 12px;
  flex-wrap: wrap;
`;

const StatsContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 8px;
  margin-top: 8px;
`;

const StatTag = styled(Tag)`
  margin: 0;
  display: flex;
  align-items: center;
  gap: 4px;
`;

const BranchCommitList = ({ owner, repo, selectedBranch }) => {
  const [commits, setCommits] = useState([]);
  const [loading, setLoading] = useState(false);
  const [pagination, setPagination] = useState({ current: 1, pageSize: 10, total: 0 });
  const [selectedCommit, setSelectedCommit] = useState(null);
  const [modalVisible, setModalVisible] = useState(false);
  useEffect(() => {
    if (selectedBranch) {
      fetchCommits();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [selectedBranch, pagination.current]);

  const fetchCommits = async () => {
    if (!selectedBranch) return;

    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    setLoading(true);
    try {
      const offset = (pagination.current - 1) * pagination.pageSize;
      const response = await axios.get(
        `http://localhost:8000/api/commits/${owner}/${repo}/branches/${selectedBranch}/commits?limit=${pagination.pageSize}&offset=${offset}`,
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );

      setCommits(response.data.commits || []);
      setPagination(prev => ({
        ...prev,
        total: response.data.total_found || response.data.count || 0
      }));

    } catch (error) {
      console.error("Error fetching commits:", error);
      if (error.response?.status === 404) {
        message.warning(`Ch∆∞a c√≥ commits n√†o trong database cho branch "${selectedBranch}". H√£y ƒë·ªìng b·ªô d·ªØ li·ªáu tr∆∞·ªõc!`);
        setCommits([]);
      } else {
        message.error("Kh√¥ng th·ªÉ l·∫•y danh s√°ch commits!");
      }
    } finally {
      setLoading(false);
    }
  };

  const formatDate = (dateString) => {
    if (!dateString) return 'N/A';
    try {
      return new Date(dateString).toLocaleString('vi-VN');
    } catch {
      return dateString;
    }
  };

  const getCommitType = (message) => {
    const lowerMessage = message.toLowerCase();
    if (lowerMessage.startsWith('feat:') || lowerMessage.includes('feature')) {
      return { type: 'feat', color: 'blue' };
    } else if (lowerMessage.startsWith('fix:') || lowerMessage.includes('bug')) {
      return { type: 'fix', color: 'red' };
    } else if (lowerMessage.startsWith('docs:')) {
      return { type: 'docs', color: 'green' };
    } else if (lowerMessage.startsWith('style:')) {
      return { type: 'style', color: 'purple' };
    } else if (lowerMessage.startsWith('refactor:')) {
      return { type: 'refactor', color: 'orange' };
    } else if (lowerMessage.startsWith('test:')) {
      return { type: 'test', color: 'cyan' };
    } else {
      return { type: 'other', color: 'default' };
    }
  };

  const showCommitDetails = (commit) => {
    setSelectedCommit(commit);
    setModalVisible(true);
  };

  const handlePageChange = (page) => {
    setPagination(prev => ({ ...prev, current: page }));
  };

  if (!selectedBranch) {
    return (
      <Card>
        <Empty 
          description="Vui l√≤ng ch·ªçn branch ƒë·ªÉ xem commits"
          image={<BranchesOutlined style={{ fontSize: 48, color: '#d9d9d9' }} />}
        />
      </Card>
    );
  }

  return (
    <>
      <Card 
        title={
          <Space>
            <GitlabOutlined style={{ color: '#1890ff' }} />
            <span>Commits - Branch: {selectedBranch}</span>
            <Tag color="blue">{commits.length} commits</Tag>
          </Space>
        }
        extra={
          <Button 
            type="primary" 
            size="small" 
            onClick={fetchCommits}
            loading={loading}
          >
            L√†m m·ªõi
          </Button>
        }
      >
        <Spin spinning={loading}>
          {commits.length === 0 && !loading ? (
            <Empty 
              description={`Ch∆∞a c√≥ commits n√†o cho branch "${selectedBranch}"`}
              image={<GitlabOutlined style={{ fontSize: 48, color: '#d9d9d9' }} />}
            />
          ) : (
            <List
              itemLayout="vertical"
              dataSource={commits}
              pagination={{
                current: pagination.current,
                pageSize: pagination.pageSize,
                total: pagination.total,
                onChange: handlePageChange,
                showSizeChanger: false,
                showQuickJumper: true,
                showTotal: (total, range) => 
                  `${range[0]}-${range[1]} c·ªßa ${total} commits`,
              }}
              renderItem={(commit) => {
                const commitType = getCommitType(commit.message);
                
                return (
                  <List.Item key={commit.sha}>
                    <CommitCard size="small">
                      <CommitHeader>
                        <div style={{ flex: 1 }}>
                          <Space size="small">
                            <Tag color={commitType.color}>{commitType.type}</Tag>
                            <Text code style={{ fontSize: '11px' }}>
                              {commit.sha?.substring(0, 7)}
                            </Text>
                          </Space>
                        </div>
                        <Button
                          type="text"
                          size="small"
                          icon={<EyeOutlined />}
                          onClick={() => showCommitDetails(commit)}
                        >
                          Chi ti·∫øt
                        </Button>
                      </CommitHeader>

                      <CommitMessage ellipsis={{ rows: 2, expandable: true }}>
                        {commit.message}
                      </CommitMessage>

                      <CommitMeta>
                        <Space size={4}>
                          <UserOutlined />
                          <Text>{commit.author_name}</Text>
                        </Space>
                        <Space size={4}>
                          <CalendarOutlined />
                          <Text>{formatDate(commit.date)}</Text>
                        </Space>
                        {commit.branch_name && (
                          <Space size={4}>
                            <BranchesOutlined />
                            <Text>{commit.branch_name}</Text>
                          </Space>
                        )}
                      </CommitMeta>

                      {(commit.insertions || commit.deletions || commit.files_changed) && (
                        <StatsContainer>
                          {commit.insertions && (
                            <StatTag color="green">
                              <PlusOutlined />
                              {commit.insertions}
                            </StatTag>
                          )}
                          {commit.deletions && (
                            <StatTag color="red">
                              <MinusOutlined />
                              {commit.deletions}
                            </StatTag>
                          )}
                          {commit.files_changed && (
                            <StatTag color="blue">
                              <FileOutlined />
                              {commit.files_changed} files
                            </StatTag>
                          )}
                        </StatsContainer>
                      )}
                    </CommitCard>
                  </List.Item>
                );
              }}
            />
          )}
        </Spin>
      </Card>

      <Modal
        title={
          <Space>
            <GitlabOutlined />
            <span>Chi ti·∫øt Commit</span>
            {selectedCommit && (
              <Text code>{selectedCommit.sha?.substring(0, 7)}</Text>
            )}
          </Space>
        }
        open={modalVisible}
        onCancel={() => setModalVisible(false)}
        footer={null}
        width={600}
      >
        {selectedCommit && (
          <div>
            <div style={{ marginBottom: 16 }}>
              <Text strong>SHA:</Text>
              <br />
              <Text code>{selectedCommit.sha}</Text>
            </div>

            <div style={{ marginBottom: 16 }}>
              <Text strong>Message:</Text>
              <br />
              <Paragraph>{selectedCommit.message}</Paragraph>
            </div>

            <div style={{ marginBottom: 16 }}>
              <Text strong>Author:</Text>
              <br />
              <Text>{selectedCommit.author_name} ({selectedCommit.author_email})</Text>
            </div>

            <div style={{ marginBottom: 16 }}>
              <Text strong>Date:</Text>
              <br />
              <Text>{formatDate(selectedCommit.date)}</Text>
            </div>

            {selectedCommit.committer_name && (
              <div style={{ marginBottom: 16 }}>
                <Text strong>Committer:</Text>
                <br />
                <Text>{selectedCommit.committer_name} ({selectedCommit.committer_email})</Text>
              </div>
            )}

            {(selectedCommit.insertions || selectedCommit.deletions || selectedCommit.files_changed) && (
              <div style={{ marginBottom: 16 }}>
                <Text strong>Statistics:</Text>
                <br />
                <Space>
                  {selectedCommit.insertions && (
                    <Tag color="green">+{selectedCommit.insertions} insertions</Tag>
                  )}
                  {selectedCommit.deletions && (
                    <Tag color="red">-{selectedCommit.deletions} deletions</Tag>
                  )}
                  {selectedCommit.files_changed && (
                    <Tag color="blue">{selectedCommit.files_changed} files changed</Tag>
                  )}
                </Space>
              </div>
            )}

            {selectedCommit.is_merge && (
              <div style={{ marginBottom: 16 }}>
                <Tag color="purple">Merge Commit</Tag>
                {selectedCommit.merge_from_branch && (
                  <Text> t·ª´ branch: {selectedCommit.merge_from_branch}</Text>
                )}
              </div>
            )}
          </div>
        )}
      </Modal>
    </>
  );
};

export default BranchCommitList;

```

### frontend\src\components\Branchs\BranchSelector.jsx
```jsx
import { useEffect, useState, useCallback } from "react";
import { Select, Spin, message, Tag, Typography, Button, Space, Tooltip } from "antd";
import { GithubOutlined, BranchesOutlined, SyncOutlined, DatabaseOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Option } = Select;
const { Text } = Typography;

const SelectContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 8px 12px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  flex-wrap: wrap;

  @media (max-width: 768px) {
    flex-direction: column;
    align-items: stretch;
    gap: 8px;
  }
`;

const BranchControls = styled.div`
  display: flex;
  align-items: center;
  gap: 8px;
  flex: 1;

  @media (max-width: 768px) {
    flex-direction: column;
    align-items: stretch;
  }
`;

const SyncControls = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
`;

const SyncButton = styled(Button)`
  display: flex;
  align-items: center;
  gap: 4px;
  height: 32px;
  padding: 0 12px;
  
  &.ant-btn-primary {
    background: linear-gradient(135deg, #1890ff 0%, #096dd9 100%);
    border: none;
    
    &:hover {
      background: linear-gradient(135deg, #40a9ff 0%, #1890ff 100%);
      transform: translateY(-1px);
    }
  }
  
  &.ant-btn-default {
    border-color: #52c41a;
    color: #52c41a;
    
    &:hover {
      border-color: #73d13d;
      color: #73d13d;
      background: #f6ffed;
    }
  }
`;

const StyledSelect = styled(Select)`
  min-width: 240px;
  
  .ant-select-selector {
    border-radius: 6px !important;
    border: 1px solid #d9d9d9 !important;
    transition: all 0.3s !important;
    
    &:hover {
      border-color: #1890ff !important;
    }
  }
  
  .ant-select-selection-item {
    font-weight: 500;
  }
`;

const BranchTag = styled(Tag)`
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  border-radius: 6px;
  background: #f0f5ff;
  color: #1890ff;
  border: 1px solid #d6e4ff;
`;

const BranchSelector = ({ owner, repo, onBranchChange }) => {
  const [branches, setBranches] = useState([]);
  const [loading, setLoading] = useState(true);
  const [selectedBranch, setSelectedBranch] = useState(null);
  const [syncLoading, setSyncLoading] = useState(false);
  const [commitStats, setCommitStats] = useState(null);
  const [isInitialized, setIsInitialized] = useState(false); // Prevent multiple initializations

  // Define fetchCommitStats first using useCallback
  const fetchCommitStats = useCallback(async (branchName) => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    try {
      const response = await axios.get(
        `http://localhost:8000/api/commits/${owner}/${repo}/branches/${branchName}/commits?limit=1`,
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      setCommitStats({
        totalCommits: response.data.total_found || 0,
        lastSync: new Date().toLocaleString()
      });
    } catch (err) {
      console.error("Error fetching commit stats:", err);
    }
  }, [owner, repo]);

  useEffect(() => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchBranches = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );        setBranches(response.data);
        if (response.data.length > 0 && !isInitialized) {
          const defaultBranch = response.data[0].name;
          console.log('BranchSelector: Initializing with default branch:', defaultBranch);
          setSelectedBranch(defaultBranch);
          onBranchChange(defaultBranch);
          fetchCommitStats(defaultBranch);
          setIsInitialized(true);
        }
      } catch (err) {
        console.error(err);
        message.error("Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch branch");
      } finally {
        setLoading(false);
      }
    };    fetchBranches();
  }, [owner, repo, onBranchChange, fetchCommitStats, isInitialized]);  const handleBranchChange = (value) => {
    console.log('BranchSelector: Changing branch from', selectedBranch, 'to', value);
    console.log('Available branches:', branches);
    
    if (value === selectedBranch) {
      console.log('BranchSelector: Same branch selected, skipping');
      return;
    }
    
    setSelectedBranch(value);
    onBranchChange(value);
    fetchCommitStats(value);
  };

  const syncCommitsForBranch = async () => {
    if (!selectedBranch) {
      message.warning("Vui l√≤ng ch·ªçn branch tr∆∞·ªõc!");
      return;
    }

    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    setSyncLoading(true);
    try {
      const response = await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/branches/${selectedBranch}/sync-commits?include_stats=true&per_page=100&max_pages=5`,
        {},
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      
      const { stats } = response.data;
      message.success(
        `ƒê·ªìng b·ªô th√†nh c√¥ng! ${stats.new_commits_saved} commits m·ªõi ƒë∆∞·ª£c l∆∞u cho branch "${selectedBranch}"`
      );
      
      // Update commit stats
      setCommitStats({
        totalCommits: stats.total_commits_in_database,
        newCommits: stats.new_commits_saved,
        lastSync: new Date().toLocaleString()
      });
      
      // Auto refresh commit stats after sync
      setTimeout(() => {
        fetchCommitStats(selectedBranch);
      }, 1000);
      
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô commits:", error);
      const errorMessage = error.response?.data?.detail || "Kh√¥ng th·ªÉ ƒë·ªìng b·ªô commits!";
      message.error(errorMessage);
    } finally {
      setSyncLoading(false);
    }
  };

  const viewCommitsInDB = async () => {
    if (!selectedBranch) {
      message.warning("Vui l√≤ng ch·ªçn branch tr∆∞·ªõc!");
      return;
    }

    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    try {
      const response = await axios.get(
        `http://localhost:8000/api/commits/${owner}/${repo}/branches/${selectedBranch}/commits?limit=10`,
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      
      const { commits, count } = response.data;
      if (count > 0) {
        message.success(`T√¨m th·∫•y ${count} commits trong database cho branch "${selectedBranch}"`);
        console.log("Commits in database:", commits);
        
        // Trigger parent refresh if callback available
        if (onBranchChange) {
          onBranchChange(selectedBranch, { refresh: true });
        }
      } else {
        message.info(`Ch∆∞a c√≥ commits n√†o trong database cho branch "${selectedBranch}". H√£y ƒë·ªìng b·ªô tr∆∞·ªõc!`);
      }
      
    } catch (error) {
      console.error("L·ªói khi xem commits:", error);
      message.error("Kh√¥ng th·ªÉ l·∫•y danh s√°ch commits!");
    }
  };

  if (loading) {
    console.log('BranchSelector: Loading...');
    return <Spin size="small" />;
  }

  if (!branches || branches.length === 0) {
    console.log('BranchSelector: No branches available');
  } else {
    console.log('BranchSelector: Available branches:', branches.map(b => b.name));
  }

  console.log('BranchSelector: Current selected branch:', selectedBranch);

  return (
    <div style={{ marginBottom: 16 }}>
      <SelectContainer>
        <BranchControls>
          <BranchTag>
            <BranchesOutlined />
            <Text strong>Branch:</Text>
          </BranchTag>
            <StyledSelect
            value={selectedBranch}
            onChange={handleBranchChange}
            suffixIcon={<GithubOutlined style={{ color: '#1890ff' }} />}
            popupMatchSelectWidth={false}
          >
            {branches.map((branch) => (
              <Option key={branch.name} value={branch.name}>
                <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                  <BranchesOutlined style={{ color: '#52c41a' }} />
                  <Text strong>{branch.name}</Text>
                </div>
              </Option>
            ))}
          </StyledSelect>
        </BranchControls>
        
        <SyncControls>
          <Tooltip title="ƒê·ªìng b·ªô commits t·ª´ GitHub cho branch n√†y">
            <SyncButton
              type="primary"
              size="small"
              loading={syncLoading}
              onClick={syncCommitsForBranch}
              disabled={!selectedBranch}
            >
              <SyncOutlined />
              Sync
            </SyncButton>
          </Tooltip>
          
          <Tooltip title="Xem commits ƒë√£ l∆∞u trong database">
            <SyncButton
              type="default"
              size="small"
              onClick={viewCommitsInDB}
              disabled={!selectedBranch}
            >
              <DatabaseOutlined />
              View DB
            </SyncButton>
          </Tooltip>
        </SyncControls>
      </SelectContainer>
      
      {commitStats && (
        <div style={{ 
          marginTop: 8, 
          padding: '6px 12px', 
          background: '#f0f5ff', 
          borderRadius: '6px',
          fontSize: '12px',
          color: '#1890ff'
        }}>
          <Space split={<span style={{ color: '#d9d9d9' }}>|</span>}>
            <span>üìä {commitStats.totalCommits} commits</span>
            {commitStats.newCommits && (
              <span>‚ú® {commitStats.newCommits} m·ªõi</span>
            )}
            <span>üïí {commitStats.lastSync}</span>
          </Space>
        </div>
      )}
    </div>
  );
};

export default BranchSelector;

```

### frontend\src\components\Branchs\BranchSelector_fixed.jsx
```jsx
import { useEffect, useState, useCallback } from "react";
import { Select, Spin, message, Tag, Typography, Button, Space, Tooltip } from "antd";
import { GithubOutlined, BranchesOutlined, SyncOutlined, DatabaseOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Option } = Select;
const { Text } = Typography;

const SelectContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 8px 12px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  flex-wrap: wrap;

  @media (max-width: 768px) {
    flex-direction: column;
    align-items: stretch;
    gap: 8px;
  }
`;

const BranchControls = styled.div`
  display: flex;
  align-items: center;
  gap: 8px;
  flex: 1;

  @media (max-width: 768px) {
    flex-direction: column;
    align-items: stretch;
  }
`;

const SyncControls = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
`;

const SyncButton = styled(Button)`
  display: flex;
  align-items: center;
  gap: 4px;
  height: 32px;
  padding: 0 12px;
  
  &.ant-btn-primary {
    background: linear-gradient(135deg, #1890ff 0%, #096dd9 100%);
    border: none;
    
    &:hover {
      background: linear-gradient(135deg, #40a9ff 0%, #1890ff 100%);
      transform: translateY(-1px);
    }
  }
  
  &.ant-btn-default {
    border-color: #52c41a;
    color: #52c41a;
    
    &:hover {
      border-color: #73d13d;
      color: #73d13d;
      background: #f6ffed;
    }
  }
`;

const StyledSelect = styled(Select)`
  min-width: 240px;
  
  .ant-select-selector {
    border-radius: 6px !important;
    border: 1px solid #d9d9d9 !important;
    transition: all 0.3s !important;
    
    &:hover {
      border-color: #1890ff !important;
    }
  }
  
  .ant-select-selection-item {
    font-weight: 500;
  }
`;

const BranchTag = styled(Tag)`
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  border-radius: 6px;
  background: #f0f5ff;
  color: #1890ff;
  border: 1px solid #d6e4ff;
`;

const BranchSelector = ({ owner, repo, onBranchChange }) => {
  const [branches, setBranches] = useState([]);
  const [loading, setLoading] = useState(true);
  const [selectedBranch, setSelectedBranch] = useState(null);
  const [syncLoading, setSyncLoading] = useState(false);
  const [commitStats, setCommitStats] = useState(null);

  // Define fetchCommitStats first using useCallback
  const fetchCommitStats = useCallback(async (branchName) => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    try {
      const response = await axios.get(
        `http://localhost:8000/api/commits/${owner}/${repo}/branches/${branchName}/commits?limit=1`,
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      setCommitStats({
        totalCommits: response.data.total_found || 0,
        lastSync: new Date().toLocaleString()
      });
    } catch (err) {
      console.error("Error fetching commit stats:", err);
    }
  }, [owner, repo]);

  useEffect(() => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchBranches = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setBranches(response.data);
        if (response.data.length > 0) {
          setSelectedBranch(response.data[0].name);
          onBranchChange(response.data[0].name);
          // Fetch commit stats for default branch
          fetchCommitStats(response.data[0].name);
        }
      } catch (err) {
        console.error(err);
        message.error("Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch branch");
      } finally {
        setLoading(false);
      }
    };

    fetchBranches();
  }, [owner, repo, onBranchChange, fetchCommitStats]);

  const handleBranchChange = (value) => {
    setSelectedBranch(value);
    onBranchChange(value);
    fetchCommitStats(value);
  };

  const syncCommitsForBranch = async () => {
    if (!selectedBranch) {
      message.warning("Vui l√≤ng ch·ªçn branch tr∆∞·ªõc!");
      return;
    }

    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    setSyncLoading(true);
    try {
      const response = await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/branches/${selectedBranch}/sync-commits?include_stats=true&per_page=100&max_pages=5`,
        {},
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      
      const { stats } = response.data;
      message.success(
        `ƒê·ªìng b·ªô th√†nh c√¥ng! ${stats.new_commits_saved} commits m·ªõi ƒë∆∞·ª£c l∆∞u cho branch "${selectedBranch}"`
      );
      
      // Update commit stats
      setCommitStats({
        totalCommits: stats.total_commits_in_database,
        newCommits: stats.new_commits_saved,
        lastSync: new Date().toLocaleString()
      });
      
      // Auto refresh commit stats after sync
      setTimeout(() => {
        fetchCommitStats(selectedBranch);
      }, 1000);
      
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô commits:", error);
      const errorMessage = error.response?.data?.detail || "Kh√¥ng th·ªÉ ƒë·ªìng b·ªô commits!";
      message.error(errorMessage);
    } finally {
      setSyncLoading(false);
    }
  };

  const viewCommitsInDB = async () => {
    if (!selectedBranch) {
      message.warning("Vui l√≤ng ch·ªçn branch tr∆∞·ªõc!");
      return;
    }

    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    try {
      const response = await axios.get(
        `http://localhost:8000/api/commits/${owner}/${repo}/branches/${selectedBranch}/commits?limit=10`,
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      
      const { commits, count } = response.data;
      if (count > 0) {
        message.success(`T√¨m th·∫•y ${count} commits trong database cho branch "${selectedBranch}"`);
        console.log("Commits in database:", commits);
        
        // Trigger parent refresh if callback available
        if (onBranchChange) {
          onBranchChange(selectedBranch, { refresh: true });
        }
      } else {
        message.info(`Ch∆∞a c√≥ commits n√†o trong database cho branch "${selectedBranch}". H√£y ƒë·ªìng b·ªô tr∆∞·ªõc!`);
      }
      
    } catch (error) {
      console.error("L·ªói khi xem commits:", error);
      message.error("Kh√¥ng th·ªÉ l·∫•y danh s√°ch commits!");
    }
  };

  if (loading) return <Spin size="small" />;

  return (
    <div style={{ marginBottom: 16 }}>
      <SelectContainer>
        <BranchControls>
          <BranchTag>
            <BranchesOutlined />
            <Text strong>Branch:</Text>
          </BranchTag>
          
          <StyledSelect
            value={selectedBranch}
            onChange={handleBranchChange}
            suffixIcon={<GithubOutlined style={{ color: '#1890ff' }} />}
            dropdownMatchSelectWidth={false}
          >
            {branches.map((branch) => (
              <Option key={branch.name} value={branch.name}>
                <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                  <BranchesOutlined style={{ color: '#52c41a' }} />
                  <Text strong>{branch.name}</Text>
                </div>
              </Option>
            ))}
          </StyledSelect>
        </BranchControls>
        
        <SyncControls>
          <Tooltip title="ƒê·ªìng b·ªô commits t·ª´ GitHub cho branch n√†y">
            <SyncButton
              type="primary"
              size="small"
              loading={syncLoading}
              onClick={syncCommitsForBranch}
              disabled={!selectedBranch}
            >
              <SyncOutlined />
              Sync
            </SyncButton>
          </Tooltip>
          
          <Tooltip title="Xem commits ƒë√£ l∆∞u trong database">
            <SyncButton
              type="default"
              size="small"
              onClick={viewCommitsInDB}
              disabled={!selectedBranch}
            >
              <DatabaseOutlined />
              View DB
            </SyncButton>
          </Tooltip>
        </SyncControls>
      </SelectContainer>
      
      {commitStats && (
        <div style={{ 
          marginTop: 8, 
          padding: '6px 12px', 
          background: '#f0f5ff', 
          borderRadius: '6px',
          fontSize: '12px',
          color: '#1890ff'
        }}>
          <Space split={<span style={{ color: '#d9d9d9' }}>|</span>}>
            <span>üìä {commitStats.totalCommits} commits</span>
            {commitStats.newCommits && (
              <span>‚ú® {commitStats.newCommits} m·ªõi</span>
            )}
            <span>üïí {commitStats.lastSync}</span>
          </Space>
        </div>
      )}
    </div>
  );
};

export default BranchSelector;

```

### frontend\src\components\commits\AnalyzeGitHubCommits.jsx
```jsx
import { useState } from 'react';
import { Button, Badge, Popover, List, Typography, Divider, Spin, Tag, Alert, Tooltip } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled, InfoCircleOutlined } from '@ant-design/icons';
import axios from 'axios';

const { Text, Title } = Typography;

const AnalyzeGitHubCommits = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [popoverVisible, setPopoverVisible] = useState(false);

  const analyzeCommits = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      
      if (!token) {
        throw new Error('Authentication required');
      }

      const response = await axios.get(
        `http://localhost:8000/api/commits/analyze-github/${repo.owner.login}/${repo.name}`,
        {
          headers: { 
            Authorization: `Bearer ${token}`,
            Accept: "application/json"
          },
          params: { 
            per_page: 10,
            // Add cache busting to avoid stale data
            timestamp: Date.now()
          },
          timeout: 10000 // 10 second timeout
        }
      );
      
      if (!response.data) {
        throw new Error('Invalid response data');
      }

      setAnalysis(response.data);
    } catch (err) {
      let errorMessage = 'Failed to analyze commits';
      
      if (err.response) {
        if (err.response.status === 401) {
          errorMessage = 'Please login to analyze commits';
        } else if (err.response.status === 403) {
          errorMessage = 'Access to this repository is denied';
        } else if (err.response.data?.detail) {
          errorMessage = err.response.data.detail;
        }
      } else if (err.message) {
        errorMessage = err.message;
      }

      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const handlePopoverOpen = (visible) => {
    setPopoverVisible(visible);
    if (visible && !analysis && !error) {
      analyzeCommits();
    }
  };

  const getStatusColor = () => {
    if (error) return 'warning';
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (error) return 'Error';
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical` 
      : 'No Issues';
  };

  const getStatusIcon = () => {
    if (error) return <InfoCircleOutlined />;
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };
  const renderContent = () => {
    if (loading) {
      return (
        <div style={{ textAlign: 'center', padding: '20px' }}>
          <Spin size="small" />
          <div style={{ marginTop: 8 }}>
            <Text type="secondary">Analyzing commits...</Text>
          </div>
        </div>
      );
    }

    if (error) {
      return (
        <Alert
          message="Analysis Failed"
          description={error}
          type="error"
          showIcon
        />
      );
    }

    if (!analysis) {
      return <Text type="secondary">Click to analyze commits</Text>;
    }

    return (
      <>
        <div style={{ marginBottom: 16 }}>
          <Title level={5} style={{ marginBottom: 4 }}>
            Commit Analysis Summary
          </Title>
          <Text>
            <Tag color={analysis.critical > 0 ? 'error' : 'success'}>
              {analysis.critical > 0 ? 'Needs Review' : 'All Clear'}
            </Tag>
            {analysis.critical} of {analysis.total} commits are critical
          </Text>
        </div>

        <Divider style={{ margin: '12px 0' }} />

        <List
          size="small"
          dataSource={analysis.details.slice(0, 5)}
          renderItem={item => (
            <List.Item>
              <div style={{ width: '100%' }}>
                <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                  <Tag color={item.is_critical ? 'error' : 'success'}>
                    {item.is_critical ? 'CRITICAL' : 'Normal'}
                  </Tag>
                  <Tooltip title="Commit ID">
                    <Text code style={{ fontSize: 12 }}>
                      {item.id.substring(0, 7)}
                    </Text>
                  </Tooltip>
                </div>
                <Text
                  ellipsis={{ tooltip: item.message_preview }}
                  style={{ 
                    color: item.is_critical ? '#f5222d' : 'inherit',
                    marginTop: 4,
                    display: 'block'
                  }}
                >
                  {item.message_preview}
                </Text>
              </div>
            </List.Item>
          )}
        />

        {analysis.total > 5 && (
          <Text type="secondary" style={{ display: 'block', marginTop: 8 }}>
            Showing 5 of {analysis.total} commits
          </Text>
        )}
      </>
    );
  };

  return (
    <Popover 
      content={renderContent()}
      title={
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
          <span>Commit Analysis</span>
          {analysis && (
            <Badge 
              count={`${analysis.critical_percentage}%`} 
              style={{ 
                backgroundColor: analysis.critical > 0 ? '#f5222d' : '#52c41a'
              }} 
            />
          )}
        </div>
      }
      trigger="click"
      open={popoverVisible}
      onOpenChange={handlePopoverOpen}
      overlayStyle={{ width: 350 }}
      placement="bottomRight"
    >
      <Badge 
        count={analysis?.critical || 0} 
        color={getStatusColor()}
        offset={[-10, 10]}
      >
        <Button 
          type={error ? 'default' : analysis ? (analysis.critical ? 'danger' : 'success') : 'default'}
          icon={getStatusIcon()}
          loading={loading}
          onClick={(e) => e.stopPropagation()}
          style={{ 
            marginLeft: 'auto',
            fontWeight: 500,
            borderRadius: 20,
            padding: '0 16px',
            border: error ? '1px solid #faad14' : undefined
          }}
        >
          {getStatusText()}
        </Button>
      </Badge>
    </Popover>
  );
};

export default AnalyzeGitHubCommits;
```

### frontend\src\components\commits\CommitAnalysisBadge.jsx
```jsx
// components/CommitAnalysisBadge.jsx
import { Tag, Tooltip, Popover, List, Typography, Divider, Badge, Spin } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled } from '@ant-design/icons';
import { useState } from 'react';
import axios from 'axios';

const { Text } = Typography;

const CommitAnalysisBadge = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchCommitAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 5 } // Get last 5 commits for analysis
        }
      );
      
      // Analyze the commits
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  const getStatusColor = () => {
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical Commits` 
      : 'No Critical Commits';
  };

  const getStatusIcon = () => {
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const content = (
    <div style={{ maxWidth: 300 }}>
      {loading && <Spin size="small" />}
      {error && <Text type="danger">{error}</Text>}
      {analysis && (
        <>
          <Text strong>Recent Commits Analysis</Text>
          <Divider style={{ margin: '8px 0' }} />
          <List
            size="small"
            dataSource={analysis.details.slice(0, 5)}
            renderItem={item => (
              <List.Item>
                <div style={{ width: '100%' }}>
                  <div style={{ 
                    display: 'flex', 
                    justifyContent: 'space-between',
                    marginBottom: 4
                  }}>
                    <Text 
                      ellipsis 
                      style={{ 
                        maxWidth: 180,
                        color: item.is_critical ? '#f5222d' : 'inherit'
                      }}
                    >
                      {item.message_preview}
                    </Text>
                    <Tag color={item.is_critical ? 'error' : 'success'}>
                      {item.is_critical ? 'Critical' : 'Normal'}
                    </Tag>
                  </div>
                  <Text type="secondary" style={{ fontSize: 12 }}>
                    {item.id.substring(0, 7)}
                  </Text>
                </div>
              </List.Item>
            )}
          />
          <Divider style={{ margin: '8px 0' }} />
          <Text type="secondary">
            {analysis.critical} of {analysis.total} recent commits are critical
          </Text>
        </>
      )}
    </div>
  );

  return (
    <Popover 
      content={content}
      title="Commit Analysis"
      trigger="click"
      onVisibleChange={visible => visible && !analysis && fetchCommitAnalysis()}
    >
      <Badge 
        count={analysis?.critical || 0} 
        style={{ backgroundColor: getStatusColor() }}
      >
        <Tag 
          icon={getStatusIcon()}
          color={getStatusColor()}
          style={{ cursor: 'pointer' }}
        >
          {getStatusText()}
        </Tag>
      </Badge>
    </Popover>
  );
};

export default CommitAnalysisBadge;
```

### frontend\src\components\commits\CommitAnalysisModal.jsx
```jsx
// components/CommitAnalysisModal.jsx
import { Modal, List, Typography, Tag, Divider, Spin, Tabs, Progress, Alert } from 'antd';
import { 
  ExclamationCircleOutlined, 
  CheckCircleOutlined,
  BarChartOutlined,
  FileTextOutlined 
} from '@ant-design/icons';
import axios from 'axios';
import { useState, useEffect } from 'react';

const { Title, Text } = Typography;

const CommitAnalysisModal = ({ repo, visible, onCancel }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchFullAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 100 } // Get more commits for detailed analysis
        }
      );
      
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    if (visible) {
      fetchFullAnalysis();
    }
  }, [visible]);

  const criticalPercentage = analysis 
    ? Math.round((analysis.critical / analysis.total) * 100) 
    : 0;

  return (
    <Modal
      title={<><BarChartOutlined /> Commit Analysis for {repo.name}</>}
      visible={visible}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {loading && <Spin size="large" style={{ display: 'block', margin: '40px auto' }} />}
      
      {error && (
        <Alert 
          message="Error" 
          description={error} 
          type="error" 
          showIcon 
          style={{ marginBottom: 20 }}
        />
      )}
        {analysis && (
        <Tabs 
          defaultActiveKey="1"
          items={[
            {
              key: '1',
              label: (
                <span>
                  <FileTextOutlined /> Commits
                </span>
              ),
              children: (
                <div style={{ marginBottom: 20 }}>
                  <div style={{ display: 'flex', alignItems: 'center', marginBottom: 16 }}>
                    <Progress
                      type="circle"
                      percent={criticalPercentage}
                      width={80}
                      format={percent => (
                        <Text strong style={{ fontSize: 24, color: percent > 0 ? '#f5222d' : '#52c41a' }}>
                          {percent}%
                        </Text>
                      )}
                      status={criticalPercentage > 0 ? 'exception' : 'success'}
                    />
                    <div style={{ marginLeft: 20 }}>
                      <Title level={4} style={{ marginBottom: 0 }}>
                        {analysis.critical} of {analysis.total} commits are critical
                      </Title>
                      <Text type="secondary">
                        {criticalPercentage > 0 
                          ? 'This repository contains potentially critical changes'
                          : 'No critical commits detected'}
                      </Text>
                    </div>
                  </div>
                  
                  <List
                    size="large"
                    dataSource={analysis.details}
                    renderItem={item => (
                      <List.Item>
                        <div style={{ width: '100%' }}>
                          <div style={{ display: 'flex', justifyContent: 'space-between' }}>
                            <Tag color={item.is_critical ? 'error' : 'success'}>
                              {item.is_critical ? 'CRITICAL' : 'Normal'}
                            </Tag>
                            <Text type="secondary" copyable>
                              {item.id.substring(0, 7)}
                            </Text>
                          </div>
                          <Divider style={{ margin: '8px 0' }} />
                          <Text style={{ color: item.is_critical ? '#f5222d' : 'inherit' }}>
                            {item.message_preview}
                          </Text>
                        </div>
                      </List.Item>
                    )}
                  />
                </div>
              )
            },
            {
              key: '2', 
              label: (
                <span>
                  <ExclamationCircleOutlined /> Critical Commits
                </span>
              ),
              children: analysis.critical > 0 ? (
                <List
                  dataSource={analysis.details.filter(c => c.is_critical)}
                  renderItem={item => (
                    <List.Item>
                      <Alert
                        message="Critical Commit"
                        description={
                          <>
                            <Text strong style={{ display: 'block', marginBottom: 4 }}>
                              {item.message_preview}
                            </Text>
                            <Text type="secondary">Commit ID: {item.id.substring(0, 7)}</Text>
                          </>
                        }
                        type="error"
                        showIcon
                      />
                    </List.Item>
                  )}
                />
              ) : (
                <div style={{ textAlign: 'center', padding: '40px 0' }}>
                  <CheckCircleOutlined style={{ fontSize: 48, color: '#52c41a', marginBottom: 20 }} />
                  <Title level={4} style={{ color: '#52c41a' }}>
                    No Critical Commits Found
                  </Title>
                  <Text type="secondary">
                    All analyzed commits appear to be normal changes
                  </Text>
                </div>
              )
            }
          ]}
        />
      )}
    </Modal>
  );
};

export default CommitAnalysisModal;
```

### frontend\src\components\commits\CommitList.jsx
```jsx
import { useEffect, useState } from "react";
import { List, Avatar, Typography, Spin, message, Tooltip, Card, Tag, Pagination } from "antd";
import { GithubOutlined, BranchesOutlined, ClockCircleOutlined, UserOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Title, Text } = Typography;

const CommitCard = styled(Card)`
  margin-bottom: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
  }
`;

const CommitHeader = styled.div`
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
`;

const CommitMessage = styled.div`
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  font-weight: 500;
  
  &:hover {
    white-space: normal;
    overflow: visible;
  }
`;

const CommitMeta = styled.div`
  display: flex;
  align-items: center;
  gap: 12px;
  margin-top: 8px;
  color: #666;
  font-size: 13px;
`;

const PaginationContainer = styled.div`
  display: flex;
  justify-content: center;
  margin-top: 20px;
`;

const CommitList = ({ owner, repo, branch }) => {
  const [commits, setCommits] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const pageSize = 5;

  useEffect(() => {
    if (!branch) return;

    const token = localStorage.getItem("access_token");
    if (!token) return;    const fetchCommits = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches/${encodeURIComponent(branch)}/commits`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        
        // Backend tr·∫£ v·ªÅ object v·ªõi commits array trong property "commits"
        const commitsData = response.data.commits || response.data;
        setCommits(Array.isArray(commitsData) ? commitsData : []);
      } catch (err) {
        console.error(err);
        message.error("L·ªói khi l·∫•y danh s√°ch commit");
      } finally {
        setLoading(false);
      }
    };

    setLoading(true);
    fetchCommits();
  }, [owner, repo, branch]);

  const formatDate = (dateString) => {
    const options = { year: 'numeric', month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' };
    return new Date(dateString).toLocaleDateString('vi-VN', options);
  };
  // T√≠nh to√°n d·ªØ li·ªáu hi·ªÉn th·ªã theo trang hi·ªán t·∫°i
  const paginatedCommits = Array.isArray(commits) ? commits.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  ) : [];

  if (loading) {
    return (
      <div style={{ textAlign: 'center', padding: '40px' }}>
        <Spin size="large" />
        <div style={{ marginTop: 16 }}>
          <Text>ƒêang t·∫£i commit...</Text>
        </div>
      </div>
    );
  }

  return (
    <div style={{ padding: '16px' }}>
      <div style={{ display: 'flex', alignItems: 'center', marginBottom: '20px' }}>
        <Title level={4} style={{ margin: 0 }}>
          <BranchesOutlined style={{ marginRight: '8px', color: '#1890ff' }} />
          Commit tr√™n branch: <Tag color="blue">{branch}</Tag>
          <Tag style={{ marginLeft: '8px' }}>{commits.length} commits</Tag>
        </Title>
      </div>
      
      <List
        itemLayout="vertical"
        dataSource={paginatedCommits}
        renderItem={(item) => (
          <List.Item>
            <CommitCard>
              <CommitHeader>
                <Tooltip title={item.sha} placement="topLeft">
                  <Tag icon={<GithubOutlined />} color="default">
                    {item.sha.substring(0, 7)}
                  </Tag>
                </Tooltip>
              </CommitHeader>
                <CommitMessage>
                <Tooltip title={item.message || item.commit?.message} placement="topLeft">
                  {(item.message || item.commit?.message || '').split('\n')[0]}
                </Tooltip>
              </CommitMessage>
              
              <CommitMeta>
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <Avatar 
                    src={item.author?.avatar_url} 
                    size="small" 
                    icon={<UserOutlined />}
                    style={{ marginRight: '8px' }}
                  />
                  <Text>{item.author_name || item.commit?.author?.name || 'Unknown'}</Text>
                </div>
                
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <ClockCircleOutlined style={{ marginRight: '4px' }} />
                  <Text>{formatDate(item.author_date || item.commit?.author?.date)}</Text>
                </div>
              </CommitMeta>
            </CommitCard>
          </List.Item>
        )}
      />      <PaginationContainer>
        <Pagination
          current={currentPage}
          pageSize={pageSize}
          total={Array.isArray(commits) ? commits.length : 0}
          onChange={(page) => setCurrentPage(page)}
          showSizeChanger={false}
          showQuickJumper
          style={{ marginTop: '20px' }}
        />
      </PaginationContainer>
    </div>
  );
};

export default CommitList;
```

### frontend\src\components\commits\CommitTable.jsx
```jsx
//frontend\src\components\commits\CommitTable.jsxCommitTable.jsx

import { useEffect, useState } from 'react';
import { Table } from 'antd';
import axios from 'axios';

const CommitTable = () => {
  const [commits, setCommits] = useState([]);

  useEffect(() => {
    const fetchCommits = async () => {
      try {
        const response = await axios.get('http://localhost:8000/commits');
        setCommits(response.data);
      } catch (error) {
        console.error('Failed to fetch commits:', error);
      }
    };
    fetchCommits();
  }, []);

  const columns = [
    {
      title: 'ID',
      dataIndex: 'id',
    },
    {
      title: 'Repo ID',
      dataIndex: 'repo_id',
    },
    {
      title: 'User ID',
      dataIndex: 'user_id',
    },
    {
      title: 'Message',
      dataIndex: 'message',
    },
    {
      title: 'Hash',
      dataIndex: 'commit_hash',
    },
    {
      title: 'Date',
      dataIndex: 'commit_date',
    },
  ];

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-4">L·ªãch s·ª≠ Commit</h2>
      <Table columns={columns} dataSource={commits} rowKey="id" />
    </div>
  );
};

export default CommitTable;
```

### frontend\src\components\common\SyncProgressNotification.jsx
```jsx
import React, { useState, useEffect, useLayoutEffect } from 'react';
import { Progress, Card, Typography, Button, Space } from 'antd';
import { CloseOutlined, CheckCircleOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Text } = Typography;

const ProgressContainer = styled(Card)`
  position: fixed;
  top: 80px;
  right: 20px;
  width: 320px;
  z-index: 1000;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  border-radius: 8px;
  opacity: ${props => props.visible ? 1 : 0};
  transform: ${props => props.visible ? 'translateX(0)' : 'translateX(100%)'};
  transition: opacity 0.1s ease-out, transform 0.1s ease-out;
  pointer-events: ${props => props.visible ? 'auto' : 'none'};

  .ant-card-body {
    padding: 16px;
  }

  /* Force immediate display */
  &.instant-show {
    opacity: 1 !important;
    transform: translateX(0) !important;
    transition: none !important;
  }
`;

const ProgressHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 12px;
`;

const ProgressTitle = styled(Text)`
  font-weight: 600;
  color: #1e293b;
`;

const RepoProgress = styled.div`
  margin-bottom: 8px;
  padding: 8px;
  background: #f8fafc;
  border-radius: 6px;
  border-left: 3px solid ${props => 
    props.status === 'completed' ? '#10b981' : 
    props.status === 'error' ? '#ef4444' : '#3b82f6'
  };
`;

const SyncProgressNotification = ({ 
  visible, 
  onClose, 
  totalRepos = 0, 
  completedRepos = 0, 
  currentRepo = '', 
  repoProgresses = [], 
  overallProgress = 0 
}) => {
  const [autoClose, setAutoClose] = useState(false);
  const [showInstantly, setShowInstantly] = useState(false);
  const [forceInstantShow, setForceInstantShow] = useState(false);
  // Show immediately when visible becomes true - using useLayoutEffect for immediate DOM update
  useLayoutEffect(() => {
    if (visible) {
      setShowInstantly(true);
      setForceInstantShow(true);
    }
  }, [visible]);

  useEffect(() => {
    if (visible) {
      // Reset force instant after a tiny delay to allow normal transitions
      const timer = setTimeout(() => setForceInstantShow(false), 50);
      return () => clearTimeout(timer);
    } else {
      setForceInstantShow(false);
      // Delay hiding for animation
      const timer = setTimeout(() => setShowInstantly(false), 200);
      return () => clearTimeout(timer);
    }
  }, [visible]);

  useEffect(() => {
    if (completedRepos === totalRepos && totalRepos > 0) {
      setAutoClose(true);
      const timer = setTimeout(() => {
        onClose();
      }, 3000); // T·ª± ƒë·ªông ƒë√≥ng sau 3 gi√¢y
      return () => clearTimeout(timer);
    }
  }, [completedRepos, totalRepos, onClose]);
  
  // Render even if not visible for smooth transitions
  if (!showInstantly && !visible) return null;

  const isCompleted = completedRepos === totalRepos && totalRepos > 0;
  const hasErrors = repoProgresses.some(repo => repo.status === 'error');
  return (
    <ProgressContainer 
      visible={visible} 
      className={forceInstantShow ? 'instant-show' : (visible ? 'show' : '')}
    >
      <ProgressHeader>
        <ProgressTitle>
          {isCompleted ? '‚úÖ ƒê·ªìng b·ªô ho√†n th√†nh' : 'üîÑ ƒêang ƒë·ªìng b·ªô repository'}
        </ProgressTitle>
        <Button 
          type="text" 
          size="small" 
          icon={<CloseOutlined />} 
          onClick={onClose}
        />
      </ProgressHeader>

      {/* Overall Progress */}
      <div style={{ marginBottom: 16 }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', marginBottom: 4 }}>
          <Text type="secondary">T·ªïng ti·∫øn tr√¨nh</Text>
          <Text strong>{Math.round(overallProgress)}%</Text>
        </div>
        <Progress 
          percent={overallProgress} 
          strokeColor={isCompleted ? '#10b981' : '#3b82f6'}
          showInfo={false}
          size="small"
        />
        <Text type="secondary" style={{ fontSize: '12px' }}>
          {completedRepos}/{totalRepos} repository
        </Text>
      </div>

      {/* Current Repository */}
      {currentRepo && !isCompleted && (
        <div style={{ marginBottom: 12 }}>
          <Text type="secondary" style={{ fontSize: '12px' }}>ƒêang x·ª≠ l√Ω:</Text>
          <div style={{ 
            background: '#e0f2fe', 
            padding: '4px 8px', 
            borderRadius: '4px',
            marginTop: '4px'
          }}>
            <Text style={{ fontSize: '12px', color: '#0369a1' }}>{currentRepo}</Text>
          </div>
        </div>
      )}

      {/* Repository List (hi·ªÉn th·ªã khi c√≥ nhi·ªÅu repo) */}
      {repoProgresses.length > 0 && (
        <div style={{ maxHeight: '200px', overflowY: 'auto' }}>
          {repoProgresses.slice(-5).map((repo, index) => (
            <RepoProgress key={index} status={repo.status}>
              <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                <Text style={{ fontSize: '11px', fontWeight: 500 }}>
                  {repo.name}
                </Text>
                <Space size={4}>
                  {repo.status === 'completed' && <CheckCircleOutlined style={{ color: '#10b981' }} />}
                  {repo.status === 'error' && <ExclamationCircleOutlined style={{ color: '#ef4444' }} />}
                  <Text style={{ fontSize: '10px' }}>
                    {repo.status === 'completed' ? '‚úì' : 
                     repo.status === 'error' ? '‚úó' : '...'}
                  </Text>
                </Space>
              </div>
              {repo.progress !== undefined && repo.status === 'syncing' && (
                <Progress 
                  percent={repo.progress} 
                  size="small" 
                  showInfo={false}
                  strokeColor="#3b82f6"
                  style={{ marginTop: 4 }}
                />
              )}
            </RepoProgress>
          ))}
        </div>
      )}

      {/* Summary */}
      {isCompleted && (
        <div style={{ 
          background: hasErrors ? '#fef3c7' : '#d1fae5', 
          padding: '8px', 
          borderRadius: '6px',
          marginTop: '12px'
        }}>
          <Text style={{ 
            fontSize: '12px', 
            color: hasErrors ? '#92400e' : '#047857'
          }}>
            {hasErrors 
              ? `Ho√†n th√†nh v·ªõi ${repoProgresses.filter(r => r.status === 'error').length} l·ªói`
              : 'T·∫•t c·∫£ repository ƒë√£ ƒë∆∞·ª£c ƒë·ªìng b·ªô th√†nh c√¥ng!'
            }
          </Text>
          {autoClose && (
            <div style={{ marginTop: 4 }}>
              <Text style={{ fontSize: '10px', color: '#6b7280' }}>
                T·ª± ƒë·ªông ƒë√≥ng sau 3 gi√¢y...
              </Text>
            </div>
          )}
        </div>
      )}
    </ProgressContainer>
  );
};

export default SyncProgressNotification;

```

### frontend\src\components\Dashboard\AIInsightWidget.jsx
```jsx
import React from 'react';
import { Card, Space, Typography, Button, Tag } from 'antd';
import { BulbOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Title, Text } = Typography;

// Styled components
const InsightContainer = styled(Card)`
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  background: #ffffff;
  transition: all 0.3s ease;

  &:hover {
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.12);
    transform: translateY(-2px);
  }
`;

const InsightCard = styled(Card).withConfig({
  shouldForwardProp: (prop) => !['borderColor'].includes(prop),
})`
  border-radius: 8px;
  border: 1px solid ${(props) => props.borderColor || '#f0f0f0'};
  background: #fff;
  transition: all 0.3s ease;
  padding: 12px;

  &:hover {
    border-color: ${(props) => props.borderColor || '#d9d9d9'};
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  }

  @media (max-width: 576px) {
    padding: 8px;
  }
`;

const IconWrapper = styled.div.withConfig({
  shouldForwardProp: (prop) => !['bgColor'].includes(prop),
})`
  display: flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background: ${(props) => props.bgColor || '#f0f0f0'};
`;

const ActionWrapper = styled.div`
  display: flex;
  justify-content: flex-end;
  gap: 8px;

  @media (max-width: 576px) {
    justify-content: flex-start;
    margin-top: 8px;
  }
`;

const AIInsightWidget = () => {
  const insights = [
    {
      id: 1,
      type: 'suggestion',
      title: 'Ph√¢n c√¥ng ƒë·ªÅ xu·∫•t',
      description: 'Th√™m 2 developer v√†o repo "frontend" ƒë·ªÉ ƒë·∫£m b·∫£o deadline 25/04/2025.',
    },
    {
      id: 2,
      type: 'warning',
      title: 'D·ª± ƒëo√°n ti·∫øn ƒë·ªô',
      description: 'Repo "backend" c√≥ nguy c∆° tr·ªÖ h·∫°n 3 ng√†y. Xem x√©t tƒÉng t√†i nguy√™n.',
    },
  ];

  const getInsightStyle = (type) => {
    switch (type) {
      case 'suggestion':
        return {
          icon: <BulbOutlined style={{ fontSize: 20, color: '#1890ff' }} />,
          tag: <Tag color="blue">ƒê·ªÅ xu·∫•t</Tag>,
          borderColor: '#e6f7ff',
          iconBg: '#e6f7ff',
        };
      case 'warning':
        return {
          icon: <WarningOutlined style={{ fontSize: 20, color: '#fa8c16' }} />,
          tag: <Tag color="orange">C·∫£nh b√°o</Tag>,
          borderColor: '#fff7e6',
          iconBg: '#fff7e6',
        };
      default:
        return {
          icon: null,
          tag: null,
          borderColor: '#f0f0f0',
          iconBg: '#f0f0f0',
        };
    }
  };
  return (
    <InsightContainer
      title={<Title level={4} style={{ margin: 0 }}>G·ª£i √Ω AI</Title>}
      variant="outlined"
    >
      <Space direction="vertical" size="middle" style={{ width: '100%' }}>
        {insights.map((item) => {
          const { icon, tag, borderColor, iconBg } = getInsightStyle(item.type);
          return (
            <InsightCard key={item.id} borderColor={borderColor}>
              <Space direction="horizontal" size="middle" style={{ width: '100%', alignItems: 'center' }}>
                <IconWrapper bgColor={iconBg}>{icon}</IconWrapper>
                <Space direction="vertical" size={4} style={{ flex: 1 }}>
                  <Space>
                    <Title level={5} style={{ margin: 0 }}>{item.title}</Title>
                    {tag}
                  </Space>
                  <Text type="secondary">{item.description}</Text>
                </Space>
                <ActionWrapper>
                  <Button type="primary" size="small">Th·ª±c hi·ªán</Button>
                  <Button size="small">B·ªè qua</Button>
                </ActionWrapper>
              </Space>
            </InsightCard>
          );
        })}
      </Space>
    </InsightContainer>
  );
};

export default AIInsightWidget;
```

### frontend\src\components\Dashboard\OverviewCard.jsx
```jsx
import React from 'react';
import { Card, Statistic, Space, Row, Col } from 'antd';
import { ProjectOutlined, CheckCircleOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const SidebarOverviewCard = styled(Card)`
  .ant-card-body {
    padding: 16px;
  }
`;

const StatisticItem = styled.div`
  padding: 12px;
  border-radius: 8px;
  background: #f8fafc;
  border: 1px solid #e2e8f0;
  margin-bottom: 12px;
  transition: all 0.2s ease;

  &:hover {
    border-color: #3b82f6;
    transform: translateY(-1px);
  }

  &:last-child {
    margin-bottom: 0;
  }
`;

const OverviewCard = ({ projects = 10, completedTasks = 50, overdueTasks = 5, sidebar = false }) => {
  if (sidebar) {
    return (
      <SidebarOverviewCard 
        title="T·ªïng quan d·ª± √°n" 
        variant="outlined"
        size="small"
      >
        <Space direction="vertical" style={{ width: '100%' }} size={0}>
          <StatisticItem>
            <Statistic
              title="S·ªë d·ª± √°n"
              value={projects}
              prefix={<ProjectOutlined />}
              valueStyle={{ color: '#1890ff', fontSize: '18px' }}
            />
          </StatisticItem>
          <StatisticItem>
            <Statistic
              title="C√¥ng vi·ªác ho√†n th√†nh"
              value={completedTasks}
              prefix={<CheckCircleOutlined />}
              valueStyle={{ color: '#52c41a', fontSize: '18px' }}
            />
          </StatisticItem>
          <StatisticItem>
            <Statistic
              title="C√¥ng vi·ªác tr·ªÖ h·∫°n"
              value={overdueTasks}
              prefix={<WarningOutlined />}
              valueStyle={{ color: '#ff4d4f', fontSize: '18px' }}
            />
          </StatisticItem>
        </Space>
      </SidebarOverviewCard>
    );
  }  // Layout ngang cho desktop th∆∞·ªùng
  return (
    <Card title="T·ªïng quan d·ª± √°n" variant="outlined">
      <Row gutter={16}>
        <Col span={8}>
          <Statistic
            title="S·ªë d·ª± √°n"
            value={projects}
            prefix={<ProjectOutlined />}
            valueStyle={{ color: '#1890ff' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="C√¥ng vi·ªác ho√†n th√†nh"
            value={completedTasks}
            prefix={<CheckCircleOutlined />}
            valueStyle={{ color: '#52c41a' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="C√¥ng vi·ªác tr·ªÖ h·∫°n"
            value={overdueTasks}
            prefix={<WarningOutlined />}
            valueStyle={{ color: '#ff4d4f' }}
          />
        </Col>
      </Row>
    </Card>
  );
};

export default OverviewCard;
```

### frontend\src\components\Dashboard\ProjectTaskManager.jsx
```jsx
import React, { useState } from 'react';
import { 
  Card, Button, Space, Form, message, Switch,
  Typography, Divider
} from 'antd';
import { 
  AppstoreOutlined, UnorderedListOutlined, PlusOutlined,
  ReloadOutlined, TeamOutlined 
} from '@ant-design/icons';
import styled from 'styled-components';
import dayjs from 'dayjs';

import { useProjectData } from '../../hooks/useProjectData';
import {
  filterTasks,
  calculateTaskStats,
  formatTaskForAPI
} from '../../utils/taskUtils.jsx';
import RepoSelector from './ProjectTaskManager/RepoSelector';
import StatisticsPanel from './ProjectTaskManager/StatisticsPanel';
import FiltersPanel from './ProjectTaskManager/FiltersPanel';
import TaskList from './ProjectTaskManager/TaskList';
import TaskModal from './ProjectTaskManager/TaskModal';
import KanbanBoard from './ProjectTaskManager/KanbanBoard';
import RepositoryMembers from './RepositoryMembers';

const { Title } = Typography;

const TaskCard = styled(Card)`
  margin-bottom: 12px;
  border-radius: 8px;
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    transform: translateY(-2px);
  }
`;

const TaskHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 8px;
`;

const TaskActions = styled.div`
  display: flex;
  gap: 8px;
`;

const ProjectTaskManager = ({ repositories, repoLoading }) => {
  // ==================== LOCAL STATE (UI ONLY) ====================
  const [isModalVisible, setIsModalVisible] = useState(false);
  const [editingTask, setEditingTask] = useState(null);
  const [form] = Form.useForm();
  const [viewMode, setViewMode] = useState(true); // true = Kanban, false = List
  const [activeTab, setActiveTab] = useState('tasks'); // 'tasks' or 'members'
  
  // Filter states
  const [searchText, setSearchText] = useState('');
  const [statusFilter, setStatusFilter] = useState('all');
  const [priorityFilter, setPriorityFilter] = useState('all');
  const [assigneeFilter, setAssigneeFilter] = useState('all');  // ==================== CUSTOM HOOK (DATA MANAGEMENT) ====================
  const {
    selectedRepo,
    branches,
    tasks,
    collaborators,
    tasksLoading,
    branchesLoading,
    handleRepoChange,
    getAssigneeInfo,
    createTask,
    updateTask,
    updateTaskStatus,    deleteTask,
    syncBranches,
    syncCollaborators
  } = useProjectData({ preloadedRepositories: repositories });

  // ==================== COMPUTED VALUES ====================
  const filteredTasks = filterTasks(tasks, {
    searchText,
    statusFilter,
    priorityFilter,
    assigneeFilter
  });

  const taskStats = calculateTaskStats(tasks);

  // ==================== UI HANDLERS ====================
  const showTaskModal = (task = null) => {
    setEditingTask(task);
    setIsModalVisible(true);
    
    if (task) {
      form.setFieldsValue({
        title: task.title,
        description: task.description,
        assignee: task.assignee,
        priority: task.priority,
        dueDate: task.due_date ? dayjs(task.due_date) : null
      });
    } else {
      form.resetFields();
    }
  };

  const handleTaskSubmit = async (values) => {
    try {
      const taskData = {
        ...formatTaskForAPI(values),
        status: editingTask ? editingTask.status : 'TODO',
        repo_owner: selectedRepo.owner.login,
        repo_name: selectedRepo.name
      };
      if (editingTask) {
        await updateTask(editingTask.id, taskData);
        message.success('‚úÖ C·∫≠p nh·∫≠t task th√†nh c√¥ng!');
      } else {
        await createTask(taskData);
        message.success('‚úÖ T·∫°o task th√†nh c√¥ng!');
      }

      setIsModalVisible(false);
      form.resetFields();
    } catch (error) {
      console.error('Form submission error:', error);
      message.error('‚ùå L·ªói khi l∆∞u task!');
    }
  };

  const resetFilters = () => {
    setSearchText('');
    setStatusFilter('all');
    setPriorityFilter('all');
    setAssigneeFilter('all');
  };

  // ==================== RENDER ====================
  return (
    <Card 
      title={
        <div style={{ display: 'flex', alignItems: 'center', gap: 12 }}>
          <Title level={3} style={{ margin: 0, background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)', 
          WebkitBackgroundClip: 'text', WebkitTextFillColor: 'transparent' }}>
            üéØ Qu·∫£n l√Ω Task D·ª± √°n
          </Title>
        </div>
      }
      style={{ minHeight: '80vh' }}      extra={selectedRepo && (
        <Space>
          {/* Tab Switcher */}
          <Space.Compact>
            <Button 
              type={activeTab === 'tasks' ? "primary" : "default"}
              onClick={() => setActiveTab('tasks')}
              style={{ borderRadius: '6px 0 0 6px' }}
            >
              üìã Tasks
            </Button>
            <Button 
              type={activeTab === 'members' ? "primary" : "default"}
              icon={<TeamOutlined />}
              onClick={() => setActiveTab('members')}
              style={{ borderRadius: '0 6px 6px 0' }}
            >
              üë• Members
            </Button>
          </Space.Compact>

          {/* Task View Mode (only show when on tasks tab) */}
          {activeTab === 'tasks' && (
            <Space.Compact>
              <Button 
                type={viewMode ? "primary" : "default"}
                icon={<AppstoreOutlined />}
                onClick={() => setViewMode(true)}
                style={{ borderRadius: '6px 0 0 6px' }}
              >
                Kanban
              </Button>
              <Button 
                type={!viewMode ? "primary" : "default"}
                icon={<UnorderedListOutlined />}
                onClick={() => setViewMode(false)}
                style={{ borderRadius: '0 6px 6px 0' }}
              >
                List
              </Button>
            </Space.Compact>
          )}
          
          {activeTab === 'tasks' && (
            <Button 
              type="primary" 
              icon={<PlusOutlined />}
              onClick={() => showTaskModal()}
              disabled={!selectedRepo}
              style={{ 
                borderRadius: 6,
                background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
                border: 'none'
              }}
            >
              T·∫°o Task
            </Button>
          )}
        </Space>
      )}
    >
      {/* Repository Selector - Clean & Simple */}      <RepoSelector 
        repositories={repositories}
        selectedRepo={selectedRepo}
        loading={repoLoading}
        handleRepoChange={handleRepoChange}
        branches={branches}
        collaborators={collaborators}
        branchesLoading={branchesLoading}
        onSyncBranches={syncBranches}
        onSyncCollaborators={syncCollaborators}
      />{/* Tab Content - Conditional Rendering */}
      {selectedRepo && (
        <>
          {activeTab === 'tasks' && (
            <>
              {/* Statistics Panel */}
              <StatisticsPanel 
                stats={taskStats}
                selectedRepo={selectedRepo}
                collaborators={collaborators}
              />
              <Divider />
              {/* Filters Panel */}
              <FiltersPanel 
                searchText={searchText}
                setSearchText={setSearchText}
                statusFilter={statusFilter}
                setStatusFilter={setStatusFilter}
                priorityFilter={priorityFilter}
                setPriorityFilter={setPriorityFilter}
                assigneeFilter={assigneeFilter}
                setAssigneeFilter={setAssigneeFilter}
                collaborators={collaborators}
                filteredTasks={filteredTasks}
                tasksLoading={tasksLoading}
                resetFilters={resetFilters}
              />
              <Divider />
              {/* Tasks Display */}
              {viewMode ? (
                <KanbanBoard 
                  tasks={filteredTasks}
                  getAssigneeInfo={getAssigneeInfo}
                  getPriorityColor={(priority) => {
                    switch (priority) {
                      case 'urgent': return '#ff4d4f';
                      case 'high': return '#ff7a45';
                      case 'medium': return '#faad14';
                      case 'low': return '#52c41a';
                      default: return '#1890ff';
                    }
                  }}
                  showTaskModal={showTaskModal}
                  deleteTask={deleteTask}
                  updateTaskStatus={updateTaskStatus}
                />
              ) : (
                <TaskList 
                  filteredTasks={filteredTasks}
                  tasksLoading={tasksLoading}
                  getAssigneeInfo={getAssigneeInfo}
                  getStatusIcon={(status) => {
                    switch (status) {
                      case 'todo': return 'üìã';
                      case 'in_progress': return '‚ö°';
                      case 'done': return '‚úÖ';
                      default: return 'üìã';
                    }
                  }}
                  getPriorityColor={(priority) => {
                    switch (priority) {
                      case 'urgent': return '#ff4d4f';
                      case 'high': return '#ff7a45';
                      case 'medium': return '#faad14';
                      case 'low': return '#52c41a';
                      default: return '#1890ff';
                    }
                  }}
                  updateTaskStatus={updateTaskStatus}
                  showTaskModal={showTaskModal}
                  deleteTask={deleteTask}
                />
              )}
            </>
          )}
          {activeTab === 'members' && (
            <RepositoryMembers selectedRepo={selectedRepo} />
          )}
        </>
      )}
      {/* Task Modal */}
      <TaskModal 
        isModalVisible={isModalVisible}
        editingTask={editingTask}
        form={form}
        handleTaskSubmit={handleTaskSubmit}
        setIsModalVisible={setIsModalVisible}
        collaborators={collaborators}
      />
    </Card>
  );
};
export default ProjectTaskManager;
```

### frontend\src\components\Dashboard\RepoListFilter.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col, Input, Select, Button } from 'antd';
import { SearchOutlined } from '@ant-design/icons';

const { Option } = Select;

const RepoListFilter = ({ onFilterChange }) => {
  const [searchText, setSearchText] = useState('');
  const [status, setStatus] = useState('all');
  const [assignee, setAssignee] = useState('all');

  const handleApplyFilter = () => {
    onFilterChange({ searchText, status, assignee });
  };

  return (
    <Card title="B·ªô l·ªçc Repository" variant="borderless">
      <Row gutter={16}>
        <Col span={8}>
          <Input
            placeholder="T√¨m ki·∫øm repo"
            prefix={<SearchOutlined />}
            value={searchText}
            onChange={(e) => setSearchText(e.target.value)}
          />
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={status}
            onChange={(value) => setStatus(value)}
            placeholder="Tr·∫°ng th√°i"
          >
            <Option value="all">T·∫•t c·∫£</Option>
            <Option value="active">ƒêang ho·∫°t ƒë·ªông</Option>
            <Option value="archived">ƒê√£ l∆∞u tr·ªØ</Option>
          </Select>
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={assignee}
            onChange={(value) => setAssignee(value)}
            placeholder="Ng∆∞·ªùi ph·ª• tr√°ch"
          >
            <Option value="all">T·∫•t c·∫£</Option>
            <Option value="user1">User 1</Option>
            <Option value="user2">User 2</Option>
          </Select>
        </Col>
        <Col span={4}>
          <Button type="primary" onClick={handleApplyFilter}>
            √Åp d·ª•ng
          </Button>
        </Col>
      </Row>
    </Card>
  );
};

export default RepoListFilter;
```

### frontend\src\components\Dashboard\RepositoryMembers.jsx
```jsx
import React, { useState, useEffect, useCallback } from 'react';
import { 
  Card, Avatar, List, Tag, Progress, Row, Col, Button, 
  Typography, Divider, Spin, Empty, message, Space, Switch, Select 
} from 'antd';
import { 
  UserOutlined, RobotOutlined, CodeOutlined, 
  BugOutlined, ToolOutlined, FileTextOutlined, BranchesOutlined 
} from '@ant-design/icons';
import { Pie, Bar } from 'react-chartjs-2';
import {
  Chart as ChartJS,
  ArcElement,
  Tooltip,
  Legend,
  CategoryScale,
  LinearScale,
  BarElement,
} from 'chart.js';

ChartJS.register(ArcElement, Tooltip, Legend, CategoryScale, LinearScale, BarElement);

const { Title, Text } = Typography;

const RepositoryMembers = ({ selectedRepo }) => {  const [members, setMembers] = useState([]);
  const [selectedMember, setSelectedMember] = useState(null);
  const [memberCommits, setMemberCommits] = useState(null);
  const [loading, setLoading] = useState(false);
  const [analysisLoading, setAnalysisLoading] = useState(false);
  const [showAIFeatures, setShowAIFeatures] = useState(false);
  const [useAI, setUseAI] = useState(true); // Toggle for AI analysis
  const [aiModelStatus, setAiModelStatus] = useState(null);
  const [branches, setBranches] = useState([]);
  const [selectedBranch, setSelectedBranch] = useState(null); // NEW: Branch selector
  const [branchesLoading, setBranchesLoading] = useState(false);
  // Debug: Log component render and props
  console.log('RepositoryMembers RENDER:', { 
    selectedRepo, 
    members: members.length,
    loading,
    hasSelectedRepo: !!selectedRepo,
    repoId: selectedRepo?.id 
  });

  // Load AI model status
  const _loadAIModelStatus = useCallback(async () => {
    if (!selectedRepo?.id) return;
    
    try {
      const response = await fetch(`http://localhost:8000/api/repositories/${selectedRepo.id}/ai/model-status`);
      
      if (response.ok) {
        const data = await response.json();
        console.log('AI Model Status:', data); // Debug log
        setAiModelStatus(data);
      } else {
        console.error('AI Status Error:', response.status);
      }
    } catch (error) {
      console.error('Error loading AI model status:', error);
    }
  }, [selectedRepo?.id]);

  const loadRepositoryBranches = useCallback(async () => {
    if (!selectedRepo?.id) return;
    
    setBranchesLoading(true);
    try {
      const response = await fetch(`http://localhost:8000/api/repositories/${selectedRepo.id}/branches`);
      
      if (response.ok) {
        const data = await response.json();
        console.log('Branches API Response:', data);
        setBranches(data.branches || []);        // Auto-select: Start with "All Branches" by default
        if (!selectedBranch) {
          setSelectedBranch(null); // null = "All Branches"
        }
      } else {
        console.error('Branches API Error:', response.status);
      }
    } catch (error) {
      console.error('Error loading branches:', error);
    } finally {
      setBranchesLoading(false);
    }
  }, [selectedRepo?.id, selectedBranch]);

  const loadRepositoryMembers = useCallback(async () => {
    if (!selectedRepo?.id) {
      console.log('‚ùå loadRepositoryMembers: No selectedRepo.id');
      return;
    }

    console.log('loadRepositoryMembers called with repo:', selectedRepo); // Debug log
    setLoading(true);
    try {
      const url = `http://localhost:8000/api/repositories/${selectedRepo.id}/members`;
      console.log('Fetching members from URL:', url); // Debug log
      
      // Test without token first
      const response = await fetch(url);
      
      console.log('Members API Response status:', response.status); // Debug log
      
      if (response.ok) {
        const data = await response.json();
        console.log('Members API Response data:', data); // Debug log
        setMembers(data.members || []);
        console.log('Members set:', data.members || []); // Debug log
      } else {
        console.error('Members API Error:', response.status, response.statusText);
        const errorText = await response.text();
        console.error('Error response body:', errorText);
        message.error(`Kh√¥ng th·ªÉ t·∫£i danh s√°ch th√†nh vi√™n: ${response.status}`);
      }
    } catch (error) {
      console.error('Error loading members:', error);
      message.error('L·ªói khi t·∫£i th√†nh vi√™n');
    } finally {
      setLoading(false);
    }
  }, [selectedRepo]);

  // Load members when repo changes
  useEffect(() => {
    console.log('RepositoryMembers useEffect triggered:', {
      selectedRepo,
      repoId: selectedRepo?.id,
      repoName: selectedRepo?.name,
      hasRepo: !!selectedRepo
    });
      if (selectedRepo && selectedRepo.id) {
      console.log('‚úÖ Loading members for repo:', selectedRepo.name, 'ID:', selectedRepo.id);
      loadRepositoryMembers();
      loadRepositoryBranches();
      _loadAIModelStatus();
    } else {
      console.log('‚ùå No selectedRepo or selectedRepo.id found:', {
        selectedRepo: !!selectedRepo,
        id: selectedRepo?.id
      });
      // Clear members if no repo
      setMembers([]);
    }  }, [selectedRepo, loadRepositoryMembers, loadRepositoryBranches, _loadAIModelStatus]); // Remove selectedRepo?.id to avoid redundancy

  // Re-analyze when branch changes
  useEffect(() => {
    if (selectedMember && selectedBranch !== null) {
      console.log('Branch changed, re-analyzing member:', selectedMember.login, 'on branch:', selectedBranch);
      handleMemberClick(selectedMember);
    }
  }, [selectedBranch]); // eslint-disable-line react-hooks/exhaustive-deps

  const handleMemberClick = async (member) => {
    setSelectedMember(member);
    setAnalysisLoading(true);
    
    try {
      const aiParam = useAI ? '?use_ai=true' : '?use_ai=false';
      const branchParam = selectedBranch ? `&branch_name=${encodeURIComponent(selectedBranch)}` : '';
      const response = await fetch(
        `http://localhost:8000/api/repositories/${selectedRepo.id}/members/${member.login}/commits${aiParam}${branchParam}`
      );
      
      if (response.ok) {
        const data = await response.json();
        console.log('Commit Analysis Response:', data); // Debug log
        setMemberCommits(data.data);
      } else {
        console.error('Commit Analysis Error:', response.status, response.statusText);
        message.error(`Kh√¥ng th·ªÉ ph√¢n t√≠ch commits: ${response.status}`);
      }
    } catch (error) {
      console.error('Error analyzing member:', error);
      message.error('L·ªói khi ph√¢n t√≠ch commits');
    } finally {
      setAnalysisLoading(false);
    }
  };

  const getCommitTypeIcon = (type) => {
    const icons = {
      'feat': <CodeOutlined style={{ color: '#52c41a' }} />,
      'fix': <BugOutlined style={{ color: '#f5222d' }} />,
      'chore': <ToolOutlined style={{ color: '#1890ff' }} />,
      'docs': <FileTextOutlined style={{ color: '#fa8c16' }} />,
      'refactor': '‚ôªÔ∏è',
      'test': '‚úÖ',
      'style': 'üíÑ',
      'other': 'üìù'
    };
    return icons[type] || 'üìù';
  };

  const getCommitTypeColor = (type) => {
    const colors = {
      'feat': 'green',
      'fix': 'red',
      'chore': 'blue',
      'docs': 'orange',
      'refactor': 'purple',
      'test': 'cyan',
      'style': 'magenta',
      'other': 'default'
    };
    return colors[type] || 'default';
  };

  // Chart data for commit types
  const chartData = memberCommits ? {
    labels: Object.keys(memberCommits.statistics.commit_types),
    datasets: [{
      data: Object.values(memberCommits.statistics.commit_types),
      backgroundColor: [
        '#52c41a', '#f5222d', '#1890ff', '#fa8c16', 
        '#722ed1', '#13c2c2', '#eb2f96', '#666666'
      ]
    }]
  } : null;

  if (!selectedRepo) {
    return (
      <Card>
        <Empty description="Vui l√≤ng ch·ªçn repository ƒë·ªÉ xem th√†nh vi√™n" />
      </Card>
    );
  }

  return (
    <div style={{ padding: '20px' }}>      {/* Header v·ªõi AI Button */}      <div style={{ 
        display: 'flex', 
        justifyContent: 'space-between', 
        alignItems: 'flex-start',
        marginBottom: '20px',
        flexWrap: 'wrap',
        gap: '16px'
      }}>
        <Title level={3} style={{ margin: 0 }}>
          üë• Th√†nh vi√™n - {selectedRepo.name}
        </Title>        
        <Space wrap>{/* Branch Selector */}
          <div style={{ display: 'flex', alignItems: 'center', gap: '8px' }}>
            <BranchesOutlined />
            <Text strong style={{ fontSize: '14px' }}>Nh√°nh:</Text>
            <Select
              value={selectedBranch}
              onChange={setSelectedBranch}
              placeholder="Ch·ªçn nh√°nh"
              style={{ minWidth: 150 }}
              loading={branchesLoading}
              allowClear
            >
              <Select.Option key="all" value={null}>
                <span style={{ display: 'flex', alignItems: 'center', gap: '4px' }}>
                  <Tag color="purple" size="small">T·∫•t c·∫£</Tag>
                  T·∫•t c·∫£ nh√°nh
                </span>
              </Select.Option>
              {branches.map(branch => (
                <Select.Option key={branch.name} value={branch.name}>
                  <span style={{ display: 'flex', alignItems: 'center', gap: '4px' }}>
                    {branch.is_default && <Tag color="blue" size="small">M·∫∑c ƒë·ªãnh</Tag>}
                    {branch.name}
                    <Text type="secondary" style={{ fontSize: '12px' }}>
                      ({branch.commits_count} commits)
                    </Text>
                  </span>
                </Select.Option>
              ))}
            </Select>
          </div>
          
          {/* AI Toggle Switch */}
          <div style={{ display: 'flex', alignItems: 'center', gap: '8px' }}>
            <Text>Ph√¢n t√≠ch m·∫´u</Text>
            <Switch 
              checked={useAI}
              onChange={setUseAI}
              checkedChildren="ü§ñ AI"
              unCheckedChildren="üìù C∆° b·∫£n"
              style={{
                backgroundColor: useAI ? '#52c41a' : '#d9d9d9'
              }}
            />
            <Text>M√¥ h√¨nh HAN AI</Text>
          </div>
          
          <Button 
            type="primary" 
            icon={<RobotOutlined />}
            onClick={() => setShowAIFeatures(!showAIFeatures)}
            style={{
              background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
              border: 'none'
            }}
          >
            ü§ñ AI Features
          </Button>        </Space>
      </div>

      {/* Th·ªëng k√™ t·ªïng quan */}
      {members.length > 0 && (
        <Row gutter={[16, 16]} style={{ marginBottom: '20px' }}>
          <Col xs={24} sm={8}>
            <Card size="small" style={{ textAlign: 'center' }}>
              <Title level={3} style={{ color: '#1890ff', margin: 0 }}>
                {members.length}
              </Title>
              <Text type="secondary">Th√†nh vi√™n tham gia</Text>
            </Card>
          </Col>
          <Col xs={24} sm={8}>
            <Card size="small" style={{ textAlign: 'center' }}>
              <Title level={3} style={{ color: '#52c41a', margin: 0 }}>
                {branches.length}
              </Title>
              <Text type="secondary">Nh√°nh trong d·ª± √°n</Text>
            </Card>
          </Col>
          <Col xs={24} sm={8}>
            <Card size="small" style={{ textAlign: 'center' }}>
              <Title level={3} style={{ color: '#fa8c16', margin: 0 }}>
                {members.reduce((sum, member) => sum + member.total_commits, 0)}
              </Title>
              <Text type="secondary">T·ªïng commits</Text>
            </Card>
          </Col>
        </Row>
      )}{/* AI Features Panel */}
      {showAIFeatures && (
        <Card 
          style={{ marginBottom: '20px', borderColor: '#1890ff' }}
          title={
            <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
              <span>ü§ñ AI Model Status</span>
              {aiModelStatus && (
                <Tag color={aiModelStatus.model_loaded ? 'green' : 'red'}>
                  {aiModelStatus.model_loaded ? '‚úÖ Model Loaded' : '‚ùå Model Not Available'}
                </Tag>
              )}
            </div>
          }
        >
          {aiModelStatus ? (
            <>              <Text strong>Lo·∫°i m√¥ h√¨nh: </Text>
              <Text>{aiModelStatus.model_info?.type || 'M·∫°ng HAN'}</Text>
              <br />
              <Text strong>Ch·∫ø ƒë·ªô ph√¢n t√≠ch: </Text>
              <Text>{useAI ? 'H·ªó tr·ª£ AI (M√¥ h√¨nh HAN)' : 'D·ª±a tr√™n m·∫´u (D·ª± ph√≤ng)'}</Text>
              <Divider />
            </>
          ) : null}
          
          <Row gutter={[16, 16]}>
            <Col span={6}>
              <div style={{ textAlign: 'center' }}>
                <CodeOutlined style={{ fontSize: '24px', color: useAI ? '#52c41a' : '#d9d9d9' }} />                <div>Ph√¢n lo·∫°i Commit</div>
                <Text type="secondary">feat/fix/chore/docs</Text>
              </div>
            </Col>
            <Col span={6}>
              <div style={{ textAlign: 'center' }}>
                <UserOutlined style={{ fontSize: '24px', color: useAI ? '#1890ff' : '#d9d9d9' }} />
                <div>Th√¥ng tin Developer</div>
                <Text type="secondary">Ph√¢n t√≠ch nƒÉng su·∫•t</Text>
              </div>
            </Col>
            <Col span={6}>
              <div style={{ textAlign: 'center' }}>
                <ToolOutlined style={{ fontSize: '24px', color: useAI ? '#fa8c16' : '#d9d9d9' }} />
                <div>Ph√°t hi·ªán lƒ©nh v·ª±c c√¥ng ngh·ªá</div>
                <Text type="secondary">API/Frontend/Database</Text>
              </div>
            </Col>
            <Col span={6}>
              <div style={{ textAlign: 'center' }}>
                <FileTextOutlined style={{ fontSize: '24px', color: useAI ? '#722ed1' : '#d9d9d9' }} />
                <div>Nh·∫≠n d·∫°ng m·∫´u</div>
                <Text type="secondary">M·∫´u thay ƒë·ªïi code</Text>
              </div>
            </Col>
          </Row>
        </Card>
      )}

      <Row gutter={[24, 24]}>
        {/* Members List */}
        <Col xs={24} md={8}>
          <Card title="üë• Danh s√°ch th√†nh vi√™n" loading={loading}>
            {members.length === 0 ? (
              <Empty description="Kh√¥ng c√≥ th√†nh vi√™n n√†o" />
            ) : (
              <List
                dataSource={members}
                renderItem={member => (
                  <List.Item
                    style={{
                      cursor: 'pointer',
                      padding: '12px',
                      backgroundColor: selectedMember?.login === member.login ? '#e6f7ff' : 'transparent',
                      borderRadius: '6px',
                      marginBottom: '8px'
                    }}
                    onClick={() => handleMemberClick(member)}
                  >
                    <List.Item.Meta
                      avatar={
                        <Avatar 
                          src={member.avatar_url} 
                          icon={<UserOutlined />}
                          size="large"
                        />
                      }
                      title={member.display_name}
                      description={
                        <div>
                          <div>@{member.login}</div>
                          <Text type="secondary">{member.total_commits} commits</Text>
                        </div>
                      }
                    />
                  </List.Item>
                )}
              />
            )}
          </Card>
        </Col>

        {/* Member Analysis */}
        <Col xs={24} md={16}>
          {!selectedMember ? (
            <Card>
              <Empty description="Ch·ªçn th√†nh vi√™n ƒë·ªÉ xem ph√¢n t√≠ch commits" />
            </Card>          ) : (
            <Spin spinning={analysisLoading}>
              {memberCommits && memberCommits.summary.total_commits === 0 ? (
                <Card>
                  <Empty 
                    description={
                      <div>                        <p>Kh√¥ng t√¨m th·∫•y commits cho @{selectedMember.login}</p>
                        {selectedBranch && (
                          <p>tr√™n nh√°nh <Tag color="blue">{selectedBranch}</Tag></p>
                        )}
                        {!selectedBranch && (
                          <p>tr√™n t·∫•t c·∫£ c√°c nh√°nh</p>
                        )}
                        <p>Th·ª≠:</p>
                        <ul style={{ textAlign: 'left', margin: '0 auto', display: 'inline-block' }}>
                          <li>Ch·ªçn nh√°nh kh√°c t·ª´ dropdown</li>
                          <li>Ch·ªçn "T·∫•t c·∫£ nh√°nh" ƒë·ªÉ xem to√†n b·ªô</li>
                          <li>Ki·ªÉm tra t√™n ng∆∞·ªùi d√πng c√≥ ch√≠nh x√°c kh√¥ng</li>
                        </ul>
                      </div>
                    }
                  />
                </Card>
              ) : memberCommits && (
                <>{/* Statistics Overview */}
                  <Card 
                    title={                      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                        <span>üìä Ph√¢n t√≠ch commits - @{selectedMember.login}</span>
                        <div style={{ display: 'flex', gap: '8px' }}>                          {selectedBranch && (
                            <Tag color="blue" icon={<BranchesOutlined />}>
                              Nh√°nh: {selectedBranch}
                            </Tag>
                          )}
                          {!selectedBranch && (
                            <Tag color="purple">
                              T·∫•t c·∫£ nh√°nh
                            </Tag>
                          )}
                          {memberCommits.summary?.ai_powered && (
                            <Tag color="green" icon={<RobotOutlined />}>
                              ü§ñ H·ªó tr·ª£ AI
                            </Tag>
                          )}
                          {!memberCommits.summary?.ai_powered && (
                            <Tag color="orange">
                              üìù D·ª±a tr√™n m·∫´u
                            </Tag>
                          )}
                          <Text type="secondary" style={{ fontSize: '12px' }}>
                            {new Date(memberCommits.summary.analysis_date).toLocaleString()}
                          </Text>
                        </div>
                      </div>
                    } 
                    style={{ marginBottom: '20px' }}
                  >
                    <Row gutter={[16, 16]}>
                      <Col span={8}>
                        <div style={{ textAlign: 'center' }}>
                          <Title level={2} style={{ color: '#1890ff', margin: 0 }}>
                            {memberCommits.summary.total_commits}
                          </Title>                          <Text>T·ªïng Commits</Text>
                        </div>
                      </Col>
                      <Col span={8}>
                        <div style={{ textAlign: 'center' }}>
                          <Title level={2} style={{ color: '#52c41a', margin: 0 }}>
                            +{memberCommits.statistics.productivity.total_additions}
                          </Title>
                          <Text>D√≤ng code th√™m</Text>
                        </div>
                      </Col>
                      <Col span={8}>
                        <div style={{ textAlign: 'center' }}>
                          <Title level={2} style={{ color: '#f5222d', margin: 0 }}>
                            -{memberCommits.statistics.productivity.total_deletions}
                          </Title>
                          <Text>D√≤ng code x√≥a</Text>
                        </div>
                      </Col>
                    </Row>
                  </Card>

                  <Row gutter={[16, 16]}>
                    {/* Commit Types Chart */}
                    <Col xs={24} lg={12}>                      <Card title="üè∑Ô∏è Lo·∫°i Commit" size="small">
                        {chartData && (
                          <div style={{ height: '300px', display: 'flex', justifyContent: 'center' }}>
                            <Pie 
                              data={chartData} 
                              options={{ 
                                responsive: true, 
                                maintainAspectRatio: false,
                                plugins: {
                                  legend: {
                                    position: 'bottom'
                                  }
                                }
                              }} 
                            />
                          </div>
                        )}
                      </Card>
                    </Col>

                    {/* Tech Areas */}
                    <Col xs={24} lg={12}>
                      <Card title="üõ†Ô∏è Lƒ©nh v·ª±c c√¥ng ngh·ªá" size="small">
                        <div style={{ height: '300px' }}>
                          {Object.entries(memberCommits.statistics.tech_analysis).map(([tech, count]) => (
                            <div key={tech} style={{ marginBottom: '12px' }}>
                              <div style={{ display: 'flex', justifyContent: 'space-between' }}>
                                <span>{tech}</span>
                                <span>{count}</span>
                              </div>
                              <Progress 
                                percent={(count / memberCommits.summary.total_commits) * 100} 
                                size="small"
                                showInfo={false}
                              />
                            </div>
                          ))}
                        </div>
                      </Card>
                    </Col>
                  </Row>                  {/* Recent Commits */}
                  <Card title="üìù Commits g·∫ßn ƒë√¢y" style={{ marginTop: '20px' }}>
                    <List
                      dataSource={memberCommits.commits.slice(0, 10)}
                      renderItem={commit => (
                        <List.Item>
                          <List.Item.Meta
                            title={
                              <div>
                                <span style={{ marginRight: '8px' }}>
                                  {commit.message.length > 80 ? 
                                    commit.message.substring(0, 80) + '...' : 
                                    commit.message
                                  }
                                </span>
                                <Tag 
                                  color={getCommitTypeColor(commit.analysis.type)}
                                  icon={getCommitTypeIcon(commit.analysis.type)}
                                >
                                  {commit.analysis.type_icon} {commit.analysis.type}
                                </Tag>
                                <Tag color="blue">{commit.analysis.tech_area}</Tag>
                                {commit.analysis.ai_powered && (
                                  <>                                    {commit.analysis.impact && (
                                      <Tag color={commit.analysis.impact === 'high' ? 'red' : 
                                                 commit.analysis.impact === 'medium' ? 'orange' : 'green'}>
                                        T√°c ƒë·ªông: {commit.analysis.impact === 'high' ? 'Cao' : 
                                                  commit.analysis.impact === 'medium' ? 'Trung b√¨nh' : 'Th·∫•p'}
                                      </Tag>
                                    )}
                                    {commit.analysis.urgency && (
                                      <Tag color={commit.analysis.urgency === 'urgent' ? 'red' : 
                                                 commit.analysis.urgency === 'high' ? 'orange' : 'default'}>
                                        {commit.analysis.urgency === 'urgent' ? 'Kh·∫©n c·∫•p' : 
                                         commit.analysis.urgency === 'high' ? 'Cao' : commit.analysis.urgency}
                                      </Tag>
                                    )}
                                    <Tag color="green" style={{ fontSize: '10px' }}>
                                      ü§ñ AI
                                    </Tag>
                                  </>
                                )}
                              </div>
                            }
                            description={
                              <div>
                                <Text code>{commit.sha}</Text> ‚Ä¢                                <Text type="secondary">
                                  {commit.date ? 
                                    new Date(commit.date).toLocaleDateString('vi-VN') : 
                                    'Ng√†y kh√¥ng x√°c ƒë·ªãnh'
                                  }
                                </Text> ‚Ä¢ 
                                <Text style={{ color: '#52c41a' }}>+{commit.stats?.insertions || 0}</Text> 
                                <Text style={{ color: '#f5222d' }}> -{commit.stats?.deletions || 0}</Text>
                                {commit.stats?.files_changed && (
                                  <Text type="secondary"> ‚Ä¢ {commit.stats.files_changed} files</Text>
                                )}
                              </div>
                            }
                          />
                        </List.Item>
                      )}
                    />                  </Card>
                </>
              )}
            </Spin>
          )}
        </Col></Row>
    </div>
  );
};

export default RepositoryMembers;

```

### frontend\src\components\Dashboard\TaskBoard.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col } from 'antd';
import { DndContext, closestCenter } from '@dnd-kit/core';
import { SortableContext, useSortable, arrayMove } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { Task } from '../../utils/types';

const SortableTask = ({ task }) => {
  const { attributes, listeners, setNodeRef, transform, transition } = useSortable({ id: task.id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    marginBottom: 8,
  };

  return (
    <Card ref={setNodeRef} style={style} {...attributes} {...listeners}>
      <p>{task.title}</p>
      <p>Ng∆∞·ªùi ph·ª• tr√°ch: {task.assignee}</p>
    </Card>
  );
};

const TaskBoard = ({ initialTasks = [] }) => {
  const [tasks, setTasks] = useState(initialTasks);

  const onDragEnd = (event) => {
    const { active, over } = event;
    if (active.id !== over.id) {
      setTasks((items) => {
        const oldIndex = items.findIndex((item) => item.id === active.id);
        const newIndex = items.findIndex((item) => item.id === over.id);
        return arrayMove(items, oldIndex, newIndex);
      });
    }
  };

  const columns = {
    todo: { title: 'Ch·ªù x·ª≠ l√Ω', tasks: tasks.filter((task) => task.status === 'todo') },
    inProgress: { title: 'ƒêang th·ª±c hi·ªán', tasks: tasks.filter((task) => task.status === 'inProgress') },
    done: { title: 'Ho√†n th√†nh', tasks: tasks.filter((task) => task.status === 'done') },
  };

  return (
    <Card title="B·∫£ng c√¥ng vi·ªác" variant="borderless">
      <DndContext collisionDetection={closestCenter} onDragEnd={onDragEnd}>
        <Row gutter={16}>
          {Object.keys(columns).map((columnId) => (            <Col span={8} key={columnId}>
              <Card title={columns[columnId].title} variant="outlined">
                <SortableContext items={columns[columnId].tasks.map((task) => task.id)}>
                  {columns[columnId].tasks.map((task) => (
                    <SortableTask key={task.id} task={task} />
                  ))}
                </SortableContext>
              </Card>
            </Col>
          ))}
        </Row>
      </DndContext>
    </Card>
  );
};

export default TaskBoard;
```

### frontend\src\components\Dashboard\ProjectTaskManager\DragOverlayContent.jsx
```jsx
// DragOverlayContent.jsx
import React from 'react';
import { Tag } from 'antd';
import { truncateDescription } from './kanbanUtils';
import styles from './KanbanBoard.module.css';

const DragOverlayContent = ({ activeTask, getPriorityColor }) => {
  if (!activeTask) return null;

  return (
    <div className={styles.dragOverlay}>
      <div className={styles.dragOverlayContent}>
        <div className={styles.dragOverlayTitle}>
          {activeTask.title}
        </div>
        {activeTask.description && (
          <div className={styles.dragOverlayDescription}>
            {truncateDescription(activeTask.description)}
          </div>
        )}
        <div className={styles.dragOverlayFooter}>
          <Tag 
            color={getPriorityColor(activeTask.priority)} 
            className={styles.dragOverlayTag}
          >
            {activeTask.priority?.toUpperCase()}
          </Tag>
          <span className={styles.dragOverlayId}>
            #{activeTask.id}
          </span>
        </div>
      </div>
    </div>
  );
};

export default DragOverlayContent;

```

### frontend\src\components\Dashboard\ProjectTaskManager\DroppableColumn.jsx
```jsx
// DroppableColumn.jsx
import React from 'react';
import { useDroppable } from '@dnd-kit/core';
import styles from './KanbanBoard.module.css';

const DroppableColumn = ({ columnId, children }) => {
  const { setNodeRef, isOver } = useDroppable({
    id: columnId,
  });

  return (
    <div 
      ref={setNodeRef} 
      className={`${styles.tasksContainer} ${isOver ? styles.dragOver : ''}`}
    >
      {children}
    </div>
  );
};

export default DroppableColumn;

```

### frontend\src\components\Dashboard\ProjectTaskManager\FiltersPanel.jsx
```jsx
import React from 'react';
import { Row, Col, Select, Button, Space, Badge, Input, Avatar } from 'antd';
import { FilterOutlined, SearchOutlined } from '@ant-design/icons';
import { getAvatarUrl } from '../../../utils/taskUtils.jsx';

const { Option } = Select;
const { Search } = Input;

const FiltersPanel = ({
  searchText,
  setSearchText,
  statusFilter,
  setStatusFilter,
  priorityFilter,
  setPriorityFilter,
  assigneeFilter,
  setAssigneeFilter,
  collaborators,
  filteredTasks,
  resetFilters
}) => { 
  return (
    <Row gutter={16} align="middle">
      <Col span={6}>
        <Search
          placeholder="T√¨m ki·∫øm tasks..."
          value={searchText}
          onChange={(e) => setSearchText(e.target.value)}
          prefix={<SearchOutlined />}
          allowClear
        />
      </Col>
      <Col span={4}>
        <Select
          placeholder="Tr·∫°ng th√°i"
          value={statusFilter}
          onChange={setStatusFilter}
          style={{ width: '100%' }}
        >
          <Option value="all">T·∫•t c·∫£</Option>
          <Option value="todo">Ch∆∞a b·∫Øt ƒë·∫ßu</Option>
          <Option value="in_progress">ƒêang l√†m</Option>
          <Option value="done">Ho√†n th√†nh</Option>
        </Select>
      </Col>
      <Col span={4}>
        <Select
          placeholder="ƒê·ªô ∆∞u ti√™n"
          value={priorityFilter}
          onChange={setPriorityFilter}
          style={{ width: '100%' }}
        >
          <Option value="all">T·∫•t c·∫£</Option>
          <Option value="high">Cao</Option>
          <Option value="medium">Trung b√¨nh</Option>
          <Option value="low">Th·∫•p</Option>
        </Select>
      </Col>
      <Col span={4}>
        <Select
          placeholder="Ng∆∞·ªùi th·ª±c hi·ªán"
          value={assigneeFilter}
          onChange={setAssigneeFilter}
          style={{ width: '100%' }}        >
          <Option value="all">T·∫•t c·∫£</Option>
          {Array.isArray(collaborators) && collaborators.map(collab => (
            <Option key={collab.login} value={collab.login}>              <Space>
                <Avatar src={getAvatarUrl(collab.avatar_url, collab.login)} size="small" />
                {collab.login}
              </Space>
            </Option>
          ))}
        </Select>
      </Col>      <Col span={6}>
        <Space>
          <Button
            onClick={resetFilters}
            size="small"
          >
            Reset Filters
          </Button>
          <Badge count={filteredTasks?.length || 0} showZero>
            <Button icon={<FilterOutlined />}>
              K·∫øt qu·∫£ l·ªçc
            </Button>
          </Badge>
        </Space>
      </Col>
    </Row>
  );
};

export default FiltersPanel;

```

### frontend\src\components\Dashboard\ProjectTaskManager\index.js
```js
// Export all ProjectTaskManager components
export { default as RepoSelector } from './RepoSelector';
export { default as StatisticsPanel } from './StatisticsPanel';
export { default as FiltersPanel } from './FiltersPanel';
export { default as TaskList } from './TaskList';
export { default as TaskModal } from './TaskModal';
export { default as TaskCard } from './TaskCard';

```

### frontend\src\components\Dashboard\ProjectTaskManager\KanbanBoard.jsx
```jsx
// KanbanBoard.jsx
import React from 'react';
import { Typography } from 'antd';
import { DndContext, closestCenter, DragOverlay } from '@dnd-kit/core';
import { SortableContext, verticalListSortingStrategy } from '@dnd-kit/sortable';

// Import custom modules
import { COLUMN_CONFIG } from './kanbanConstants';
import { useKanbanDragDrop } from './useKanbanDragDrop';
import { getTasksByStatus } from './kanbanUtils';
import DroppableColumn from './DroppableColumn';
import SortableTaskCard from './SortableTaskCard';
import DragOverlayContent from './DragOverlayContent';

// Import CSS Module
import styles from './KanbanBoard.module.css';

const { Title } = Typography;

const KanbanBoard = ({ 
  tasks = [], 
  getAssigneeInfo, 
  getPriorityColor, 
  showTaskModal, 
  deleteTask, 
  updateTaskStatus 
}) => {
  // Safe array check
  const safeTasks = Array.isArray(tasks) ? tasks : [];

  // Use custom hook for drag & drop logic
  const {
    sensors,
    activeTask,
    handleDragStart,
    handleDragEnd,
    dropAnimation
  } = useKanbanDragDrop({ 
    tasks: safeTasks, 
    updateTaskStatus, 
    columns: COLUMN_CONFIG 
  });  return (
    <DndContext
      sensors={sensors}
      collisionDetection={closestCenter}
      onDragStart={handleDragStart}
      onDragEnd={handleDragEnd}
    >
      <div className={styles.kanbanContainer}>
        {COLUMN_CONFIG.map(column => {
          const columnTasks = getTasksByStatus(safeTasks, column.id);
          const IconComponent = column.icon;
          
          return (
            <div key={column.id} className={styles.kanbanColumn}>
              <div className={`${styles.columnHeader} ${styles[column.cssClass + 'Border']}`}>
                <Title 
                  level={5}
                  className={`${styles.columnTitle} ${styles[column.cssClass + 'Color']}`}
                >
                  <IconComponent />
                  {column.title}
                </Title>
                <div className={`${styles.taskCount} ${styles[column.cssClass + 'Bg']}`}>
                  {columnTasks.length}
                </div>
              </div>

              <DroppableColumn columnId={column.id}>
                <SortableContext 
                  items={columnTasks.map(task => task.id)}
                  strategy={verticalListSortingStrategy}
                >
                  {columnTasks.map(task => (
                    <SortableTaskCard
                      key={task.id}
                      task={task}
                      getAssigneeInfo={getAssigneeInfo}
                      getPriorityColor={getPriorityColor}
                      showTaskModal={showTaskModal}
                      deleteTask={deleteTask}
                    />
                  ))}
                </SortableContext>
                
                {/* Empty state when no tasks */}
                {columnTasks.length === 0 && (
                  <div className={styles.emptyState}>
                    <IconComponent />
                    <span>K√©o task v√†o ƒë√¢y</span>
                  </div>
                )}
              </DroppableColumn>
            </div>
          );
        })}
      </div>      {/* Drag Overlay */}
      <DragOverlay 
        adjustScale={false}
        dropAnimation={dropAnimation}
        modifiers={[]}
        style={{
          cursor: 'grabbing',
          zIndex: 1000,
          transformOrigin: '0 0',
        }}
      >
        <div style={{ 
          transform: 'translate(-10px, -10px)', // ƒêi·ªÅu ch·ªânh v·ªã tr√≠ g·∫ßn con tr·ªè h∆°n
        }}>
          <DragOverlayContent 
            activeTask={activeTask}
            getPriorityColor={getPriorityColor}
          />
        </div>
      </DragOverlay>
    </DndContext>
  );
};

export default KanbanBoard;
```

### frontend\src\components\Dashboard\ProjectTaskManager\kanbanConstants.js
```js
// kanbanConstants.js
import { ClockCircleOutlined, ExclamationCircleOutlined, CheckCircleOutlined } from '@ant-design/icons';

export const COLUMN_CONFIG = [
  {
    id: 'TODO',
    title: 'To Do',
    icon: ClockCircleOutlined,
    color: '#faad14',
    bgColor: '#faad14',
    borderColor: '#faad14',
    cssClass: 'todo'
  },
  {
    id: 'IN_PROGRESS',
    title: 'In Progress',
    icon: ExclamationCircleOutlined,
    color: '#1890ff',
    bgColor: '#1890ff',
    borderColor: '#1890ff',
    cssClass: 'inProgress'
  },
  {
    id: 'DONE',
    title: 'Done',
    icon: CheckCircleOutlined,
    color: '#52c41a',
    bgColor: '#52c41a',
    borderColor: '#52c41a',
    cssClass: 'done'
  }
];

export const DRAG_CONFIG = {
  ACTIVATION_DISTANCE: 0,
  DROP_ANIMATION: {
    duration: 200,
    easing: 'cubic-bezier(0.18, 0.67, 0.6, 1.22)',
  }
};

export const TASK_CARD_CONFIG = {
  DESCRIPTION_MAX_LENGTH: 35,
  AVATAR_SIZE: 24
};

```

### frontend\src\components\Dashboard\ProjectTaskManager\kanbanUtils.js
```js
// kanbanUtils.js
import { TASK_CARD_CONFIG } from './kanbanConstants';

/**
 * L·ªçc tasks theo status
 */
export const getTasksByStatus = (tasks, status) => {
  const filtered = tasks.filter(task => task.status === status);
  return filtered;
};

/**
 * Truncate description 
 */
export const truncateDescription = (description) => {
  if (!description) return '';
  
  return description.length > TASK_CARD_CONFIG.DESCRIPTION_MAX_LENGTH
    ? description.substring(0, TASK_CARD_CONFIG.DESCRIPTION_MAX_LENGTH) + '...'
    : description;
};

/**
 * Format date cho display
 */
export const formatDate = (dateString) => {
  if (!dateString) return null;
  
  return new Date(dateString).toLocaleDateString('vi-VN', { 
    month: 'short', 
    day: 'numeric' 
  });
};

/**
 * Format full date cho tooltip
 */
export const formatFullDate = (dateString) => {
  if (!dateString) return '';
  
  return new Date(dateString).toLocaleDateString('vi-VN');
};

```

### frontend\src\components\Dashboard\ProjectTaskManager\RepoSelector.jsx
```jsx
import React, { useState } from 'react';
import { Select, Avatar, Space, Tag, Button, Card } from 'antd';
import { BranchesOutlined, TeamOutlined, SyncOutlined } from '@ant-design/icons';

const { Option } = Select;

const RepoSelector = ({ 
  repositories, 
  selectedRepo, 
  loading, 
  handleRepoChange,
  branches = [],
  collaborators = [],
  branchesLoading = false,
  onSyncCollaborators = null,
  onSyncBranches = null     
}) => {
  const [isSyncingCollaborators, setIsSyncingCollaborators] = useState(false);
  const [isSyncingBranches, setIsSyncingBranches] = useState(false);
  const handleRepoSelect = async (repoId) => {
    await handleRepoChange(repoId);
  };

  const handleSyncCollaborators = async () => {
    if (!onSyncCollaborators || !selectedRepo || isSyncingCollaborators) return;
    
    setIsSyncingCollaborators(true);
    try {
      await onSyncCollaborators();
    } catch (error) {
      console.error('Sync collaborators failed:', error);
    } finally {
      setIsSyncingCollaborators(false);
    }
  };

  const handleSyncBranches = async () => {
    if (!onSyncBranches || !selectedRepo || isSyncingBranches) return;
    
    setIsSyncingBranches(true);
    try {
      await onSyncBranches();
    } catch (error) {
      console.error('Sync branches failed:', error);
    } finally {
      setIsSyncingBranches(false);
    }
  };

  return (
    <div>
      <Select
        style={{ width: '100%' }}
        placeholder="Ch·ªçn repository ƒë·ªÉ qu·∫£n l√Ω tasks"
        loading={loading}
        value={selectedRepo?.id}
        onChange={handleRepoSelect}
        showSearch
        optionFilterProp="children"
      >        {repositories.map(repo => (
          <Option key={repo.id} value={repo.id}>
            <Space>
              <Avatar 
                src={repo.owner?.avatar_url} 
                size="small"
                style={{ backgroundColor: '#1890ff' }}
              >
                {!repo.owner?.avatar_url && repo.owner?.login?.charAt(0)?.toUpperCase()}
              </Avatar>
              <span style={{ fontWeight: 500 }}>
                {repo.owner?.login}/{repo.name}
              </span>
            </Space>
          </Option>
        ))}
      </Select>

      {selectedRepo && (
        <Card 
          size="small" 
          style={{ 
            marginTop: 16, 
            borderRadius: 8,
            background: 'linear-gradient(135deg, #f6f8fa 0%, #e1e7ed 100%)'
          }}
        >
          <Space direction="vertical" style={{ width: '100%' }}>            <Space>
              <Avatar 
                src={selectedRepo.owner?.avatar_url} 
                size="small"
                style={{ backgroundColor: '#1890ff' }}
              >
                {!selectedRepo.owner?.avatar_url && selectedRepo.owner?.login?.charAt(0)?.toUpperCase()}
              </Avatar>
              <span style={{ fontWeight: 600, color: '#1890ff' }}>
                {selectedRepo.owner?.login}/{selectedRepo.name}
              </span>
              {selectedRepo.private && (
                <Tag color="orange" size="small">Private</Tag>
              )}
            </Space>            <Space style={{ width: '100%', justifyContent: 'space-between' }}>
              <Space>
                <BranchesOutlined style={{ color: '#52c41a', fontSize: '14px' }} />
                <span style={{ fontSize: '13px', color: '#666', fontWeight: '500' }}>
                  {branchesLoading ? 'Loading...' : `${branches.length} branches`}
                </span>
                {onSyncBranches && (
                  <Button
                    size="small"
                    type="text"
                    icon={<SyncOutlined spin={isSyncingBranches} />}
                    onClick={handleSyncBranches}
                    disabled={!selectedRepo || isSyncingBranches}
                    style={{ 
                      fontSize: '12px',
                      height: '20px',
                      padding: '0 4px',
                      color: '#52c41a'
                    }}
                    title="Sync branches t·ª´ GitHub"
                  />
                )}
              </Space>

              <Space>
                <TeamOutlined style={{ color: '#1890ff', fontSize: '14px' }} />
                <span style={{ fontSize: '13px', color: '#666', fontWeight: '500' }}>
                  {collaborators.length} collaborators
                </span>
                {onSyncCollaborators && (
                  <Button
                    size="small"
                    type="text"
                    icon={<SyncOutlined spin={isSyncingCollaborators} />}
                    onClick={handleSyncCollaborators}
                    disabled={!selectedRepo || isSyncingCollaborators}
                    style={{ 
                      fontSize: '12px',
                      height: '20px',
                      padding: '0 4px',
                      color: '#1890ff'
                    }}
                    title="Sync collaborators t·ª´ GitHub"
                  />
                )}
              </Space>
            </Space>            <div style={{ 
              fontSize: '11px', 
              color: '#999', 
              textAlign: 'center',
              marginTop: '8px'
            }}>
              üí° D·ªØ li·ªáu t·ª± ƒë·ªông t·∫£i t·ª´ database, ·∫•n sync ƒë·ªÉ c·∫≠p nh·∫≠t t·ª´ GitHub
            </div>
          </Space>
        </Card>
      )}
    </div>
  );
};

export default RepoSelector;

```

### frontend\src\components\Dashboard\ProjectTaskManager\SortableTaskCard.jsx
```jsx
// SortableTaskCard.jsx
import React from 'react';
import { Card, Avatar, Tag, Space, Typography, Button, Tooltip } from 'antd';
import { EditOutlined, DeleteOutlined, UserOutlined, CalendarOutlined } from '@ant-design/icons';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { formatDate, formatFullDate } from './kanbanUtils';
import { getAvatarUrl } from '../../../utils/taskUtils.jsx';
import { TASK_CARD_CONFIG } from './kanbanConstants';
import styles from './KanbanBoard.module.css';

const { Text } = Typography;

const SortableTaskCard = ({ 
  task, 
  getAssigneeInfo, 
  getPriorityColor, 
  showTaskModal, 
  deleteTask 
}) => {  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging,
  } = useSortable({ 
    id: task.id,
  });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition: transition || 'transform 200ms cubic-bezier(0.18, 0.67, 0.6, 1.22)',
    opacity: isDragging ? 0.5 : 1,
    zIndex: isDragging ? 999 : 'auto',
    cursor: isDragging ? 'grabbing' : 'grab',
    touchAction: 'none',
  };

  const assigneeInfo = getAssigneeInfo(task.assignee);

  return (    <Card
      ref={setNodeRef}
      style={style}
      {...attributes}
      {...listeners}
      className={`${styles.taskCard} ${isDragging ? styles.isDragging : ''}`}
      styles={{ body: { padding: 12 } }}
    >
      <div className={styles.taskCardHeader}>
        <div className={styles.taskTitle}>{task.title}</div>
        <div className={styles.taskActions}>
          <Tooltip title="Ch·ªânh s·ª≠a">
            <Button
              type="text"
              size="small"
              icon={<EditOutlined />}
              onClick={(e) => {
                e.stopPropagation();
                showTaskModal(task);
              }}
            />
          </Tooltip>
          <Tooltip title="X√≥a">
            <Button
              type="text"
              size="small"
              danger
              icon={<DeleteOutlined />}
              onClick={(e) => {
                e.stopPropagation();
                deleteTask(task.id);
              }}
            />
          </Tooltip>
        </div>
      </div>

      {task.description && (
        <Text className={styles.taskDescription}>
          {task.description}
        </Text>
      )}

      <div className={styles.taskMeta}>
        <Space size="small">
          <Tag color={getPriorityColor(task.priority)} style={{ margin: 0 }}>
            {task.priority?.toUpperCase()}
          </Tag>
          {task.due_date && (
            <Tooltip title={`H·∫°n: ${formatFullDate(task.due_date)}`}>
              <div style={{ display: 'flex', alignItems: 'center', gap: 4, color: '#666' }}>
                <CalendarOutlined style={{ fontSize: 12 }} />
                <Text style={{ fontSize: 11, color: '#666' }}>
                  {formatDate(task.due_date)}
                </Text>
              </div>
            </Tooltip>
          )}
        </Space>
      </div>

      <div className={styles.taskFooter}>        <Space>
          <Avatar 
            size={TASK_CARD_CONFIG.AVATAR_SIZE} 
            src={getAvatarUrl(assigneeInfo.avatar_url, assigneeInfo.login)} 
            icon={<UserOutlined />}
          />
          <Text style={{ fontSize: 12, color: '#666' }}>
            {assigneeInfo.login}
          </Text>
        </Space>
        <Text style={{ fontSize: 11, color: '#999' }}>
          #{task.id}
        </Text>
      </div>
    </Card>
  );
};

export default SortableTaskCard;

```

### frontend\src\components\Dashboard\ProjectTaskManager\StatisticsPanel.jsx
```jsx
import React from 'react';
import { Card, Row, Col, Statistic, Progress } from 'antd';
import { BarChartOutlined, CheckCircleOutlined, ExclamationCircleOutlined } from '@ant-design/icons';

const StatisticsPanel = ({ stats = {} }) => {
  // Fallback values n·∫øu stats undefined
  const safeStats = {
    total: stats?.total || 0,
    completed: stats?.completed || 0,
    inProgress: stats?.inProgress || 0,
    todo: stats?.todo || 0,
    completionPercentage: stats?.completionPercentage || 0
  };

  return (
    <div style={{ marginBottom: 16 }}>
      <Row gutter={16}>
        <Col span={6}>
          <Card size="small">
            <Statistic 
              title="T·ªïng tasks" 
              value={safeStats.total}
              prefix={<BarChartOutlined />}
            />
          </Card>
        </Col>
        <Col span={6}>
          <Card size="small">
            <Statistic 
              title="Ho√†n th√†nh" 
              value={safeStats.completed}
              valueStyle={{ color: '#52c41a' }}
              prefix={<CheckCircleOutlined />}
            />
          </Card>
        </Col>        <Col span={6}>
          <Card size="small">
            <Statistic 
              title="ƒêang l√†m" 
              value={safeStats.inProgress}
              valueStyle={{ color: '#1890ff' }}
              prefix={<ExclamationCircleOutlined />}
            />
          </Card>
        </Col>
        <Col span={6}>
          <Card size="small">
            <Statistic 
              title="T·ª∑ l·ªá ho√†n th√†nh" 
              value={safeStats.completionPercentage}
              suffix="%"
              valueStyle={{ color: safeStats.completionPercentage > 70 ? '#52c41a' : '#fa8c16' }}
            />
            <Progress 
              percent={safeStats.completionPercentage} 
              showInfo={false}
              size="small"
              strokeColor={safeStats.completionPercentage > 70 ? '#52c41a' : '#fa8c16'}
            />
          </Card>
        </Col>
      </Row>
    </div>
  );
};

export default StatisticsPanel;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskCard.jsx
```jsx
import React from 'react';
import { Card, Space, Button, Tooltip, Tag, Avatar, Select } from 'antd';
import { EditOutlined, DeleteOutlined, UserOutlined, CalendarOutlined } from '@ant-design/icons';
import { getAvatarUrl } from '../../../utils/taskUtils.jsx';
import styled from 'styled-components';

const { Option } = Select;

const TaskHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 8px;
`;

const TaskActions = styled.div`
  display: flex;
  gap: 8px;
`;

const PriorityTag = styled(Tag)`
  font-weight: 500;
`;

const TaskCard = ({
  task,
  getAssigneeInfo,
  getStatusIcon,
  getPriorityColor,
  updateTaskStatus,
  showTaskModal,
  deleteTask
}) => {
  const assigneeInfo = getAssigneeInfo(task.assignee);
  return (
    <Card size="small">
      <TaskHeader>
        <Space>
          {getStatusIcon(task.status)}
          <strong>{task.title}</strong>
          <PriorityTag color={getPriorityColor(task.priority)}>
            {task.priority?.toUpperCase()}
          </PriorityTag>
        </Space>
        <TaskActions>
          <Tooltip title="Ch·ªânh s·ª≠a">
            <Button 
              size="small" 
              icon={<EditOutlined />}
              onClick={() => showTaskModal(task)}
            />
          </Tooltip>
          <Tooltip title="X√≥a">
            <Button 
              size="small" 
              danger
              icon={<DeleteOutlined />}
              onClick={() => deleteTask(task.id)}
            />
          </Tooltip>
        </TaskActions>
      </TaskHeader>
      <div style={{ marginBottom: 8 }}>
        {task.description}
      </div>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
        <Space>          <Avatar 
            src={getAvatarUrl(assigneeInfo.avatar_url, assigneeInfo.login || assigneeInfo.github_username)} 
            icon={<UserOutlined />}
            size="small"
          /><div style={{ display: 'flex', flexDirection: 'column' }}>
            <span style={{ fontSize: 12, fontWeight: 500 }}>
              {assigneeInfo.display_name || assigneeInfo.login || assigneeInfo.github_username}
            </span>
            {(assigneeInfo.type || assigneeInfo.is_owner || assigneeInfo.role) && (
              <Tag 
                size="small" 
                color={
                  assigneeInfo.is_owner ? 'gold' : 
                  assigneeInfo.type === 'Owner' ? 'gold' : 
                  'blue'
                }
                style={{ fontSize: '9px', marginTop: 2 }}
              >
                {assigneeInfo.is_owner ? 'Owner' : assigneeInfo.type || assigneeInfo.role}
              </Tag>
            )}
          </div>
        </Space>
        <Space>
          {task.due_date && (
            <Space style={{ fontSize: 12, color: '#666' }}>
              <CalendarOutlined />
              {task.due_date}
            </Space>
          )}
          <Select 
            size="small"
            value={task.status}
            onChange={(newStatus) => updateTaskStatus(task.id, newStatus)}
            style={{ width: 100 }}
          >
            <Option value="todo">To Do</Option>
            <Option value="in_progress">ƒêang l√†m</Option>
            <Option value="done">Ho√†n th√†nh</Option>
          </Select>
        </Space>
      </div>
    </Card>
  );
};

export default TaskCard;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskList.jsx
```jsx
import React from 'react';
import { List, Empty, Spin } from 'antd';
import TaskCard from './TaskCard';

const TaskList = ({ filteredTasks = [], tasksLoading = false, getAssigneeInfo, getStatusIcon, getPriorityColor, updateTaskStatus, showTaskModal, deleteTask }) => {
  // Double safety check
  const safeTasks = Array.isArray(filteredTasks) ? filteredTasks : [];
  
  return (
    <Spin spinning={tasksLoading}>
      {safeTasks.length === 0 ? (
        <Empty 
          description="Ch∆∞a c√≥ task n√†o cho repository n√†y"
          image={Empty.PRESENTED_IMAGE_SIMPLE}
        />
      ) : (
        <List
          dataSource={safeTasks}
        renderItem={task => (
          <TaskCard
            task={task}
            getAssigneeInfo={getAssigneeInfo}
            getStatusIcon={getStatusIcon}
            getPriorityColor={getPriorityColor}            updateTaskStatus={updateTaskStatus}
            showTaskModal={showTaskModal}
            deleteTask={deleteTask}
          />
        )}
      />
    )}
  </Spin>
  );
};

export default TaskList;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskModal.jsx
```jsx
import React from 'react';
import { 
  Modal, Form, Input, Select, DatePicker, Button, Space, Avatar, Tag, 
  Card, Divider, Typography, Row, Col 
} from 'antd';
import { 
  UserOutlined, CalendarOutlined, FlagOutlined, 
  FileTextOutlined, TeamOutlined 
} from '@ant-design/icons';
import { getAvatarUrl } from '../../../utils/taskUtils.jsx';

const { Option } = Select;
const { TextArea } = Input;
const { Title, Text } = Typography;

const TaskModal = ({
  isModalVisible,
  editingTask,
  form,
  handleTaskSubmit,
  setIsModalVisible,
  collaborators
}) => {
  console.log('üéØ TaskModal rendered with collaborators:', collaborators);
  console.log('üéØ TaskModal collaborators type:', typeof collaborators);
  console.log('üéØ TaskModal collaborators isArray:', Array.isArray(collaborators));

  // Get current user and ensure they're in the assignee list
  const getCurrentUser = () => {
    try {
      const profile = JSON.parse(localStorage.getItem('github_profile') || '{}');
      return {
        login: profile.login,
        github_username: profile.login,
        avatar_url: profile.avatar_url,
        display_name: profile.name || profile.login,
        is_current_user: true
      };
    } catch {
      return null;
    }
  };

  const currentUser = getCurrentUser();
  
  // Combine current user with collaborators, avoiding duplicates
  const allAssignees = (() => {
    const assignees = Array.isArray(collaborators) ? [...collaborators] : [];
    
    if (currentUser && !assignees.some(c => c.login === currentUser.login)) {
      assignees.unshift(currentUser); // Add current user at the beginning
    }
    
    return assignees;
  })();
    return (
    <Modal
      title={null}
      open={isModalVisible}
      onCancel={() => setIsModalVisible(false)}
      footer={null}
      width={600}
      style={{ top: 20 }}
    >
      <div style={{ padding: '20px 0' }}>
        {/* Modal Header */}
        <div style={{ 
          textAlign: 'center', 
          marginBottom: 30,
          background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
          margin: '-24px -24px 30px -24px',
          padding: '20px 24px',
          borderRadius: '8px 8px 0 0'
        }}>
          <Title level={3} style={{ color: 'white', margin: 0 }}>
            {editingTask ? "‚úèÔ∏è Ch·ªânh s·ª≠a Task" : "‚ûï T·∫°o Task M·ªõi"}
          </Title>
          <Text style={{ color: 'rgba(255,255,255,0.8)', fontSize: '14px' }}>
            {editingTask ? "C·∫≠p nh·∫≠t th√¥ng tin task" : "T·∫°o task m·ªõi cho d·ª± √°n"}
          </Text>
        </div>

        <Form
          form={form}
          layout="vertical"
          onFinish={handleTaskSubmit}
          size="large"
        >
          {/* Task Title */}
          <Card 
            size="small" 
            title={
              <Space>
                <FileTextOutlined style={{ color: '#1890ff' }} />
                <span>Th√¥ng tin c∆° b·∫£n</span>
              </Space>
            }
            style={{ marginBottom: 20, borderRadius: 8 }}
          >
            <Form.Item
              name="title"
              label="Ti√™u ƒë·ªÅ Task"
              rules={[{ required: true, message: 'Vui l√≤ng nh·∫≠p ti√™u ƒë·ªÅ!' }]}
            >
              <Input 
                placeholder="Nh·∫≠p ti√™u ƒë·ªÅ task..." 
                style={{ borderRadius: 6 }}
              />
            </Form.Item>
            
            <Form.Item
              name="description"
              label="M√¥ t·∫£ chi ti·∫øt"
              rules={[{ required: true, message: 'Vui l√≤ng nh·∫≠p m√¥ t·∫£!' }]}
            >
              <TextArea 
                rows={4} 
                placeholder="M√¥ t·∫£ chi ti·∫øt v·ªÅ task n√†y..."
                style={{ borderRadius: 6 }}
              />
            </Form.Item>
          </Card>

          {/* Assignment & Priority */}
          <Card 
            size="small" 
            title={
              <Space>
                <TeamOutlined style={{ color: '#52c41a' }} />
                <span>Ph√¢n c√¥ng & ∆Øu ti√™n</span>
              </Space>
            }
            style={{ marginBottom: 20, borderRadius: 8 }}
          >
            <Row gutter={16}>
              <Col span={12}>
                <Form.Item
                  name="assignee"
                  label="Giao cho"
                  rules={[{ required: true, message: 'Vui l√≤ng ch·ªçn ng∆∞·ªùi th·ª±c hi·ªán!' }]}
                >
                  <Select 
                    placeholder="Ch·ªçn th√†nh vi√™n..."
                    showSearch
                    optionFilterProp="children"
                    style={{ borderRadius: 6 }}
                    filterOption={(input, option) =>
                      option.children.props.children[1].toLowerCase().indexOf(input.toLowerCase()) >= 0
                    }                  >                    {allAssignees.map((collab, index) => {
                      const username = collab.login || collab.github_username;
                      const uniqueKey = username ? `${username}-${index}` : `unknown-${index}`;
                      
                      return (
                        <Option 
                          key={uniqueKey}
                          value={username}
                        >
                          <Space>
                            <Avatar src={getAvatarUrl(collab.avatar_url, username)} size="small" />
                            <span>{collab.display_name || username || 'Unknown User'}</span>
                            {collab.is_current_user && <Tag color="green" size="small">B·∫£n th√¢n</Tag>}
                            {collab.is_owner && <Tag color="gold" size="small">Owner</Tag>}
                            {collab.role && !collab.is_owner && <Tag color="blue" size="small">{collab.role}</Tag>}
                          </Space>
                        </Option>
                      );
                    })}
                  </Select>
                </Form.Item>
              </Col>
              
              <Col span={12}>
                <Form.Item
                  name="priority"
                  label="ƒê·ªô ∆∞u ti√™n"
                  rules={[{ required: true, message: 'Vui l√≤ng ch·ªçn ƒë·ªô ∆∞u ti√™n!' }]}
                >
                  <Select placeholder="Ch·ªçn ƒë·ªô ∆∞u ti√™n..." style={{ borderRadius: 6 }}>
                    <Option value="low">
                      <Space>
                        <FlagOutlined style={{ color: '#52c41a' }} />
                        <Tag color="#52c41a">Th·∫•p</Tag>
                      </Space>
                    </Option>
                    <Option value="medium">
                      <Space>
                        <FlagOutlined style={{ color: '#fa8c16' }} />
                        <Tag color="#fa8c16">Trung b√¨nh</Tag>
                      </Space>
                    </Option>
                    <Option value="high">
                      <Space>
                        <FlagOutlined style={{ color: '#f5222d' }} />
                        <Tag color="#f5222d">Cao</Tag>
                      </Space>
                    </Option>
                  </Select>
                </Form.Item>
              </Col>
            </Row>
          </Card>

          {/* Due Date */}
          <Card 
            size="small" 
            title={
              <Space>
                <CalendarOutlined style={{ color: '#fa8c16' }} />
                <span>Th·ªùi gian</span>
              </Space>
            }
            style={{ marginBottom: 20, borderRadius: 8 }}
          >
            <Form.Item
              name="dueDate"
              label="H·∫°n ho√†n th√†nh"
            >
              <DatePicker 
                style={{ width: '100%', borderRadius: 6 }} 
                placeholder="Ch·ªçn ng√†y h·∫øt h·∫°n..."
              />
            </Form.Item>
          </Card>

          {/* Action Buttons */}
          <div style={{ 
            display: 'flex', 
            justifyContent: 'flex-end', 
            gap: 12,
            marginTop: 30,
            paddingTop: 20,
            borderTop: '1px solid #f0f0f0'
          }}>
            <Button 
              size="large"
              onClick={() => setIsModalVisible(false)}
              style={{ minWidth: 100, borderRadius: 6 }}
            >
              H·ªßy b·ªè
            </Button>
            <Button 
              type="primary" 
              htmlType="submit"
              size="large"
              style={{ 
                minWidth: 120, 
                borderRadius: 6,
                background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
                border: 'none'
              }}
            >
              {editingTask ? 'üíæ C·∫≠p nh·∫≠t' : 'üöÄ T·∫°o Task'}
            </Button>
          </div>
        </Form>
      </div>
    </Modal>
  );
};

export default TaskModal;

```

### frontend\src\components\Dashboard\ProjectTaskManager\useKanbanDragDrop.js
```js
// useKanbanDragDrop.js
import { useState } from 'react';
import { useSensor, useSensors, PointerSensor, KeyboardSensor } from '@dnd-kit/core';
import { DRAG_CONFIG } from './kanbanConstants';

export const useKanbanDragDrop = ({ tasks, updateTaskStatus, columns }) => {
  const [activeId, setActiveId] = useState(null);
  // C·∫•u h√¨nh sensors ƒë·ªÉ cursor g·∫ßn h∆°n v·ªõi task
  const sensors = useSensors(
    useSensor(PointerSensor, {
      activationConstraint: {
        distance: DRAG_CONFIG.ACTIVATION_DISTANCE,
      },
      // ƒêi·ªÅu ch·ªânh ƒë·ªÉ cursor g·∫ßn task h∆°n
      coordinateGetter: (event) => ({
        x: event.clientX,
        y: event.clientY,
      }),
    }),
    useSensor(KeyboardSensor)
  );

  const handleDragStart = (event) => {
    setActiveId(event.active.id);
  };
  const handleDragEnd = (event) => {
    const { active, over } = event;
    setActiveId(null);
    
    if (!over) return;

    const draggedTaskId = active.id;
    const targetId = over.id;

    try {
      // T√¨m task ƒëang ƒë∆∞·ª£c k√©o
      const draggedTask = tasks.find(t => t.id === draggedTaskId);
      if (!draggedTask) {
        console.error(`Dragged task with ID ${draggedTaskId} not found`);
        return;
      }

      // Ki·ªÉm tra xem target c√≥ ph·∫£i l√† column kh√¥ng
      const targetColumn = columns.find(col => col.id === targetId);
      
      if (targetColumn) {
        // K√©o v√†o column
        if (draggedTask.status !== targetColumn.id) {
          console.log(`Moving task ${draggedTaskId} to column ${targetColumn.id}`);
          updateTaskStatus(draggedTaskId, targetColumn.id);
        }
      } else {
        // K√©o v√†o task kh√°c - t√¨m column ch·ª©a task ƒë√≥
        const targetTask = tasks.find(t => t.id === targetId);
        if (targetTask && draggedTask.status !== targetTask.status) {
          console.log(`Moving task ${draggedTaskId} to column ${targetTask.status} (via task)`);
          updateTaskStatus(draggedTaskId, targetTask.status);
        }
      }
    } catch (error) {
      console.error('Error in handleDragEnd:', error);
      // Kh√¥ng show message error ƒë·ªÉ tr√°nh l√†m crash UI
    }
  };

  const activeTask = activeId ? tasks.find(t => t.id === activeId) : null;

  return {
    sensors,
    activeId,
    activeTask,
    handleDragStart,
    handleDragEnd,
    dropAnimation: DRAG_CONFIG.DROP_ANIMATION
  };
};

```

### frontend\src\components\repo\RepoList.jsx
```jsx
import { useEffect, useState } from "react";
import { Avatar, Typography, Spin, message, Card, Tag, Pagination } from "antd";
import { useNavigate } from "react-router-dom";
import { GithubOutlined, StarFilled, EyeFilled, ForkOutlined, CalendarOutlined } from "@ant-design/icons";
import styled from "styled-components";
import axios from "axios";

const { Title, Text } = Typography;

const RepoContainer = styled.div`
  max-width: 900px;
  margin: 0 auto;
  padding: 24px;
`;

const RepoCard = styled(Card)`
  margin-bottom: 20px;
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
  cursor: pointer;
  border: none;
  
  &:hover {
    box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    transform: translateY(-5px);
  }
`;

const RepoHeader = styled.div`
  display: flex;
  align-items: flex-start;
  margin-bottom: 12px;
`;

const RepoTitle = styled.div`
  flex: 1;
  min-width: 0;
`;

const RepoName = styled(Text)`
  display: block;
  font-size: 18px;
  font-weight: 600;
  color: #24292e;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
`;

const RepoDescription = styled(Text)`
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
  color: #586069;
  margin: 8px 0;
`;

const RepoMeta = styled.div`
  display: flex;
  flex-wrap: wrap;
  gap: 16px;
  margin-top: 16px;
  align-items: center;
`;

const MetaItem = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 14px;
  color: #586069;
`;

const StyledPagination = styled(Pagination)`
  margin-top: 32px;
  text-align: center;
  
  .ant-pagination-item-active {
    border-color: #1890ff;
    background: #1890ff;
    
    a {
      color: white;
    }
  }
`;

const HighlightTag = styled(Tag)`
  font-weight: 500;
  border-radius: 12px;
  padding: 0 10px;
`;

const RepoList = () => {
  const [repos, setRepos] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const [totalRepos, setTotalRepos] = useState(0);
  const navigate = useNavigate();
  const pageSize = 8;

  useEffect(() => {
    const fetchRepos = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) return message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");

      try {
        setLoading(true);
        const response = await axios.get("http://localhost:8000/api/github/repos", {
          headers: { Authorization: `token ${token}` },
          params: { sort: 'updated', direction: 'desc' } // S·∫Øp x·∫øp theo m·ªõi nh·∫•t
        });
        
        // S·∫Øp x·∫øp l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o m·ªõi nh·∫•t l√™n ƒë·∫ßu
        const sortedRepos = response.data.sort((a, b) => 
          new Date(b.updated_at) - new Date(a.updated_at)
        );
        
        setRepos(sortedRepos);
        setTotalRepos(sortedRepos.length);
      } catch (error) {
        message.error("Kh√¥ng th·ªÉ t·∫£i danh s√°ch repository!");
        console.error(error);
      } finally {
        setLoading(false);
      }
    };

    fetchRepos();
  }, []);

  const formatDate = (dateString) => {
    return new Date(dateString).toLocaleDateString('vi-VN', {
      day: '2-digit',
      month: '2-digit',
      year: 'numeric'
    });
  };

  const paginatedRepos = repos.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );
  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', marginTop: '100px' }}>
        <Spin size="large" />
        <div style={{ marginLeft: 16 }}>
          <Text>ƒêang t·∫£i d·ªØ li·ªáu...</Text>
        </div>
      </div>
    );
  }

  return (
    <RepoContainer>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '24px' }}>
        <Title level={2} style={{ margin: 0, color: '#24292e' }}>
          <GithubOutlined style={{ marginRight: '12px', color: '#1890ff' }} />
          GitHub Repositories
        </Title>
        <Text strong style={{ fontSize: '16px' }}>
          T·ªïng c·ªông: {totalRepos} repositories
        </Text>
      </div>

      {paginatedRepos.map((repo) => (
        <RepoCard 
          key={repo.id} 
          onClick={() => navigate(`/repo/${repo.owner.login}/${repo.name}`)}
        >
          <RepoHeader>
            <Avatar 
              src={repo.owner.avatar_url} 
              size={48}
              style={{ marginRight: '16px', flexShrink: 0 }}
            />
            <RepoTitle>
              <div style={{ display: 'flex', alignItems: 'center' }}>
                <RepoName>{repo.name}</RepoName>
                {repo.private ? (
                  <HighlightTag color="error" style={{ marginLeft: '12px' }}>
                    Private
                  </HighlightTag>
                ) : (
                  <HighlightTag color="success" style={{ marginLeft: '12px' }}>
                    Public
                  </HighlightTag>
                )}
              </div>
              
              <RepoDescription type="secondary">
                {repo.description || "Kh√¥ng c√≥ m√¥ t·∫£"}
              </RepoDescription>
            </RepoTitle>
          </RepoHeader>

          <RepoMeta>
            <MetaItem>
              <StarFilled style={{ color: '#ffc53d' }} />
              <Text strong>{repo.stargazers_count}</Text>
              <Text>stars</Text>
            </MetaItem>
            
            <MetaItem>
              <EyeFilled style={{ color: '#1890ff' }} />
              <Text strong>{repo.watchers_count}</Text>
              <Text>watchers</Text>
            </MetaItem>
            
            <MetaItem>
              <ForkOutlined style={{ color: '#73d13d' }} />
              <Text strong>{repo.forks_count}</Text>
              <Text>forks</Text>
            </MetaItem>
            
            {repo.language && (
              <MetaItem>
                <div style={{
                  width: 12,
                  height: 12,
                  borderRadius: '50%',
                  backgroundColor: '#1890ff',
                  marginRight: 6
                }} />
                <Text>{repo.language}</Text>
              </MetaItem>
            )}
            
            <MetaItem style={{ marginLeft: 'auto' }}>
              <CalendarOutlined />
              <Text>C·∫≠p nh·∫≠t: {formatDate(repo.updated_at)}</Text>
            </MetaItem>
          </RepoMeta>
        </RepoCard>
      ))}

      <StyledPagination
        current={currentPage}
        pageSize={pageSize}
        total={totalRepos}
        onChange={(page) => setCurrentPage(page)}
        showSizeChanger={false}
        showQuickJumper
      />
    </RepoContainer>
  );
};

export default RepoList;
```

### frontend\src\contexts\SyncContext.jsx
```jsx

```

### frontend\src\features\github\GithubRepoFetcher.jsx
```jsx

```

### frontend\src\hooks\useCommits.js
```js

```

### frontend\src\hooks\useProjectData.js
```js
// frontend/src/hooks/useProjectData.js
import { useState, useEffect, useCallback } from 'react';
import { message } from 'antd';
import { repositoryAPI, taskAPI, collaboratorAPI, branchAPI } from '../services/api';

// ==================== AUTHENTICATION HELPER ====================
const redirectToLogin = () => {
  window.location.href = '/login';
};

const checkAuthentication = () => {
  const token = localStorage.getItem('access_token');
  if (!token) {
    message.error('üîí Vui l√≤ng ƒëƒÉng nh·∫≠p ƒë·ªÉ ti·∫øp t·ª•c');
    redirectToLogin();
    return false;
  }
  return true;
};

// ==================== REPOSITORY HOOK ====================
export const useRepositories = (dataSourcePreference = 'auto') => {
  const [repositories, setRepositories] = useState([]);
  const [loading, setLoading] = useState(false);
  const [dataSource, setDataSource] = useState('database');  const fetchRepositories = useCallback(async () => {
    console.log('fetchRepositories: checking authentication...'); // Debug
    
    // Check authentication before making API calls
    if (!checkAuthentication()) {
      setRepositories([]);
      return;
    }
    
    const token = localStorage.getItem('access_token');
    console.log('fetchRepositories: token preview:', token ? `${token.substring(0, 10)}...` : 'No token'); // Debug

    setLoading(true);
    try {
      let result;
      
      // Handle forced data source preference
      if (dataSourcePreference === 'database') {
        console.log('Fetching from database...'); // Debug
        const data = await repositoryAPI.getFromDatabase();
        console.log('Database result:', data); // Debug
        result = { data, source: 'database' };
      } else if (dataSourcePreference === 'github') {
        const data = await repositoryAPI.getFromGitHub();
        result = { data, source: 'github' };
      } else {
        // Auto mode - intelligent fallback
        result = await repositoryAPI.getIntelligent();
      }
      
      setRepositories(result.data);
      setDataSource(result.source);
      
      console.log('Repositories loaded:', result.data.length, 'repos'); // Debug
      
      // User feedback
      if (result.source === 'github') {
        message.info('üì° Repositories loaded from GitHub API');
      } else {
        message.success('üíæ Repositories loaded from local database');
      }    } catch (error) {
      console.error('Error fetching repositories:', error);
      console.error('Error details:', {
        status: error.response?.status,
        statusText: error.response?.statusText,
        data: error.response?.data,
        message: error.message,
        code: error.code
      });
      setRepositories([]);      if (error.response?.status === 401) {
        message.error('üîí Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. ƒêang chuy·ªÉn h∆∞·ªõng ƒë·∫øn trang ƒëƒÉng nh·∫≠p...');
        localStorage.removeItem('access_token');
        setTimeout(() => redirectToLogin(), 1500);
      } else if (error.response?.status === 404 && error.config?.url?.includes('/repositories')) {
        // 404 on repositories endpoint usually means auth issue
        message.error('üîí Kh√¥ng c√≥ quy·ªÅn truy c·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.');
        localStorage.removeItem('access_token');
        setTimeout(() => redirectToLogin(), 1500);
      } else if (error.code === 'ECONNREFUSED' || error.code === 'ERR_NETWORK') {
        message.error('‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi server. Vui l√≤ng ki·ªÉm tra server c√≥ ƒëang ch·∫°y?');
      } else if (error.code === 'ECONNABORTED') {
        message.error('‚è±Ô∏è K·∫øt n·ªëi b·ªã timeout. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng v√† th·ª≠ l·∫°i.');
      } else {
        message.error(error.message || 'Failed to load repositories');
      }
    } finally {
      setLoading(false);
    }}, [dataSourcePreference]);useEffect(() => {
    fetchRepositories();
  }, [fetchRepositories]);

  return {
    repositories,
    loading,
    dataSource,
    refetch: fetchRepositories
  };
};

// ==================== TASKS HOOK ====================
export const useTasks = (selectedRepo, dataSourcePreference = 'auto') => {
  const [tasks, setTasks] = useState([]);
  const [loading, setLoading] = useState(false);
  const [dataSource, setDataSource] = useState('database');

  const fetchTasks = useCallback(async () => {
    if (!selectedRepo) {
      setTasks([]);
      return;
    }

    setLoading(true);
    try {
      let result;
      
      // Handle forced data source preference
      if (dataSourcePreference === 'database') {
        const data = await taskAPI.getByRepo(selectedRepo.owner.login, selectedRepo.name);
        result = { data, source: 'database' };
      } else if (dataSourcePreference === 'fallback') {
        const data = await taskAPI.getAll(selectedRepo.owner.login, selectedRepo.name);
        result = { data, source: 'fallback' };
      } else {
        // Auto mode - intelligent fallback
        result = await taskAPI.getIntelligent(
          selectedRepo.owner.login,
          selectedRepo.name
        );
      }
      
      setTasks(result.data);
      setDataSource(result.source);
      
      if (result.data.length === 0) {
        message.info('Kh√¥ng c√≥ tasks n√†o cho repository n√†y');
      } else if (result.source === 'fallback') {
        message.info('üì° Tasks loaded from general database');
      }
    } catch (error) {
      console.error('Error fetching tasks:', error);
      setTasks([]);
      message.error('Cannot load tasks');
    } finally {
      setLoading(false);
    }
  }, [selectedRepo, dataSourcePreference]);

  // API operations v·ªõi fallback local
  const createTask = useCallback(async (taskData) => {
    try {
      await taskAPI.create(
        selectedRepo.owner.login,
        selectedRepo.name,
        taskData
      );
      await fetchTasks(); // Refresh t·ª´ server
      message.success('T·∫°o task m·ªõi th√†nh c√¥ng!');
    } catch (error) {
      console.log('API failed, using local creation:', error);
      // Local fallback
      const newTask = {
        id: Date.now(),
        ...taskData,
        created_at: new Date().toISOString().split('T')[0]
      };
      setTasks(prev => [...prev, newTask]);
      message.success('T·∫°o task m·ªõi th√†nh c√¥ng (local)!');
    }
  }, [selectedRepo, fetchTasks]);

  const updateTask = useCallback(async (taskId, taskData) => {
    try {
      await taskAPI.update(
        selectedRepo.owner.login,
        selectedRepo.name,
        taskId,
        taskData
      );
      await fetchTasks(); // Refresh t·ª´ server
      message.success('C·∫≠p nh·∫≠t task th√†nh c√¥ng!');
    } catch (error) {
      console.log('API failed, using local update:', error);
      // Local fallback
      setTasks(prev => prev.map(task => 
        task.id === taskId ? { ...task, ...taskData } : task
      ));
      message.success('C·∫≠p nh·∫≠t task th√†nh c√¥ng (local)!');
    }
  }, [selectedRepo, fetchTasks]);

  const updateTaskStatus = useCallback(async (taskId, newStatus) => {
    const taskToUpdate = tasks.find(t => t.id === taskId);
    if (!taskToUpdate) return;

    await updateTask(taskId, { ...taskToUpdate, status: newStatus });
  }, [tasks, updateTask]);

  const deleteTask = useCallback(async (taskId) => {
    try {
      await taskAPI.delete(
        selectedRepo.owner.login,
        selectedRepo.name,
        taskId
      );
      await fetchTasks(); // Refresh t·ª´ server
      message.success('X√≥a task th√†nh c√¥ng!');
    } catch (error) {
      console.log('API failed, using local delete:', error);
      // Local fallback
      setTasks(prev => prev.filter(task => task.id !== taskId));
      message.success('X√≥a task th√†nh c√¥ng (local)!');
    }
  }, [selectedRepo, fetchTasks]);  useEffect(() => {
    fetchTasks();
  }, [fetchTasks]);

  return {
    tasks,
    loading,
    dataSource,
    createTask,
    updateTask,
    updateTaskStatus,
    deleteTask,
    refetch: fetchTasks
  };
};

// ==================== COLLABORATORS HOOK ====================
export const useCollaborators = (selectedRepo) => {
  const [collaborators, setCollaborators] = useState([]);
  const [dataSource, setDataSource] = useState('mixed');
  const [syncStatus, setSyncStatus] = useState(null);  const fetchCollaborators = useCallback(async () => {
    if (!selectedRepo) {
      console.log('üö´ No selected repo, clearing collaborators');
      setCollaborators([]);
      setSyncStatus(null);
      return;
    }

    const repoKey = `${selectedRepo.owner.login}/${selectedRepo.name}`;
    console.log(`üîÑ Fetching collaborators for ${repoKey}`);

    try {
      // üìä Ch·ªâ l·∫•y t·ª´ database (ƒë∆°n gi·∫£n)
      const result = await collaboratorAPI.getCollaborators(
        selectedRepo.owner.login, 
        selectedRepo.name
      );
      
      console.log(`üìä Result for ${repoKey}:`, result);
      
      setCollaborators(result.collaborators);
      setDataSource('database');
      setSyncStatus({
        hasSyncedData: result.hasSyncedData,
        message: result.message
      });
      
      console.log(`‚úÖ Loaded ${result.collaborators.length} collaborators for ${repoKey}`);
    } catch (error) {
      console.error(`‚ùå Error fetching collaborators for ${repoKey}:`, error);
      setCollaborators([]);
      setSyncStatus({
        hasSyncedData: false,
        message: 'Kh√¥ng th·ªÉ t·∫£i collaborators. Vui l√≤ng th·ª≠ l·∫°i.'
      });
    }
  }, [selectedRepo]);  useEffect(() => {
    fetchCollaborators();
  }, [fetchCollaborators]);  // Utility function ƒë·ªÉ get assignee info v·ªõi fallback ƒë·∫øn current user
  const getAssigneeInfo = useCallback((assigneeLogin) => {
    // T√¨m trong collaborators tr∆∞·ªõc
    const found = collaborators.find(c => 
      c.login === assigneeLogin || 
      c.github_username === assigneeLogin
    );
    
    if (found) {
      return found;
    }
    
    // Fallback ƒë·∫øn current user n·∫øu assignee l√† ch√≠nh m√¨nh
    try {
      const currentUserProfile = JSON.parse(localStorage.getItem('github_profile') || '{}');
      if (currentUserProfile.login === assigneeLogin) {
        return {
          login: currentUserProfile.login,
          github_username: currentUserProfile.login,
          avatar_url: currentUserProfile.avatar_url,
          display_name: currentUserProfile.name || currentUserProfile.login
        };
      }
    } catch (error) {
      console.warn('Failed to parse github_profile from localStorage:', error);
    }
    
    // Default fallback
    return { 
      login: assigneeLogin, 
      github_username: assigneeLogin,
      avatar_url: null, 
      display_name: assigneeLogin 
    };
  }, [collaborators]);
  // Function to manually clear collaborators data
  const clearCollaborators = useCallback(() => {
    console.log('üßπ Manually clearing collaborators data');
    setCollaborators([]);
    setSyncStatus(null);
    setDataSource('mixed');
  }, []);  // üîÑ Sync collaborators t·ª´ GitHub 
  const syncCollaborators = useCallback(async () => {
    if (!selectedRepo) return;

    const repoKey = `${selectedRepo.owner.login}/${selectedRepo.name}`;
    console.log(`üîÑ Syncing collaborators for ${repoKey}`);

    try {
      await collaboratorAPI.sync(selectedRepo.owner.login, selectedRepo.name);
      console.log(`‚úÖ Sync completed for ${repoKey}`);
      
      // Refresh data sau khi sync
      await fetchCollaborators();
    } catch (error) {
      console.error(`‚ùå Sync failed for ${repoKey}:`, error);
      throw error;
    }
  }, [selectedRepo, fetchCollaborators]);

  return {
    collaborators,
    dataSource,
    syncStatus,
    getAssigneeInfo,
    clearCollaborators,
    syncCollaborators,
    refetch: fetchCollaborators
  };
};

// ==================== COMPOSITE HOOK FOR ALL PROJECT DATA ====================
export const useProjectData = (options = {}) => {
  const { dataSourcePreference = 'database', preloadedRepositories } = options;
  
  // Individual hooks - ch·ªâ use repositories n·∫øu kh√¥ng c√≥ preloaded
  const repositoriesHook = useRepositories(dataSourcePreference);
  
  // S·ª≠ d·ª•ng preloaded repositories n·∫øu c√≥, fallback ƒë·∫øn hook
  const repositories = {
    repositories: preloadedRepositories || repositoriesHook.repositories,
    loading: preloadedRepositories ? false : repositoriesHook.loading,
    dataSource: repositoriesHook.dataSource,
    refetch: repositoriesHook.refetch
  };
  
  const [selectedRepo, setSelectedRepo] = useState(null);
  const tasks = useTasks(selectedRepo, 'database');
  const collaborators = useCollaborators(selectedRepo);
  const [branches, setBranches] = useState([]);
  const [branchesLoading, setBranchesLoading] = useState(false);

  // New function: Auto-sync repository data  // Note: Removed syncRepositoryData function - no longer needed for auto-sync
  // Use individual sync functions (syncBranches, syncCollaborators) instead
  // Load branches from database
  const loadBranches = useCallback(async (repo) => {
    if (!repo) {
      setBranches([]);
      return;
    }

    // Check authentication before loading branches
    if (!checkAuthentication()) {
      setBranches([]);
      return;
    }

    setBranchesLoading(true);
    try {
      console.log(`üåø Loading branches from database for ${repo.owner.login}/${repo.name}`);
      const branchesData = await branchAPI.getBranches(repo.owner.login, repo.name);
      setBranches(branchesData);
      console.log(`‚úÖ Loaded ${branchesData.length} branches from database`);
    } catch (error) {
      console.error('Failed to load branches:', error);
      setBranches([]);
      
      // Handle authentication errors for branches
      if (error.response?.status === 401 || error.response?.status === 404) {
        message.error('üîí Kh√¥ng th·ªÉ t·∫£i branches. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.');
        localStorage.removeItem('access_token');
        setTimeout(() => redirectToLogin(), 1500);
      }
      // Don't show error message for other cases - it's expected to be empty sometimes
    } finally {
      setBranchesLoading(false);
    }
  }, []);
  // Handle repository selection - ONLY load from database, NO auto-sync
  const handleRepoChange = useCallback(async (repoId) => {
    const repo = repositories.repositories.find(r => r.id === repoId);
    setSelectedRepo(repo);
    
    if (repo) {
      console.log(`ÔøΩ Repository selected: ${repo.owner.login}/${repo.name} - Loading from database only`);
      
      // ONLY load branches from database (no auto-sync)
      await loadBranches(repo);
    } else {
      // Clear branches when no repo selected
      setBranches([]);
    }
  }, [repositories.repositories, loadBranches]);
  // Sync branches only
  const syncBranches = useCallback(async () => {
    if (!selectedRepo) return;

    const token = localStorage.getItem('access_token');
    if (!token) {
      message.error('Vui l√≤ng ƒëƒÉng nh·∫≠p ƒë·ªÉ sync branches');
      return;
    }

    setBranchesLoading(true);
    try {
      console.log(`üìÇ Syncing branches for ${selectedRepo.owner.login}/${selectedRepo.name}`);
      
      const branchData = await branchAPI.sync(selectedRepo.owner.login, selectedRepo.name);
      setBranches(branchData.branches || []);
      message.success(`‚úÖ ƒê√£ sync ${branchData.branches?.length || 0} branches t·ª´ GitHub`);
      
    } catch (error) {
      console.error('Failed to sync branches:', error);
      if (error.response?.status === 401) {
        message.error('üîí Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.');
      } else {
        message.error('‚ùå Kh√¥ng th·ªÉ sync branches t·ª´ GitHub');
      }
    } finally {
      setBranchesLoading(false);
    }  }, [selectedRepo]);

  return {
    // States
    selectedRepo,
    branches,
    
    // Data
    repositories: repositories.repositories,
    tasks: tasks.tasks,
    collaborators: collaborators.collaborators,
      // Loading states
    repositoriesLoading: repositories.loading,
    tasksLoading: tasks.loading,
    branchesLoading,
    
    // Actions
    handleRepoChange,
    getAssigneeInfo: collaborators.getAssigneeInfo,
    
    // Task operations
    createTask: tasks.createTask,
    updateTask: tasks.updateTask,
    updateTaskStatus: tasks.updateTaskStatus,
    deleteTask: tasks.deleteTask,    // Manual refresh functions (no auto-sync)
    refetchRepositories: repositories.refetch,
    refetchTasks: tasks.refetch,
    refetchCollaborators: collaborators.refetch,

    // Manual sync functions (user-initiated only)
    syncBranches,
    syncCollaborators: collaborators.syncCollaborators
  };
};

```

### frontend\src\pages\AuthSuccess.jsx
```jsx
// src/pages/AuthSuccess.jsx
import React, { useEffect } from "react";
import { useNavigate, useLocation } from "react-router-dom";
import { message } from "antd";

const AuthSuccess = () => {
  const navigate = useNavigate();
  const location = useLocation();

  useEffect(() => {
    const params = new URLSearchParams(location.search);
    const token = params.get("token");
    const username = params.get("username");
    const email = params.get("email");

    if (token) {
      const profile = {
        token,
        username,
        email,
        avatar_url: params.get("avatar_url"),
      };      localStorage.setItem("github_profile", JSON.stringify(profile));
      localStorage.setItem("access_token", token);

      // Chuy·ªÉn h∆∞·ªõng ngay l·∫≠p t·ª©c, ƒë·ªÉ Dashboard x·ª≠ l√Ω ƒë·ªìng b·ªô
      message.success("ƒêƒÉng nh·∫≠p th√†nh c√¥ng!");
      navigate("/dashboard");
    } else {
      navigate("/login");
    }
  }, [location, navigate]);

  return (
    <div className="h-screen flex items-center justify-center">
      <p className="text-xl">ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu...</p>
    </div>
  );
};

export default AuthSuccess;
```

### frontend\src\pages\Dashboard.jsx
```jsx
import React, { useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button, Typography, Avatar, Card, Grid, Space, Divider, Badge, message, Spin } from 'antd';
import { LogoutOutlined, GithubOutlined, NotificationOutlined } from '@ant-design/icons';
import styled from 'styled-components';
import RepoList from '../components/repo/RepoList';
import OverviewCard from '../components/Dashboard/OverviewCard';
import AIInsightWidget from '../components/Dashboard/AIInsightWidget';
import ProjectTaskManager from '../components/Dashboard/ProjectTaskManager';
import RepoListFilter from '../components/Dashboard/RepoListFilter';
import TaskBoard from '../components/Dashboard/TaskBoard';
import SyncProgressNotification from '../components/common/SyncProgressNotification';
import axios from 'axios';

const { Title, Text } = Typography;
const { useBreakpoint } = Grid;

// Styled components v·ªõi theme hi·ªán ƒë·∫°i
const DashboardContainer = styled.div`
  padding: 24px;
  max-width: 1440px;
  margin: 0 auto;
  background: #f8fafc;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  gap: 24px;

  @media (max-width: 768px) {
    padding: 16px;
    gap: 16px;
  }
`;

const MainLayout = styled.div`
  display: grid;
  grid-template-columns: 280px 1fr;
  gap: 24px;
  min-height: calc(100vh - 200px);

  @media (max-width: 1200px) {
    grid-template-columns: 250px 1fr;
    gap: 16px;
  }

  @media (max-width: 768px) {
    grid-template-columns: 1fr;
    gap: 16px;
  }
`;

const Sidebar = styled.div`
  position: sticky;
  top: 24px;
  height: fit-content;
  display: flex;
  flex-direction: column;
  gap: 16px;

  @media (max-width: 768px) {
    position: static;
    order: 2;
  }
`;

const MainContent = styled.div`
  display: flex;
  flex-direction: column;
  gap: 24px;
  min-width: 0; /* ƒê·ªÉ tr√°nh overflow */
`;

const SidebarCard = styled(Card)`
  border-radius: 12px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);

  .ant-card-body {
    padding: 16px;
  }

  .ant-card-head {
    padding: 12px 16px;
    border-bottom: 1px solid #f1f5f9;
  }
`;

const HeaderCard = styled(Card)`
  border-radius: 16px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  
  .ant-card-body {
    padding: 24px;
  }
`;

const DashboardCard = styled(Card)`
  border-radius: 16px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  transition: all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);
  
  &:hover {
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
    transform: translateY(-2px);
  }

  .ant-card-head {
    border-bottom: 1px solid #f1f5f9;
    padding: 16px 24px;
  }

  .ant-card-body {
    padding: 24px;
  }

  @media (max-width: 768px) {
    .ant-card-body {
      padding: 16px;
    }
  }
`;

const PrimaryButton = styled(Button)`
  border-radius: 8px;
  font-weight: 500;
  height: 40px;
  padding: 0 20px;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const UserInfoContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 16px;
`;

const UserAvatar = styled(Avatar)`
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  border: 2px solid #ffffff;
`;

const ContentSection = styled.section`
  display: flex;
  flex-direction: column;
  gap: 24px;
`;

const SectionTitle = styled(Title)`
  margin-bottom: 0 !important;
  font-weight: 600 !important;
  color: #1e293b !important;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const NotificationBadge = styled(Badge)`
  .ant-badge-count {
    background: #3b82f6;
    box-shadow: 0 0 0 1px #fff;
  }
`;

const Dashboard = () => {
  const [user, setUser] = useState(null);
  const [loading] = useState(false);
  const navigate = useNavigate();
  const screens = useBreakpoint();
  // Progress notification states
  const [syncProgress, setSyncProgress] = useState({
    visible: false,
    totalRepos: 0,
    completedRepos: 0,
    currentRepo: '',
    repoProgresses: [],
    overallProgress: 0
  });

  const [isSyncing, setIsSyncing] = useState(false);
  const [repositories, setRepositories] = useState([]);
  const [repoLoading, setRepoLoading] = useState(true);

  // Pre-fetch repositories t·ª´ database ng·∫ßm trong n·ªÅn
  const preloadRepositoriesFromDB = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) return;

    try {
      // G·ªçi API l·∫•y repos t·ª´ database (kh√¥ng ph·∫£i GitHub API)
      const response = await axios.get('http://localhost:8000/api/repositories', {
        headers: { Authorization: `Bearer ${token}` },
      });
      
      setRepositories(response.data);
      console.log(`Pre-loaded ${response.data.length} repositories from database`);
    } catch (error) {
      console.error('L·ªói khi pre-load repositories:', error);
    } finally {
      setRepoLoading(false);
    }
  };

  // Pre-load repositories ngay khi v√†o Dashboard
  useEffect(() => {
    preloadRepositoriesFromDB();
  }, []);

  const syncAllRepositories = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) {
      message.error('Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!');
      return;
    }

    // Hi·ªÉn th·ªã progress ngay l·∫≠p t·ª©c TR∆Ø·ªöC khi set loading
    setSyncProgress({
      visible: true,
      totalRepos: 0,
      completedRepos: 0,
      currentRepo: 'ƒêang l·∫•y danh s√°ch repository...',
      repoProgresses: [],
      overallProgress: 0
    });

    setIsSyncing(true);

    try {
      // Th√™m timeout nh·ªè ƒë·ªÉ ƒë·∫£m b·∫£o UI render progress tr∆∞·ªõc
      await new Promise(resolve => setTimeout(resolve, 50));
      
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: {
          Authorization: `token ${token}`,
        },
      });

      const repositories = response.data;
      
      // C·∫≠p nh·∫≠t v·ªõi danh s√°ch repository th·ª±c t·∫ø
      setSyncProgress(prev => ({
        ...prev,
        totalRepos: repositories.length,
        currentRepo: 'Chu·∫©n b·ªã ƒë·ªìng b·ªô...',
        repoProgresses: repositories.map(repo => ({
          name: `${repo.owner.login}/${repo.name}`,
          status: 'pending',
          progress: 0
        }))
      }));

      let completedCount = 0;
      
      // ƒê·ªìng b·ªô t·ª´ng repository m·ªôt c√°ch tu·∫ßn t·ª± ƒë·ªÉ tracking d·ªÖ h∆°n
      for (const repo of repositories) {
        const repoName = `${repo.owner.login}/${repo.name}`;
        
        // C·∫≠p nh·∫≠t repository hi·ªán t·∫°i
        setSyncProgress(prev => ({
          ...prev,
          currentRepo: repoName,
          repoProgresses: prev.repoProgresses.map(r => 
            r.name === repoName ? { ...r, status: 'syncing', progress: 0 } : r
          )
        }));

        try {
          // ƒê·ªìng b·ªô repository
          await axios.post(
            `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
            {},
            {
              headers: {
                Authorization: `token ${token}`,
              },
            }
          );

          completedCount++;
          
          // C·∫≠p nh·∫≠t tr·∫°ng th√°i ho√†n th√†nh
          setSyncProgress(prev => ({
            ...prev,
            completedRepos: completedCount,
            overallProgress: (completedCount / repositories.length) * 100,
            repoProgresses: prev.repoProgresses.map(r => 
              r.name === repoName ? { ...r, status: 'completed', progress: 100 } : r
            )
          }));

        } catch (error) {
          console.error(`L·ªói ƒë·ªìng b·ªô ${repoName}:`, error);
          
          // C·∫≠p nh·∫≠t tr·∫°ng th√°i l·ªói
          setSyncProgress(prev => ({
            ...prev,
            repoProgresses: prev.repoProgresses.map(r => 
              r.name === repoName ? { ...r, status: 'error', progress: 0 } : r
            )
          }));
          
          completedCount++; // V·∫´n t√≠nh l√† completed ƒë·ªÉ ti·∫øp t·ª•c
        }
      }

      message.success('ƒê·ªìng b·ªô t·∫•t c·∫£ repository ho√†n th√†nh!');

    } catch (error) {
      console.error('L·ªói khi l·∫•y danh s√°ch repository:', error);
      message.error('Kh√¥ng th·ªÉ l·∫•y danh s√°ch repository!');
      setSyncProgress(prev => ({ ...prev, visible: false }));
    } finally {
      setIsSyncing(false);
    }
  };  useEffect(() => {
    const storedProfile = localStorage.getItem('github_profile');
    if (!storedProfile) {
      navigate('/login');
    } else {
      setUser(JSON.parse(storedProfile));
      
      // Removed automatic sync - now only manual sync is allowed
      // Users must manually sync repositories using the sync buttons
    }  }, [navigate]);

  const handleLogout = () => {
    localStorage.removeItem('github_profile');
    localStorage.removeItem('access_token');
    navigate('/login');
  };

  const handleFilterChange = (filters) => {
    console.log('Applied filters:', filters);
  };

  const handleStatusChange = (taskId, newStatus) => {
    console.log(`Updated task ${taskId} status to ${newStatus}`);
  };

  if (loading) {
    return <Spin tip="ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu..." size="large" />;
  }

  return (
    <DashboardContainer>
      {/* Header Section */}
      <HeaderCard variant="borderless">
        <Space 
          direction={screens.md ? 'horizontal' : 'vertical'} 
          align={screens.md ? 'center' : 'start'}
          style={{ width: '100%', justifyContent: 'space-between' }}
        >
          <UserInfoContainer>
            <UserAvatar src={user?.avatar_url} size={screens.md ? 72 : 56} />
            <div>
              <Title level={4} style={{ margin: 0, color: '#1e293b' }}>
                Welcome back, {user?.username || 'User'}!
              </Title>
              <Text type="secondary" style={{ color: '#64748b' }}>
                {user?.email || 'No email provided'}
              </Text>
            </div>
          </UserInfoContainer>          <Space size={screens.md ? 16 : 8}>
            
            <Button 
              type="default" 
              onClick={syncAllRepositories}
              loading={isSyncing}
              disabled={isSyncing}
              style={{ backgroundColor: '#f8fafc', borderColor: '#e2e8f0' }}
            >
              {isSyncing ? 'ƒêang ƒë·ªìng b·ªô...' : 'ƒê·ªìng b·ªô ƒë·∫ßy ƒë·ªß'}
            </Button>
            
            {/* Test button for instant progress */}
            <Button 
              onClick={() => {
                setSyncProgress({
                  visible: true,
                  totalRepos: 5,
                  completedRepos: 0,
                  currentRepo: 'Test repository...',
                  repoProgresses: [],
                  overallProgress: 0
                });
              }}
              style={{ background: '#10b981', borderColor: '#10b981', color: 'white' }}
            >
              Test Progress
            </Button>
            
            <NotificationBadge count={3} size="small">
              <Button 
                icon={<NotificationOutlined />} 
                shape="circle" 
                style={{ border: 'none' }}
              />
            </NotificationBadge>
            <PrimaryButton 
              type="primary" 
              danger 
              onClick={handleLogout}
              icon={<LogoutOutlined />}
            >
              {screens.md ? 'Log Out' : ''}
            </PrimaryButton>
          </Space>        </Space>
      </HeaderCard>

      {/* Main Layout v·ªõi Sidebar v√† Content */}
      <MainLayout>        {/* Sidebar b√™n tr√°i */}
        <Sidebar>
          {/* Overview Metrics trong Sidebar */}
          <OverviewCard sidebar={true} />
          
          {/* Quick Actions */}
          <SidebarCard 
            title={<SectionTitle level={5} style={{ fontSize: '14px' }}>Thao t√°c nhanh</SectionTitle>}
            size="small"
          >
            <Space direction="vertical" style={{ width: '100%' }} size="small">              <Button 
                type="default" 
                onClick={syncAllRepositories}
                loading={isSyncing}
                disabled={isSyncing}
                block
                size="small"
                style={{ backgroundColor: '#f8fafc', borderColor: '#e2e8f0' }}
              >
                {isSyncing ? 'ƒêang ƒë·ªìng b·ªô...' : 'ƒê·ªìng b·ªô ƒë·∫ßy ƒë·ªß'}
              </Button>
            </Space>
          </SidebarCard>

          {/* Activity Summary */}
          <SidebarCard 
            title={<SectionTitle level={5} style={{ fontSize: '14px' }}>Ho·∫°t ƒë·ªông g·∫ßn ƒë√¢y</SectionTitle>}
            size="small"
          >
            <Space direction="vertical" style={{ width: '100%' }} size="small">
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ Task "Tr√≤ game tƒÉng ƒë·ªô kh√≥" ƒë√£ ho√†n th√†nh
              </div>
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ 2 repositories m·ªõi ƒë∆∞·ª£c ƒë·ªìng b·ªô
              </div>
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ AI ph√¢n t√≠ch 15 commits m·ªõi
              </div>
            </Space>
          </SidebarCard>
        </Sidebar>

        {/* Main Content b√™n ph·∫£i */}
        <MainContent>          {/* Project Task Manager - Full Width */}
          <DashboardCard>
            <ProjectTaskManager 
              repositories={repositories}
              repoLoading={repoLoading}
            />
          </DashboardCard>

          {/* Repository Analysis */}
          <DashboardCard 
            title={
              <SectionTitle level={5}>
                <GithubOutlined />
                Repository Analysis
              </SectionTitle>
            }
          >
            <AIInsightWidget />
          </DashboardCard>

          {/* Filters Section */}
          <DashboardCard 
            title={<SectionTitle level={5}>Filters & Settings</SectionTitle>}
          >
            <RepoListFilter onFilterChange={handleFilterChange} />
          </DashboardCard>

          {/* Main Content Sections */}
          <ContentSection>
            <DashboardCard 
              title={
                <SectionTitle level={5}>
                  My Repositories
                  <Text type="secondary" style={{ fontSize: 14, marginLeft: 8 }}>
                    (24 repositories)
                  </Text>
                </SectionTitle>
              }
            >
              <RepoList />
            </DashboardCard>

            <DashboardCard 
              title={<SectionTitle level={5}>Project Tasks</SectionTitle>}
            >
              <TaskBoard onStatusChange={handleStatusChange} />
            </DashboardCard>
          </ContentSection>
        </MainContent>
      </MainLayout>

      {/* Progress Notification */}
      <SyncProgressNotification
        visible={syncProgress.visible}
        onClose={() => setSyncProgress(prev => ({ ...prev, visible: false }))}
        totalRepos={syncProgress.totalRepos}
        completedRepos={syncProgress.completedRepos}
        currentRepo={syncProgress.currentRepo}
        repoProgresses={syncProgress.repoProgresses}
        overallProgress={syncProgress.overallProgress}
      />
    </DashboardContainer>
  );
};

export default Dashboard;
```

### frontend\src\pages\Login.jsx
```jsx
// src/pages/Login.jsx
import React from "react";
import { Button, Card, Typography } from "antd";
import { GithubOutlined } from "@ant-design/icons";

const { Title } = Typography;

const Login = () => {
  console.log("Login component is rendering...");
  
  const handleGitHubLogin = () => {
    console.log("GitHub login button clicked");
    window.location.href = "http://localhost:8000/api/login"; // backend redirect to GitHub OAuth
  };

  console.log("Login component rendered successfully");
  
  return (
    <div style={{
      height: '100vh',
      display: 'flex',
      alignItems: 'center',
      justifyContent: 'center',
      background: 'linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%)',
      padding: '20px'
    }}>
      <Card
        style={{ 
          textAlign: "center", 
          padding: "3rem 2rem",
          width: '100%',
          maxWidth: '400px',
          borderRadius: '16px',
          boxShadow: '0 10px 25px rgba(0,0,0,0.1)'
        }}
      >
        <Title level={2} style={{ marginBottom: "2rem", color: '#2c3e50' }}>
          ƒêƒÉng nh·∫≠p v√†o <span style={{ color: "#1890ff" }}>TaskFlowAI</span>
        </Title>
        
        <Button
          type="primary"
          icon={<GithubOutlined />}
          size="large"
          onClick={handleGitHubLogin}
          style={{
            backgroundColor: "#000",
            borderColor: "#000",
            width: "100%",
            height: '48px',
            fontSize: '16px',
            borderRadius: '8px'
          }}
        >
          ƒêƒÉng nh·∫≠p v·ªõi GitHub
        </Button>
        
        <div style={{ marginTop: '24px', color: '#6c757d', fontSize: '14px' }}>
          <p>S·ª≠ d·ª•ng t√†i kho·∫£n GitHub ƒë·ªÉ ƒëƒÉng nh·∫≠p</p>
          <p>H·ªá th·ªëng qu·∫£n l√Ω task v√† ph√¢n t√≠ch commits</p>
        </div>
      </Card>
    </div>
  );
};

export default Login;
```

### frontend\src\pages\RepoDetails.jsx
```jsx
import { useEffect, useState, useCallback } from "react";
import { useParams } from "react-router-dom";
import { message, Button, Card, Typography, Alert, Progress, Row, Col } from "antd";
import { SyncOutlined, SaveOutlined, GithubOutlined } from "@ant-design/icons";
import BranchSelector from "../components/Branchs/BranchSelector";
import BranchCommitList from "../components/Branchs/BranchCommitList";
import CommitList from "../components/commits/CommitList";
import axios from "axios";

const { Title, Text } = Typography;

const RepoDetails = () => {
  const { owner, repo } = useParams();
  const [branch, setBranch] = useState("");
  const [loading, setLoading] = useState(false);
  const [syncing, setSyncing] = useState(false);
  const [syncProgress, setSyncProgress] = useState(0);
  const [refreshKey, setRefreshKey] = useState(0); // For refreshing child components

  // Handle branch change with optional refresh
  const handleBranchChange = (newBranch, shouldRefresh = false) => {
    setBranch(newBranch);
    if (shouldRefresh) {
      setRefreshKey(prev => prev + 1); // Trigger refresh
    }
  };

  // Sync repository trong background kh√¥ng block UI
  const syncRepositoryInBackground = useCallback(async () => {
    const token = localStorage.getItem("access_token");
    if (!token || syncing) return;

    try {
      setSyncing(true);
      setSyncProgress(0);
      
      // Hi·ªÉn th·ªã th√¥ng b√°o b·∫Øt ƒë·∫ßu sync
      message.info(`ƒêang ƒë·ªìng b·ªô repository ${repo} trong background...`, 2);
      
      // Sync c∆° b·∫£n tr∆∞·ªõc (nhanh)
      setSyncProgress(30);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-basic`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      
      // Sync ƒë·∫ßy ƒë·ªß
      setSyncProgress(70);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      
      setSyncProgress(100);
      message.success(`ƒê·ªìng b·ªô repository ${repo} th√†nh c√¥ng!`);
      
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô repository:", error);
      message.error("ƒê·ªìng b·ªô repository th·∫•t b·∫°i!");
    } finally {
      setSyncing(false);
      setTimeout(() => setSyncProgress(0), 2000);
    }
  }, [owner, repo, syncing]);

  // Ki·ªÉm tra v√† sync repository trong background
  const checkAndSyncRepository = useCallback(async () => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    try {
      // Ki·ªÉm tra xem repo ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a
      const checkResponse = await axios.get(
        `http://localhost:8000/api/github/${owner}/${repo}/branches`,
        {
          headers: { Authorization: `token ${token}` },
        }
      );

      // N·∫øu c√≥ d·ªØ li·ªáu r·ªìi th√¨ kh√¥ng c·∫ßn sync
      if (checkResponse.data && checkResponse.data.length > 0) {
        console.log('Repository ƒë√£ c√≥ d·ªØ li·ªáu, kh√¥ng c·∫ßn sync');
        return;
      }
    } catch {
      console.log('Repository ch∆∞a c√≥ d·ªØ li·ªáu, b·∫Øt ƒë·∫ßu sync...');
    }

    // Sync repository trong background
    syncRepositoryInBackground();
  }, [owner, repo, syncRepositoryInBackground]);

  // Load trang ngay l·∫≠p t·ª©c v·ªõi d·ªØ li·ªáu c√≥ s·∫µn
  useEffect(() => {
    // Sync trong background n·∫øu c·∫ßn
    checkAndSyncRepository();
  }, [owner, repo, checkAndSyncRepository]);

  // Sync th·ªß c√¥ng
  const manualSync = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    try {
      setLoading(true);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      message.success("ƒê·ªìng b·ªô d·ªØ li·ªáu th√†nh c√¥ng!");
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô d·ªØ li·ªáu:", error);
      message.error("Kh√¥ng th·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu!");
    } finally {
      setLoading(false);
    }
  };
  const saveCommits = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    if (!branch) {
      message.error("Vui l√≤ng ch·ªçn branch tr∆∞·ªõc!");
      return;
    }

    try {
      const response = await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/branches/${branch}/sync-commits?include_stats=true&per_page=100&max_pages=5`,
        {},
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      
      const { stats } = response.data;
      message.success(
        `ƒê·ªìng b·ªô th√†nh c√¥ng! ${stats.new_commits_saved} commits m·ªõi ƒë∆∞·ª£c l∆∞u cho branch "${branch}"`
      );
    } catch (error) {
      console.error("L·ªói khi l∆∞u commit:", error);
      message.error("Kh√¥ng th·ªÉ l∆∞u commit!");
    }
  };

  // Hi·ªÉn th·ªã trang ngay l·∫≠p t·ª©c, kh√¥ng ƒë·ª£i sync
  return (
    <div style={{ padding: 24 }}>
      <Card>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: 16 }}>
          <Title level={2} style={{ margin: 0 }}>
            <GithubOutlined /> {owner}/{repo}
          </Title>
          
          <div style={{ display: 'flex', gap: 8, alignItems: 'center' }}>
            {syncing && (
              <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                <Progress 
                  type="circle" 
                  size={24} 
                  percent={syncProgress}
                  showInfo={false}
                />
                <Text type="secondary">ƒêang ƒë·ªìng b·ªô...</Text>
              </div>
            )}
            
            <Button 
              icon={<SyncOutlined />} 
              onClick={manualSync}
              loading={loading}
              disabled={loading || syncing}
            >
              ƒê·ªìng b·ªô th·ªß c√¥ng
            </Button>
            
            <Button 
              type="primary" 
              icon={<SaveOutlined />} 
              onClick={saveCommits}
              disabled={!branch}
            >
              L∆∞u Commit
            </Button>
          </div>
        </div>

        {syncing && (
          <Alert
            message="ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu trong background"
            description="B·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c s·ª≠ d·ª•ng trang n√†y, vi·ªác ƒë·ªìng b·ªô s·∫Ω ho√†n th√†nh trong gi√¢y l√°t."
            type="info"
            showIcon
            style={{ marginBottom: 16 }}
          />        )}        <BranchSelector owner={owner} repo={repo} onBranchChange={handleBranchChange} />
      </Card>

      <Row gutter={16} style={{ marginTop: 16 }}>
        <Col xs={24} lg={12}>
          <BranchCommitList 
            key={`branch-commits-${refreshKey}`}
            owner={owner} 
            repo={repo} 
            selectedBranch={branch} 
          />
        </Col>
        <Col xs={24} lg={12}>
          <Card title="Commits t·ª´ GitHub API (Real-time)">
            <CommitList 
              key={`real-time-commits-${refreshKey}`}
              owner={owner} 
              repo={repo} 
              branch={branch} 
            />
          </Card>
        </Col>
      </Row>
    </div>
  );
};

export default RepoDetails;

```

### frontend\src\pages\TestPage.jsx
```jsx
import React from 'react';

const TestPage = () => {
  return (
    <div style={{ padding: '20px', background: 'lightblue', minHeight: '100vh' }}>
      <h1>üöÄ Test Page - App ƒëang ho·∫°t ƒë·ªông!</h1>
      <p>Th·ªùi gian: {new Date().toLocaleString()}</p>
      <div>
        <button style={{ padding: '10px', background: 'green', color: 'white', border: 'none', borderRadius: '5px' }}>
          Click me!
        </button>
      </div>
    </div>
  );
};

export default TestPage;

```

### frontend\src\services\api.js
```js
// frontend/src/services/api.js
import axios from 'axios';
import { message } from 'antd';

// Centralized API configuration
const API_BASE_URL = 'http://localhost:8000/api';

// Axios instance v·ªõi common config
const apiClient = axios.create({
  baseURL: API_BASE_URL,
  timeout: 10000,
});

// Create a separate client for long-running operations like sync
const apiClientLongTimeout = axios.create({
  baseURL: API_BASE_URL,
  timeout: 30000, // 30 seconds for sync operations
});

// Create a separate API client without auth for public endpoints (currently unused)
// const publicApiClient = axios.create({
//   baseURL: API_BASE_URL,
//   timeout: 10000,
// });

// Request interceptor ƒë·ªÉ t·ª± ƒë·ªông th√™m token
apiClient.interceptors.request.use(
  (config) => {
    const token = localStorage.getItem('access_token');
    console.log(`üöÄ API Request: ${config.method?.toUpperCase()} ${config.url}`);
    console.log(`üîë Token exists: ${!!token}`);
    if (token) {
      config.headers.Authorization = `token ${token}`;
      console.log(`üîë Authorization header set: token ${token.substring(0, 10)}...`);
    }
    return config;
  },
  (error) => {
    console.error('‚ùå Request interceptor error:', error);
    return Promise.reject(error);
  }
);

// Response interceptor ƒë·ªÉ x·ª≠ l√Ω l·ªói chung
apiClient.interceptors.response.use(
  (response) => {
    console.log(`‚úÖ API Response: ${response.status} for ${response.config.url}`);
    return response;
  },
  (error) => {
    console.error(`‚ùå API Error:`, {
      url: error.config?.url,
      method: error.config?.method,
      status: error.response?.status,
      statusText: error.response?.statusText,
      data: error.response?.data,
      message: error.message
    });
    
    if (error.response?.status === 401) {
      message.error('Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n');
      // C√≥ th·ªÉ redirect ƒë·∫øn login page
    } else if (error.response?.status === 429) {
      message.warning('Qu√° nhi·ªÅu requests, vui l√≤ng th·ª≠ l·∫°i sau');
    }
    return Promise.reject(error);
  }
);

// Add interceptors for long timeout client
apiClientLongTimeout.interceptors.request.use(
  (config) => {
    const token = localStorage.getItem('access_token');
    if (token) {
      config.headers.Authorization = `token ${token}`;
    }
    return config;
  },
  (error) => Promise.reject(error)
);

apiClientLongTimeout.interceptors.response.use(
  (response) => response,
  (error) => {
    if (error.response?.status === 401) {
      message.error('Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n');
      // C√≥ th·ªÉ redirect ƒë·∫øn login page
    } else if (error.response?.status === 429) {
      message.warning('Qu√° nhi·ªÅu requests, vui l√≤ng th·ª≠ l·∫°i sau');
    }
    return Promise.reject(error);
  }
);

// ==================== REPOSITORY API ====================
export const repositoryAPI = {  // L·∫•y repos t·ª´ database (requires authentication to filter by user)
  getFromDatabase: async () => {
    console.log('üîç repositoryAPI.getFromDatabase: Making request to /repositories');
    const response = await apiClient.get('/repositories');
    console.log('‚úÖ repositoryAPI.getFromDatabase: Response received', {
      status: response.status,
      dataLength: response.data?.length
    });
    return response.data || [];
  },

  // L·∫•y repos t·ª´ GitHub API (fallback)
  getFromGitHub: async () => {
    const response = await apiClient.get('/github/repos');
    return response.data || [];
  },

  // Intelligent fetch v·ªõi fallback
  getIntelligent: async () => {
    try {
      console.log('üîç Trying database first...');
      const data = await repositoryAPI.getFromDatabase();
      if (data && data.length > 0) {
        console.log('‚úÖ Loaded from database');
        return { data, source: 'database' };
      }
    } catch (error) {
      console.log('‚ùå Database failed:', error.message);
    }

    try {
      console.log('üîç Trying GitHub API fallback...');
      const data = await repositoryAPI.getFromGitHub();
      console.log('‚ö†Ô∏è Loaded from GitHub API');
      return { data, source: 'github' };
    } catch (error) {
      console.log('‚ùå GitHub API failed:', error.message);
      throw new Error('Kh√¥ng th·ªÉ t·∫£i repositories t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o');
    }
  }
};

// ==================== TASK API ====================
export const taskAPI = {
  // L·∫•y tasks theo repo c·ª• th·ªÉ
  getByRepo: async (owner, repoName) => {
    const response = await apiClient.get(`/projects/${owner}/${repoName}/tasks`);
    return response.data || [];
  },

  // L·∫•y t·∫•t c·∫£ tasks (fallback)
  getAll: async (owner, repoName) => {
    const response = await apiClient.get('/tasks', {
      params: { limit: 100, offset: 0 }
    });
    const allTasks = response.data || [];
    return allTasks.filter(task => 
      task.repo_owner === owner && task.repo_name === repoName
    );
  },

  // Intelligent fetch v·ªõi fallback
  getIntelligent: async (owner, repoName) => {
    try {
      console.log('üîç Trying repo-specific endpoint...');
      const data = await taskAPI.getByRepo(owner, repoName);
      if (data && data.length > 0) {
        console.log('‚úÖ Loaded repo-specific tasks');
        return { data, source: 'database' };
      }
    } catch (error) {
      console.log('‚ùå Repo-specific failed:', error.message);
    }

    try {
      console.log('üîç Trying general tasks with filter...');
      const data = await taskAPI.getAll(owner, repoName);
      console.log('‚ö†Ô∏è Loaded from general tasks');
      return { data, source: 'fallback' };
    } catch (error) {
      console.log('‚ùå All task sources failed:', error.message);
      return { data: [], source: 'failed' };
    }
  },

  // T·∫°o task m·ªõi
  create: async (owner, repoName, taskData) => {
    const response = await apiClient.post(`/projects/${owner}/${repoName}/tasks`, taskData);
    return response.data;
  },

  // C·∫≠p nh·∫≠t task
  update: async (owner, repoName, taskId, taskData) => {
    const response = await apiClient.put(`/projects/${owner}/${repoName}/tasks/${taskId}`, taskData);
    return response.data;
  },

  // X√≥a task
  delete: async (owner, repoName, taskId) => {
    await apiClient.delete(`/projects/${owner}/${repoName}/tasks/${taskId}`);
  }
};

// ==================== COLLABORATOR API ====================
export const collaboratorAPI = {
  // üìä L·∫•y collaborators t·ª´ database
  getCollaborators: async (owner, repoName) => {
    console.log(`üîç Getting collaborators from database for ${owner}/${repoName}`);
    const timestamp = Date.now();
    const response = await apiClient.get(`/contributors/${owner}/${repoName}?t=${timestamp}`);
    
    const result = response.data;
    console.log('üìä Database response:', result);
    
    return {
      collaborators: result?.collaborators || [],
      hasSyncedData: result?.has_synced_data || false,
      message: result?.message || '',
      repository: result?.repository
    };
  },
  // ÔøΩ Sync collaborators t·ª´ GitHub v√†o database
  sync: async (owner, repoName) => {
    console.log(`üîÑ Syncing collaborators for ${owner}/${repoName}`);
    const response = await apiClientLongTimeout.post(`/contributors/${owner}/${repoName}/sync`);
    console.log('‚úÖ Sync response:', response.data);
    return response.data;  }
};

// ==================== BRANCH API ====================
export const branchAPI = {  // üåø L·∫•y branches t·ª´ database
  getBranches: async (owner, repoName) => {
    console.log(`üîç Getting branches from database for ${owner}/${repoName}`);
    const response = await apiClient.get(`/${owner}/${repoName}/branches`);
    
    console.log('üåø Branch database response:', response.data);
    return response.data?.branches || [];
  },

  // üîÑ Sync branches t·ª´ GitHub v√†o database
  sync: async (owner, repoName) => {
    console.log(`üîÑ Syncing branches for ${owner}/${repoName}`);
    const response = await apiClientLongTimeout.post(`/github/${owner}/${repoName}/sync-branches`);
    console.log('‚úÖ Branch sync response:', response.data);
    return response.data;
  }
};

export default apiClient;

```

### frontend\src\utils\taskUtils.js
```js

```

### frontend\src\utils\taskUtils.jsx
```jsx
// frontend/src/utils/taskUtils.jsx
import React from 'react';
import { 
  ClockCircleOutlined, 
  ExclamationCircleOutlined, 
  CheckCircleOutlined 
} from '@ant-design/icons';

// ==================== AVATAR UTILITIES ====================
export const getDefaultAvatarUrl = (username) => {
  // Generate default avatar using GitHub's default avatar pattern or a placeholder service
  return `https://github.com/identicons/${username}.png`;
};

export const getAvatarUrl = (avatarUrl, username) => {
  // Return provided avatar or fallback to default
  return avatarUrl || getDefaultAvatarUrl(username);
};

// ==================== TASK STATUS UTILITIES ====================
export const getStatusIcon = (status) => {
  const statusMap = {
    'TODO': React.createElement(ClockCircleOutlined, { style: { color: '#faad14' } }),
    'IN_PROGRESS': React.createElement(ExclamationCircleOutlined, { style: { color: '#1890ff' } }),
    'DONE': React.createElement(CheckCircleOutlined, { style: { color: '#52c41a' } }),
  };
  return statusMap[status] || React.createElement(ClockCircleOutlined);
};

export const getStatusText = (status) => {
  const statusMap = {
    'TODO': 'Ch·ªù th·ª±c hi·ªán',
    'IN_PROGRESS': 'ƒêang th·ª±c hi·ªán',
    'DONE': 'Ho√†n th√†nh'
  };
  return statusMap[status] || 'Kh√¥ng x√°c ƒë·ªãnh';
};

export const getStatusColor = (status) => {
  const colorMap = {
    'TODO': '#faad14',
    'IN_PROGRESS': '#1890ff', 
    'DONE': '#52c41a'
  };
  return colorMap[status] || '#d9d9d9';
};

// ==================== PRIORITY UTILITIES ====================
export const getPriorityColor = (priority) => {
  const colorMap = {
    'high': '#f5222d',
    'medium': '#fa8c16',
    'low': '#52c41a'
  };
  return colorMap[priority] || '#d9d9d9';
};

export const getPriorityText = (priority) => {
  const textMap = {
    'high': 'Cao',
    'medium': 'Trung b√¨nh',
    'low': 'Th·∫•p'
  };
  return textMap[priority] || 'Kh√¥ng x√°c ƒë·ªãnh';
};

export const getPriorityValue = (priority) => {
  const valueMap = {
    'high': 3,
    'medium': 2,
    'low': 1
  };
  return valueMap[priority] || 0;
};

// ==================== TASK FILTERING ====================
export const filterTasks = (tasks, filters) => {
  const { searchText, statusFilter, priorityFilter, assigneeFilter } = filters;
  
  return tasks.filter(task => {
    // Search filter
    const matchesSearch = !searchText || (
      task.title.toLowerCase().includes(searchText.toLowerCase()) ||
      task.description?.toLowerCase().includes(searchText.toLowerCase()) ||
      task.assignee.toLowerCase().includes(searchText.toLowerCase())
    );
    
    // Status filter
    const matchesStatus = statusFilter === 'all' || task.status === statusFilter;
    
    // Priority filter
    const matchesPriority = priorityFilter === 'all' || task.priority === priorityFilter;
    
    // Assignee filter
    const matchesAssignee = assigneeFilter === 'all' || task.assignee === assigneeFilter;
    
    return matchesSearch && matchesStatus && matchesPriority && matchesAssignee;
  });
};

// ==================== TASK STATISTICS ====================
export const calculateTaskStats = (tasks) => {
  const total = tasks.length;
  const completed = tasks.filter(t => t.status === 'DONE').length;
  const inProgress = tasks.filter(t => t.status === 'IN_PROGRESS').length;
  const todo = tasks.filter(t => t.status === 'TODO').length;
  const highPriority = tasks.filter(t => t.priority === 'high').length;
  
  return {
    total,
    completed,
    inProgress,
    todo,
    highPriority,
    completionRate: total > 0 ? Math.round((completed / total) * 100) : 0
  };
};

// ==================== TASK GROUPING ====================
export const groupTasksByStatus = (tasks) => {
  return {
    todo: tasks.filter(t => t.status === 'TODO'),
    inProgress: tasks.filter(t => t.status === 'IN_PROGRESS'),
    done: tasks.filter(t => t.status === 'DONE')
  };
};

export const groupTasksByPriority = (tasks) => {
  return {
    high: tasks.filter(t => t.priority === 'high'),
    medium: tasks.filter(t => t.priority === 'medium'),
    low: tasks.filter(t => t.priority === 'low')
  };
};

export const groupTasksByAssignee = (tasks) => {
  const groups = {};
  tasks.forEach(task => {
    const assignee = task.assignee || 'unassigned';
    if (!groups[assignee]) {
      groups[assignee] = [];
    }
    groups[assignee].push(task);
  });
  return groups;
};

// ==================== TASK SORTING ====================
export const sortTasks = (tasks, sortBy = 'created_at', sortOrder = 'desc') => {
  return [...tasks].sort((a, b) => {
    let aVal = a[sortBy];
    let bVal = b[sortBy];
    
    // Special handling for different data types
    if (sortBy === 'priority') {
      aVal = getPriorityValue(aVal);
      bVal = getPriorityValue(bVal);
    } else if (sortBy === 'due_date' || sortBy === 'created_at') {
      aVal = new Date(aVal || 0);
      bVal = new Date(bVal || 0);
    } else if (typeof aVal === 'string') {
      aVal = aVal.toLowerCase();
      bVal = bVal.toLowerCase();
    }
    
    if (sortOrder === 'asc') {
      return aVal > bVal ? 1 : -1;
    } else {
      return aVal < bVal ? 1 : -1;
    }
  });
};

// ==================== TASK VALIDATION ====================
export const validateTask = (taskData) => {
  const errors = {};
  
  if (!taskData.title || taskData.title.trim() === '') {
    errors.title = 'Ti√™u ƒë·ªÅ task kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng';
  }
  
  if (!taskData.assignee || taskData.assignee.trim() === '') {
    errors.assignee = 'Ph·∫£i ch·ªâ ƒë·ªãnh ng∆∞·ªùi th·ª±c hi·ªán';
  }
  
  if (!taskData.priority) {
    errors.priority = 'Ph·∫£i ch·ªçn m·ª©c ƒë·ªô ∆∞u ti√™n';
  }
  
  if (taskData.due_date && new Date(taskData.due_date) < new Date()) {
    errors.due_date = 'Ng√†y h·∫øt h·∫°n kh√¥ng th·ªÉ l√† qu√° kh·ª©';
  }
  
  return {
    isValid: Object.keys(errors).length === 0,
    errors
  };
};

// ==================== FORMAT HELPERS ====================
export const formatTaskForAPI = (formValues) => {
  return {
    ...formValues,
    due_date: formValues.dueDate ? formValues.dueDate.format('YYYY-MM-DD') : null,
    status: formValues.status || 'TODO'
  };
};

export const formatTaskForForm = (task) => {
  return {
    title: task.title,
    description: task.description,
    assignee: task.assignee,
    priority: task.priority,
    status: task.status,
    dueDate: task.due_date ? new Date(task.due_date) : null
  };
};

// ==================== TASK OPERATIONS ====================
export const getNextStatus = (currentStatus) => {
  const statusFlow = {
    'TODO': 'IN_PROGRESS',
    'IN_PROGRESS': 'DONE',
    'DONE': 'TODO' // Reset cycle
  };
  return statusFlow[currentStatus] || 'TODO';
};

export const canEditTask = (task, currentUser) => {
  // Business logic for task editing permissions
  return task.assignee === currentUser.login || 
         task.created_by === currentUser.login ||
         currentUser.role === 'admin';
};

export const getTaskDeadlineStatus = (dueDate) => {
  if (!dueDate) return 'no-deadline';
  
  const today = new Date();
  const deadline = new Date(dueDate);
  const diffDays = Math.ceil((deadline - today) / (1000 * 60 * 60 * 24));
  
  if (diffDays < 0) return 'overdue';
  if (diffDays === 0) return 'due-today';
  if (diffDays <= 3) return 'due-soon';
  return 'on-track';
};

export default {
  getDefaultAvatarUrl,
  getAvatarUrl,
  getStatusIcon,
  getStatusText,
  getStatusColor,
  getPriorityColor,
  getPriorityText,
  filterTasks,
  calculateTaskStats,
  groupTasksByStatus,
  sortTasks,
  validateTask,
  formatTaskForAPI,
  formatTaskForForm,
  getNextStatus,
  canEditTask,
  getTaskDeadlineStatus
};

```

### frontend\src\utils\types.js
```js
export const Task = {
  id: '',
  title: '',
  assignee: '',
  status: '', // 'todo', 'inProgress', 'done'
};
```
