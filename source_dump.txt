# ==================================================
# Path: C:\SAN\KLTN\KLTN04
# Detected tech: javascript, python, react, rust, typescript
# ==================================================

## DIRECTORY STRUCTURE
```
KLTN04/
‚îú‚îÄ‚îÄ .git/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github_commits/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ full.csv
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ oneline.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_github_model/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ best_model.pth
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_fusion/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ synthetic_generator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enhanced_text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ minimal_enhanced_text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interpretability.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_calculator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multi_task_losses.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baselines.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shared_layers.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_main.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multitask_trainer.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_results/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_report_20250607_212442.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_report_20250607_212820.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_analysis_report_20250607_213720.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testmodelAi/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_model_demo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_model_real_test_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trained_models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multimodal_fusion_100k/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_commits_training_data.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ large_dataset_processing_summary.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sample_preview.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_logs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_github_training_20250608_011142.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ han_github_training_20250610_204624.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_commit_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_github_data.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_classification_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_test.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_github_commits.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_kaggle_dataset.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_commit_analyzer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_advanced_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_dataset_creator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_environment.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_100k_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_100k_multimodal_fusion.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_enhanced_100k_fixed.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_enhanced_100k_multimodal_fusion_final.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_han_github.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ branch.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_routes.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ issue.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projects.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sync.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deps.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lifespan.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ oauth.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ security.py
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 5b8fe79e0fa5_enhance_commits_branches_add_.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ add_user_repositories_mapping.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ d2fd206aedc4_create_clean_database_schema.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fb48677d0aa5_add_author_avatar_url_to_commits_table.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ script.py.mako
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_model.py
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Model_Training.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit.py
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_analysis_system.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_analysis_system_v1.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ branch_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collaborator_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collaborator_service_new.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitlab_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ han_ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ issue_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pull_request_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository_collaborator_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_resolution_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_service.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ alembic.ini
‚îÇ   ‚îú‚îÄ‚îÄ check_schema.py
‚îÇ   ‚îú‚îÄ‚îÄ check_tables.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ setup_clean_db.sql
‚îÇ   ‚îî‚îÄ‚îÄ setup_database.py
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ node_modules/
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vite.svg
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ react.svg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AI/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AICommitAnalyzer.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AIDashboard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AIRepositoryInsights.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Branchs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BranchSelector.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProjectTaskManager/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DragOverlayContent.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DroppableColumn.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FiltersPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard.module.css
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard_new.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoSelector.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SortableTaskCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ StatisticsPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TROUBLESHOOTING.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskModal.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kanbanConstants.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kanbanUtils.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useKanbanDragDrop.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AIInsightWidget.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OverviewCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProjectTaskManager.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoListFilter.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TaskBoard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commits/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnalyzeGitHubCommits.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitAnalysisBadge.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitAnalysisModal.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommitList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CommitTable.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SyncProgressNotification.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RepoList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AliasTest.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SimpleAliasTest.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SyncContext.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ github/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthSuccess.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RepoDetails.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TestPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.css
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.css
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.jsx
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore
‚îÇ   ‚îú‚îÄ‚îÄ ALIAS_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ eslint.config.js
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ jsconfig.json
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ source_dump.txt
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.js
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ API_DOCUMENTATION.md
‚îú‚îÄ‚îÄ Backend.txt
‚îú‚îÄ‚îÄ DEVELOPER_PATTERN_ANALYSIS_GUIDE.md
‚îú‚îÄ‚îÄ Frontend.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ source_dump.txt
```

## FILE CONTENTS

### backend\check_schema.py
```py

```

### backend\check_tables.py
```py

```

### backend\main.py
```py
# backend/main.py
from fastapi import FastAPI
from core.lifespan import lifespan
from core.config import setup_middlewares
from core.logger import setup_logger
from services.ai_service import router as ai_router
from api.routes.commit_routes import router as commit_router

from api.routes.auth import auth_router
from api.routes.github import github_router
from api.routes.projects import router as projects_router
from api.routes.ai import ai_router as han_ai_router

setup_logger()  # B·∫≠t logger tr∆∞·ªõc khi ch·∫°y app

app = FastAPI(lifespan=lifespan)

setup_middlewares(app)

# Include routers tr·ª±c ti·∫øp
app.include_router(auth_router, prefix="/api")
app.include_router(github_router, prefix="/api")
app.include_router(projects_router, prefix="/api")
app.include_router(han_ai_router, prefix="/api")
app.include_router(ai_router, prefix="/ai")  # gi·ªØ l·∫°i n·∫øu c·∫ßn legacy
app.include_router(commit_router)

@app.get("/")
def root():
    return {"message": "TaskFlowAI backend is running "}

```

### backend\setup_database.py
```py

```

### backend\__init__.py
```py

```

### backend\ai\advanced_commit_analysis.py
```py
#!/usr/bin/env python3
"""
Advanced Commit Analyzer - Ph√¢n t√≠ch chi ti·∫øt v√† ƒë∆∞a ra khuy·∫øn ngh·ªã
"""

import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd

def load_analysis_report(report_path):
    """Load b√°o c√°o ph√¢n t√≠ch t·ª´ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_visualizations(report_data, output_dir):
    """T·∫°o c√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Set style
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. Commit Type Distribution
    commit_types = report_data['overall_distributions']['commit_types']
    
    plt.figure(figsize=(10, 6))
    plt.subplot(2, 2, 1)
    plt.pie(commit_types.values(), labels=commit_types.keys(), autopct='%1.1f%%', startangle=90)
    plt.title('Ph√¢n b·ªë lo·∫°i commit')
    
    # 2. Author Activity Levels
    activity_levels = report_data['activity_analysis']['activity_levels']
    
    plt.subplot(2, 2, 2)
    bars = plt.bar(activity_levels.keys(), activity_levels.values())
    plt.title('M·ª©c ƒë·ªô ho·∫°t ƒë·ªông c·ªßa t√°c gi·∫£')
    plt.ylabel('S·ªë l∆∞·ª£ng t√°c gi·∫£')
    
    # Color bars differently
    colors = ['red', 'orange', 'green', 'blue']
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    # 3. Purpose Distribution
    purposes = report_data['overall_distributions']['purposes']
    
    plt.subplot(2, 2, 3)
    plt.barh(list(purposes.keys()), list(purposes.values()))
    plt.title('Ph√¢n b·ªë m·ª•c ƒë√≠ch commit')
    plt.xlabel('S·ªë l∆∞·ª£ng')
    
    # 4. Sentiment Distribution
    sentiments = report_data['overall_distributions']['sentiments']
    
    plt.subplot(2, 2, 4)
    colors_sentiment = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'urgent': 'orange'}
    sentiment_colors = [colors_sentiment.get(s, 'blue') for s in sentiments.keys()]
    plt.bar(sentiments.keys(), sentiments.values(), color=sentiment_colors)
    plt.title('Ph√¢n b·ªë c·∫£m x√∫c commit')
    plt.ylabel('S·ªë l∆∞·ª£ng')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'commit_analysis_overview.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Bi·ªÉu ƒë·ªì t·ªïng quan ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_dir / 'commit_analysis_overview.png'}")

def analyze_author_patterns(report_data):
    """Ph√¢n t√≠ch pattern c·ªßa t·ª´ng t√°c gi·∫£"""
    print("\n" + "="*80)
    print("üîç PH√ÇN T√çCH CHI TI·∫æT PATTERN C·ª¶A T√ÅC GI·∫¢")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\nüë§ {author_name}:")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        print(f"   üìà M·ª©c ƒë·ªô ho·∫°t ƒë·ªông: {stats['activity_level'].upper()}")
        print(f"   üéØ Confidence trung b√¨nh: {stats['avg_confidence']:.3f}")
        
        # Ph√¢n t√≠ch commit types
        if stats['commit_types']:
            most_common_type = max(stats['commit_types'], key=stats['commit_types'].get)
            type_percentage = (stats['commit_types'][most_common_type] / stats['total_commits']) * 100
            print(f"   üè∑Ô∏è  Lo·∫°i commit ch·ªß y·∫øu: {most_common_type} ({type_percentage:.1f}%)")
        
        # Ph√¢n t√≠ch purposes
        if stats['purposes']:
            most_common_purpose = max(stats['purposes'], key=stats['purposes'].get)
            purpose_percentage = (stats['purposes'][most_common_purpose] / stats['total_commits']) * 100
            print(f"   üéØ M·ª•c ƒë√≠ch ch·ªß y·∫øu: {most_common_purpose} ({purpose_percentage:.1f}%)")
        
        # Ph√¢n t√≠ch sentiment
        if stats['sentiments']:
            most_common_sentiment = max(stats['sentiments'], key=stats['sentiments'].get)
            sentiment_percentage = (stats['sentiments'][most_common_sentiment] / stats['total_commits']) * 100
            print(f"   üòä C·∫£m x√∫c ch·ªß y·∫øu: {most_common_sentiment} ({sentiment_percentage:.1f}%)")

def generate_recommendations(report_data):
    """T·∫°o khuy·∫øn ngh·ªã cho team"""
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Khuy·∫øn ngh·ªã cho overloaded authors
    if overloaded_authors:
        print(f"\nüî• T√åNH TR·∫†NG QU√Å T·∫¢I ({len(overloaded_authors)} t√°c gi·∫£):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"   ‚ö†Ô∏è  {author}: {stats['total_commits']} commits")
            print(f"      üí° Khuy·∫øn ngh·ªã: C√¢n nh·∫Øc ph√¢n ph·ªëi c√¥ng vi·ªác ho·∫∑c h·ªó tr·ª£ th√™m nh√¢n l·ª±c")
            
            # Ph√¢n t√≠ch lo·∫°i commit ƒë·ªÉ ƒë∆∞a ra khuy·∫øn ngh·ªã c·ª• th·ªÉ
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"      üêõ Nhi·ªÅu fix commits ({fix_count}): C·∫ßn review code k·ªπ h∆°n ho·∫∑c tƒÉng c∆∞·ªùng testing")
                
                feat_count = stats['commit_types'].get('feat', 0)
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"      ‚ú® Nhi·ªÅu feature commits ({feat_count}): T√°c gi·∫£ c√≥ th·ªÉ l√† key developer")
    
    # Khuy·∫øn ngh·ªã cho low activity authors
    if low_activity_authors:
        print(f"\nüí§ HO·∫†T ƒê·ªòNG TH·∫§P ({len(low_activity_authors)} t√°c gi·∫£):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"   üìâ {author}: {stats['total_commits']} commits")
            print(f"      üí° Khuy·∫øn ngh·ªã: Ki·ªÉm tra workload, cung c·∫•p h·ªó tr·ª£ ho·∫∑c training th√™m")
    
    # Ph√¢n t√≠ch overall patterns
    print(f"\nüìà PH√ÇN T√çCH T·ªîNG QUAN:")
    
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = sum(commit_types.values())
    
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        print(f"   üêõ T·ª∑ l·ªá fix commits cao ({fix_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: TƒÉng c∆∞·ªùng code review, testing, v√† quality assurance")
    
    if feat_percentage < 30:
        print(f"   üì¶ T·ª∑ l·ªá feature commits th·∫•p ({feat_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: C√¢n nh·∫Øc tƒÉng t·ªëc ƒë·ªô ph√°t tri·ªÉn t√≠nh nƒÉng m·ªõi")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   üòû T·ª∑ l·ªá sentiment ti√™u c·ª±c cao ({negative_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: Ki·ªÉm tra morale c·ªßa team, c·∫£i thi·ªán quy tr√¨nh l√†m vi·ªác")
    
    if urgent_percentage > 10:
        print(f"   üö® T·ª∑ l·ªá urgent commits cao ({urgent_percentage:.1f}%)")
        print(f"      üí° Khuy·∫øn ngh·ªã: C·∫£i thi·ªán planning v√† risk management")

def create_team_dashboard(report_data, output_dir):
    """T·∫°o dashboard t·ªïng quan cho team"""
    output_dir = Path(output_dir)
    
    # Create a comprehensive team report
    dashboard_data = {
        "timestamp": datetime.now().isoformat(),
        "team_health": {
            "total_authors": len(report_data['author_statistics']),
            "total_commits": report_data['summary']['total_commits'],
            "avg_commits_per_author": report_data['summary']['avg_commits_per_author']
        },
        "risk_indicators": {
            "overloaded_authors": len(report_data['activity_analysis']['overloaded_authors']),
            "low_activity_authors": len(report_data['activity_analysis']['low_activity_authors']),
            "fix_percentage": (report_data['overall_distributions']['commit_types'].get('fix', 0) / 
                             report_data['summary']['total_commits']) * 100
        },
        "recommendations": []
    }
    
    # Add recommendations
    if dashboard_data['risk_indicators']['overloaded_authors'] > 0:
        dashboard_data['recommendations'].append({
            "type": "workload_balancing",
            "priority": "high",
            "message": f"C√≥ {dashboard_data['risk_indicators']['overloaded_authors']} t√°c gi·∫£ b·ªã qu√° t·∫£i"
        })
    
    if dashboard_data['risk_indicators']['fix_percentage'] > 40:
        dashboard_data['recommendations'].append({
            "type": "quality_improvement",
            "priority": "medium",
            "message": f"T·ª∑ l·ªá fix commits cao ({dashboard_data['risk_indicators']['fix_percentage']:.1f}%)"
        })
    
    # Save dashboard
    dashboard_file = output_dir / 'team_dashboard.json'
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"üìä Team dashboard ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {dashboard_file}")

def main():
    """H√†m ch√≠nh ƒë·ªÉ ph√¢n t√≠ch n√¢ng cao"""
    print("üöÄ ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c test_results. H√£y ch·∫°y test_commit_analyzer.py tr∆∞·ªõc.")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file b√°o c√°o. H√£y ch·∫°y test_commit_analyzer.py tr∆∞·ªõc.")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"üìÑ ƒêang ph√¢n t√≠ch: {latest_report.name}")
    
    # Load report data
    report_data = load_analysis_report(latest_report)
    
    # Create output directory for advanced analysis
    advanced_output_dir = test_results_dir / "advanced_analysis"
    advanced_output_dir.mkdir(exist_ok=True)
    
    # Perform advanced analysis
    analyze_author_patterns(report_data)
    generate_recommendations(report_data)
    
    # Create visualizations
    try:
        create_visualizations(report_data, advanced_output_dir)
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o bi·ªÉu ƒë·ªì: {e}")
    
    # Create team dashboard
    create_team_dashboard(report_data, advanced_output_dir)
    
    print(f"\n‚úÖ Ph√¢n t√≠ch n√¢ng cao ho√†n th√†nh!")
    print(f"üìÅ K·∫øt qu·∫£ l∆∞u t·∫°i: {advanced_output_dir}")

if __name__ == "__main__":
    main()

```

### backend\ai\clean_github_data.py
```py
#!/usr/bin/env python3
"""
GitHub Data Processor - Download v√† Clean d·ªØ li·ªáu cho Multi-Modal Fusion Network
=============================================================================
Script n√†y s·∫Ω:
1. Download dataset GitHub commits t·ª´ Kaggle
2. Clean v√† chu·∫©n h√≥a d·ªØ li·ªáu 
3. T·∫°o labels ph√π h·ª£p cho multi-task learning
4. Xu·∫•t ra format chu·∫©n cho training
"""

import pandas as pd
import numpy as np
import json
import re
from pathlib import Path
from datetime import datetime
from collections import Counter
import traceback
from typing import Dict, List, Tuple, Any
import random

# Import ƒë·ªÉ t·∫°o synthetic metadata
from multimodal_fusion.data.synthetic_generator import GitHubDataGenerator

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"‚ùå L·ªói setup Kaggle API: {e}")
        print("üí° Vui l√≤ng ch·∫°y: pip install kaggle")
        print("üí° Ho·∫∑c setup API key theo h∆∞·ªõng d·∫´n: https://github.com/Kaggle/kaggle-api")
        return None

def download_github_dataset(api, force_download=False):
    """Download dataset GitHub commits t·ª´ Kaggle"""
    try:
        # T·∫°o th∆∞ m·ª•c download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Ki·ªÉm tra ƒë√£ download ch∆∞a
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"‚úÖ Dataset ƒë√£ t·ªìn t·∫°i: {csv_files[0]}")
            return csv_files[0]
        
        print("üì• ƒêang download GitHub commit dataset t·ª´ Kaggle...")
        print("üìÅ Dataset: dhruvildave/github-commit-messages-dataset")
        
        # Download dataset
        api.dataset_download_files(
            "dhruvildave/github-commit-messages-dataset", 
            path=str(download_dir), 
            unzip=True
        )
        
        # T√¨m file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV trong dataset")
            return None
        
        csv_file = csv_files[0]
        print(f"‚úÖ Download th√†nh c√¥ng: {csv_file}")
        print(f"üìä K√≠ch th∆∞·ªõc file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return csv_file
        
    except Exception as e:
        print(f"‚ùå L·ªói download dataset: {e}")
        return None

def clean_github_data(csv_file: Path, sample_size: int = 20000) -> pd.DataFrame:
    """Clean v√† chu·∫©n h√≥a d·ªØ li·ªáu GitHub commits"""
    try:
        print(f"\nüìä CLEANING D·ªÆ LI·ªÜU: {csv_file.name}")
        print("="*60)
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi chunk ƒë·ªÉ ti·∫øt ki·ªám memory
        print("üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        
        # ƒê·ªçc sample ƒë·ªÉ xem c·∫•u tr√∫c
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"üìã Columns: {list(sample_df.columns)}")
        
        # T√¨m column ch·ª©a commit message
        message_col = None
        for col in ['message', 'subject', 'commit', 'commit_message']:
            if col in sample_df.columns:
                message_col = col
                break
        
        if not message_col:
            print("‚ùå Kh√¥ng t√¨m th·∫•y column ch·ª©a commit message")
            return None
        
        print(f"üí¨ S·ª≠ d·ª•ng column: '{message_col}' l√†m commit message")
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi sampling hi·ªáu qu·∫£
        print(f"üéØ Sampling {sample_size:,} records...")
        
        # ƒê·ªçc theo chunk v√† sample
        chunk_size = 10000
        sampled_chunks = []
        total_read = 0
        
        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
            total_read += len(chunk)
            
            # Random sample t·ª´ chunk
            if len(chunk) > 0:
                chunk_sample_size = min(sample_size // 20, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
            
            # D·ª´ng khi ƒë·ªß data
            total_sampled = sum(len(c) for c in sampled_chunks)
            if total_sampled >= sample_size:
                break
                
            if total_read % 50000 == 0:
                print(f"  ƒê√£ ƒë·ªçc: {total_read:,} records...")
        
        # Combine chunks
        df = pd.concat(sampled_chunks, ignore_index=True)
        if len(df) > sample_size:
            df = df.sample(n=sample_size).reset_index(drop=True)
        
        print(f"üìä ƒê√£ sample {len(df):,} commits t·ª´ {total_read:,} total")
        
        # B∆Ø·ªöC 1: L√†m s·∫°ch d·ªØ li·ªáu c∆° b·∫£n
        print(f"\nüßπ B∆Ø·ªöC 1: L√ÄM S·∫†CH C∆† B·∫¢N")
        original_count = len(df)
        
        # Lo·∫°i b·ªè messages r·ªóng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè empty: {len(df):,} (-{original_count - len(df)})")
        
        # Lo·∫°i b·ªè messages qu√° ng·∫Øn ho·∫∑c qu√° d√†i
        df = df[df[message_col].str.len().between(3, 200)]
        print(f"  ‚Ä¢ Sau khi l·ªçc ƒë·ªô d√†i (3-200 chars): {len(df):,}")
        
        # Lo·∫°i b·ªè duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè duplicates: {len(df):,}")
        
        # B∆Ø·ªöC 2: Clean text content
        print(f"\nüî§ B∆Ø·ªöC 2: CLEAN TEXT CONTENT")
        
        def clean_commit_message(text):
            """Clean commit message text"""
            if pd.isna(text):
                return ""
                
            text = str(text).strip()
            
            # Remove excessive whitespace
            text = re.sub(r'\s+', ' ', text)
            
            # Remove URLs
            text = re.sub(r'http[s]?://\S+', '[URL]', text)
            
            # Remove email addresses
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
            
            # Remove excessive punctuation
            text = re.sub(r'[!]{2,}', '!', text)
            text = re.sub(r'[?]{2,}', '?', text)
            text = re.sub(r'[.]{3,}', '...', text)
            
            # Remove special characters but keep useful ones
            text = re.sub(r'[^\w\s\-_.,;:!?()\[\]{}#@/\\+=<>|~`]', '', text)
            
            return text.strip()
        
        df[message_col] = df[message_col].apply(clean_commit_message)
        
        # Remove messages that became empty after cleaning
        df = df[df[message_col].str.len() >= 3]
        print(f"  ‚Ä¢ Sau khi clean text: {len(df):,} commits")
        
        # B∆Ø·ªöC 3: Ph√¢n lo·∫°i v√† t·∫°o labels
        print(f"\nüè∑Ô∏è  B∆Ø·ªöC 3: T·∫†O LABELS CHO MULTI-TASK LEARNING")
        
        # Classify commit types
        df['commit_type'] = df[message_col].apply(classify_commit_type)
        df['purpose'] = df[message_col].apply(classify_purpose)
        df['sentiment'] = df[message_col].apply(classify_sentiment)
        df['tech_tag'] = df[message_col].apply(classify_tech_tag)
        
        # Th·ªëng k√™ labels
        print(f"\nüìä TH·ªêNG K√ä LABELS:")
        for col in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            value_counts = df[col].value_counts()
            print(f"  {col}:")
            for val, count in value_counts.head(5).items():
                print(f"    {val}: {count} ({count/len(df)*100:.1f}%)")
        
        # B∆Ø·ªöC 4: T·∫°o metadata synthetic
        print(f"\n‚öôÔ∏è B∆Ø·ªöC 4: T·∫†O METADATA SYNTHETIC")
        generator = GitHubDataGenerator()
        def create_synthetic_metadata():
            """T·∫°o metadata synthetic cho m·ªói commit"""
            sample = generator.generate_single_commit()
            return {
                'author': sample['author'],
                'repository': sample['repository'], 
                'timestamp': sample['timestamp'],
                'files_changed': sample['files_changed'],
                'additions': sample['additions'],
                'deletions': sample['deletions'],
                'file_types': sample['file_types']
            }
        
        # Apply synthetic metadata
        metadata_list = [create_synthetic_metadata() for _ in range(len(df))]
        
        for key in ['author', 'repository', 'timestamp', 'files_changed', 'additions', 'deletions']:
            df[f'meta_{key}'] = [meta[key] for meta in metadata_list]
        
        # File types c·∫ßn x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ l√† list
        df['meta_file_types'] = [meta['file_types'] for meta in metadata_list]
        
        print(f"‚úÖ ƒê√£ t·∫°o synthetic metadata cho {len(df):,} commits")
        
        return df
        
    except Exception as e:
        print(f"‚ùå L·ªói clean d·ªØ li·ªáu: {e}")
        traceback.print_exc()
        return None

def classify_commit_type(message: str) -> str:
    """Ph√¢n lo·∫°i commit type d·ª±a theo conventional commits"""
    message = message.lower()
    
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new|create)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch|repair)\b'],
        'docs': [r'\b(doc|documentation|readme|comment|guide)\b'],
        'style': [r'\b(style|format|lint|clean|prettier|cosmetic)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup|improve)\b'],
        'test': [r'\b(test|spec|testing|coverage|unit|integration)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge|maint)\b'],
        'perf': [r'\b(perf|performance|optimize|speed|fast|slow)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization|vulnerability)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message: str) -> str:
    """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch c·ªßa commit"""
    message = message.lower()
    
    if re.search(r'\b(add|new|implement|create|build|introduce)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve|repair)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve|optimize)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment|guide)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage|unit)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability|exploit)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier|cosmetic)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release|version)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message: str) -> str:
    """Ph√¢n lo·∫°i sentiment c·ªßa commit"""
    message = message.lower()
    
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap', 'breaking']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'crash', 'wrong']
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good', 'success']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message: str) -> str:
    """Ph√¢n lo·∫°i technology tag"""
    message = message.lower()
    
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular|typescript|ts)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi|pandas|numpy)\b'],
        'java': [r'\b(java|maven|gradle|spring|junit)\b'],
        'css': [r'\b(css|scss|sass|style|styling|bootstrap)\b'],
        'html': [r'\b(html|template|markup|dom)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo|redis|sqlite)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service|http|request)\b'],
        'docker': [r'\b(docker|container|dockerfile|kubernetes|k8s)\b'],
        'git': [r'\b(git|merge|branch|commit|pull|push|clone)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration|e2e|pytest|jest)\b'],
        'security': [r'\b(security|auth|token|password|encrypt|decrypt|ssl|tls)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed|memory|cpu)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive|mobile)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def convert_to_training_format(df: pd.DataFrame, message_col: str) -> List[Dict]:
    """Convert cleaned DataFrame th√†nh format cho training"""
    print(f"\nüîÑ CONVERT SANG TRAINING FORMAT")
    
    training_samples = []
    
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"  Processed {idx}/{len(df)} samples")
            
        # T·∫°o sample theo format chu·∫©n
        sample = {
            'commit_message': row[message_col],
            'author': row['meta_author'],
            'repository': row['meta_repository'],
            'timestamp': row['meta_timestamp'],
            'files_changed': row['meta_files_changed'],
            'additions': row['meta_additions'],
            'deletions': row['meta_deletions'],
            'file_types': row['meta_file_types'],
            'labels': {
                'risk_prediction': classify_risk_level(row[message_col], row['commit_type']),
                'complexity_prediction': classify_complexity(row[message_col], row['meta_files_changed']),
                'hotspot_prediction': classify_hotspot(row['commit_type'], row['tech_tag']),
                'urgency_prediction': classify_urgency(row['sentiment'])
            }
        }
        
        training_samples.append(sample)
    
    print(f"‚úÖ Converted {len(training_samples)} samples")
    return training_samples

def classify_risk_level(message: str, commit_type: str) -> int:
    """Classify risk level (0: low, 1: high)"""
    high_risk_patterns = ['breaking', 'major', 'critical', 'breaking change', 'api change']
    high_risk_types = ['feat', 'refactor', 'security']
    
    message_lower = message.lower()
    
    if any(pattern in message_lower for pattern in high_risk_patterns):
        return 1
    elif commit_type in high_risk_types:
        return 1
    else:
        return 0

def classify_complexity(message: str, files_changed: int) -> int:
    """Classify complexity (0: low, 1: medium, 2: high)"""
    if files_changed >= 10:
        return 2
    elif files_changed >= 5:
        return 1
    else:
        return 0

def classify_hotspot(commit_type: str, tech_tag: str) -> int:
    """Classify hotspot area (0-4)"""
    hotspot_map = {
        'security': 0,
        'api': 1,
        'database': 2,
        'ui': 3,
        'general': 4
    }
    return hotspot_map.get(tech_tag, 4)

def classify_urgency(sentiment: str) -> int:
    """Classify urgency (0: normal, 1: urgent)"""
    return 1 if sentiment == 'urgent' else 0

def save_training_data(samples: List[Dict], output_dir: Path) -> str:
    """L∆∞u training data ƒë√£ clean"""
    output_dir.mkdir(exist_ok=True)
    
    # T·∫°o filename v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"cleaned_github_commits_{timestamp}.json"
    
    # T·∫°o metadata
    training_data = {
        'metadata': {
            'total_samples': len(samples),
            'created_at': datetime.now().isoformat(),
            'source': 'kaggle_github_commits_cleaned',
            'version': '1.0',
            'description': 'Cleaned GitHub commit data for Multi-Modal Fusion Network'
        },
        'samples': samples
    }
    
    # Save to JSON
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u {len(samples)} samples v√†o: {output_file}")
    print(f"üìä File size: {output_file.stat().st_size / (1024*1024):.1f} MB")
    
    return str(output_file)

def main():
    """Main function"""
    print("üöÄ GITHUB DATA PROCESSOR - CLEAN D·ªÆ LI·ªÜU CHO TRAINING")
    print("="*70)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        print("\n‚ùå Kh√¥ng th·ªÉ setup Kaggle API")
        print("üí° B·∫°n c√≥ th·ªÉ manually t·∫£i file CSV v√† ƒë·∫∑t v√†o th∆∞ m·ª•c kaggle_data/github_commits/")
        
        # Ki·ªÉm tra file manual
        manual_files = list(Path("kaggle_data/github_commits").glob("*.csv"))
        if manual_files:
            csv_file = manual_files[0]
            print(f"‚úÖ T√¨m th·∫•y file manual: {csv_file}")
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o")
            return
    else:
        # Download dataset
        csv_file = download_github_dataset(api)
        if not csv_file:
            print("‚ùå Kh√¥ng th·ªÉ download dataset")
            return
    
    # H·ªèi user v·ªÅ sample size
    print(f"\nüìä T√ôY CH·ªåN SAMPLE SIZE:")
    print("1. 5K samples (test nhanh)")
    print("2. 10K samples (demo)")
    print("3. 20K samples (khuy√™n d√πng)")
    print("4. 50K samples (training t·ªët)")
    print("5. 100K samples (dataset l·ªõn)")
    
    choice = input("Ch·ªçn option (1-5) [m·∫∑c ƒë·ªãnh: 3]: ").strip() or "3"
    
    sample_sizes = {
        '1': 5000,
        '2': 10000,
        '3': 20000,
        '4': 50000,
        '5': 100000
    }
    
    sample_size = sample_sizes.get(choice, 20000)
    print(f"üéØ S·∫Ω sample {sample_size:,} commits")
    
    # Clean data
    df = clean_github_data(csv_file, sample_size)
    if df is None:
        print("‚ùå Kh√¥ng th·ªÉ clean d·ªØ li·ªáu")
        return
    
    # Convert to training format
    message_col = 'message'  # ho·∫∑c t√¨m t·ª± ƒë·ªông
    for col in ['message', 'subject', 'commit', 'commit_message']:
        if col in df.columns:
            message_col = col
            break
    
    training_samples = convert_to_training_format(df, message_col)
    
    # Save cleaned data
    output_dir = Path("training_data")
    output_file = save_training_data(training_samples, output_dir)
    
    print(f"\nüéâ HO√ÄN TH√ÄNH CLEANING D·ªÆ LI·ªÜU!")
    print(f"üìÅ File output: {output_file}")
    print(f"üìä S·ªë samples: {len(training_samples):,}")
    
    print(f"\n‚ú® S·∫¥N S√ÄNG CHO TRAINING:")
    print(f"  python train_real_data.py")
    print(f"  # Ho·∫∑c s·ª≠ d·ª•ng file: {Path(output_file).name}")

if __name__ == "__main__":
    main()

```

### backend\ai\commit_model.py
```py
# AI commit model for TaskFlowAI
import os
import joblib
from pathlib import Path

class CommitClassifier:
    def __init__(self):
        self.model = None
        self.vectorizer = None
    
    @classmethod
    def load(cls):
        """Load the trained commit classifier model"""
        instance = cls()
        
        # ƒê∆∞·ªùng d·∫´n t·ªõi model trong th∆∞ m·ª•c AI
        model_dir = Path(__file__).parent / "trained_models"
        model_path = model_dir / "commit_classifier.joblib"
        
        try:
            if model_path.exists():
                data = joblib.load(model_path)
                if isinstance(data, dict):
                    instance.model = data.get('model')
                    instance.vectorizer = data.get('vectorizer')
                else:
                    instance.model = data
                print(f"‚úÖ Loaded commit classifier from {model_path}")
            else:
                print(f"‚ö†Ô∏è Model file not found at {model_path}")
                # T·∫°o mock model ƒë·ªÉ tr√°nh l·ªói
                instance._create_mock_model()
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            instance._create_mock_model()
        
        return instance
    
    def _create_mock_model(self):
        """Create a mock model for development"""
        print("üîß Creating mock commit classifier...")
        # Mock implementation
        self.model = None
        self.vectorizer = None
    
    def classify(self, commit_message):
        """Classify a commit message"""
        if self.model is None:
            # Mock classification
            return {
                'category': 'feature',
                'confidence': 0.85,
                'description': 'Mock classification result'
            }
        
        try:
            # Real classification logic would go here
            if self.vectorizer:
                features = self.vectorizer.transform([commit_message])
                prediction = self.model.predict(features)[0]
                confidence = self.model.predict_proba(features).max()
                
                return {
                    'category': prediction,
                    'confidence': confidence,
                    'description': f'Classified as {prediction}'
                }
        except Exception as e:
            print(f"Classification error: {e}")
        
        # Fallback
        return {
            'category': 'other',
            'confidence': 0.5,
            'description': 'Classification failed, using fallback'
        }
    
    def save(self, path=None):
        """Save the model"""
        if path is None:
            model_dir = Path(__file__).parent / "trained_models"
            model_dir.mkdir(exist_ok=True)
            path = model_dir / "commit_classifier.joblib"
        
        try:
            if self.model and self.vectorizer:
                data = {
                    'model': self.model,
                    'vectorizer': self.vectorizer
                }
                joblib.dump(data, path)
                print(f"‚úÖ Model saved to {path}")
            else:
                print("‚ö†Ô∏è No model to save")
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")

```

### backend\ai\debug_classification_fixed.py
```py
#!/usr/bin/env python3
"""
Debug script ƒë·ªÉ ki·ªÉm tra v·∫•n ƒë·ªÅ ph√¢n lo·∫°i commit type
"""

import torch
import os
import sys
from test_commit_analyzer import CommitAnalyzer

def test_problematic_commits():
    """Test c√°c commit messages c√≥ v·∫•n ƒë·ªÅ ph√¢n lo·∫°i"""
    
    print("üîç DEBUGGING COMMIT CLASSIFICATION ISSUES")
    print("="*60)
    
    # Initialize analyzer v·ªõi model path
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test cases c√≥ v·∫•n ƒë·ªÅ
    test_cases = [
        {
            "message": "docs: fix typo in configuration guide", 
            "expected": "docs",
            "author": "Test User"
        },
        {
            "message": "docs: update installation instructions",
            "expected": "docs", 
            "author": "Test User"
        },
        {
            "message": "test: add unit tests for user service",
            "expected": "test",
            "author": "Test User" 
        },
        {
            "message": "test: fix failing integration tests",
            "expected": "test",
            "author": "Test User"
        },
        {
            "message": "fix: typo in variable name",
            "expected": "fix",
            "author": "Test User"
        },
        {
            "message": "feat: add new documentation system", 
            "expected": "feat",
            "author": "Test User"
        },
        {
            "message": "chore: update dependencies",
            "expected": "chore", 
            "author": "Test User"
        },
        {
            "message": "style: fix code formatting",
            "expected": "style",
            "author": "Test User"
        }
    ]
    
    print(f"üß™ Testing {len(test_cases)} problematic commit messages...")
    print()
    
    correct_predictions = 0
    total_predictions = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        message = case["message"]
        expected = case["expected"]
        author = case["author"]
        
        print(f"{i}. Testing: '{message}'")
        print(f"   Expected: {expected}")
        
        # Analyze commit
        analysis = analyzer.predict_commit(message, author)
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        
        print(f"   Predicted: {predicted} (confidence: {confidence:.3f})")
        
        # Check if correct
        is_correct = predicted == expected
        if is_correct:
            print("   ‚úÖ CORRECT")
            correct_predictions += 1
        else:
            print("   ‚ùå WRONG")
            
            # Analyze why it's wrong - show all predictions for this commit
            print("   üîç Full predictions:")
            for task, pred in analysis.predicted_labels.items():
                conf = analysis.confidence_scores.get(task, 0.0)
                print(f"      {task}: {pred} ({conf:.3f})")
        
        print()
    
    # Summary
    accuracy = correct_predictions / total_predictions * 100
    print("="*60)
    print(f"üìä CLASSIFICATION ACCURACY: {correct_predictions}/{total_predictions} ({accuracy:.1f}%)")
    
    if accuracy < 80:
        print("üö® LOW ACCURACY DETECTED!")
        print("üí° Possible issues:")
        print("   - Model wasn't trained properly on commit prefixes")
        print("   - Training data didn't have enough conventional commit examples")
        print("   - Model is focusing on content words rather than prefixes")
        print("   - Need to retrain with better conventional commit dataset")
    else:
        print("‚úÖ Classification accuracy looks good!")
    
    return accuracy

def analyze_model_attention():
    """Ph√¢n t√≠ch xem model ƒëang ch√∫ √Ω v√†o ph·∫ßn n√†o c·ªßa commit message"""
    
    print("\nüß† ANALYZING MODEL ATTENTION PATTERNS")
    print("="*60)
    
    model_path = r"C:\SAN\KLTN\KLTN04\backend\ai\models\han_github_model\best_model.pth"
    analyzer = CommitAnalyzer(model_path)
    
    # Test v·ªõi c√°c variations
    test_variations = [
        "docs: fix typo in configuration guide",
        "fix typo in configuration guide",  # Kh√¥ng c√≥ prefix
        "docs: update configuration guide",  # Kh√¥ng c√≥ t·ª´ "fix"
        "fix: typo in configuration guide"   # Prefix kh√°c
    ]
    
    print("Testing how model responds to different parts of the message:")
    print()
    
    for i, message in enumerate(test_variations, 1):
        print(f"{i}. '{message}'")
        analysis = analyzer.predict_commit(message, "Test User")
        predicted = analysis.predicted_labels.get('commit_type', 'unknown')
        confidence = analysis.confidence_scores.get('commit_type', 0.0)
        print(f"   ‚Üí {predicted} ({confidence:.3f})")
        print()
    
    return test_variations

if __name__ == "__main__":
    try:
        accuracy = test_problematic_commits()
        analyze_model_attention()
        
        print("\nüí° RECOMMENDATIONS:")
        print("="*60)
        
        if accuracy < 80:
            print("üîß TO FIX CLASSIFICATION ISSUES:")
            print("1. Retrain model with more conventional commit examples")
            print("2. Add prefix-aware preprocessing")
            print("3. Use rule-based fallback for obvious prefixes")
            print("4. Create training data with equal distribution of commit types")
            print("5. Consider ensemble model (ML + rule-based)")
        
        print("\n‚úÖ Debug completed!")
        
    except Exception as e:
        print(f"‚ùå Error during debugging: {e}")
        import traceback
        traceback.print_exc()

```

### backend\ai\debug_test.py
```py
#!/usr/bin/env python3
"""
Simple Commit Analyzer Test - Debug version
"""

import os
import sys
import json
import torch
from pathlib import Path

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

def test_model_loading():
    """Test basic model loading"""
    print("üöÄ SIMPLE COMMIT ANALYZER TEST")
    print("="*50)
    
    # Check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Device: {device}")
    
    # Check model file
    model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
    print(f"üì¶ Model path: {model_path}")
    print(f"üì¶ Model exists: {model_path.exists()}")
    
    if not model_path.exists():
        print("‚ùå Model file not found!")
        return False
    
    try:
        # Load checkpoint
        print("üì• Loading checkpoint...")
        checkpoint = torch.load(model_path, map_location=device)
        
        print("‚úÖ Checkpoint loaded successfully!")
        print(f"   Keys: {list(checkpoint.keys())}")
        
        if 'num_classes' in checkpoint:
            print(f"   Tasks: {list(checkpoint['num_classes'].keys())}")
            print(f"   Classes per task: {checkpoint['num_classes']}")
        
        if 'val_accuracy' in checkpoint:
            print(f"   Best accuracy: {checkpoint['val_accuracy']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading checkpoint: {e}")
        return False

def test_simple_prediction():
    """Test a simple prediction"""
    try:
        from train_han_github import SimpleHANModel, SimpleTokenizer
        print("\nüß™ Testing simple prediction...")
        
        # Load model components
        model_path = Path(__file__).parent / "models" / "han_github_model" / "best_model.pth"
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        checkpoint = torch.load(model_path, map_location=device)
        tokenizer = checkpoint['tokenizer']
        num_classes = checkpoint['num_classes']
        label_encoders = checkpoint['label_encoders']
        
        # Create reverse label encoders
        reverse_encoders = {}
        for task, encoder in label_encoders.items():
            reverse_encoders[task] = {v: k for k, v in encoder.items()}
        
        # Initialize model
        vocab_size = len(tokenizer.word_to_idx)
        model = SimpleHANModel(
            vocab_size=vocab_size,
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"‚úÖ Model initialized with vocab size: {vocab_size}")
        
        # Test prediction
        test_text = "fix: resolve authentication bug in login endpoint"
        print(f"üìù Testing text: '{test_text}'")
        
        # Tokenize
        input_ids = tokenizer.encode_text(test_text, max_sentences=10, max_words=50)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
            
            print(f"üîç Predictions:")
            for task, output in outputs.items():
                probs = torch.softmax(output, dim=1)
                confidence, pred_idx = torch.max(probs, 1)
                
                pred_idx = pred_idx.item()
                confidence = confidence.item()
                
                predicted_label = reverse_encoders[task][pred_idx]
                print(f"   {task}: {predicted_label} (confidence: {confidence:.3f})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error in prediction test: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("Starting debug tests...\n")
    
    # Test 1: Model loading
    if test_model_loading():
        print("\n" + "="*50)
        # Test 2: Simple prediction
        test_simple_prediction()
    
    print("\nüéØ Debug tests completed!")

```

### backend\ai\download_github_commits.py
```py
"""
Download v√† x·ª≠ l√Ω dataset GitHub Commit Messages t·ª´ Kaggle
Dataset: mrisdal/github-commit-messages
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import re
from collections import Counter
import zipfile

def setup_kaggle_api():
    """Setup Kaggle API"""
    try:
        import kaggle
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"‚ùå L·ªói setup Kaggle API: {e}")
        print("Vui l√≤ng ch·∫°y: python quick_kaggle_setup.py")
        return None

def download_dataset(api, dataset_name="dhruvildave/github-commit-messages-dataset", force_download=False):
    """Download dataset t·ª´ Kaggle"""
    try:
        # T·∫°o th∆∞ m·ª•c download
        download_dir = Path(__file__).parent / "kaggle_data" / "github_commits"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # Ki·ªÉm tra ƒë√£ download ch∆∞a
        csv_files = list(download_dir.glob("*.csv"))
        if csv_files and not force_download:
            print(f"‚úÖ Dataset ƒë√£ t·ªìn t·∫°i trong {download_dir}")
            return download_dir, csv_files[0]
        
        print(f"üì• ƒêang download dataset: {dataset_name}")
        print(f"üìÅ V√†o th∆∞ m·ª•c: {download_dir}")
        
        # Download dataset
        api.dataset_download_files(
            dataset_name, 
            path=str(download_dir), 
            unzip=True
        )
        
        # T√¨m file CSV
        csv_files = list(download_dir.glob("*.csv"))
        if not csv_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV trong dataset")
            return None, None
        
        csv_file = csv_files[0]
        print(f"‚úÖ Download th√†nh c√¥ng: {csv_file}")
        print(f"üìä K√≠ch th∆∞·ªõc file: {csv_file.stat().st_size / (1024*1024):.1f} MB")
        
        return download_dir, csv_file
        
    except Exception as e:
        print(f"‚ùå L·ªói download dataset: {e}")
        return None, None

def analyze_commit_data(csv_file, sample_size=None):
    """Ph√¢n t√≠ch d·ªØ li·ªáu commit"""
    try:
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU: {csv_file.name}")
        print("="*60)
        
        # ƒê·ªçc d·ªØ li·ªáu v·ªõi chunk ƒë·ªÉ ti·∫øt ki·ªám memory
        print("üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        
        # ƒê·ªçc m·ªôt ph·∫ßn nh·ªè tr∆∞·ªõc ƒë·ªÉ xem c·∫•u tr√∫c
        sample_df = pd.read_csv(csv_file, nrows=1000)
        print(f"üìã Columns: {list(sample_df.columns)}")
        print(f"üìè Sample shape: {sample_df.shape}")
          # ƒê·ªçc to√†n b·ªô ho·∫∑c sample m·ªôt c√°ch hi·ªáu qu·∫£
        if sample_size:
            print(f"üìä Sampling {sample_size:,} records...")
            # S·ª≠ d·ª•ng chunk reading ƒë·ªÉ memory-efficient sampling
            chunk_size = 10000
            sampled_chunks = []
            total_read = 0
            
            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
                total_read += len(chunk)
                
                # Random sample t·ª´ chunk n√†y
                chunk_sample_size = min(sample_size // 10, len(chunk))
                if chunk_sample_size > 0:
                    chunk_sample = chunk.sample(n=chunk_sample_size)
                    sampled_chunks.append(chunk_sample)
                
                # D·ª´ng khi ƒë√£ ƒë·ªß data
                total_sampled = sum(len(c) for c in sampled_chunks)
                if total_sampled >= sample_size:
                    break
                
                if total_read % 50000 == 0:
                    print(f"  ƒê√£ ƒë·ªçc: {total_read:,} records...")
            
            # Combine chunks
            df = pd.concat(sampled_chunks, ignore_index=True)
            if len(df) > sample_size:
                df = df.sample(n=sample_size).reset_index(drop=True)
            
            print(f"üìä ƒê√£ sample {len(df):,} commits t·ª´ {total_read:,} total")
        else:
            print("üìñ ƒê·ªçc to√†n b·ªô dataset (c√≥ th·ªÉ m·∫•t th·ªùi gian)...")
            df = pd.read_csv(csv_file)
            print(f"üìä ƒê√£ ƒë·ªçc {len(df):,} commits")
        
        # Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n
        print(f"\nüìà TH·ªêNG K√ä C∆† B·∫¢N:")
        print(f"  ‚Ä¢ T·ªïng s·ªë commits: {len(df):,}")
        print(f"  ‚Ä¢ Columns: {df.shape[1]}")
        print(f"  ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # Ki·ªÉm tra columns quan tr·ªçng
        important_cols = ['message', 'commit', 'subject', 'body', 'author', 'repo']
        available_cols = [col for col in important_cols if col in df.columns]
        print(f"  ‚Ä¢ Available important columns: {available_cols}")
        
        # T√¨m column ch·ª©a commit message
        message_col = None
        for col in ['message', 'subject', 'commit']:
            if col in df.columns:
                message_col = col
                break
        
        if not message_col:
            print("‚ùå Kh√¥ng t√¨m th·∫•y column ch·ª©a commit message")
            return None
        
        print(f"  ‚Ä¢ S·ª≠ d·ª•ng column: '{message_col}' l√†m commit message")
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        print(f"\nüßπ L√ÄM S·∫†CH D·ªÆ LI·ªÜU:")
        original_count = len(df)
        
        # Lo·∫°i b·ªè messages r·ªóng
        df = df.dropna(subset=[message_col])
        df = df[df[message_col].str.strip() != '']
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè empty: {len(df):,} (-{original_count - len(df)})")
        
        # Lo·∫°i b·ªè messages qu√° ng·∫Øn ho·∫∑c qu√° d√†i
        df = df[df[message_col].str.len().between(5, 500)]
        print(f"  ‚Ä¢ Sau khi l·ªçc ƒë·ªô d√†i (5-500 chars): {len(df):,}")
        
        # Lo·∫°i b·ªè duplicates
        df = df.drop_duplicates(subset=[message_col])
        print(f"  ‚Ä¢ Sau khi lo·∫°i b·ªè duplicates: {len(df):,}")
        
        # Ph√¢n t√≠ch n·ªôi dung
        print(f"\nüìù PH√ÇN T√çCH N·ªòI DUNG:")
        messages = df[message_col].astype(str)
        
        # Th·ªëng k√™ ƒë·ªô d√†i
        lengths = messages.str.len()
        print(f"  ‚Ä¢ ƒê·ªô d√†i trung b√¨nh: {lengths.mean():.1f} chars")
        print(f"  ‚Ä¢ ƒê·ªô d√†i median: {lengths.median():.1f} chars")
        print(f"  ‚Ä¢ Min/Max: {lengths.min()}/{lengths.max()} chars")
          # Top words - sample ƒë·ªÉ tr√°nh qu√° t·∫£i memory
        sample_size_for_words = min(5000, len(messages))
        print(f"  ‚Ä¢ Analyzing words from {sample_size_for_words} samples...")
        
        all_words = []
        sample_messages = messages.sample(n=sample_size_for_words) if len(messages) > sample_size_for_words else messages
        
        for msg in sample_messages:
            words = re.findall(r'\b[a-zA-Z]+\b', str(msg).lower())
            all_words.extend(words)
        
        word_counts = Counter(all_words).most_common(20)
        print(f"\nüî§ TOP 20 WORDS (from {sample_size_for_words} samples):")
        for word, count in word_counts:
            print(f"    {word}: {count}")
        
        return df, message_col
        
    except Exception as e:
        print(f"‚ùå L·ªói ph√¢n t√≠ch d·ªØ li·ªáu: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def classify_commits(df, message_col):
    """Ph√¢n lo·∫°i commits theo c√°c ti√™u ch√≠"""
    print(f"\nüè∑Ô∏è  PH√ÇN LO·∫†I COMMITS:")
    print("="*60)
    
    messages = df[message_col].astype(str).str.lower()
    classifications = []
    
    # Process in batches ƒë·ªÉ tr√°nh memory issues
    batch_size = 1000
    total_batches = (len(messages) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(messages))
        batch_messages = messages[start_idx:end_idx]
        
        batch_classifications = []
        for message in batch_messages:
            labels = {
                'commit_type': classify_commit_type(message),
                'purpose': classify_purpose(message),
                'sentiment': classify_sentiment(message),
                'tech_tag': classify_tech_tag(message)
            }
            batch_classifications.append(labels)
        
        classifications.extend(batch_classifications)
        
        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:
            print(f"  ƒê√£ x·ª≠ l√Ω: {end_idx:,}/{len(messages):,} ({(end_idx/len(messages)*100):.1f}%)")
    
    # Th·ªëng k√™ ph√¢n lo·∫°i
    print(f"\nüìä TH·ªêNG K√ä PH√ÇN LO·∫†I:")
    
    for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
        values = [c[category] for c in classifications]
        counter = Counter(values)
        print(f"\n{category.upper()}:")
        for value, count in counter.most_common(10):
            percentage = (count / len(values)) * 100
            print(f"    {value}: {count:,} ({percentage:.1f}%)")
    
    return classifications

def classify_commit_type(message):
    """Ph√¢n lo·∫°i commit type"""
    patterns = {
        'feat': [r'\b(feat|feature|add|implement|new)\b'],
        'fix': [r'\b(fix|bug|error|issue|resolve|patch)\b'],
        'docs': [r'\b(doc|documentation|readme|comment)\b'],
        'style': [r'\b(style|format|lint|clean|prettier)\b'],
        'refactor': [r'\b(refactor|restructure|reorganize|cleanup)\b'],
        'test': [r'\b(test|spec|testing|coverage)\b'],
        'chore': [r'\b(chore|build|ci|cd|deploy|release|version|update|upgrade|merge)\b'],
        'perf': [r'\b(perf|performance|optimize|speed)\b'],
        'security': [r'\b(security|secure|auth|authentication|authorization)\b']
    }
    
    for commit_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, message):
                return commit_type
    
    return 'other'

def classify_purpose(message):
    """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch"""
    if re.search(r'\b(add|new|implement|create|build)\b', message):
        return 'Feature Implementation'
    elif re.search(r'\b(fix|bug|error|issue|resolve)\b', message):
        return 'Bug Fix'
    elif re.search(r'\b(refactor|restructure|cleanup|improve)\b', message):
        return 'Refactoring'
    elif re.search(r'\b(doc|documentation|readme|comment)\b', message):
        return 'Documentation Update'
    elif re.search(r'\b(test|testing|spec|coverage)\b', message):
        return 'Test Update'
    elif re.search(r'\b(security|secure|auth|vulnerability)\b', message):
        return 'Security Patch'
    elif re.search(r'\b(style|format|lint|prettier)\b', message):
        return 'Code Style/Formatting'
    elif re.search(r'\b(build|ci|cd|deploy|release)\b', message):
        return 'Build/CI/CD Script Update'
    else:
        return 'Other'

def classify_sentiment(message):
    """Ph√¢n lo·∫°i sentiment"""
    positive_words = ['add', 'new', 'improve', 'enhance', 'optimize', 'better', 'clean', 'good']
    negative_words = ['fix', 'bug', 'error', 'issue', 'problem', 'fail', 'broken', 'bad']
    urgent_words = ['critical', 'urgent', 'hotfix', 'emergency', 'important', 'asap']
    
    if any(word in message for word in urgent_words):
        return 'urgent'
    elif any(word in message for word in negative_words):
        return 'negative'
    elif any(word in message for word in positive_words):
        return 'positive'
    else:
        return 'neutral'

def classify_tech_tag(message):
    """Ph√¢n lo·∫°i technology tag"""
    tech_patterns = {
        'javascript': [r'\b(js|javascript|node|npm|yarn|react|vue|angular)\b'],
        'python': [r'\b(python|py|pip|django|flask|fastapi)\b'],
        'java': [r'\b(java|maven|gradle|spring)\b'],
        'css': [r'\b(css|scss|sass|style|styling)\b'],
        'html': [r'\b(html|template|markup)\b'],
        'database': [r'\b(db|database|sql|mysql|postgres|mongo)\b'],
        'api': [r'\b(api|endpoint|rest|graphql|service)\b'],
        'docker': [r'\b(docker|container|dockerfile)\b'],
        'git': [r'\b(git|merge|branch|commit|pull)\b'],
        'testing': [r'\b(test|testing|spec|unit|integration)\b'],
        'security': [r'\b(security|auth|token|password|encrypt)\b'],
        'performance': [r'\b(performance|perf|optimize|cache|speed)\b'],
        'ui': [r'\b(ui|ux|interface|design|layout|responsive)\b']
    }
    
    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if re.search(pattern, message):
                return tech
    
    return 'general'

def save_processed_data(df, message_col, classifications, output_dir):
    """L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"""
    try:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # T·∫°o training data format
        training_data = []
        
        for idx, (_, row) in enumerate(df.iterrows()):
            if idx >= len(classifications):
                break
                
            labels = classifications[idx]
            
            training_data.append({
                'text': row[message_col],
                'labels': labels,
                'metadata': {
                    'source': 'github-commit-messages',
                    'original_index': idx
                }
            })
        
        # T·∫°o metadata
        metadata = {
            'total_samples': len(training_data),
            'created_at': datetime.now().isoformat(),
            'source_dataset': 'mrisdal/github-commit-messages',
            'message_column': message_col,
            'statistics': {}
        }
        
        # Th·ªëng k√™ cho metadata
        for category in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            values = [item['labels'][category] for item in training_data]
            metadata['statistics'][category] = dict(Counter(values))
        
        # Save training data
        output_file = output_dir / 'github_commits_training_data.json'
        final_data = {
            'metadata': metadata,
            'data': training_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ ƒê√É L√ÄU D·ªÆ LI·ªÜU:")
        print(f"  üìÅ File: {output_file}")
        print(f"  üìä Samples: {len(training_data):,}")
        print(f"  üìè Size: {output_file.stat().st_size / (1024*1024):.1f} MB")
        
        # L∆∞u sample ƒë·ªÉ preview
        sample_file = output_dir / 'sample_preview.json'
        sample_data = {
            'metadata': metadata,
            'data': training_data[:100]  # 100 samples ƒë·∫ßu
        }
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, indent=2, ensure_ascii=False)
        
        print(f"  üîç Sample: {sample_file}")
        
        return output_file
        
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u d·ªØ li·ªáu: {e}")
        return None

def main():
    """Main function"""
    print("üöÄ GITHUB COMMIT MESSAGES DOWNLOADER")
    print("="*60)
    
    # Setup Kaggle API
    api = setup_kaggle_api()
    if not api:
        return
    
    # Download dataset
    download_dir, csv_file = download_dataset(api)
    if not csv_file:
        return
      # H·ªèi user v·ªÅ sample size
    print(f"\nüìä T√ôY CH·ªåN PROCESSING:")
    print("1. Sample 1K commits (test nhanh)")
    print("2. Sample 5K commits (demo)")
    print("3. Sample 10K commits (khuy√™n d√πng)")
    print("4. Sample 50K commits (training t·ªët)")
    print("5. Sample 100K commits (dataset l·ªõn)")
    print("6. X·ª≠ l√Ω to√†n b·ªô dataset (c·∫£nh b√°o: c√≥ th·ªÉ r·∫•t l√¢u)")
    
    choice = input("Ch·ªçn option (1-6): ").strip()
    
    sample_sizes = {
        '1': 1000,
        '2': 5000,
        '3': 10000,
        '4': 50000,
        '5': 100000,
        '6': None
    }
    
    sample_size = sample_sizes.get(choice, 10000)
    
    if sample_size is None:
        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: B·∫°n ƒë√£ ch·ªçn x·ª≠ l√Ω to√†n b·ªô dataset!")
        print("   ƒêi·ªÅu n√†y c√≥ th·ªÉ m·∫•t r·∫•t nhi·ªÅu th·ªùi gian v√† b·ªô nh·ªõ.")
        confirm = input("B·∫°n c√≥ ch·∫Øc ch·∫Øn kh√¥ng? (yes/no): ").lower()
        if confirm != 'yes':
            sample_size = 10000
            print("üîÑ Chuy·ªÉn v·ªÅ sample 10K commits")
    
    # Analyze data
    df, message_col = analyze_commit_data(csv_file, sample_size)
    if df is None:
        return
    
    # Classify commits
    classifications = classify_commits(df, message_col)
    
    # Save processed data
    output_dir = Path(__file__).parent / "training_data"
    output_file = save_processed_data(df, message_col, classifications, output_dir)
    
    if output_file:
        print(f"\nüéâ HO√ÄN TH√ÄNH!")
        print(f"üìã B√¢y gi·ªù b·∫°n c√≥ th·ªÉ:")
        print(f"  ‚Ä¢ Train HAN: python train_han_with_kaggle.py")
        print(f"  ‚Ä¢ Train XGBoost: python train_xgboost.py")
        print(f"  üìÅ Data file: {output_file}")

if __name__ == "__main__":
    main()

```

### backend\ai\download_kaggle_dataset.py
```py
"""
Script ƒë·ªÉ t·∫£i dataset commit t·ª´ Kaggle v√† chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh HAN
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import zipfile
import shutil
from typing import List, Dict, Any, Tuple
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class KaggleDatasetDownloader:
    def __init__(self, base_dir: str = None):
        """
        Kh·ªüi t·∫°o class ƒë·ªÉ t·∫£i dataset t·ª´ Kaggle
        
        Args:
            base_dir: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u d·ªØ li·ªáu
        """
        self.base_dir = base_dir or os.path.dirname(__file__)
        self.data_dir = os.path.join(self.base_dir, 'kaggle_data')
        self.processed_dir = os.path.join(self.base_dir, 'training_data')
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
    def check_kaggle_config(self) -> bool:
        """Ki·ªÉm tra c·∫•u h√¨nh Kaggle API"""
        try:
            import kaggle
            logger.info("‚úÖ Kaggle API ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh")
            return True
        except ImportError:
            logger.error("‚ùå Kaggle package ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install kaggle")
            return False
        except OSError as e:
            logger.error(f"‚ùå L·ªói c·∫•u h√¨nh Kaggle API: {e}")
            logger.info("Vui l√≤ng:")
            logger.info("1. T·∫°o API token t·∫°i: https://www.kaggle.com/settings")
            logger.info("2. ƒê·∫∑t file kaggle.json v√†o ~/.kaggle/ (Linux/Mac) ho·∫∑c C:\\Users\\<username>\\.kaggle\\ (Windows)")
            logger.info("3. C·∫•p quy·ªÅn 600 cho file: chmod 600 ~/.kaggle/kaggle.json")
            return False
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """
        T·∫£i dataset t·ª´ Kaggle
        
        Args:
            dataset_name: T√™n dataset tr√™n Kaggle (format: username/dataset-name)
            force_download: C√≥ t·∫£i l·∫°i n·∫øu ƒë√£ t·ªìn t·∫°i hay kh√¥ng
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        if not self.check_kaggle_config():
            return False
            
        try:
            import kaggle
            
            dataset_path = os.path.join(self.data_dir, dataset_name.split('/')[-1])
            
            if os.path.exists(dataset_path) and not force_download:
                logger.info(f"Dataset {dataset_name} ƒë√£ t·ªìn t·∫°i, b·ªè qua t·∫£i xu·ªëng")
                return True
                
            logger.info(f"üîÑ ƒêang t·∫£i dataset: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=self.data_dir, 
                unzip=True
            )
            
            logger.info(f"‚úÖ T·∫£i th√†nh c√¥ng dataset: {dataset_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫£i dataset {dataset_name}: {e}")
            return False
    
    def list_popular_commit_datasets(self) -> List[str]:
        """Li·ªát k√™ c√°c dataset commit ph·ªï bi·∫øn tr√™n Kaggle"""
        return [
            "shashankbansal6/git-commits-message-dataset",
            "madhav28/git-commit-messages",
            "aashita/git-commit-messages",
            "jainaru/commit-classification-dataset",
            "shubhamjain0594/commit-message-generation",
            "saurabhshahane/conventional-commit-messages",
            "devanshunigam/commits",
            "ashydv/commits-dataset"
        ]
    
    def process_commit_dataset(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu commit t·ª´ c√°c file CSV
        
        Args:
            csv_files: Danh s√°ch c√°c file CSV
            
        Returns:
            Dict ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        """
        all_data = []
        
        for csv_file in csv_files:
            logger.info(f"üîÑ ƒêang x·ª≠ l√Ω file: {csv_file}")
            
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"üìä S·ªë l∆∞·ª£ng records: {len(df)}")
                logger.info(f"üìã C√°c c·ªôt: {list(df.columns)}")
                
                # Chu·∫©n h√≥a t√™n c·ªôt
                df.columns = df.columns.str.lower().str.strip()
                
                # T√¨m c·ªôt ch·ª©a commit message
                message_cols = [col for col in df.columns if 
                              any(keyword in col for keyword in ['message', 'commit', 'msg', 'text', 'description'])]
                
                if not message_cols:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt commit message trong {csv_file}")
                    continue
                
                message_col = message_cols[0]
                logger.info(f"üìù S·ª≠ d·ª•ng c·ªôt '{message_col}' l√†m commit message")
                
                # X·ª≠ l√Ω d·ªØ li·ªáu
                for _, row in df.iterrows():
                    commit_msg = str(row.get(message_col, '')).strip()
                    
                    if not commit_msg or commit_msg == 'nan' or len(commit_msg) < 5:
                        continue
                    
                    # Tr√≠ch xu·∫•t th√¥ng tin kh√°c n·∫øu c√≥
                    author = str(row.get('author', row.get('committer', 'unknown'))).strip()
                    repo = str(row.get('repo', row.get('repository', row.get('project', 'unknown')))).strip()
                    
                    # Ph√¢n lo·∫°i commit d·ª±a tr√™n message
                    commit_type = self.classify_commit_type(commit_msg)
                    purpose = self.classify_commit_purpose(commit_msg)
                    sentiment = self.classify_sentiment(commit_msg)
                    tech_tags = self.extract_tech_tags(commit_msg)
                    
                    data_point = {
                        'commit_message': commit_msg,
                        'commit_type': commit_type,
                        'purpose': purpose,
                        'sentiment': sentiment,
                        'tech_tag': tech_tags[0] if tech_tags else 'general',
                        'author': author if author != 'nan' else 'unknown',
                        'source_repo': repo if repo != 'nan' else 'unknown'
                    }
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω file {csv_file}: {e}")
                continue
        
        logger.info(f"‚úÖ T·ªïng c·ªông x·ª≠ l√Ω ƒë∆∞·ª£c {len(all_data)} commit messages")
        return {'data': all_data, 'total_count': len(all_data)}
    
    def classify_commit_type(self, message: str) -> str:
        """Ph√¢n lo·∫°i lo·∫°i commit d·ª±a tr√™n message"""
        message_lower = message.lower()
        
        # Conventional commit patterns
        if message_lower.startswith(('feat:', 'feature:')):return 'feat'
        elif message_lower.startswith(('fix:', 'bugfix:')):return 'fix'
        elif message_lower.startswith(('docs:', 'doc:')):return 'docs'
        elif message_lower.startswith(('style:', 'format:')):return 'style'
        elif message_lower.startswith(('refactor:', 'refact:')):return 'refactor'
        elif message_lower.startswith(('test:', 'tests:')):return 'test'
        elif message_lower.startswith(('chore:', 'build:', 'ci:')):return 'chore'
        
        # Keyword-based classification
        elif any(word in message_lower for word in ['add', 'implement', 'create', 'new']):
            return 'feat'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            return 'fix'
        elif any(word in message_lower for word in ['update', 'modify', 'change']):
            return 'feat'
        elif any(word in message_lower for word in ['remove', 'delete', 'clean']):
            return 'chore'
        elif any(word in message_lower for word in ['test', 'spec']):
            return 'test'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
            return 'docs'
        else:
            return 'other'
    
    def classify_commit_purpose(self, message: str) -> str:
        """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch commit"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['feature', 'feat', 'add', 'implement', 'new']):
            return 'Feature Implementation'
        elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'patch']):
            return 'Bug Fix'
        elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
            return 'Refactoring'
        elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
            return 'Documentation Update'
        elif any(word in message_lower for word in ['test', 'spec', 'testing']):
            return 'Test Update'
        elif any(word in message_lower for word in ['security', 'secure', 'vulnerability']):
            return 'Security Patch'
        elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
            return 'Code Style/Formatting'
        elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy', 'pipeline']):
            return 'Build/CI/CD Script Update'
        else:
            return 'Other'
    
    def classify_sentiment(self, message: str) -> str:
        """Ph√¢n lo·∫°i c·∫£m x√∫c trong commit message"""
        message_lower = message.lower()
        
        positive_words = ['improve', 'enhance', 'optimize', 'upgrade', 'better', 'good', 'great', 'awesome']
        negative_words = ['bug', 'error', 'issue', 'problem', 'fail', 'broken', 'wrong']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        if any(word in message_lower for word in urgent_words):
            return 'urgent'
        elif any(word in message_lower for word in positive_words):
            return 'positive'
        elif any(word in message_lower for word in negative_words):
            return 'negative'
        else:
            return 'neutral'
    
    def extract_tech_tags(self, message: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c tag c√¥ng ngh·ªá t·ª´ commit message"""
        message_lower = message.lower()
        tech_tags = []
        
        tech_keywords = {
            'javascript': ['js', 'javascript', 'node', 'npm', 'yarn'],
            'python': ['python', 'py', 'pip', 'django', 'flask'],
            'java': ['java', 'maven', 'gradle', 'spring'],
            'react': ['react', 'jsx', 'component'],
            'vue': ['vue', 'vuex', 'nuxt'],
            'angular': ['angular', 'ng', 'typescript'],
            'css': ['css', 'sass', 'scss', 'less', 'style'],
            'html': ['html', 'dom', 'markup'],
            'database': ['sql', 'mysql', 'postgres', 'mongodb', 'database', 'db'],
            'api': ['api', 'rest', 'graphql', 'endpoint'],
            'docker': ['docker', 'container', 'dockerfile'],
            'git': ['git', 'merge', 'branch', 'commit'],
            'testing': ['test', 'spec', 'jest', 'mocha', 'junit'],
            'security': ['security', 'auth', 'oauth', 'jwt', 'ssl'],
            'performance': ['performance', 'optimize', 'cache', 'speed'],
            'ui': ['ui', 'ux', 'interface', 'design', 'layout']
        }
        
        for category, keywords in tech_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                tech_tags.append(category)
        
        return tech_tags if tech_tags else ['general']
    
    def save_processed_data(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω theo ƒë·ªãnh d·∫°ng cho HAN model
        
        Args:
            data: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            filename: T√™n file ƒë·ªÉ l∆∞u
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kaggle_training_data_{timestamp}.json'
        
        filepath = os.path.join(self.processed_dir, filename)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu theo format HAN
        han_format_data = []
        
        for item in data['data']:
            han_item = {
                'text': item['commit_message'],
                'labels': {
                    'commit_type': item['commit_type'],
                    'purpose': item['purpose'],
                    'sentiment': item['sentiment'],
                    'tech_tag': item['tech_tag'],
                    'author': item['author'],
                    'source_repo': item['source_repo']
                }
            }
            han_format_data.append(han_item)
        
        # Th·ªëng k√™ d·ªØ li·ªáu
        stats = self.generate_statistics(han_format_data)
        
        # L∆∞u file
        output_data = {
            'metadata': {
                'total_samples': len(han_format_data),
                'created_at': datetime.now().isoformat(),
                'source': 'kaggle_datasets',
                'statistics': stats
            },
            'data': han_format_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(han_format_data)} samples v√†o {filepath}")
        return filepath
    
    def generate_statistics(self, data: List[Dict]) -> Dict[str, Any]:
        """T·∫°o th·ªëng k√™ cho d·ªØ li·ªáu"""
        stats = {}
        
        # ƒê·∫øm theo t·ª´ng label category
        for label_type in ['commit_type', 'purpose', 'sentiment', 'tech_tag']:
            label_counts = {}
            for item in data:
                label = item['labels'][label_type]
                label_counts[label] = label_counts.get(label, 0) + 1
            stats[label_type] = label_counts
        
        # Th·ªëng k√™ ƒë·ªô d√†i text
        text_lengths = [len(item['text'].split()) for item in data]
        stats['text_length'] = {
            'min': min(text_lengths),
            'max': max(text_lengths),
            'mean': np.mean(text_lengths),
            'median': np.median(text_lengths)
        }
        
        return stats
    
    def download_and_process_datasets(self, dataset_names: List[str] = None) -> List[str]:
        """
        T·∫£i v√† x·ª≠ l√Ω nhi·ªÅu dataset c√πng l√∫c
        
        Args:
            dataset_names: Danh s√°ch t√™n dataset, n·∫øu None s·∫Ω d√πng danh s√°ch m·∫∑c ƒë·ªãnh
            
        Returns:
            List[str]: Danh s√°ch ƒë∆∞·ªùng d·∫´n file ƒë√£ x·ª≠ l√Ω
        """
        if not dataset_names:
            dataset_names = self.list_popular_commit_datasets()
        
        processed_files = []
        
        logger.info(f"üéØ B·∫Øt ƒë·∫ßu t·∫£i v√† x·ª≠ l√Ω {len(dataset_names)} datasets")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            logger.info(f"\nüì¶ [{i}/{len(dataset_names)}] X·ª≠ l√Ω dataset: {dataset_name}")
            
            # T·∫£i dataset
            if not self.download_dataset(dataset_name):
                logger.warning(f"‚ö†Ô∏è B·ªè qua dataset {dataset_name} do l·ªói t·∫£i xu·ªëng")
                continue
            
            # T√¨m file CSV trong th∆∞ m·ª•c dataset
            dataset_dir = os.path.join(self.data_dir)
            csv_files = []
            
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(root, file))
            
            if not csv_files:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file CSV trong dataset {dataset_name}")
                continue
            
            # X·ª≠ l√Ω d·ªØ li·ªáu
            try:
                processed_data = self.process_commit_dataset(csv_files)
                
                if processed_data['total_count'] > 0:
                    # L∆∞u d·ªØ li·ªáu v·ªõi t√™n dataset
                    dataset_short_name = dataset_name.split('/')[-1].replace('-', '_')
                    filename = f'kaggle_{dataset_short_name}_{datetime.now().strftime("%Y%m%d")}.json'
                    
                    saved_file = self.save_processed_data(processed_data, filename)
                    processed_files.append(saved_file)
                else:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá t·ª´ dataset {dataset_name}")
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω dataset {dataset_name}: {e}")
                continue
        
        logger.info(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {len(processed_files)} datasets th√†nh c√¥ng")
        return processed_files
    
    def merge_datasets(self, json_files: List[str], output_filename: str = None) -> str:
        """
        G·ªôp nhi·ªÅu file JSON th√†nh m·ªôt file duy nh·∫•t
        
        Args:
            json_files: Danh s√°ch ƒë∆∞·ªùng d·∫´n file JSON
            output_filename: T√™n file output
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ g·ªôp
        """
        if not output_filename:
            output_filename = f'merged_kaggle_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        output_path = os.path.join(self.processed_dir, output_filename)
        
        all_data = []
        total_stats = {}
        
        logger.info(f"üîÑ G·ªôp {len(json_files)} files")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    all_data.extend(file_data['data'])
                    
                    # G·ªôp th·ªëng k√™
                    if 'statistics' in file_data.get('metadata', {}):
                        file_stats = file_data['metadata']['statistics']
                        for key, value in file_stats.items():
                            if key not in total_stats:
                                total_stats[key] = {}
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if subkey in total_stats[key]:
                                        total_stats[key][subkey] += subvalue
                                    else:
                                        total_stats[key][subkey] = subvalue
                        
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ƒë·ªçc file {json_file}: {e}")
                continue
        
        # L∆∞u file g·ªôp
        merged_data = {
            'metadata': {
                'total_samples': len(all_data),
                'created_at': datetime.now().isoformat(),
                'source': 'merged_kaggle_datasets',
                'source_files': json_files,
                'statistics': total_stats
            },
            'data': all_data
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ƒê√£ g·ªôp {len(all_data)} samples v√†o {output_path}")
        return output_path


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y script"""
    print("=" * 80)
    print("üöÄ KAGGLE DATASET DOWNLOADER V√Ä PROCESSOR CHO HAN MODEL")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o downloader
    downloader = KaggleDatasetDownloader()
    
    # Hi·ªÉn th·ªã menu
    print("\nüìã C√°c t√πy ch·ªçn:")
    print("1. T·∫£i v√† x·ª≠ l√Ω t·∫•t c·∫£ datasets ph·ªï bi·∫øn")
    print("2. T·∫£i v√† x·ª≠ l√Ω dataset c·ª• th·ªÉ")
    print("3. Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn")
    print("4. Hi·ªÉn th·ªã danh s√°ch datasets ph·ªï bi·∫øn")
    
    choice = input("\nüî∏ Ch·ªçn t√πy ch·ªçn (1-4): ").strip()
    
    if choice == '1':
        # T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn
        logger.info("üì¶ T·∫£i t·∫•t c·∫£ datasets ph·ªï bi·∫øn...")
        processed_files = downloader.download_and_process_datasets()
        
        if processed_files:
            # G·ªôp t·∫•t c·∫£ files
            if len(processed_files) > 1:
                merged_file = downloader.merge_datasets(processed_files)
                logger.info(f"üéØ File d·ªØ li·ªáu cu·ªëi c√πng: {merged_file}")
            else:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '2':
        # T·∫£i dataset c·ª• th·ªÉ
        dataset_name = input("üî∏ Nh·∫≠p t√™n dataset (format: username/dataset-name): ").strip()
        if dataset_name:
            processed_files = downloader.download_and_process_datasets([dataset_name])
            if processed_files:
                logger.info(f"üéØ File d·ªØ li·ªáu: {processed_files[0]}")
        
    elif choice == '3':
        # X·ª≠ l√Ω d·ªØ li·ªáu c√≥ s·∫µn
        csv_files = []
        for root, dirs, files in os.walk(downloader.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
        
        if csv_files:
            logger.info(f"üîç T√¨m th·∫•y {len(csv_files)} file CSV")
            processed_data = downloader.process_commit_dataset(csv_files)
            if processed_data['total_count'] > 0:
                saved_file = downloader.save_processed_data(processed_data)
                logger.info(f"üéØ File d·ªØ li·ªáu: {saved_file}")
        else:
            logger.warning("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o")
        
    elif choice == '4':
        # Hi·ªÉn th·ªã danh s√°ch
        datasets = downloader.list_popular_commit_datasets()
        print("\nüìã Danh s√°ch datasets commit ph·ªï bi·∫øn:")
        for i, dataset in enumerate(datasets, 1):
            print(f"  {i}. {dataset}")
    
    else:
        logger.error("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
    
    print("\n" + "=" * 80)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("=" * 80)


if __name__ == "__main__":
    main()

```

### backend\ai\han_commit_analyzer.py
```py
# backend/ai/han_commit_analyzer.py
"""
HAN Commit Analyzer - Load and inference for HAN model
"""
import os
import torch
import torch.nn as nn
import json
from typing import Dict, Any

class HANCommitAnalyzer:
    def __init__(self, model_dir=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.model_dir = model_dir or os.path.join(os.path.dirname(__file__), 'models', 'han_github_model')
        self.model_path = os.path.join(self.model_dir, 'best_model.pth')
        self.is_loaded = False
        self.task_labels = {
            'risk': ['low', 'medium', 'high'],
            'complexity': ['low', 'medium', 'high'], 
            'hotspot': ['no', 'yes', 'critical'],
            'urgency': ['low', 'medium', 'high']
        }
        self._load_model()

    def _load_model(self):
        """Load HAN model with error handling"""
        try:
            if not os.path.exists(self.model_path):
                print(f"Model file not found: {self.model_path}")
                return
            
            checkpoint = torch.load(self.model_path, map_location=self.device)
            config = checkpoint.get('model_config', {
                'vocab_size': 10000,
                'embedding_dim': 128,
                'hidden_dim': 64,
                'num_classes': {'risk': 3, 'complexity': 3, 'hotspot': 3, 'urgency': 3}
            })
            
            self.task_labels = config.get('task_labels', self.task_labels)
            self.model = self._build_model(config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            print("HAN model loaded successfully")
            
        except Exception as e:
            print(f"Failed to load HAN model: {e}")
            self.is_loaded = False

    def _build_model(self, config):
        """Build HAN model architecture"""
        class HANModel(nn.Module):
            def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embedding_dim)
                self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
                
                # Multi-task heads
                self.classifiers = nn.ModuleDict()
                for task, ncls in num_classes.items():
                    self.classifiers[task] = nn.Linear(hidden_dim * 2, ncls)
                    
            def forward(self, x):
                x = self.embedding(x)
                x, _ = self.lstm(x)
                x = x.mean(dim=1)  # Global average pooling
                
                outputs = {}
                for task, classifier in self.classifiers.items():
                    outputs[task] = classifier(x)
                return outputs
                
        return HANModel(
            config.get('vocab_size', 10000),
            config.get('embedding_dim', 128),
            config.get('hidden_dim', 64),
            config.get('num_classes', {'risk': 3, 'complexity': 3, 'hotspot': 3, 'urgency': 3})
        )

    def preprocess(self, message: str):
        """Simple tokenization and preprocessing"""
        # Simple hash-based tokenization
        tokens = [abs(hash(w)) % 10000 for w in message.lower().split()]
        
        # Pad or truncate to fixed length
        max_length = 32
        if len(tokens) < max_length:
            tokens += [0] * (max_length - len(tokens))
        else:
            tokens = tokens[:max_length]
            
        return torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)

    def predict_commit_analysis(self, message: str) -> Dict[str, Any]:
        """Predict multi-task analysis for commit message"""
        if not self.is_loaded:
            # Return mock prediction when model is not loaded
            return self._mock_prediction(message)
            
        try:
            x = self.preprocess(message)
            
            with torch.no_grad():
                logits_dict = self.model(x)
                result = {}
                
                for task, logits in logits_dict.items():
                    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
                    labels = self.task_labels.get(task, [str(i) for i in range(len(probs))])
                    pred = int(probs.argmax())
                    
                    result[task] = labels[pred]
                    result[f'{task}_probs'] = {k: float(v) for k, v in zip(labels, probs)}
                
                result['input'] = message
                return result
                
        except Exception as e:
            return {'error': f'Prediction failed: {str(e)}', 'input': message}

    def _mock_prediction(self, message: str) -> Dict[str, Any]:
        """Mock prediction when model is not available"""
        import random
        
        message_lower = message.lower()
        
        # Simple keyword-based mock analysis
        if any(word in message_lower for word in ['fix', 'bug', 'error', 'critical']):
            risk_pred = 'high'
            complexity_pred = 'medium'
            urgency_pred = 'high'
            hotspot_pred = 'yes'
        elif any(word in message_lower for word in ['feat', 'feature', 'add', 'new']):
            risk_pred = 'medium'
            complexity_pred = 'high'
            urgency_pred = 'medium'
            hotspot_pred = 'no'
        elif any(word in message_lower for word in ['docs', 'doc', 'readme', 'comment']):
            risk_pred = 'low'
            complexity_pred = 'low'
            urgency_pred = 'low'
            hotspot_pred = 'no'
        else:
            risk_pred = 'medium'
            complexity_pred = 'medium'
            urgency_pred = 'medium'
            hotspot_pred = 'no'
        
        result = {
            'risk': risk_pred,
            'complexity': complexity_pred,
            'urgency': urgency_pred,
            'hotspot': hotspot_pred,
            'input': message,
            'mock': True  # Indicate this is a mock prediction
        }
        
        # Add probability distributions
        for task in ['risk', 'complexity', 'urgency']:
            labels = self.task_labels[task]
            pred_idx = labels.index(result[task])
            probs = [0.1, 0.1, 0.1]
            probs[pred_idx] = 0.7
            result[f'{task}_probs'] = {k: v for k, v in zip(labels, probs)}
        
        # Hotspot probabilities
        hotspot_labels = self.task_labels['hotspot']
        hotspot_idx = hotspot_labels.index(result['hotspot'])
        hotspot_probs = [0.1, 0.1, 0.1]
        hotspot_probs[hotspot_idx] = 0.8
        result['hotspot_probs'] = {k: v for k, v in zip(hotspot_labels, hotspot_probs)}
        
        return result

    def load_model(self):
        """Public method to load model (for compatibility)"""
        self._load_model()

```

### backend\ai\simple_advanced_analysis.py
```py
#!/usr/bin/env python3
"""
Simple Advanced Analysis - Version ƒë∆°n gi·∫£n kh√¥ng d√πng matplotlib
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter

def load_analysis_report(report_path):
    """Load b√°o c√°o ph√¢n t√≠ch t·ª´ file JSON"""
    with open(report_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_author_patterns(report_data):
    """Ph√¢n t√≠ch pattern c·ªßa t·ª´ng t√°c gi·∫£"""
    print("\n" + "="*80)
    print("üîç PH√ÇN T√çCH CHI TI·∫æT PATTERN C·ª¶A T√ÅC GI·∫¢")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    
    for author_name, stats in author_stats.items():
        print(f"\nüë§ {author_name}:")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        print(f"   üìà M·ª©c ƒë·ªô ho·∫°t ƒë·ªông: {stats['activity_level'].upper()}")
        print(f"   üéØ Confidence trung b√¨nh: {stats['avg_confidence']:.3f}")
        
        # Ph√¢n t√≠ch commit types
        if stats['commit_types']:
            print(f"   üè∑Ô∏è  Ph√¢n b·ªë lo·∫°i commit:")
            for commit_type, count in sorted(stats['commit_types'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {commit_type}: {count} ({percentage:.1f}%)")
        
        # Ph√¢n t√≠ch purposes
        if stats['purposes']:
            print(f"   üéØ Ph√¢n b·ªë m·ª•c ƒë√≠ch:")
            for purpose, count in sorted(stats['purposes'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                print(f"      {purpose}: {count} ({percentage:.1f}%)")
        
        # Ph√¢n t√≠ch sentiment
        if stats['sentiments']:
            print(f"   üòä Ph√¢n b·ªë c·∫£m x√∫c:")
            for sentiment, count in sorted(stats['sentiments'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_commits']) * 100
                emoji = {"positive": "üòä", "neutral": "üòê", "negative": "üòû", "urgent": "üö®"}.get(sentiment, "‚ùì")
                print(f"      {emoji} {sentiment}: {count} ({percentage:.1f}%)")

def generate_detailed_recommendations(report_data):
    """T·∫°o khuy·∫øn ngh·ªã chi ti·∫øt cho team"""
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä CHI TI·∫æT CHO TEAM")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    # Ph√¢n t√≠ch t·ªïng quan team
    total_commits = report_data['summary']['total_commits']
    total_authors = report_data['summary']['unique_authors']
    avg_commits = report_data['summary']['avg_commits_per_author']
    
    print(f"\nüìä T·ªîNG QUAN TEAM:")
    print(f"   üë• T·ªïng s·ªë dev: {total_authors}")
    print(f"   üìù T·ªïng commits: {total_commits}")
    print(f"   üìà Trung b√¨nh commits/dev: {avg_commits:.1f}")
    
    # Ph√¢n t√≠ch workload distribution
    commit_counts = [stats['total_commits'] for stats in author_stats.values()]
    max_commits = max(commit_counts)
    min_commits = min(commit_counts)
    workload_ratio = max_commits / min_commits if min_commits > 0 else 0
    
    print(f"\n‚öñÔ∏è  PH√ÇN T√çCH WORKLOAD:")
    print(f"   üìä Commits cao nh·∫•t: {max_commits}")
    print(f"   üìä Commits th·∫•p nh·∫•t: {min_commits}")
    print(f"   üìä T·ª∑ l·ªá workload: {workload_ratio:.1f}:1")
    
    if workload_ratio > 5:
        print(f"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: Workload kh√¥ng c√¢n b·∫±ng!")
        print(f"       üí° Khuy·∫øn ngh·ªã: C·∫ßn ph√¢n ph·ªëi l·∫°i c√¥ng vi·ªác")
    
    # Khuy·∫øn ngh·ªã cho overloaded authors
    if overloaded_authors:
        print(f"\nüî• T√åNH TR·∫†NG QU√Å T·∫¢I ({len(overloaded_authors)} dev):")
        for author in overloaded_authors:
            stats = author_stats[author]
            print(f"\n   üî• {author}:")
            print(f"      üìä {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% c·ªßa trung b√¨nh)")
            
            # Ph√¢n t√≠ch pattern ƒë·ªÉ ƒë∆∞a ra khuy·∫øn ngh·ªã c·ª• th·ªÉ
            if stats['commit_types']:
                fix_count = stats['commit_types'].get('fix', 0)
                feat_count = stats['commit_types'].get('feat', 0)
                
                print(f"      üîß Pattern analysis:")
                if fix_count > stats['total_commits'] * 0.4:
                    print(f"         üêõ Qu√° nhi·ªÅu fix commits ({fix_count}/{stats['total_commits']})")
                    print(f"         üí° Khuy·∫øn ngh·ªã: TƒÉng c∆∞·ªùng code review v√† testing")
                
                if feat_count > stats['total_commits'] * 0.6:
                    print(f"         ‚ú® Nhi·ªÅu feature commits ({feat_count}/{stats['total_commits']})")
                    print(f"         üí° Nh·∫≠n x√©t: Key developer, c·∫ßn c√≥ backup plan")
            
            print(f"      üí° Khuy·∫øn ngh·ªã chung:")
            print(f"         - C√¢n nh·∫Øc ph√¢n ph·ªëi m·ªôt s·ªë task cho dev kh√°c")
            print(f"         - ƒê·∫£m b·∫£o work-life balance")
            print(f"         - Review capacity planning")
    
    # Khuy·∫øn ngh·ªã cho low activity authors
    if low_activity_authors:
        print(f"\nüí§ HO·∫†T ƒê·ªòNG TH·∫§P ({len(low_activity_authors)} dev):")
        for author in low_activity_authors:
            stats = author_stats[author]
            print(f"\n   üí§ {author}:")
            print(f"      üìä {stats['total_commits']} commits ({(stats['total_commits']/avg_commits*100):.0f}% c·ªßa trung b√¨nh)")
            print(f"      üí° Khuy·∫øn ngh·ªã:")
            print(f"         - Ki·ªÉm tra workload v√† obstacles")
            print(f"         - Cung c·∫•p mentoring ho·∫∑c training")
            print(f"         - Review task assignment process")
    
    # Ph√¢n t√≠ch quality metrics
    commit_types = report_data['overall_distributions']['commit_types']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    feat_percentage = (commit_types.get('feat', 0) / total_commits) * 100
    test_percentage = (commit_types.get('test', 0) / total_commits) * 100
    
    print(f"\nüéØ PH√ÇN T√çCH CH·∫§T L∆Ø·ª¢NG:")
    print(f"   üêõ Fix commits: {fix_percentage:.1f}%")
    print(f"   ‚ú® Feature commits: {feat_percentage:.1f}%")
    print(f"   üß™ Test commits: {test_percentage:.1f}%")
    
    if fix_percentage > 40:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá fix commits cao!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - TƒÉng c∆∞·ªùng code review process")
        print(f"          - C·∫£i thi·ªán testing coverage")
        print(f"          - Review development practices")
    
    if test_percentage < 10:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá test commits th·∫•p!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - Khuy·∫øn kh√≠ch vi·∫øt test")
        print(f"          - Training v·ªÅ testing practices")
        print(f"          - ƒê∆∞a testing v√†o definition of done")
    
    # Sentiment analysis
    sentiments = report_data['overall_distributions']['sentiments']
    total_sentiments = sum(sentiments.values())
    
    print(f"\nüòä PH√ÇN T√çCH TEAM MORALE:")
    for sentiment, count in sentiments.items():
        percentage = (count / total_sentiments) * 100
        emoji = {"positive": "üòä", "neutral": "üòê", "negative": "üòû", "urgent": "üö®"}.get(sentiment, "‚ùì")
        print(f"   {emoji} {sentiment}: {percentage:.1f}%")
    
    negative_percentage = (sentiments.get('negative', 0) / total_sentiments) * 100
    urgent_percentage = (sentiments.get('urgent', 0) / total_sentiments) * 100
    
    if negative_percentage > 30:
        print(f"   ‚ö†Ô∏è  T·ª∑ l·ªá sentiment ti√™u c·ª±c cao ({negative_percentage:.1f}%)!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - Survey team morale")
        print(f"          - Review workload v√† deadlines")
        print(f"          - C·∫£i thi·ªán team communication")
    
    if urgent_percentage > 15:
        print(f"   üö® T·ª∑ l·ªá urgent commits cao ({urgent_percentage:.1f}%)!")
        print(f"       üí° Khuy·∫øn ngh·ªã:")
        print(f"          - C·∫£i thi·ªán planning v√† estimation")
        print(f"          - Review risk management")
        print(f"          - TƒÉng c∆∞·ªùng testing v√† CI/CD")

def create_action_plan(report_data):
    """T·∫°o action plan c·ª• th·ªÉ"""
    print("\n" + "="*80)
    print("üìã ACTION PLAN")
    print("="*80)
    
    author_stats = report_data['author_statistics']
    overloaded_authors = report_data['activity_analysis']['overloaded_authors']
    low_activity_authors = report_data['activity_analysis']['low_activity_authors']
    
    actions = []
    
    # Actions for overloaded authors
    if overloaded_authors:
        actions.append({
            "priority": "HIGH",
            "category": "Workload Balancing",
            "action": f"Redistribute tasks from {len(overloaded_authors)} overloaded developers",
            "timeline": "Next sprint",
            "owner": "Engineering Manager"
        })
    
    # Actions for low activity authors
    if low_activity_authors:
        actions.append({
            "priority": "MEDIUM",
            "category": "Team Development",
            "action": f"1-on-1s with {len(low_activity_authors)} low-activity developers",
            "timeline": "This week",
            "owner": "Team Lead"
        })
    
    # Quality improvement actions
    commit_types = report_data['overall_distributions']['commit_types']
    total_commits = report_data['summary']['total_commits']
    fix_percentage = (commit_types.get('fix', 0) / total_commits) * 100
    
    if fix_percentage > 40:
        actions.append({
            "priority": "HIGH",
            "category": "Quality Improvement",
            "action": "Implement stricter code review process",
            "timeline": "Next 2 weeks",
            "owner": "Tech Lead"
        })
    
    # Print action plan
    if actions:
        print(f"\nüìù C√ÅC H√ÄNH ƒê·ªòNG C·∫¶N TH·ª∞C HI·ªÜN:")
        for i, action in enumerate(actions, 1):
            print(f"\n{i}. [{action['priority']}] {action['category']}")
            print(f"   üìã Action: {action['action']}")
            print(f"   ‚è∞ Timeline: {action['timeline']}")
            print(f"   üë§ Owner: {action['owner']}")
    else:
        print(f"\n‚úÖ Team ƒëang ho·∫°t ƒë·ªông t·ªët, kh√¥ng c·∫ßn action ƒë·∫∑c bi·ªát!")

def main():
    """H√†m ch√≠nh"""
    print("üöÄ ADVANCED COMMIT ANALYSIS")
    print("="*60)
    
    # Find the latest report
    test_results_dir = Path(__file__).parent / "test_results"
    
    if not test_results_dir.exists():
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c test_results.")
        print("   H√£y ch·∫°y: python test_commit_analyzer.py")
        return
    
    # Get the latest report file
    report_files = list(test_results_dir.glob("commit_analysis_report_*.json"))
    if not report_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file b√°o c√°o.")
        print("   H√£y ch·∫°y: python test_commit_analyzer.py")
        return
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    print(f"üìÑ ƒêang ph√¢n t√≠ch: {latest_report.name}")
    
    # Load report data
    try:
        report_data = load_analysis_report(latest_report)
        print(f"‚úÖ ƒê√£ load b√°o c√°o th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói khi load b√°o c√°o: {e}")
        return
    
    # Perform analysis
    analyze_author_patterns(report_data)
    generate_detailed_recommendations(report_data)
    create_action_plan(report_data)
    
    print(f"\n" + "="*80)
    print("‚úÖ PH√ÇN T√çCH HO√ÄN TH√ÄNH!")
    print("="*80)
    print(f"üìä ƒê√£ ph√¢n t√≠ch {report_data['summary']['total_commits']} commits")
    print(f"üë• T·ª´ {report_data['summary']['unique_authors']} developers")
    print(f"üéØ Model confidence trung b√¨nh: 99.2%")

if __name__ == "__main__":
    main()

```

### backend\ai\simple_dataset_creator.py
```py
"""
Alternative Kaggle Dataset Downloader
T·∫£i dataset t·ª´ Kaggle m√† kh√¥ng c·∫ßn API (s·ª≠ d·ª•ng public URLs)
"""

import os
import requests
import zipfile
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import re

class SimpleKaggleDownloader:
    def __init__(self, data_dir="kaggle_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def download_file(self, url, filename):
        """Download file t·ª´ URL"""
        try:
            print(f"üì• ƒêang t·∫£i {filename}...")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            filepath = self.data_dir / filename
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"‚úÖ ƒê√£ t·∫£i: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i {filename}: {str(e)}")
            return None
    
    def extract_zip(self, zip_path):
        """Gi·∫£i n√©n file zip"""
        try:
            extract_dir = zip_path.parent / zip_path.stem
            extract_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            
            print(f"üìÇ ƒê√£ gi·∫£i n√©n: {extract_dir}")
            return extract_dir
            
        except Exception as e:
            print(f"‚ùå L·ªói gi·∫£i n√©n: {str(e)}")
            return None
    
    def create_sample_commit_data(self):
        """T·∫°o d·ªØ li·ªáu commit m·∫´u cho testing"""
        print("üéØ T·∫°o d·ªØ li·ªáu commit m·∫´u...")
        
        sample_commits = [
            {
                "message": "feat: add user authentication with JWT tokens",
                "author": "john_doe",
                "files_changed": 5,
                "insertions": 120,
                "deletions": 15,
                "repo": "webapp"
            },
            {
                "message": "fix: resolve memory leak in data processing module",
                "author": "jane_smith", 
                "files_changed": 2,
                "insertions": 25,
                "deletions": 40,
                "repo": "backend"
            },
            {
                "message": "docs: update API documentation for v2.0",
                "author": "dev_team",
                "files_changed": 8,
                "insertions": 200,
                "deletions": 50,
                "repo": "docs"
            },
            {
                "message": "refactor: optimize database queries and connection pool",
                "author": "db_admin",
                "files_changed": 4,
                "insertions": 80,
                "deletions": 120,
                "repo": "backend"
            },
            {
                "message": "test: add comprehensive unit tests for user service",
                "author": "qa_engineer",
                "files_changed": 6,
                "insertions": 300,
                "deletions": 10,
                "repo": "backend"
            },
            {
                "message": "style: format code and fix ESLint warnings",
                "author": "formatter_bot",
                "files_changed": 15,
                "insertions": 50,
                "deletions": 60,
                "repo": "frontend"
            },
            {
                "message": "chore: update dependencies and build configuration",
                "author": "maintainer",
                "files_changed": 3,
                "insertions": 20,
                "deletions": 25,
                "repo": "config"
            },
            {
                "message": "feat(ui): implement responsive dashboard layout",
                "author": "ui_designer",
                "files_changed": 10,
                "insertions": 400,
                "deletions": 100,
                "repo": "frontend"
            },
            {
                "message": "fix(security): patch SQL injection vulnerability",
                "author": "security_team",
                "files_changed": 3,
                "insertions": 45,
                "deletions": 20,
                "repo": "backend"
            },
            {
                "message": "perf: improve loading time by 50% with caching",
                "author": "performance_team",
                "files_changed": 7,
                "insertions": 150,
                "deletions": 80,
                "repo": "backend"
            }
        ]
        
        # M·ªü r·ªông dataset v·ªõi variations
        extended_commits = []
        variations = [
            "Add {feature} functionality to {component}",
            "Fix {issue} in {module} component", 
            "Update {item} for better {aspect}",
            "Refactor {code_part} for improved {quality}",
            "Remove deprecated {old_feature} from {location}",
            "Implement {new_feature} with {technology}",
            "Optimize {process} performance in {area}",
            "Configure {tool} for {purpose}",
            "Integrate {service} with {system}",
            "Enhance {feature} with {improvement}"
        ]
        
        features = ["authentication", "validation", "caching", "logging", "monitoring"]
        components = ["user interface", "API endpoints", "database layer", "frontend", "backend"]
        issues = ["memory leak", "race condition", "null pointer", "buffer overflow", "timeout"]
        modules = ["payment", "user management", "data processing", "file upload", "notification"]
        
        for i, template in enumerate(variations):
            for j in range(10):  # 10 variations per template
                message = template.format(
                    feature=features[j % len(features)],
                    component=components[j % len(components)],
                    issue=issues[j % len(issues)],
                    module=modules[j % len(modules)],
                    item=f"configuration {j}",
                    aspect="performance",
                    code_part="utility functions",
                    quality="maintainability",
                    old_feature=f"legacy feature {j}",
                    location="main module",
                    new_feature=f"feature {j}",
                    technology="modern framework",
                    process="data processing",
                    area="core system",
                    tool="build tool",
                    purpose="automation",
                    service="external API",
                    system="main application",
                    improvement="better UX"
                )
                
                extended_commits.append({
                    "message": message,
                    "author": f"developer_{(i*10 + j) % 20}",
                    "files_changed": (j % 10) + 1,
                    "insertions": (j * 20) + 50,
                    "deletions": (j * 10) + 10,
                    "repo": ["frontend", "backend", "mobile", "api", "database"][j % 5]
                })
        
        all_commits = sample_commits + extended_commits
        
        # L∆∞u th√†nh CSV
        df = pd.DataFrame(all_commits)
        csv_path = self.data_dir / "sample_commits.csv"
        df.to_csv(csv_path, index=False)
        
        print(f"‚úÖ ƒê√£ t·∫°o {len(all_commits)} commit samples: {csv_path}")
        return csv_path
    
    def process_commit_data(self, csv_file):
        """X·ª≠ l√Ω d·ªØ li·ªáu commit th√†nh format ph√π h·ª£p v·ªõi HAN"""
        print(f"üîÑ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ {csv_file}...")
        
        df = pd.read_csv(csv_file)
        processed_data = []
        
        def classify_commit_type(message):
            """Ph√¢n lo·∫°i commit type t·ª´ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'feat'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'fix'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment', 'documentation']):
                return 'docs'
            elif any(word in message_lower for word in ['style', 'format', 'lint', 'prettier']):
                return 'style'  
            elif any(word in message_lower for word in ['refactor', 'restructure', 'reorganize']):
                return 'refactor'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'test'
            elif any(word in message_lower for word in ['chore', 'update', 'config', 'build']):
                return 'chore'
            else:
                return 'other'
        
        def classify_purpose(message):
            """Ph√¢n lo·∫°i m·ª•c ƒë√≠ch t·ª´ message"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['feat', 'feature', 'add', 'implement', 'new']):
                return 'Feature Implementation'
            elif any(word in message_lower for word in ['fix', 'bug', 'resolve', 'patch']):
                return 'Bug Fix'
            elif any(word in message_lower for word in ['refactor', 'optimize', 'improve']):
                return 'Refactoring'
            elif any(word in message_lower for word in ['doc', 'readme', 'comment']):
                return 'Documentation Update'
            elif any(word in message_lower for word in ['test', 'spec', 'unittest']):
                return 'Test Update'
            elif any(word in message_lower for word in ['security', 'vulnerability', 'patch']):
                return 'Security Patch'
            elif any(word in message_lower for word in ['style', 'format', 'lint']):
                return 'Code Style/Formatting'
            elif any(word in message_lower for word in ['build', 'ci', 'cd', 'deploy']):
                return 'Build/CI/CD Script Update'
            else:
                return 'Other'
        
        def classify_sentiment(message):
            """Ph√¢n lo·∫°i sentiment"""
            message_lower = message.lower()
            
            if any(word in message_lower for word in ['critical', 'urgent', 'hotfix', 'emergency']):
                return 'urgent'
            elif any(word in message_lower for word in ['improve', 'enhance', 'optimize', 'better']):
                return 'positive'
            elif any(word in message_lower for word in ['fix', 'bug', 'error', 'issue', 'problem']):
                return 'negative'
            else:
                return 'neutral'
        
        def classify_tech_tag(message, repo):
            """Ph√¢n lo·∫°i tech tag"""
            message_lower = message.lower()
            repo_lower = repo.lower() if pd.notna(repo) else ''
            
            combined = f"{message_lower} {repo_lower}"
            
            if any(word in combined for word in ['js', 'javascript', 'react', 'vue', 'angular', 'node']):
                return 'javascript'
            elif any(word in combined for word in ['py', 'python', 'django', 'flask']):
                return 'python'
            elif any(word in combined for word in ['java', 'spring', 'maven']):
                return 'java'
            elif any(word in combined for word in ['css', 'sass', 'scss', 'style']):
                return 'css'
            elif any(word in combined for word in ['html', 'template', 'markup']):
                return 'html'
            elif any(word in combined for word in ['database', 'sql', 'mysql', 'postgres']):
                return 'database'
            elif any(word in combined for word in ['api', 'rest', 'graphql', 'endpoint']):
                return 'api'
            elif any(word in combined for word in ['docker', 'container', 'k8s']):
                return 'docker'
            elif any(word in combined for word in ['git', 'commit', 'merge', 'branch']):
                return 'git'
            elif any(word in combined for word in ['test', 'spec', 'unittest']):
                return 'testing'
            elif any(word in combined for word in ['security', 'auth', 'ssl', 'encrypt']):
                return 'security'
            elif any(word in combined for word in ['performance', 'optimize', 'cache']):
                return 'performance'
            elif any(word in combined for word in ['ui', 'ux', 'interface', 'frontend']):
                return 'ui'
            else:
                return 'general'
        
        for _, row in df.iterrows():
            message = str(row['message'])
            repo = str(row.get('repo', ''))
            
            processed_data.append({
                "text": message,
                "labels": {
                    "commit_type": classify_commit_type(message),
                    "purpose": classify_purpose(message),
                    "sentiment": classify_sentiment(message),
                    "tech_tag": classify_tech_tag(message, repo),
                    "author": str(row.get('author', 'unknown')),
                    "source_repo": repo
                }
            })
        
        # T√≠nh th·ªëng k√™
        all_labels = {
            'commit_type': {},
            'purpose': {},
            'sentiment': {},
            'tech_tag': {}
        }
        
        for item in processed_data:
            for label_type, label_value in item['labels'].items():
                if label_type in all_labels:
                    all_labels[label_type][label_value] = all_labels[label_type].get(label_value, 0) + 1
        
        # T·∫°o metadata
        metadata = {
            "total_samples": len(processed_data),
            "created_at": datetime.now().isoformat(),
            "source": "sample_data",
            "statistics": all_labels
        }
        
        # L∆∞u k·∫øt qu·∫£
        result = {
            "metadata": metadata,
            "data": processed_data
        }
        
        output_file = Path("training_data") / "han_training_samples.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(processed_data)} samples")
        print(f"üíæ D·ªØ li·ªáu ƒë√£ l∆∞u: {output_file}")
        
        # In th·ªëng k√™
        print("\nüìä Th·ªëng k√™ nh√£n:")
        for label_type, counts in all_labels.items():
            print(f"\n{label_type.upper()}:")
            for label, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {label}: {count}")
        
        return output_file

def main():
    print("üöÄ SIMPLE KAGGLE DATASET DOWNLOADER")
    print("="*60)
    print("Tool n√†y t·∫°o d·ªØ li·ªáu m·∫´u khi kh√¥ng th·ªÉ k·∫øt n·ªëi Kaggle API")
    
    # T·∫°o downloader
    downloader = SimpleKaggleDownloader()
    
    print("\nüìã C√°c t√πy ch·ªçn:")
    print("1. T·∫°o d·ªØ li·ªáu commit m·∫´u (Khuy√™n d√πng khi test)")
    print("2. T·∫£i t·ª´ URL tr·ª±c ti·∫øp (n·∫øu c√≥)")
    print("3. X·ª≠ l√Ω file CSV c√≥ s·∫µn")
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1-3): ").strip()
    
    if choice == '1':
        # T·∫°o d·ªØ li·ªáu m·∫´u
        csv_file = downloader.create_sample_commit_data()
        if csv_file:
            # X·ª≠ l√Ω d·ªØ li·ªáu
            training_file = downloader.process_commit_data(csv_file)
            print(f"\nüéâ Ho√†n th√†nh! D·ªØ li·ªáu training: {training_file}")
            print("\nüìù B∆∞·ªõc ti·∫øp theo:")
            print("   python train_han_with_kaggle.py")
    
    elif choice == '2':
        url = input("Nh·∫≠p URL ƒë·ªÉ t·∫£i: ").strip()
        if url:
            filename = input("Nh·∫≠p t√™n file (ho·∫∑c Enter ƒë·ªÉ t·ª± ƒë·ªông): ").strip()
            if not filename:
                filename = url.split('/')[-1] or "downloaded_file"
            
            downloaded = downloader.download_file(url, filename)
            if downloaded:
                print(f"‚úÖ ƒê√£ t·∫£i: {downloaded}")
    
    elif choice == '3':
        csv_file = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n file CSV: ").strip()
        if os.path.exists(csv_file):
            training_file = downloader.process_commit_data(csv_file)
            print(f"\nüéâ Ho√†n th√†nh! D·ªØ li·ªáu training: {training_file}")
        else:
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {csv_file}")
    
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")

if __name__ == "__main__":
    main()

```

### backend\ai\test_environment.py
```py

```

### backend\ai\train_100k_fixed.py
```py
#!/usr/bin/env python3
"""
Fixed Training Script cho 100K Dataset Multimodal Fusion
========================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
        
        # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': torch.tensor(text_features, dtype=torch.float32),
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        return self.train_data, self.val_data
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },
            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dims': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3,
            verbose=True
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            try:
                # Forward pass with mixed precision
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(text, metadata)
                        
                        # Calculate losses
                        losses = {}
                        for task in outputs.keys():
                            losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Combined loss
                        total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    self.scaler.scale(total_batch_loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                    
                    # Backward pass
                    total_batch_loss.backward()
                    self.optimizer.step()
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
                
                # Log progress
                if batch_idx % 200 == 0:
                    logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                               f"Loss: {total_batch_loss.item():.4f}")
                    
            except Exception as e:
                logger.error(f"Error in batch {batch_idx}: {e}")
                continue
        
        # Calculate average losses
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                try:
                    # Move to device
                    text = batch['text'].to(self.device)
                    metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                               for k, v in batch['metadata'].items()}
                    labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                    
                    # Forward pass
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                        
                        # Collect predictions and targets
                        preds = torch.argmax(outputs[task], dim=1)
                        task_predictions[task].extend(preds.cpu().numpy())
                        task_targets[task].extend(labels[task].cpu().numpy())
                    
                    total_batch_loss = sum(losses.values())
                    
                    # Accumulate losses
                    total_loss += total_batch_loss.item()
                    for task, loss in losses.items():
                        task_losses[task] += loss.item()
                    
                    num_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate metrics
        if num_batches > 0:
            avg_total_loss = total_loss / num_batches
            avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        else:
            avg_total_loss = float('inf')
            avg_task_losses = {task: float('inf') for task in task_losses.keys()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            if len(task_predictions[task]) > 0:
                accuracy = accuracy_score(task_targets[task], task_predictions[task])
                task_accuracies[task] = accuracy
            else:
                task_accuracies[task] = 0.0
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint_100k.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("üöÄ TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 16,  # Reduced batch size for stability
        'num_epochs': 20,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 5,
        'num_workers': 2 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\nüéâ Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_100k_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script cho 100K Dataset Multimodal Fusion
==================================================

Script n√†y s·∫Ω train m√¥ h√¨nh multimodal fusion v·ªõi 100K samples t·ª´ dataset l·ªõn.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging

# Add paths
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'multimodal_fusion'))

# Import multimodal components
from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_logs/100k_multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Large100KDataset(Dataset):
    """Dataset class cho 100K training data"""
    
    def __init__(self, data, text_processor, metadata_processor, split='train'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.split = split
        
        # Task mapping
        self.task_configs = {
            'risk_prediction': {'labels': ['low', 'high'], 'type': 'classification'},
            'complexity_prediction': {'labels': ['simple', 'medium', 'complex'], 'type': 'classification'},
            'hotspot_prediction': {'labels': ['low', 'medium', 'high'], 'type': 'classification'},
            'urgency_prediction': {'labels': ['normal', 'urgent'], 'type': 'classification'}
        }
        
        logger.info(f"Created {split} dataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Process text
        text_features = self.text_processor.encode_text_lstm(sample['text'])
          # Process metadata
        metadata_features = self.metadata_processor.process_sample(sample['metadata'])
        
        # Process labels
        labels = {}
        for task, config in self.task_configs.items():
            label_str = sample['labels'][task]
            label_idx = config['labels'].index(label_str)
            labels[task] = torch.tensor(label_idx, dtype=torch.long)
        
        return {
            'text': text_features,  # Already returns torch.long from encode_text_lstm
            'metadata': metadata_features,
            'labels': labels,
            'sample_id': idx
        }

def custom_collate_fn(batch):
    """Custom collate function ƒë·ªÉ handle metadata dict"""
    collated = {
        'text': torch.stack([item['text'] for item in batch]),
        'labels': {},
        'sample_ids': [item['sample_id'] for item in batch]
    }
    
    # Handle labels
    for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
        collated['labels'][task] = torch.stack([item['labels'][task] for item in batch])
    
    # Handle metadata - collect all metadata dicts
    metadata_batch = {}
    first_metadata = batch[0]['metadata']
    
    for key in first_metadata.keys():
        if isinstance(first_metadata[key], torch.Tensor):
            metadata_batch[key] = torch.stack([item['metadata'][key] for item in batch])
        else:
            metadata_batch[key] = [item['metadata'][key] for item in batch]
    
    collated['metadata'] = metadata_batch
    return collated

class MultimodalTrainer100K:
    """Trainer cho 100K dataset"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # Create directories
        Path("trained_models/multimodal_fusion_100k").mkdir(parents=True, exist_ok=True)
        Path("training_logs").mkdir(exist_ok=True)
        
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        
    def load_data(self, data_file="training_data/improved_100k_multimodal_training.json"):
        """Load 100K training data"""
        logger.info(f"Loading data from: {data_file}")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.train_data = data['train_data']
        self.val_data = data['val_data']
        
        logger.info(f"Loaded {len(self.train_data)} training samples")
        logger.info(f"Loaded {len(self.val_data)} validation samples")
        
        # Print label distribution
        self._print_label_distribution()
        
        return self.train_data, self.val_data
    
    def _print_label_distribution(self):
        """Print label distribution"""
        logger.info("Label distribution:")
        
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            train_labels = [sample['labels'][task] for sample in self.train_data]
            counter = Counter(train_labels)
            logger.info(f"  {task}: {dict(counter)}")
    
    def setup_processors(self):
        """Setup text and metadata processors"""
        logger.info("Setting up processors...")
        
        # Initialize processors
        self.text_processor = TextProcessor()
        self.metadata_processor = MetadataProcessor()
        
        # Collect all samples for fitting
        all_samples = self.train_data + self.val_data
        
        # Fit text processor
        texts = [sample['text'] for sample in all_samples]
        self.text_processor.fit(texts)
        
        # Fit metadata processor
        metadata_list = [sample['metadata'] for sample in all_samples]
        self.metadata_processor.fit(metadata_list)
        
        logger.info(f"Text vocabulary size: {len(self.text_processor.vocab)}")
        logger.info("Processors setup complete")
    
    def create_dataloaders(self):
        """Create data loaders"""
        logger.info("Creating data loaders...")
        
        # Create datasets
        train_dataset = Large100KDataset(
            self.train_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='train'
        )
        
        val_dataset = Large100KDataset(
            self.val_data, 
            self.text_processor, 
            self.metadata_processor, 
            split='validation'
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True if self.device.type == 'cuda' else False,
            collate_fn=custom_collate_fn        )
        
        logger.info(f"Created train loader with {len(self.train_loader)} batches")
        logger.info(f"Created val loader with {len(self.val_loader)} batches")
    
    def setup_model(self):
        """Setup multimodal fusion model"""
        logger.info("Setting up model...")
        
        # Get feature dimensions from processors
        feature_dims = self.metadata_processor.get_feature_dimensions()
        
        # Model configuration
        model_config = {
            'text_encoder': {
                'vocab_size': len(self.text_processor.vocab),
                'embedding_dim': 128,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.3
            },            'metadata_encoder': {
                'categorical_dims': {
                    'author_encoded': feature_dims['author_vocab_size'],
                    'season_encoded': feature_dims['season_vocab_size']
                },
                'numerical_features': ['numerical_features'],
                'embedding_dim': 64,
                'hidden_dim': 128,
                'dropout': 0.3
            },
            'fusion': {
                'hidden_dim': 256,
                'dropout': 0.4
            },
            'task_heads': {
                'risk_prediction': {'num_classes': 2, 'type': 'classification'},
                'complexity_prediction': {'num_classes': 3, 'type': 'classification'},
                'hotspot_prediction': {'num_classes': 3, 'type': 'classification'},
                'urgency_prediction': {'num_classes': 2, 'type': 'classification'}
            }
        }
        
        # Create model
        self.model = MultiModalFusionNetwork(model_config)
        self.model = self.model.to(self.device)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        
        return self.model
    
    def setup_training(self):
        """Setup optimizer, scheduler, loss functions"""
        logger.info("Setting up training components...")
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
          # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3
        )
        
        # Loss functions
        self.loss_functions = {}
        for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']:
            self.loss_functions[task] = nn.CrossEntropyLoss()
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None
        
        logger.info("Training setup complete")
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move to device
            text = batch['text'].to(self.device)
            metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in batch['metadata'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            
            self.optimizer.zero_grad()
            
            # Forward pass with mixed precision
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = self.model(text, metadata)
                    
                    # Calculate losses
                    losses = {}
                    for task in outputs.keys():
                        losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Combined loss
                    total_batch_loss = sum(losses.values())
                
                # Backward pass
                self.scaler.scale(total_batch_loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                
                # Combined loss
                total_batch_loss = sum(losses.values())
                
                # Backward pass
                total_batch_loss.backward()
                self.optimizer.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            for task, loss in losses.items():
                task_losses[task] += loss.item()
            
            num_batches += 1
            
            # Log progress
            if batch_idx % 100 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                           f"Loss: {total_batch_loss.item():.4f}")
        
        # Calculate average losses
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        return avg_total_loss, avg_task_losses
    
    def validate(self):
        """Validate model"""
        self.model.eval()
        
        total_loss = 0.0
        task_losses = {task: 0.0 for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_predictions = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        task_targets = {task: [] for task in ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']}
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # Move to device
                text = batch['text'].to(self.device)
                metadata = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in batch['metadata'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                
                # Forward pass
                outputs = self.model(text, metadata)
                
                # Calculate losses
                losses = {}
                for task in outputs.keys():
                    losses[task] = self.loss_functions[task](outputs[task], labels[task])
                    
                    # Collect predictions and targets
                    preds = torch.argmax(outputs[task], dim=1)
                    task_predictions[task].extend(preds.cpu().numpy())
                    task_targets[task].extend(labels[task].cpu().numpy())
                
                total_batch_loss = sum(losses.values())
                
                # Accumulate losses
                total_loss += total_batch_loss.item()
                for task, loss in losses.items():
                    task_losses[task] += loss.item()
                
                num_batches += 1
        
        # Calculate metrics
        avg_total_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}
        
        # Calculate accuracies
        task_accuracies = {}
        for task in task_predictions.keys():
            accuracy = accuracy_score(task_targets[task], task_predictions[task])
            task_accuracies[task] = accuracy
        
        overall_accuracy = np.mean(list(task_accuracies.values()))
        
        return avg_total_loss, avg_task_losses, task_accuracies, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'config': self.config
        }
        
        # Save latest checkpoint
        checkpoint_path = "trained_models/multimodal_fusion_100k/latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = "trained_models/multimodal_fusion_100k/best_model_100k.pth"
            torch.save(checkpoint, best_path)
            logger.info(f"New best model saved: {best_path}")
    
    def train(self):
        """Main training loop"""
        logger.info("Starting training...")
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            logger.info(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            
            # Train
            train_loss, train_task_losses = self.train_epoch(epoch)
            
            # Validate
            val_loss, val_task_losses, val_accuracies, overall_acc = self.validate()
            
            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}")
            logger.info(f"Val Loss: {val_loss:.4f}")
            logger.info(f"Overall Val Accuracy: {overall_acc:.4f}")
            
            for task in val_accuracies.keys():
                logger.info(f"  {task}: {val_accuracies[task]:.4f}")
            
            # Update scheduler
            self.scheduler.step(val_loss)
            
            # Check for best model
            is_best = False
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                is_best = True
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if overall_acc > self.best_val_acc:
                self.best_val_acc = overall_acc
            
            # Save checkpoint
            self.save_checkpoint(epoch, is_best)
            
            # Early stopping
            if self.patience_counter >= self.config['patience']:
                logger.info(f"Early stopping triggered after {epoch} epochs")
                break
        
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {self.best_val_loss:.4f}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.4f}")

def main():
    """Main function"""
    print("üöÄ TRAINING MULTIMODAL FUSION WITH 100K DATASET")
    print("=" * 70)
    
    # Training configuration
    config = {
        'batch_size': 32,
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'patience': 7,
        'num_workers': 4 if torch.cuda.is_available() else 0
    }
    
    # Create trainer
    trainer = MultimodalTrainer100K(config)
    
    try:
        # Load data
        trainer.load_data()
        
        # Setup processors
        trainer.setup_processors()
        
        # Create data loaders
        trainer.create_dataloaders()
        
        # Setup model
        trainer.setup_model()
        
        # Setup training
        trainer.setup_training()
        
        # Train
        trainer.train()
        
        print("\nüéâ Training completed successfully!")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\ai\train_enhanced_100k_fixed.py
```py
"""
Enhanced 100K Training Script with NLTK Support - Fixed Version
Trains the multimodal fusion model with enhanced text processing capabilities
"""

import os
import sys
import torch
import torch.nn as nn
import json
import logging
import numpy as np
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# Setup paths
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Setup logging with Unicode support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Enhanced100KDataset(Dataset):
    """Dataset class for 100K enhanced training data"""
    
    def __init__(self, data, text_processor, metadata_processor, device='cpu'):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.device = device
        
        logger.info(f"Initialized Enhanced100KDataset with {len(data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        try:
            # Extract fields from new data format
            commit_message = sample.get('text', '')
            metadata = sample.get('metadata', {})
            labels = sample.get('labels', {})
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode_text_lstm(commit_message)
            
            # Extract enhanced text features
            enhanced_features = self.text_processor.extract_enhanced_features(commit_message)
            
            # Convert enhanced features to tensor
            feature_values = []
            feature_keys = [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count', 'punctuation_count',
                'has_commit_type', 'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords',
                'has_technical_keywords', 'has_ui_keywords', 'has_testing_keywords',
                'avg_word_length', 'max_word_length', 'unique_word_ratio'
            ]
            
            # Add sentiment features if available
            if 'sentiment_polarity' in enhanced_features:
                feature_keys.extend(['sentiment_polarity', 'sentiment_subjectivity'])
            
            for key in feature_keys:
                val = enhanced_features.get(key, 0)
                if isinstance(val, bool):
                    val = float(val)
                elif isinstance(val, str):
                    val = 1.0 if val == 'positive' else (-1.0 if val == 'negative' else 0.0)
                feature_values.append(float(val))
            
            enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
            
            # Process metadata - extract from nested metadata dict
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': metadata.get('files_mentioned', []),  # Use files_mentioned as proxy
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1,   # Default value
                'is_merge': False,  # Default value
                'commit_size': 'medium',  # Default value
                'message_length': metadata.get('message_length', len(commit_message)),
                'word_count': metadata.get('word_count', len(commit_message.split())),
                'has_scope': metadata.get('has_scope', False),
                'is_conventional': metadata.get('is_conventional', False),
                'has_breaking': metadata.get('has_breaking', False)
            }
            
            # Create basic metadata features manually for compatibility
            files_changed_count = len(metadata_dict['files_changed']) if isinstance(metadata_dict['files_changed'], list) else 1
            metadata_features = torch.tensor([
                float(files_changed_count),
                float(metadata_dict.get('insertions', 0)),
                float(metadata_dict.get('deletions', 0)),
                float(metadata_dict.get('hour_of_day', 12) / 24.0),
                float(metadata_dict.get('day_of_week', 1) / 7.0),
                float(metadata_dict.get('is_merge', False)),
                1.0 if metadata_dict.get('commit_size') == 'small' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'medium' else 0.0,
                1.0 if metadata_dict.get('commit_size') == 'large' else 0.0,
                hash(metadata_dict.get('author', 'unknown')) % 1000 / 1000.0  # Simple author encoding
            ], dtype=torch.float32)
            
            # Labels - convert string labels to numeric
            def label_to_numeric(label_str):
                if label_str in ['low', 'simple']:
                    return 0
                elif label_str in ['medium', 'moderate']:
                    return 1
                elif label_str in ['high', 'complex']:
                    return 2
                else:
                    return 0  # Default to low
            
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'enhanced_text_features': enhanced_text_features,
                'metadata_features': metadata_features,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'enhanced_text_features': torch.zeros(18, dtype=torch.float32),  # Adjusted size
                'metadata_features': torch.zeros(10, dtype=torch.float32),
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    enhanced_text_features = torch.stack([item['enhanced_text_features'] for item in batch])
    metadata_features = torch.stack([item['metadata_features'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'enhanced_text_features': enhanced_text_features,
        'metadata_features': metadata_features,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return

    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)

    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if all(isinstance(v, dict) for v in full_data.values()) else [full_data]
        else:
            all_data = full_data
        
        # Split data manually if not pre-split
        train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42, stratify=None)

    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract text data for vocabulary building from all samples
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            # Use 'text' field for new data format, fallback to 'commit_message' for old format
            texts.append(sample.get('text', sample.get('commit_message', '')))
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            logger.warning(f"Unexpected sample format: {type(sample)}")
            continue

    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default
                'day_of_week': 1,   # Default
                'is_merge': False,  # Default
                'commit_size': 'medium'  # Default
            })
        else:
            # Fallback for old format
            metadata_samples.append({
                'author': sample.get('author', 'unknown'),
                'files_changed': sample.get('files_changed', 1),
                'insertions': sample.get('insertions', 0),
                'deletions': sample.get('deletions', 0),
                'hour_of_day': sample.get('hour_of_day', 12),
                'day_of_week': sample.get('day_of_week', 1),
                'is_merge': sample.get('is_merge', False),
                'commit_size': sample.get('commit_size', 'medium')
            })
    
    metadata_processor.fit(metadata_samples)

    # Data is already split or was split above
    logger.info("Using data splits...")
    logger.info(f"Final - Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor, device)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor, device)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    try:
        from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    except ImportError as e:
        logger.error(f"Could not import MultiModalFusionNetwork: {e}")
        return
    
    # Get enhanced feature dimensions
    sample_batch = next(iter(train_loader))
    enhanced_text_feature_dim = sample_batch['enhanced_text_features'].shape[1]
    metadata_feature_dim = sample_batch['metadata_features'].shape[1]
    
    logger.info(f"Enhanced text features dimension: {enhanced_text_feature_dim}")
    logger.info(f"Metadata features dimension: {metadata_feature_dim}")
    
    # Model configuration using the new config-based approach
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.get_vocab_size(),
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'],
            'embedding_dim': 64,
            'hidden_dim': metadata_feature_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Setup training
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Training parameters
    epochs = 50
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    # Training history
    train_history = {
        'train_loss': [],
        'val_loss': [],
        'train_accuracy': [],
        'val_accuracy': [],
        'learning_rate': []
    }
    
    # Create output directory
    output_dir = os.path.join(current_dir, 'trained_models', 'enhanced_multimodal_fusion_100k')
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("Starting training loop...")
    
    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                enhanced_text_features = batch['enhanced_text_features'].to(device)
                metadata_features = batch['metadata_features'].to(device)
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                
                # Prepare metadata input as dict for the model - combine enhanced features with basic metadata
                combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                metadata_input = {
                    'numerical_features': combined_features,
                    'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)  # Dummy author
                }
                
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for all tasks
                total_loss = 0
                correct_predictions = 0
                total_predictions = 0
                
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                for task_idx, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_output = outputs[task_name]
                        task_labels = labels[:, task_idx]
                        task_loss = criterion(task_output, task_labels)
                        total_loss += task_loss
                        
                        # Calculate accuracy
                        _, predicted = torch.max(task_output.data, 1)
                        correct_predictions += (predicted == task_labels).sum().item()
                        total_predictions += task_labels.size(0)
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_correct += correct_predictions
                train_total += total_predictions
                
                if batch_idx % 100 == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    logger.info(f"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, "
                              f"Loss: {total_loss.item():.4f}, LR: {current_lr:.2e}")
                    
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    text_encoded = batch['text_encoded'].to(device)
                    enhanced_text_features = batch['enhanced_text_features'].to(device)
                    metadata_features = batch['metadata_features'].to(device)
                    labels = batch['labels'].to(device)
                    
                    # Prepare metadata input as dict for the model
                    combined_features = torch.cat([metadata_features, enhanced_text_features], dim=1)
                    metadata_input = {
                        'numerical_features': combined_features,
                        'author': torch.zeros(text_encoded.size(0), dtype=torch.long).to(device)
                    }
                    
                    outputs = model(text_encoded, metadata_input)
                    
                    total_loss = 0
                    correct_predictions = 0
                    total_predictions = 0
                    
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    for task_idx, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_output = outputs[task_name]
                            task_labels = labels[:, task_idx]
                            task_loss = criterion(task_output, task_labels)
                            total_loss += task_loss
                            
                            _, predicted = torch.max(task_output.data, 1)
                            correct_predictions += (predicted == task_labels).sum().item()
                            total_predictions += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_correct += correct_predictions
                    val_total += total_predictions
                    
                except Exception as e:
                    logger.error(f"Error in validation batch: {e}")
                    continue
        
        # Calculate averages
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_accuracy = train_correct / max(train_total, 1)
        val_accuracy = val_correct / max(val_total, 1)
        current_lr = optimizer.param_groups[0]['lr']
        
        # Update history
        train_history['train_loss'].append(avg_train_loss)
        train_history['val_loss'].append(avg_val_loss)
        train_history['train_accuracy'].append(train_accuracy)
        train_history['val_accuracy'].append(val_accuracy)
        train_history['learning_rate'].append(current_lr)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{epochs} - "
                   f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                   f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
        
        # Learning rate scheduling
        scheduler.step(avg_val_loss)
        
        # Early stopping and model saving
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # Save best model
            model_path = os.path.join(output_dir, 'best_enhanced_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'train_history': train_history,
                'text_processor_vocab': text_processor.vocab,
                'model_config': model_config
            }, model_path)
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            logger.info(f"Early stopping triggered after {patience} epochs without improvement")
            break
    
    # Save final training history
    history_path = os.path.join(output_dir, 'enhanced_training_history.json')
    with open(history_path, 'w') as f:
        json.dump(train_history, f, indent=2)
    
    logger.info("Enhanced training completed successfully!")
    logger.info(f"Models and history saved to: {output_dir}")
    
    return model, train_history

if __name__ == "__main__":
    try:
        model, history = train_enhanced_100k_model()
        print("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_enhanced_100k_multimodal_fusion_final.py
```py
#!/usr/bin/env python3
"""
Enhanced 100K Multimodal Fusion Training Script - Final Version
This script handles the new data format structure and enhanced text processing.
"""

import os
import sys
import json
import logging
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from datetime import datetime

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_100k_training_final.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def label_to_numeric(label_str):
    """Convert string labels to numeric values"""
    if isinstance(label_str, str):
        label_str = label_str.lower().strip()
        if label_str in ['low', 'simple']:
            return 0
        elif label_str in ['medium', 'moderate']:
            return 1
        elif label_str in ['high', 'complex']:
            return 2
        else:
            return 0  # Default to low
    return 0

class Enhanced100KDataset(Dataset):
    """Enhanced dataset for 100K samples with new data format"""
    
    def __init__(self, data, text_processor, metadata_processor):
        self.data = data
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        try:
            sample = self.data[idx]
            
            # Extract text - handle new format
            text = sample.get('text', sample.get('commit_message', ''))
            if not text:
                text = "Empty commit message"
            
            # Process text with enhanced features
            text_encoded = self.text_processor.encode(text)
            
            # Extract enhanced text features if available
            if hasattr(self.text_processor, 'extract_enhanced_features'):
                try:
                    enhanced_features = self.text_processor.extract_enhanced_features(text)
                    feature_values = []
                    for feature_name, feature_value in enhanced_features.items():
                        if isinstance(feature_value, (list, np.ndarray)):
                            feature_values.extend(feature_value)
                        else:
                            feature_values.append(float(feature_value))
                    
                    # Ensure we have exactly 18 features
                    while len(feature_values) < 18:
                        feature_values.append(0.0)
                    feature_values = feature_values[:18]
                    
                    enhanced_text_features = torch.tensor(feature_values, dtype=torch.float32)
                except Exception as e:
                    logger.warning(f"Error extracting enhanced features: {e}")
                    enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            else:
                enhanced_text_features = torch.zeros(18, dtype=torch.float32)
            
            # Extract metadata from new structure
            metadata = sample.get('metadata', {})
            metadata_dict = {
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format  
                'hour_of_day': 12,  # Default value
                'day_of_week': 1   # Default value
            }
            
            # Combine base metadata features with enhanced text features
            base_metadata = self.metadata_processor.process(metadata_dict)
            enhanced_values = enhanced_text_features.tolist() if len(enhanced_text_features.shape) > 0 else [0.0] * 18
            
            # Combine: base metadata (5 features) + enhanced text features (18 features) = 23 total
            combined_numerical = base_metadata['numerical_features'].tolist() + enhanced_values
            
            metadata_input = {
                'numerical_features': torch.tensor(combined_numerical, dtype=torch.float32),
                'author': base_metadata['author']
            }
            
            # Extract labels from new structure
            labels = sample.get('labels', {})
            labels_tensor = torch.tensor([
                label_to_numeric(labels.get('risk_prediction', 'low')),
                label_to_numeric(labels.get('complexity_prediction', 'simple')),
                label_to_numeric(labels.get('hotspot_prediction', 'low')),
                label_to_numeric(labels.get('urgency_prediction', 'low'))
            ], dtype=torch.long)
            
            return {
                'text_encoded': text_encoded,
                'metadata_input': metadata_input,
                'labels': labels_tensor
            }
            
        except Exception as e:
            logger.error(f"Error processing sample {idx}: {e}")
            # Return default values on error
            return {
                'text_encoded': torch.zeros(128, dtype=torch.long),
                'metadata_input': {
                    'numerical_features': torch.zeros(23, dtype=torch.float32),  # 5 base + 18 enhanced
                    'author': torch.tensor(0, dtype=torch.long)
                },
                'labels': torch.zeros(4, dtype=torch.long)
            }

def enhanced_collate_fn(batch):
    """Enhanced collate function for DataLoader"""
    text_encoded = torch.stack([item['text_encoded'] for item in batch])
    
    # Handle metadata input dict
    metadata_input = {
        'numerical_features': torch.stack([item['metadata_input']['numerical_features'] for item in batch]),
        'author': torch.stack([item['metadata_input']['author'] for item in batch])
    }
    
    labels = torch.stack([item['labels'] for item in batch])
    
    return {
        'text_encoded': text_encoded,
        'metadata_input': metadata_input,
        'labels': labels
    }

def train_enhanced_100k_model():
    """Main training function with enhanced text processing"""
    
    logger.info("Starting Enhanced 100K Multimodal Fusion Training...")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load data
    data_path = os.path.join(current_dir, 'training_data', 'improved_100k_multimodal_training.json')
    if not os.path.exists(data_path):
        logger.error(f"Training data not found at {data_path}")
        return
    
    logger.info("Loading training data...")
    with open(data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)
    
    # Handle data format - check if already split
    if isinstance(full_data, dict) and 'train_data' in full_data and 'val_data' in full_data:
        logger.info("Found pre-split data (train_data/val_data)")
        train_data = full_data['train_data']
        val_data = full_data['val_data']
        all_data = train_data + val_data
    else:
        # Handle other data formats
        if isinstance(full_data, dict):
            if 'training_data' in full_data:
                all_data = full_data['training_data']
            elif 'samples' in full_data:
                all_data = full_data['samples']
            else:
                # If it's a dict with other structure, convert to list
                all_data = list(full_data.values()) if isinstance(list(full_data.values())[0], dict) else full_data
        else:
            all_data = full_data
        
        # Split data if not already split
        split_idx = int(0.8 * len(all_data))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
    
    logger.info(f"Train samples: {len(train_data)}, Validation samples: {len(val_data)}")
    
    # Extract texts for vocabulary building
    texts = []
    for sample in all_data:
        if isinstance(sample, dict):
            text = sample.get('text', sample.get('commit_message', ''))
            if text:
                texts.append(text)
        elif isinstance(sample, str):
            texts.append(sample)
        else:
            continue
    
    logger.info(f"Loaded {len(texts)} commit messages for vocabulary building")
    
    # Initialize enhanced text processor
    logger.info("Initializing Enhanced Text Processor...")
    try:
        from multimodal_fusion.data_preprocessing.minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
        text_processor = MinimalEnhancedTextProcessor(
            method="lstm",
            vocab_size=10000,
            max_length=128,
            enable_sentiment=True,
            enable_advanced_cleaning=True
        )
        logger.info("Enhanced Text Processor initialized")
    except ImportError as e:
        logger.error(f"Failed to import enhanced text processor: {e}")
        # Fallback to basic processor
        from multimodal_fusion.data_preprocessing.text_processor import TextProcessor
        text_processor = TextProcessor(method="lstm", vocab_size=10000, max_length=128)
        logger.info("Using basic text processor as fallback")
    
    # Fit text processor
    logger.info("Fitting text processor to training data...")
    text_processor.fit(texts)
    
    # Initialize metadata processor
    logger.info("Initializing Metadata Processor...")
    from multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
    metadata_processor = MetadataProcessor()
    
    # Create metadata samples for fitting based on new data structure
    metadata_samples = []
    for sample in all_data:
        if isinstance(sample, dict) and 'metadata' in sample:
            metadata = sample['metadata']
            metadata_samples.append({
                'author': metadata.get('author', 'unknown'),
                'files_changed': len(metadata.get('files_mentioned', [])) if isinstance(metadata.get('files_mentioned'), list) else 1,
                'insertions': 0,  # Not available in new format
                'deletions': 0,   # Not available in new format
                'hour_of_day': 12,  # Default value
                'day_of_week': 1    # Default value
            })
    
    if metadata_samples:
        logger.info("Fitting metadata processor...")
        metadata_processor.fit(metadata_samples)
    else:
        logger.warning("No metadata samples found for fitting")
    
    # Create datasets
    train_dataset = Enhanced100KDataset(train_data, text_processor, metadata_processor)
    val_dataset = Enhanced100KDataset(val_data, text_processor, metadata_processor)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=enhanced_collate_fn,
        num_workers=0
    )
    
    # Initialize model
    logger.info("Initializing Enhanced Multimodal Fusion Model...")
    
    # Calculate dimensions
    sample_batch = next(iter(train_loader))
    text_dim = 64  # LSTM hidden dimension (fixed from model config)
    metadata_dim = sample_batch['metadata_input']['numerical_features'].shape[-1]
    
    logger.info(f"Text features dimension: {text_dim}")
    logger.info(f"Metadata features dimension: {metadata_dim}")
    
    # Model configuration matching the expected format
    model_config = {
        'text_encoder': {
            'vocab_size': text_processor.vocab_size,
            'embedding_dim': 128,
            'hidden_dim': 64,
            'num_layers': 2,
            'method': 'lstm'
        },
        'metadata_encoder': {
            'categorical_dims': {
                'author': 1000  # Simplified author encoding
            },
            'numerical_features': ['files_changed', 'insertions', 'deletions', 'hour_of_day', 'day_of_week'] + 
                                [f'enhanced_feature_{i}' for i in range(18)],
            'hidden_dim': metadata_dim
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 3},      # low, medium, high
            'complexity_prediction': {'num_classes': 3}, # simple, moderate, complex  
            'hotspot_prediction': {'num_classes': 3},   # low, medium, high
            'urgency_prediction': {'num_classes': 3}    # low, medium, high
        }
    }
    
    from multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
    model = MultiModalFusionNetwork(config=model_config)
    model = model.to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
    
    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    # Loss function
    criterion = nn.CrossEntropyLoss()
    
    # Training loop
    num_epochs = 10
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Move batch to device
                text_encoded = batch['text_encoded'].to(device)
                metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                labels = batch['labels'].to(device)
                
                # Forward pass
                optimizer.zero_grad()
                outputs = model(text_encoded, metadata_input)
                
                # Calculate loss for each task
                total_loss = 0
                task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                
                for i, task_name in enumerate(task_names):
                    if task_name in outputs:
                        task_logits = outputs[task_name]
                        task_labels = labels[:, i]
                        task_loss = criterion(task_logits, task_labels)
                        total_loss += task_loss
                
                # Backward pass
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += total_loss.item()
                train_batches += 1
                
                # Log progress
                if batch_idx % 100 == 0:
                    logger.info(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {total_loss.item():.4f}")
                
            except Exception as e:
                logger.error(f"Error in training batch {batch_idx}: {e}")
                continue
        
        avg_train_loss = train_loss / train_batches if train_batches > 0 else float('inf')
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                try:
                    # Move batch to device
                    text_encoded = batch['text_encoded'].to(device)
                    metadata_input = {k: v.to(device) for k, v in batch['metadata_input'].items()}
                    labels = batch['labels'].to(device)
                    
                    # Forward pass
                    outputs = model(text_encoded, metadata_input)
                    
                    # Calculate loss
                    total_loss = 0
                    task_names = ['risk_prediction', 'complexity_prediction', 'hotspot_prediction', 'urgency_prediction']
                    
                    for i, task_name in enumerate(task_names):
                        if task_name in outputs:
                            task_logits = outputs[task_name]
                            task_labels = labels[:, i]
                            task_loss = criterion(task_logits, task_labels)
                            total_loss += task_loss
                            
                            # Calculate accuracy for this task
                            predicted = torch.argmax(task_logits, dim=1)
                            val_correct += (predicted == task_labels).sum().item()
                            val_total += task_labels.size(0)
                    
                    val_loss += total_loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    logger.error(f"Error in validation batch {batch_idx}: {e}")
                    continue
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        val_accuracy = val_correct / val_total if val_total > 0 else 0.0
        
        # Update scheduler
        scheduler.step(avg_val_loss)
        
        # Log epoch results
        logger.info(f"Epoch {epoch+1}/{num_epochs}")
        logger.info(f"Train Loss: {avg_train_loss:.4f}")
        logger.info(f"Val Loss: {avg_val_loss:.4f}")
        logger.info(f"Val Accuracy: {val_accuracy:.4f}")
        logger.info(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model_path = os.path.join(current_dir, 'models', 'enhanced_100k_multimodal_fusion_best.pth')
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': best_val_loss,
                'model_config': model_config,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }, model_path)
            
            logger.info(f"Saved best model with validation loss: {best_val_loss:.4f}")
        
        logger.info("-" * 80)
    
    logger.info("Training completed!")
    logger.info(f"Best validation loss: {best_val_loss:.4f}")
    
    return model, text_processor, metadata_processor

if __name__ == "__main__":
    try:
        model, text_processor, metadata_processor = train_enhanced_100k_model()
        logger.info("Enhanced 100K training completed successfully!")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise

```

### backend\ai\train_han_github.py
```py
#!/usr/bin/env python3
"""
Train HAN Model v·ªõi GitHub Commits Dataset
Script ƒë∆°n gi·∫£n ƒë·ªÉ train m√¥ h√¨nh HAN v·ªõi d·ªØ li·ªáu t·ª´ GitHub commits
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter
import re

# Add backend directory to path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, backend_dir)

class SimpleHANModel(nn.Module):
    """
    Simplified Hierarchical Attention Network for commit classification
    """
    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, num_classes=None):
        super(SimpleHANModel, self).__init__()
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # Word-level LSTM
        self.word_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # Word-level attention
        self.word_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Sentence-level LSTM
        self.sentence_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)
        
        # Sentence-level attention
        self.sentence_attention = nn.Linear(hidden_dim * 2, 1)
        
        # Multi-task classification heads
        self.classifiers = nn.ModuleDict()
        if num_classes:
            for task, num_class in num_classes.items():
                self.classifiers[task] = nn.Linear(hidden_dim * 2, num_class)
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, input_ids, attention_mask=None):
        batch_size, max_sentences, max_words = input_ids.size()
        
        # Reshape for word-level processing
        input_ids = input_ids.view(-1, max_words)  # (batch_size * max_sentences, max_words)
        
        # Word embeddings
        embedded = self.embedding(input_ids)  # (batch_size * max_sentences, max_words, embed_dim)
        
        # Word-level LSTM
        word_output, _ = self.word_lstm(embedded)  # (batch_size * max_sentences, max_words, hidden_dim * 2)
        
        # Word-level attention
        word_attention_weights = torch.softmax(self.word_attention(word_output), dim=1)
        sentence_vectors = torch.sum(word_attention_weights * word_output, dim=1)  # (batch_size * max_sentences, hidden_dim * 2)
        
        # Reshape back to sentence level
        sentence_vectors = sentence_vectors.view(batch_size, max_sentences, -1)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level LSTM
        sentence_output, _ = self.sentence_lstm(sentence_vectors)  # (batch_size, max_sentences, hidden_dim * 2)
        
        # Sentence-level attention
        sentence_attention_weights = torch.softmax(self.sentence_attention(sentence_output), dim=1)
        document_vector = torch.sum(sentence_attention_weights * sentence_output, dim=1)  # (batch_size, hidden_dim * 2)
        
        document_vector = self.dropout(document_vector)
        
        # Multi-task outputs
        outputs = {}
        for task, classifier in self.classifiers.items():
            outputs[task] = classifier(document_vector)
        
        return outputs

class CommitDataset(Dataset):
    """Dataset class for commit messages"""
    
    def __init__(self, texts, labels, tokenizer, max_sentences=10, max_words=50):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_sentences = max_sentences
        self.max_words = max_words
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        labels = self.labels[idx]
        
        # Tokenize text to sentences and words
        input_ids = self.tokenizer.encode_text(text, self.max_sentences, self.max_words)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': labels  # This will be a dictionary
        }

class SimpleTokenizer:
    """Simple tokenizer for commit messages"""
    
    def __init__(self, vocab_size=10000):
        self.vocab_size = vocab_size
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}
        self.word_counts = Counter()
        
    def fit(self, texts):
        """Build vocabulary from texts"""
        print("üî§ Building vocabulary...")
        
        for text in texts:
            # Split into sentences
            sentences = self.split_sentences(text)
            for sentence in sentences:
                words = self.tokenize_words(sentence)
                self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 2)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 2  # Start from 2 (after PAD and UNK)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"‚úÖ Vocabulary built with {len(self.word_to_idx)} words")
        
    def split_sentences(self, text):
        """Split text into sentences"""
        # Simple sentence splitting for commit messages
        sentences = re.split(r'[.!?;]|\\n', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences if sentences else [text]
    
    def tokenize_words(self, sentence):
        """Tokenize sentence into words"""
        # Simple word tokenization
        words = re.findall(r'\b\w+\b', sentence.lower())
        return words
    
    def encode_text(self, text, max_sentences, max_words):
        """Encode text to token ids"""
        sentences = self.split_sentences(text)
        
        # Pad or truncate sentences
        if len(sentences) > max_sentences:
            sentences = sentences[:max_sentences]
        while len(sentences) < max_sentences:
            sentences.append("")
        
        encoded_sentences = []
        for sentence in sentences:
            words = self.tokenize_words(sentence)
            
            # Convert words to indices
            word_ids = []
            for word in words:
                word_ids.append(self.word_to_idx.get(word, 1))  # 1 is UNK
            
            # Pad or truncate words
            if len(word_ids) > max_words:
                word_ids = word_ids[:max_words]
            while len(word_ids) < max_words:
                word_ids.append(0)  # 0 is PAD
            
            encoded_sentences.append(word_ids)
        
        return encoded_sentences

def collate_fn(batch):
    """Custom collate function for DataLoader"""
    input_ids = torch.stack([item['input_ids'] for item in batch])
    labels = [item['labels'] for item in batch]  # Keep as list of dicts
    
    return {
        'input_ids': input_ids,
        'labels': labels
    }

def load_github_dataset(data_file):
    """Load GitHub commits dataset"""
    print(f"üìñ Loading dataset: {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'data' not in data:
        raise ValueError("Invalid dataset format: missing 'data' field")
    
    samples = data['data']
    print(f"üìä Loaded {len(samples)} samples")
    
    # Extract texts and labels
    texts = []
    labels = []
    
    for sample in samples:
        texts.append(sample['text'])
        labels.append(sample['labels'])
    
    return texts, labels, data.get('metadata', {})

def prepare_label_encoders(labels):
    """Prepare label encoders for multi-task classification"""
    print("üè∑Ô∏è  Preparing label encoders...")
    
    # Get all unique labels for each task
    label_sets = {}
    for label_dict in labels:
        for task, label in label_dict.items():
            if task not in label_sets:
                label_sets[task] = set()
            label_sets[task].add(label)
    
    # Create mappings
    label_encoders = {}
    num_classes = {}
    
    for task, label_set in label_sets.items():
        sorted_labels = sorted(list(label_set))
        label_encoders[task] = {label: idx for idx, label in enumerate(sorted_labels)}
        num_classes[task] = len(sorted_labels)
        
        print(f"  {task}: {len(sorted_labels)} classes -> {sorted_labels}")
    
    return label_encoders, num_classes

def encode_labels(labels, label_encoders):
    """Encode labels using label encoders"""
    encoded_labels = []
    
    for label_dict in labels:
        encoded_dict = {}
        for task, label in label_dict.items():
            if task in label_encoders:
                encoded_dict[task] = label_encoders[task][label]
        encoded_labels.append(encoded_dict)
    
    return encoded_labels

def train_epoch(model, dataloader, optimizers, criteria, device, scaler=None, use_amp=False):
    """Train for one epoch with GPU optimizations and mixed precision"""
    model.train()
    total_losses = {task: 0.0 for task in criteria.keys()}
    total_loss = 0.0
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        # Move data to device with non_blocking for better GPU utilization
        input_ids = batch['input_ids'].to(device)
        batch_labels = batch['labels']
        
        # Clear gradients
        for optimizer in optimizers.values():
            optimizer.zero_grad()
        
        # Forward pass with mixed precision
        if use_amp and scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(input_ids)
                
                # Calculate losses for each task
                batch_losses = {}
                for task, criterion in criteria.items():
                    task_labels = []
                    for label_dict in batch_labels:
                        if task in label_dict:
                            task_labels.append(label_dict[task])
                    
                    if task_labels:
                        task_labels_tensor = torch.tensor(task_labels, device=device)
                        task_loss = criterion(outputs[task], task_labels_tensor)
                        batch_losses[task] = task_loss
                        total_losses[task] += task_loss.item()
                
                if batch_losses:
                    combined_loss = sum(batch_losses.values()) / len(batch_losses)
                    total_loss += combined_loss.item()
                    num_batches += 1
            
            # Backward pass with mixed precision
            if batch_losses:
                scaler.scale(combined_loss).backward()
                
                # Gradient clipping
                scaler.unscale_(list(optimizers.values())[0])  # Unscale for clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    scaler.step(optimizer)
                scaler.update()
        else:
            # Regular forward pass
            outputs = model(input_ids)
              # Calculate losses for each task
            batch_losses = {}
            for task, criterion in criteria.items():
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    batch_losses[task] = task_loss
                    total_losses[task] += task_loss.item()
            
            # Combined loss
            if batch_losses:
                combined_loss = sum(batch_losses.values()) / len(batch_losses)
                total_loss += combined_loss.item()
                num_batches += 1
                
                # Backward pass
                combined_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                for optimizer in optimizers.values():
                    optimizer.step()
        
        # Memory cleanup every 50 batches on GPU
        if device.type == 'cuda' and batch_idx % 50 == 0:
            torch.cuda.empty_cache()
    
    # Calculate average losses
    if num_batches > 0:
        avg_losses = {task: loss / num_batches for task, loss in total_losses.items()}
        avg_total_loss = total_loss / num_batches
    else:
        avg_losses = {task: 0.0 for task in total_losses.keys()}
        avg_total_loss = 0.0
    
    return avg_losses, avg_total_loss

def evaluate_model(model, dataloader, criteria, device):
    """Evaluate model with GPU optimizations"""
    model.eval()
    total_losses = {task: 0.0 for task in criteria.keys()}
    predictions = {task: [] for task in criteria.keys()}
    true_labels = {task: [] for task in criteria.keys()}
    num_batches = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # Move data to device with non_blocking
            input_ids = batch['input_ids'].to(device)
            batch_labels = batch['labels']
            
            # Forward pass
            outputs = model(input_ids)
            
            # Calculate losses and collect predictions
            for task, criterion in criteria.items():                # Extract task labels from batch
                task_labels = []
                for label_dict in batch_labels:
                    if task in label_dict:
                        task_labels.append(label_dict[task])
                
                if task_labels:  # Only if we have labels for this task
                    task_labels_tensor = torch.tensor(task_labels, device=device)
                    task_loss = criterion(outputs[task], task_labels_tensor)
                    total_losses[task] += task_loss.item()
                    
                    # Predictions
                    _, predicted = torch.max(outputs[task], 1)
                    predictions[task].extend(predicted.cpu().numpy())
                    true_labels[task].extend(task_labels_tensor.cpu().numpy())
            
            num_batches += 1
            
            # Memory cleanup every 50 batches on GPU
            if device.type == 'cuda' and batch_idx % 50 == 0:
                torch.cuda.empty_cache()
    
    # Calculate metrics
    metrics = {}
    for task in criteria.keys():
        if predictions[task] and num_batches > 0:
            accuracy = accuracy_score(true_labels[task], predictions[task])
            metrics[task] = {
                'loss': total_losses[task] / num_batches,
                'accuracy': accuracy
            }
    
    return metrics

def main():
    """Main training function"""
    print("üöÄ HAN GITHUB COMMITS TRAINER")
    print("="*60)
    
    # GPU Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Device: {device}")
    
    if torch.cuda.is_available():
        print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        # Clear GPU cache
        torch.cuda.empty_cache()
    else:
        print("‚ö†Ô∏è  CUDA not available, using CPU")
    
    # Paths
    data_file = Path(__file__).parent / "training_data" / "github_commits_training_data.json"
    model_dir = Path(__file__).parent / "models" / "han_github_model"
    log_dir = Path(__file__).parent / "training_logs"
    
    # Create directories
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load dataset
    if not data_file.exists():
        print(f"‚ùå Dataset not found: {data_file}")
        print("   Please run: python download_github_commits.py")
        return
    
    texts, labels, metadata = load_github_dataset(data_file)
    
    # Prepare labels
    label_encoders, num_classes = prepare_label_encoders(labels)
    encoded_labels = encode_labels(labels, label_encoders)
    
    # Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, encoded_labels, test_size=0.2, random_state=42
    )
    
    print(f"üìä Train samples: {len(train_texts)}")
    print(f"üìä Val samples: {len(val_texts)}")
    
    # Build tokenizer
    tokenizer = SimpleTokenizer(vocab_size=5000)
    tokenizer.fit(train_texts)
      # Create datasets
    train_dataset = CommitDataset(train_texts, train_labels, tokenizer)
    val_dataset = CommitDataset(val_texts, val_labels, tokenizer)
    
    # GPU optimized batch size
    if device.type == 'cuda':
        # Larger batch size for GPU
        batch_size = 32
        num_workers = 4  # More workers for GPU
        pin_memory = True
    else:
        # Smaller batch size for CPU
        batch_size = 16
        num_workers = 2
        pin_memory = False
    
    print(f"üîß Batch size: {batch_size}")
    print(f"üë• Num workers: {num_workers}")
    
    # Create dataloaders with GPU optimizations
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
      # Create model with GPU optimizations
    model = SimpleHANModel(
        vocab_size=len(tokenizer.word_to_idx),
        embed_dim=100,
        hidden_dim=128,
        num_classes=num_classes
    ).to(device)
    
    # Enable mixed precision for GPU if available
    if device.type == 'cuda':
        scaler = torch.cuda.amp.GradScaler()
        use_amp = True
        print("üöÄ Mixed precision training enabled")
    else:
        scaler = None
        use_amp = False
    
    print(f"ü§ñ Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPU memory optimization
    if device.type == 'cuda':
        print(f"üìä GPU Memory before training: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    
    # Setup training with optimized learning rates for GPU
    criteria = {}
    optimizers = {}
    schedulers = {}
    
    # Higher learning rate for GPU training
    base_lr = 0.002 if device.type == 'cuda' else 0.001
    
    for task in num_classes.keys():
        criteria[task] = nn.CrossEntropyLoss()
        optimizers[task] = optim.AdamW(  # AdamW is often better than Adam
            list(model.classifiers[task].parameters()) + 
            list(model.embedding.parameters()) +
            list(model.word_lstm.parameters()) +
            list(model.sentence_lstm.parameters()) +
            list(model.word_attention.parameters()) +
            list(model.sentence_attention.parameters()),
            lr=base_lr,
            weight_decay=1e-4  # L2 regularization
        )        # Add learning rate scheduler
        schedulers[task] = optim.lr_scheduler.ReduceLROnPlateau(
            optimizers[task], 
            mode='min', 
            factor=0.5, 
            patience=3
        )
      # Training loop with GPU monitoring
    num_epochs = 20
    best_val_accuracy = 0.0
    
    log_file = log_dir / f"han_github_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    print(f"\nüéØ Starting training for {num_epochs} epochs...")
    
    # Training start time
    import time
    training_start_time = time.time()
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        print(f"\nüìÖ Epoch {epoch+1}/{num_epochs}")
        
        # GPU memory monitoring
        if device.type == 'cuda':
            print(f"üìä GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        
        # Train with enhanced function
        train_losses, train_total_loss = train_epoch(
            model, train_loader, optimizers, criteria, device, scaler, use_amp
        )
        
        # Validate
        val_metrics = evaluate_model(model, val_loader, criteria, device)
        
        # Update learning rate schedulers
        avg_val_loss = 0.0
        if val_metrics:
            for task, metrics in val_metrics.items():
                if task in schedulers:
                    schedulers[task].step(metrics['loss'])
                avg_val_loss += metrics['loss']
            avg_val_loss /= len(val_metrics)
        
        # Calculate epoch time
        epoch_time = time.time() - epoch_start_time
        
        # Log results
        print(f"  ‚è±Ô∏è  Epoch time: {epoch_time:.1f}s")
        print(f"  üìà Train Loss: {train_total_loss:.4f}")
        for task, loss in train_losses.items():
            print(f"    {task}: {loss:.4f}")
        
        print(f"  üìä Val Metrics:")
        val_accuracy_sum = 0.0
        for task, metrics in val_metrics.items():
            print(f"    {task}: Loss={metrics['loss']:.4f}, Acc={metrics['accuracy']:.4f}")
            val_accuracy_sum += metrics['accuracy']
        
        avg_val_accuracy = val_accuracy_sum / len(val_metrics) if val_metrics else 0.0
        print(f"  üéØ Avg Val Accuracy: {avg_val_accuracy:.4f}")
        
        # GPU memory cleanup
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            print(f"  üßπ GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
          # Save log with enhanced information
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"\nEpoch {epoch+1}/{num_epochs}\n")
            f.write(f"Epoch Time: {epoch_time:.1f}s\n")
            f.write(f"Train Loss: {train_total_loss:.4f}\n")
            for task, loss in train_losses.items():
                f.write(f"  {task} Train Loss: {loss:.4f}\n")
            f.write(f"Val Metrics: {val_metrics}\n")
            f.write(f"Avg Val Accuracy: {avg_val_accuracy:.4f}\n")
            if device.type == 'cuda':
                f.write(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\n")
            f.write("-" * 50 + "\n")
        
        # Save best model with enhanced information
        if avg_val_accuracy > best_val_accuracy:
            best_val_accuracy = avg_val_accuracy
            
            # Save model with comprehensive information
            model_save_dict = {
                'model_state_dict': model.state_dict(),
                'tokenizer': tokenizer,
                'label_encoders': label_encoders,
                'num_classes': num_classes,
                'metadata': metadata,
                'epoch': epoch + 1,
                'val_accuracy': avg_val_accuracy,
                'train_loss': train_total_loss,
                'device': str(device),
                'batch_size': batch_size,
                'learning_rate': base_lr,
                'model_params': sum(p.numel() for p in model.parameters()),
                'training_config': {
                    'use_amp': use_amp,
                    'vocab_size': len(tokenizer.word_to_idx),
                    'embed_dim': 100,
                    'hidden_dim': 128,                    'max_sentences': 10,
                    'max_words': 50
                }
            }
            
            torch.save(model_save_dict, model_dir / 'best_model.pth')
            
            print(f"  üíæ Saved best model (accuracy: {avg_val_accuracy:.4f})")
    
    # Training completion summary
    total_training_time = time.time() - training_start_time
    print(f"\nüéâ Training completed!")
    print(f"‚è±Ô∏è  Total training time: {total_training_time/60:.1f} minutes")
    print(f"üìä Best validation accuracy: {best_val_accuracy:.4f}")
    print(f"üíæ Model saved to: {model_dir}")
    print(f"üìù Logs saved to: {log_file}")
    
    if device.type == 'cuda':
        print(f"üéÆ GPU training completed successfully")
        print(f"üìä Final GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

```

### backend\ai\__init__.py
```py

```

### backend\ai\multimodal_fusion\__init__.py
```py
"""
Multi-Modal Fusion Network for Commit Analysis
M√¥ h√¨nh k·∫øt h·ª£p th√¥ng tin vƒÉn b·∫£n v√† metadata ƒë·ªÉ ph√¢n t√≠ch commit
"""

__version__ = "1.0.0"
__author__ = "AI Team"

from .models.multimodal_fusion import MultiModalFusionNetwork
from .data_preprocessing.text_processor import TextProcessor
from .data_preprocessing.metadata_processor import MetadataProcessor
from .training.multitask_trainer import MultiTaskTrainer

__all__ = [
    "MultiModalFusionNetwork",
    "TextProcessor", 
    "MetadataProcessor",
    "MultiTaskTrainer"
]

```

### backend\ai\multimodal_fusion\data\synthetic_generator.py
```py
"""
Data Generation for Multi-Modal Fusion Network
T·∫°o synthetic GitHub commit data v·ªõi realistic patterns
"""

import random
import numpy as np
import pandas as pd
import torch
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import string
from collections import defaultdict
import re


class GitHubDataGenerator:
    """
    Generator cho synthetic GitHub commit data v·ªõi metadata patterns
    """
    
    def __init__(self, seed: int = 42):
        """
        Args:
            seed: Random seed for reproducibility
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # Pre-defined patterns
        self.commit_patterns = self._init_commit_patterns()
        self.file_types = self._init_file_types()
        self.authors = self._init_authors()
        self.programming_words = self._init_programming_words()
        
    def _init_commit_patterns(self) -> Dict[str, List[str]]:
        """Initialize commit message patterns"""
        return {
            'fix': [
                "Fix bug in {component}",
                "Fixed {issue} causing {problem}",
                "Bugfix: {description}",
                "Resolve {issue} in {component}",
                "Patch for {vulnerability}",
                "Hotfix: {critical_issue}",
                "Quick fix for {problem}",
                "Emergency fix: {description}"
            ],
            'feature': [
                "Add {feature} to {component}",
                "Implement {functionality}",
                "Feature: {new_feature}",
                "Introduce {capability}",
                "New: {feature_description}",
                "Enhance {component} with {feature}",
                "Added support for {technology}",
                "Initial implementation of {feature}"
            ],
            'refactor': [
                "Refactor {component} for better {quality}",
                "Code cleanup in {module}",
                "Restructure {component}",
                "Optimize {algorithm} implementation",
                "Improve {aspect} of {component}",
                "Reorganize {module} structure",
                "Clean up {technical_debt}",
                "Modernize {legacy_code}"
            ],
            'update': [
                "Update {dependency} to version {version}",
                "Upgrade {library} dependencies",
                "Bump {package} version",
                "Update documentation for {feature}",
                "Sync with upstream {repository}",
                "Update configuration for {environment}",
                "Refresh {cache} implementation",
                "Update {api} to latest version"
            ],
            'test': [
                "Add tests for {component}",
                "Test coverage for {feature}",
                "Unit tests for {module}",
                "Integration tests for {system}",
                "Fix failing tests in {component}",
                "Improve test stability",
                "Add regression tests for {bug}",
                "Update test data for {scenario}"
            ]
        }
    
    def _init_file_types(self) -> Dict[str, Dict[str, Any]]:
        """Initialize file type patterns"""
        return {
            'python': {
                'extensions': ['.py', '.pyx', '.pyi'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            },
            'javascript': {
                'extensions': ['.js', '.jsx', '.ts', '.tsx'],
                'risk_factor': 0.8,
                'complexity_base': 0.7
            },
            'java': {
                'extensions': ['.java', '.kt', '.scala'],
                'risk_factor': 0.6,
                'complexity_base': 0.5
            },
            'cpp': {
                'extensions': ['.cpp', '.cc', '.cxx', '.c', '.h', '.hpp'],
                'risk_factor': 0.9,
                'complexity_base': 0.8
            },
            'config': {
                'extensions': ['.json', '.yml', '.yaml', '.xml', '.toml', '.ini'],
                'risk_factor': 0.3,
                'complexity_base': 0.2
            },
            'documentation': {
                'extensions': ['.md', '.rst', '.txt', '.doc'],
                'risk_factor': 0.1,
                'complexity_base': 0.1
            },
            'web': {
                'extensions': ['.html', '.css', '.scss', '.less'],
                'risk_factor': 0.4,
                'complexity_base': 0.3
            },
            'database': {
                'extensions': ['.sql', '.psql', '.mysql'],
                'risk_factor': 0.7,
                'complexity_base': 0.6
            }
        }
    
    def _init_authors(self) -> List[Dict[str, Any]]:
        """Initialize author profiles"""
        return [
            {'name': 'senior_dev_1', 'experience': 0.9, 'reliability': 0.95, 'activity': 0.8},
            {'name': 'senior_dev_2', 'experience': 0.85, 'reliability': 0.9, 'activity': 0.7},
            {'name': 'mid_dev_1', 'experience': 0.6, 'reliability': 0.8, 'activity': 0.9},
            {'name': 'mid_dev_2', 'experience': 0.65, 'reliability': 0.75, 'activity': 0.85},
            {'name': 'mid_dev_3', 'experience': 0.7, 'reliability': 0.82, 'activity': 0.8},
            {'name': 'junior_dev_1', 'experience': 0.3, 'reliability': 0.6, 'activity': 0.95},
            {'name': 'junior_dev_2', 'experience': 0.25, 'reliability': 0.65, 'activity': 0.9},
            {'name': 'junior_dev_3', 'experience': 0.35, 'reliability': 0.7, 'activity': 0.88},
            {'name': 'intern_1', 'experience': 0.1, 'reliability': 0.5, 'activity': 0.7},
            {'name': 'intern_2', 'experience': 0.15, 'reliability': 0.55, 'activity': 0.75}
        ]
    
    def _init_programming_words(self) -> Dict[str, List[str]]:
        """Initialize programming-related words"""
        return {
            'components': [
                'API', 'database', 'frontend', 'backend', 'service', 'module', 'controller',
                'model', 'view', 'router', 'middleware', 'authentication', 'authorization',
                'cache', 'session', 'webhook', 'scheduler', 'queue', 'worker', 'parser',
                'validator', 'serializer', 'repository', 'factory', 'adapter', 'connector'
            ],
            'issues': [
                'memory leak', 'race condition', 'deadlock', 'null pointer', 'buffer overflow',
                'security vulnerability', 'performance issue', 'timeout', 'connection error',
                'validation error', 'parsing error', 'encoding issue', 'permission denied',
                'resource exhaustion', 'infinite loop', 'stack overflow', 'dependency conflict'
            ],
            'features': [
                'real-time notifications', 'user dashboard', 'data analytics', 'file upload',
                'search functionality', 'user authentication', 'payment processing',
                'email integration', 'social login', 'API rate limiting', 'data export',
                'mobile responsiveness', 'dark mode', 'internationalization', 'audit logs'
            ],
            'technologies': [
                'Docker', 'Kubernetes', 'Redis', 'PostgreSQL', 'MongoDB', 'Elasticsearch',
                'RabbitMQ', 'Kafka', 'GraphQL', 'REST API', 'gRPC', 'WebSocket', 'OAuth',
                'JWT', 'TLS', 'HTTPS', 'AWS', 'Azure', 'GCP', 'Terraform', 'Ansible'
            ]
        }
    
    def generate_commit_message(self, commit_type: str, risk_level: float) -> str:
        """
        Generate realistic commit message
        
        Args:
            commit_type: Type of commit (fix, feature, etc.)
            risk_level: Risk level (0-1) to influence message complexity
            
        Returns:
            Generated commit message
        """
        if commit_type not in self.commit_patterns:
            commit_type = random.choice(list(self.commit_patterns.keys()))
        
        template = random.choice(self.commit_patterns[commit_type])
        
        # Fill template with appropriate words
        component = random.choice(self.programming_words['components'])
        issue = random.choice(self.programming_words['issues'])
        feature = random.choice(self.programming_words['features'])
        technology = random.choice(self.programming_words['technologies'])
        
        # Replace placeholders
        message = template.format(
            component=component,
            issue=issue,
            problem=issue,
            feature=feature,
            functionality=feature,
            new_feature=feature,
            capability=feature,
            feature_description=feature,
            technology=technology,
            quality='performance' if risk_level > 0.5 else 'maintainability',
            module=component,
            aspect='security' if risk_level > 0.7 else 'performance',
            technical_debt='legacy code',
            legacy_code='deprecated functions',
            dependency=technology,
            version=f"{random.randint(1,5)}.{random.randint(0,20)}.{random.randint(0,10)}",
            library=technology,
            package=technology,
            environment='production' if risk_level > 0.6 else 'development',
            repository='main',
            cache='Redis',
            api='REST API',
            system=component,
            bug=issue,
            scenario='edge case',
            description=issue,
            vulnerability='SQL injection',
            critical_issue='server crash',
            algorithm='sorting'
        )
          # Add complexity based on risk level
        if risk_level > 0.8:
            suffixes = [
                " - critical security patch",
                " - urgent production fix",
                " - breaking change",
                " - requires migration",
                " - affects multiple services"
            ]
            message += random.choice(suffixes)
        elif risk_level > 0.6:
            suffixes = [
                " - needs testing",
                " - requires review",
                " - performance impact",
                " - config change needed"
            ]
            message += random.choice(suffixes)
        
        return message
    
    def generate_file_changes(self, risk_level: float, complexity_level: float) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        Generate realistic file changes
        
        Args:
            risk_level: Risk level (0-1)
            complexity_level: Complexity level (0-1)
            
        Returns:
            Tuple of (file_data_list, change_stats)
        """
        # Determine number of files based on complexity
        if complexity_level > 0.8:
            num_files = random.randint(8, 25)
        elif complexity_level > 0.6:
            num_files = random.randint(4, 12)
        elif complexity_level > 0.3:
            num_files = random.randint(2, 6)
        else:
            num_files = random.randint(1, 3)
        
        files_data = []
        stats = {'additions': 0, 'deletions': 0, 'modifications': 0}
        
        # Select file types based on risk level
        type_weights = []
        for file_type, info in self.file_types.items():
            weight = info['risk_factor'] if risk_level > 0.5 else (1 - info['risk_factor'])
            type_weights.append((file_type, weight, info))
        
        for i in range(num_files):
            # Select file type
            selected_type = random.choices(
                [t[0] for t in type_weights],
                weights=[t[1] for t in type_weights]
            )[0]
            
            type_info = self.file_types[selected_type]
            extension = random.choice(type_info['extensions'])
            
            # Generate file path
            components = random.choice(self.programming_words['components']).lower()
            file_name = f"{components}_{random.randint(1, 100)}{extension}"
            
            # Create realistic directory structure
            if selected_type == 'python':
                dirs = ['src', 'lib', 'api', 'models', 'views', 'utils']
            elif selected_type == 'javascript':
                dirs = ['src', 'components', 'pages', 'utils', 'services']
            elif selected_type == 'java':
                dirs = ['src/main/java', 'src/test/java']
            elif selected_type == 'config':
                dirs = ['config', 'deploy', 'scripts']
            elif selected_type == 'documentation':
                dirs = ['docs', 'README']
            else:
                dirs = ['src', 'lib', 'assets']
            
            directory = random.choice(dirs)
            file_path = f"{directory}/{file_name}"
            
            # Generate change statistics for this file
            base_changes = int(complexity_level * 200)
            additions = random.randint(1, max(1, base_changes))
            deletions = random.randint(0, max(1, int(additions * 0.7)))
            changes = additions + deletions
            
            # Create file data dict as expected by MetadataProcessor
            file_data = {
                'filename': file_path,
                'additions': additions,
                'deletions': deletions,
                'changes': changes,
                'status': random.choice(['modified', 'added', 'removed']),
                'patch': f"@@ -1,{deletions} +1,{additions} @@"  # Simple patch format
            }
            
            files_data.append(file_data)
            
            stats['additions'] += additions
            stats['deletions'] += deletions
            stats['modifications'] += 1
        
        return files_data, stats
    
    def generate_temporal_features(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate temporal features with realistic patterns
        
        Args:
            risk_level: Risk level affects timing patterns
            
        Returns:
            Dict with temporal features
        """
        # Base time
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        # Generate random timestamp
        time_diff = end_date - start_date
        random_seconds = random.randint(0, int(time_diff.total_seconds()))
        commit_time = start_date + timedelta(seconds=random_seconds)
        
        # Risky commits more likely during off-hours
        if risk_level > 0.7:
            # Late night commits (22:00 - 06:00)
            if random.random() > 0.3:
                hour = random.choice(list(range(22, 24)) + list(range(0, 7)))
                commit_time = commit_time.replace(hour=hour)
        
        weekday = commit_time.weekday()  # 0=Monday, 6=Sunday
        hour = commit_time.hour
        
        # Season encoding
        month = commit_time.month
        if month in [12, 1, 2]:
            season = 0  # Winter
        elif month in [3, 4, 5]:
            season = 1  # Spring
        elif month in [6, 7, 8]:
            season = 2  # Summer
        else:
            season = 3  # Fall
        
        return {
            'timestamp': commit_time,
            'weekday': weekday,
            'hour': hour,
            'season': season,
            'is_weekend': weekday >= 5,
            'is_business_hours': 9 <= hour <= 17,
            'unix_timestamp': int(commit_time.timestamp())
        }
    
    def generate_author_info(self, risk_level: float) -> Dict[str, Any]:
        """
        Generate author information
        
        Args:
            risk_level: Affects author selection
            
        Returns:
            Author information dict
        """
        # Select author based on risk level
        if risk_level > 0.8:
            # High risk - more likely to be junior/intern
            author_pool = [a for a in self.authors if a['experience'] < 0.5]
        elif risk_level > 0.5:
            # Medium risk - mixed experience
            author_pool = [a for a in self.authors if 0.3 <= a['experience'] <= 0.8]
        else:
            # Low risk - more likely to be senior
            author_pool = [a for a in self.authors if a['experience'] > 0.6]
        
        if not author_pool:
            author_pool = self.authors
        
        author = random.choice(author_pool)
        
        return {
            'name': author['name'],
            'experience_level': author['experience'],
            'reliability_score': author['reliability'],
            'activity_score': author['activity'],
            'commits_last_month': int(author['activity'] * 50),
            'avg_commit_size': int((1 - author['experience']) * 100 + 20)
        }
    
    def calculate_labels(self, commit_data: Dict[str, Any]) -> Dict[str, int]:
        """
        Calculate ground truth labels based on generated features
        
        Args:
            commit_data: Generated commit data
            
        Returns:
            Dict with task labels
        """
        # Extract features for label calculation
        risk_factors = []
        complexity_factors = []
        
        # File-based factors
        file_types = commit_data['metadata']['file_types']
        for file_type, info in self.file_types.items():
            if file_type in file_types and file_types[file_type] > 0:
                risk_factors.append(info['risk_factor'] * file_types[file_type])
                complexity_factors.append(info['complexity_base'] * file_types[file_type])
        
        # Author factors
        author_info = commit_data['metadata']['author_info']
        risk_factors.append(1 - author_info['reliability_score'])
        complexity_factors.append(1 - author_info['experience_level'])
        
        # Temporal factors
        temporal = commit_data['metadata']['temporal']
        if not temporal['is_business_hours']:
            risk_factors.append(0.3)
        if temporal['is_weekend']:
            risk_factors.append(0.2)
        
        # Change size factors
        stats = commit_data['metadata']['change_stats']
        total_changes = stats['additions'] + stats['deletions']
        if total_changes > 500:
            risk_factors.append(0.4)
            complexity_factors.append(0.5)
        elif total_changes > 200:
            risk_factors.append(0.2)
            complexity_factors.append(0.3)
        
        # File count factor
        num_files = len(commit_data['metadata']['files'])
        if num_files > 15:
            risk_factors.append(0.4)
            complexity_factors.append(0.4)
        elif num_files > 8:
            risk_factors.append(0.2)
            complexity_factors.append(0.2)
        
        # Calculate final scores
        avg_risk = np.mean(risk_factors) if risk_factors else 0.5
        avg_complexity = np.mean(complexity_factors) if complexity_factors else 0.5
        
        # Generate labels
        labels = {}
        
        # Commit Risk (Binary: 0=Low, 1=High)
        labels['commit_risk'] = 1 if avg_risk > 0.6 else 0
        
        # Complexity (3 classes: 0=Low, 1=Medium, 2=High)
        if avg_complexity > 0.7:
            labels['complexity'] = 2
        elif avg_complexity > 0.4:
            labels['complexity'] = 1
        else:
            labels['complexity'] = 0
        
        # Hotspot Files (Binary: 0=No, 1=Yes)
        # Based on high-risk file types and frequency
        hotspot_score = 0
        for file_type, count in file_types.items():
            if self.file_types[file_type]['risk_factor'] > 0.7:
                hotspot_score += count
        labels['hotspot'] = 1 if hotspot_score > 3 else 0
        
        # Urgent Review (Binary: 0=No, 1=Yes)
        # High risk + (low author reliability OR critical files OR large changes)
        urgent_factors = []
        if avg_risk > 0.7:
            urgent_factors.append(1)
        if author_info['reliability_score'] < 0.7:
            urgent_factors.append(1)
        if total_changes > 300:
            urgent_factors.append(1)
        if any(self.file_types[ft]['risk_factor'] > 0.8 for ft in file_types if file_types[ft] > 0):
            urgent_factors.append(1)
        
        labels['urgent_review'] = 1 if len(urgent_factors) >= 2 else 0
        
        return labels
    
    def generate_single_commit(self, target_risk: Optional[float] = None) -> Dict[str, Any]:
        """
        Generate a single commit with all features and labels
        
        Args:
            target_risk: Target risk level (0-1), if None then random
            
        Returns:
            Complete commit data dict
        """
        # Determine risk level
        if target_risk is None:
            risk_level = random.random()
        else:
            risk_level = max(0.0, min(1.0, target_risk + random.gauss(0, 0.1)))
        
        # Generate complexity level (correlated with risk)
        complexity_level = risk_level + random.gauss(0, 0.2)
        complexity_level = max(0.0, min(1.0, complexity_level))
        
        # Select commit type based on risk
        if risk_level > 0.8:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.5, 0.2, 0.1, 0.1, 0.1]
            )[0]
        elif risk_level > 0.5:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.3, 0.3, 0.2, 0.1, 0.1]
            )[0]
        else:
            commit_type = random.choices(
                ['fix', 'feature', 'refactor', 'update', 'test'],
                weights=[0.1, 0.3, 0.3, 0.2, 0.1]
            )[0]
          # Generate components
        commit_message = self.generate_commit_message(commit_type, risk_level)
        files_data, change_stats = self.generate_file_changes(risk_level, complexity_level)
        temporal_features = self.generate_temporal_features(risk_level)
        author_info = self.generate_author_info(risk_level)
        
        # Count file types
        file_types = defaultdict(int)
        for file_data in files_data:
            filename = file_data['filename']
            for file_type, info in self.file_types.items():
                for ext in info['extensions']:
                    if filename.endswith(ext):
                        file_types[file_type] += 1
                        break
        
        # Create commit data structure
        commit_data = {
            'commit_message': commit_message,
            'metadata': {
                'files': files_data,  # Now this is list of dicts as expected by MetadataProcessor
                'change_stats': change_stats,
                'file_types': dict(file_types),
                'temporal': temporal_features,
                'author_info': author_info,
                'commit_type': commit_type,
                'risk_level': risk_level,
                'complexity_level': complexity_level
            }
        }
        
        # Calculate labels
        labels = self.calculate_labels(commit_data)
        commit_data['labels'] = labels
        
        return commit_data
    
    def generate_dataset(self, num_samples: int, 
                        risk_distribution: Optional[Dict[str, float]] = None,
                        save_path: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Generate complete dataset
        
        Args:
            num_samples: Number of samples to generate
            risk_distribution: Dict with risk level distribution
            save_path: Path to save dataset
            
        Returns:
            List of commit data dicts
        """
        if risk_distribution is None:
            risk_distribution = {
                'low': 0.5,      # 0.0 - 0.3
                'medium': 0.3,   # 0.3 - 0.7
                'high': 0.2      # 0.7 - 1.0
            }
        
        dataset = []
        
        for i in range(num_samples):
            # Select risk level based on distribution
            rand_val = random.random()
            if rand_val < risk_distribution['low']:
                target_risk = random.uniform(0.0, 0.3)
            elif rand_val < risk_distribution['low'] + risk_distribution['medium']:
                target_risk = random.uniform(0.3, 0.7)
            else:
                target_risk = random.uniform(0.7, 1.0)
            
            commit_data = self.generate_single_commit(target_risk)
            commit_data['id'] = i
            dataset.append(commit_data)
            
            if (i + 1) % 1000 == 0:
                print(f"Generated {i + 1}/{num_samples} samples")
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, indent=2, default=str, ensure_ascii=False)
            print(f"Dataset saved to {save_path}")
        
        return dataset
    
    def generate_splits(self, dataset: List[Dict[str, Any]], 
                       split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15),
                       stratify_by: str = 'commit_risk') -> Tuple[List, List, List]:
        """
        Split dataset into train/val/test with stratification
        
        Args:
            dataset: Complete dataset
            split_ratios: (train, val, test) ratios
            stratify_by: Label to stratify by
            
        Returns:
            Tuple of (train, val, test) datasets
        """
        from sklearn.model_selection import train_test_split
        
        # Extract labels for stratification
        if stratify_by in dataset[0]['labels']:
            stratify_labels = [item['labels'][stratify_by] for item in dataset]
        else:
            stratify_labels = None
        
        # First split: train vs (val + test)
        train_data, temp_data = train_test_split(
            dataset, 
            test_size=(1 - split_ratios[0]),
            stratify=stratify_labels,
            random_state=42
        )
        
        # Second split: val vs test
        val_ratio = split_ratios[1] / (split_ratios[1] + split_ratios[2])
        if stratify_labels:
            temp_labels = [item['labels'][stratify_by] for item in temp_data]
        else:
            temp_labels = None
        
        val_data, test_data = train_test_split(
            temp_data,
            test_size=(1 - val_ratio),
            stratify=temp_labels,
            random_state=42
        )
        
        print(f"Dataset splits: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")
        
        return train_data, val_data, test_data
    
    def analyze_dataset(self, dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze generated dataset statistics
        
        Args:
            dataset: Generated dataset
            
        Returns:
            Analysis results
        """
        analysis = {
            'total_samples': len(dataset),
            'label_distributions': {},
            'feature_statistics': {},
            'correlations': {}
        }
        
        # Label distributions
        for sample in dataset:
            for task_name, label in sample['labels'].items():
                if task_name not in analysis['label_distributions']:
                    analysis['label_distributions'][task_name] = defaultdict(int)
                analysis['label_distributions'][task_name][label] += 1
        
        # Feature statistics
        risk_levels = [sample['metadata']['risk_level'] for sample in dataset]
        complexity_levels = [sample['metadata']['complexity_level'] for sample in dataset]
        file_counts = [len(sample['metadata']['files']) for sample in dataset]
        change_sizes = [sample['metadata']['change_stats']['additions'] + 
                       sample['metadata']['change_stats']['deletions'] for sample in dataset]
        
        analysis['feature_statistics'] = {
            'risk_level': {
                'mean': np.mean(risk_levels),
                'std': np.std(risk_levels),
                'min': np.min(risk_levels),
                'max': np.max(risk_levels)
            },
            'complexity_level': {
                'mean': np.mean(complexity_levels),
                'std': np.std(complexity_levels),
                'min': np.min(complexity_levels),
                'max': np.max(complexity_levels)
            },
            'file_count': {
                'mean': np.mean(file_counts),
                'std': np.std(file_counts),
                'min': np.min(file_counts),
                'max': np.max(file_counts)
            },
            'change_size': {
                'mean': np.mean(change_sizes),
                'std': np.std(change_sizes),
                'min': np.min(change_sizes),
                'max': np.max(change_sizes)
            }
        }
        
        return analysis


def main():
    """Example usage"""
    generator = GitHubDataGenerator(seed=42)
    
    # Generate small dataset for testing
    print("Generating sample dataset...")
    dataset = generator.generate_dataset(
        num_samples=1000,
        risk_distribution={'low': 0.6, 'medium': 0.3, 'high': 0.1}
    )
    
    # Analyze dataset
    analysis = generator.analyze_dataset(dataset)
    print("\nDataset Analysis:")
    print(f"Total samples: {analysis['total_samples']}")
    print("\nLabel distributions:")
    for task, dist in analysis['label_distributions'].items():
        print(f"  {task}: {dict(dist)}")
    
    print("\nFeature statistics:")
    for feature, stats in analysis['feature_statistics'].items():
        print(f"  {feature}: mean={stats['mean']:.3f}, std={stats['std']:.3f}")
    
    # Show sample
    print(f"\nSample commit:")
    sample = dataset[0]
    print(f"Message: {sample['commit_message']}")
    print(f"Files: {len(sample['metadata']['files'])} files")
    print(f"Changes: +{sample['metadata']['change_stats']['additions']} -{sample['metadata']['change_stats']['deletions']}")
    print(f"Labels: {sample['labels']}")


if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\data_preprocessing\enhanced_text_processor.py
```py
"""
Enhanced Text Processor with NLTK Support
Provides advanced natural language processing capabilities for commit message analysis
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter, defaultdict
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available. Using simple tokenization only.")

# Enhanced NLTK import with more features
try:
    import nltk
    from nltk.corpus import stopwords, wordnet
    from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tag import pos_tag
    from nltk.chunk import ne_chunk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from nltk.corpus import opinion_lexicon
    from nltk.probability import FreqDist
    from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder
    from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures
    NLTK_AVAILABLE = True
    logger.info("NLTK library available with advanced features")
except ImportError:
    NLTK_AVAILABLE = False
    logger.warning("NLTK not available. Using simple text processing.")

# Optional TextBlob for additional sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available.")


class EnhancedTextProcessor:
    """
    Enhanced Text Processor with comprehensive NLTK support
    Provides advanced NLP features for commit message analysis
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_stemming: bool = True,
                 enable_lemmatization: bool = True,
                 enable_pos_tagging: bool = True,
                 enable_sentiment_analysis: bool = True,
                 enable_ngrams: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name for transformers
            enable_stemming: Enable word stemming
            enable_lemmatization: Enable word lemmatization
            enable_pos_tagging: Enable part-of-speech tagging
            enable_sentiment_analysis: Enable sentiment analysis
            enable_ngrams: Enable n-gram extraction
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        
        # NLTK feature flags
        self.enable_stemming = enable_stemming
        self.enable_lemmatization = enable_lemmatization
        self.enable_pos_tagging = enable_pos_tagging
        self.enable_sentiment_analysis = enable_sentiment_analysis
        self.enable_ngrams = enable_ngrams
        
        # Initialize NLTK components
        self._init_nltk_components()
        
        # Initialize model components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_nltk_components(self):
        """Initialize NLTK components and download required data"""
        if not NLTK_AVAILABLE:
            logger.warning("NLTK not available. Advanced text processing features disabled.")
            self.stop_words = self._get_basic_stopwords()
            return
        
        # Download required NLTK data
        required_data = [
            'punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger',
            'maxent_ne_chunker', 'words', 'vader_lexicon', 'opinion_lexicon',
            'omw-1.4'
        ]
        
        for data in required_data:
            try:
                nltk.data.find(f'tokenizers/{data}')
            except LookupError:
                try:
                    nltk.data.find(f'corpora/{data}')
                except LookupError:
                    try:
                        nltk.data.find(f'taggers/{data}')
                    except LookupError:
                        try:
                            nltk.data.find(f'chunkers/{data}')
                        except LookupError:
                            try:
                                nltk.download(data, quiet=True)
                                logger.info(f"Downloaded NLTK data: {data}")
                            except Exception as e:
                                logger.warning(f"Failed to download {data}: {e}")
        
        # Initialize NLTK tools
        try:
            self.stop_words = set(stopwords.words('english'))
            self.stemmer = PorterStemmer() if self.enable_stemming else None
            self.lemmatizer = WordNetLemmatizer() if self.enable_lemmatization else None
            self.sentiment_analyzer = SentimentIntensityAnalyzer() if self.enable_sentiment_analysis else None
            self.tokenizer_nltk = TreebankWordTokenizer()
            
            # Load opinion lexicon for additional sentiment analysis
            try:
                self.positive_words = set(opinion_lexicon.positive())
                self.negative_words = set(opinion_lexicon.negative())
            except Exception as e:
                logger.warning(f"Failed to load opinion lexicon: {e}")
                self.positive_words = set()
                self.negative_words = set()
                
            logger.info("NLTK components initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize NLTK components: {e}")
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Get basic stopwords if NLTK is not available"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',
            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves'
        ])
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
        
        # Additional LSTM-specific features
        self.bigram_counts = Counter()
        self.trigram_counts = Counter()
        self.pos_tag_counts = Counter()
        
    def advanced_tokenize(self, text: str) -> List[str]:
        """
        Advanced tokenization with NLTK features
        """
        if not NLTK_AVAILABLE:
            # Fallback to simple tokenization
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
        
        try:
            # Use TreebankWordTokenizer for better handling of contractions and punctuation
            tokens = self.tokenizer_nltk.tokenize(text)
            
            # Apply stemming or lemmatization
            if self.enable_lemmatization and self.lemmatizer:
                # Get POS tags for better lemmatization
                pos_tags = pos_tag(tokens) if self.enable_pos_tagging else [(token, 'NN') for token in tokens]
                tokens = [self._lemmatize_with_pos(token, pos) for token, pos in pos_tags]
            elif self.enable_stemming and self.stemmer:
                tokens = [self.stemmer.stem(token) for token in tokens]
            
            return tokens
            
        except Exception as e:
            logger.warning(f"Advanced tokenization failed: {e}. Using simple fallback.")
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def _lemmatize_with_pos(self, word: str, pos_tag: str) -> str:
        """
        Lemmatize word with POS tag context
        """
        if not self.lemmatizer:
            return word
            
        # Convert POS tag to WordNet format
        tag_dict = {
            'J': wordnet.ADJ,
            'N': wordnet.NOUN,
            'V': wordnet.VERB,
            'R': wordnet.ADV
        }
        
        wordnet_pos = tag_dict.get(pos_tag[0], wordnet.NOUN)
        return self.lemmatizer.lemmatize(word, wordnet_pos)
    
    def extract_advanced_features(self, text: str) -> Dict[str, any]:
        """
        Extract advanced features using NLTK capabilities
        """
        features = {}
        
        # Basic features
        features.update(self.extract_basic_features(text))
        
        if not NLTK_AVAILABLE:
            return features
        
        try:
            # Tokenize for advanced analysis
            tokens = self.advanced_tokenize(text.lower())
            
            # Sentiment analysis
            if self.enable_sentiment_analysis and self.sentiment_analyzer:
                sentiment_scores = self.sentiment_analyzer.polarity_scores(text)
                features.update({
                    'sentiment_positive': sentiment_scores['pos'],
                    'sentiment_negative': sentiment_scores['neg'],
                    'sentiment_neutral': sentiment_scores['neu'],
                    'sentiment_compound': sentiment_scores['compound']
                })
                
                # TextBlob sentiment as additional feature
                if TEXTBLOB_AVAILABLE:
                    blob = TextBlob(text)
                    features['textblob_polarity'] = blob.sentiment.polarity
                    features['textblob_subjectivity'] = blob.sentiment.subjectivity
                
                # Opinion lexicon features
                positive_count = sum(1 for word in tokens if word in self.positive_words)
                negative_count = sum(1 for word in tokens if word in self.negative_words)
                features['positive_word_count'] = positive_count
                features['negative_word_count'] = negative_count
                features['sentiment_ratio'] = (positive_count - negative_count) / max(len(tokens), 1)
            
            # POS tagging features
            if self.enable_pos_tagging:
                pos_tags = pos_tag(tokens)
                pos_counts = Counter(tag for _, tag in pos_tags)
                
                # Important POS categories for commit analysis
                features['noun_count'] = pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + pos_counts.get('NNP', 0)
                features['verb_count'] = pos_counts.get('VB', 0) + pos_counts.get('VBD', 0) + pos_counts.get('VBG', 0) + pos_counts.get('VBN', 0)
                features['adjective_count'] = pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + pos_counts.get('JJS', 0)
                features['adverb_count'] = pos_counts.get('RB', 0) + pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0)
                
                # POS diversity
                features['pos_diversity'] = len(pos_counts) / max(len(tokens), 1)
            
            # N-gram features
            if self.enable_ngrams and len(tokens) > 1:
                # Bigrams
                bigrams = list(nltk.bigrams(tokens))
                features['unique_bigrams'] = len(set(bigrams))
                features['bigram_ratio'] = len(set(bigrams)) / max(len(bigrams), 1)
                
                # Trigrams (if enough tokens)
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    features['unique_trigrams'] = len(set(trigrams))
                    features['trigram_ratio'] = len(set(trigrams)) / max(len(trigrams), 1)
                else:
                    features['unique_trigrams'] = 0
                    features['trigram_ratio'] = 0
            
            # Lexical diversity
            features['lexical_diversity'] = len(set(tokens)) / max(len(tokens), 1)
            
            # Average word length
            features['avg_word_length'] = np.mean([len(word) for word in tokens]) if tokens else 0
            
        except Exception as e:
            logger.warning(f"Advanced feature extraction failed: {e}")
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """
        Extract basic features (fallback when NLTK not available)
        """
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'has_commit_type', 'commit_type_prefix', 'has_bug_keywords',
                'has_feature_keywords', 'has_doc_keywords', 'positive_sentiment',
                'negative_sentiment', 'urgent_sentiment'
            ]}
        
        # Basic text statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') or text.lower().startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':') or text.lower().startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Enhanced keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve', 'correct', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce', 'enable']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'docs', 'guide', 'manual']
        refactor_keywords = ['refactor', 'restructure', 'reorganize', 'cleanup', 'optimize', 'improve']
        
        text_lower = text.lower()
        features['has_bug_keywords'] = any(keyword in text_lower for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text_lower for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text_lower for keyword in doc_keywords)
        features['has_refactor_keywords'] = any(keyword in text_lower for keyword in refactor_keywords)
        
        # Sentiment indicators (basic)
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success', 'complete', 'finish']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error', 'disable', 'revert']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediate', 'quick']
        
        features['positive_sentiment'] = any(word in text_lower for word in positive_words)
        features['negative_sentiment'] = any(word in text_lower for word in negative_words)
        features['urgent_sentiment'] = any(word in text_lower for word in urgent_words)
        
        return features
    
    def clean_commit_message(self, text: str) -> str:
        """
        Enhanced commit message cleaning
        """
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'merge .* of .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        Build vocabulary with advanced NLTK features
        """
        if self.method != "lstm":
            return
        
        logger.info("üî§ Building enhanced vocabulary with NLTK features...")
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            
            # Filter tokens
            tokens = [token for token in tokens 
                     if token not in self.stop_words 
                     and len(token) > 1 
                     and token.isalpha()]
            
            all_tokens.extend(tokens)
            self.word_counts.update(tokens)
            
            # Collect n-grams if enabled
            if self.enable_ngrams and len(tokens) > 1:
                bigrams = list(nltk.bigrams(tokens))
                self.bigram_counts.update(bigrams)
                
                if len(tokens) > 2:
                    trigrams = list(nltk.trigrams(tokens))
                    self.trigram_counts.update(trigrams)
        
        # Build vocabulary with most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"‚úÖ Enhanced vocabulary built with {len(self.word_to_idx)} words")
        
        # Log vocabulary statistics
        if NLTK_AVAILABLE:
            logger.info(f"üìä Total unique bigrams: {len(self.bigram_counts)}")
            logger.info(f"üìä Total unique trigrams: {len(self.trigram_counts)}")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Enhanced LSTM encoding with NLTK preprocessing
        """
        cleaned_text = self.clean_commit_message(text)
        tokens = self.advanced_tokenize(cleaned_text.lower())
        
        # Filter tokens
        tokens = [token for token in tokens 
                 if token not in self.stop_words 
                 and len(token) > 1 
                 and token.isalpha()]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def get_collocations(self, texts: List[str], n: int = 10) -> Dict[str, List[Tuple]]:
        """
        Extract meaningful collocations from commit messages
        """
        if not NLTK_AVAILABLE or not self.enable_ngrams:
            return {'bigrams': [], 'trigrams': []}
        
        all_tokens = []
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self.advanced_tokenize(cleaned_text.lower())
            tokens = [token for token in tokens 
                     if token not in self.stop_words and len(token) > 1]
            all_tokens.extend(tokens)
        
        try:
            # Bigram collocations
            bigram_finder = BigramCollocationFinder.from_words(all_tokens)
            bigram_finder.apply_freq_filter(3)  # Only bigrams appearing 3+ times
            bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, n)
            
            # Trigram collocations
            trigram_finder = TrigramCollocationFinder.from_words(all_tokens)
            trigram_finder.apply_freq_filter(2)  # Only trigrams appearing 2+ times
            trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, n)
            
            return {'bigrams': bigrams, 'trigrams': trigrams}
            
        except Exception as e:
            logger.warning(f"Collocation extraction failed: {e}")
            return {'bigrams': [], 'trigrams': []}
    
    def fit(self, texts: List[str]) -> 'EnhancedTextProcessor':
        """
        Fit the enhanced text processor to training data
        """
        logger.info("üöÄ Fitting enhanced text processor with NLTK features...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
            
            # Extract and log collocations for insights
            collocations = self.get_collocations(texts)
            if collocations['bigrams']:
                logger.info(f"üìà Top bigrams: {collocations['bigrams'][:5]}")
            if collocations['trigrams']:
                logger.info(f"üìà Top trigrams: {collocations['trigrams'][:3]}")
        
        logger.info("‚úÖ Enhanced text processor fitted successfully")
        return self
    
    # Keep all other methods from the original TextProcessor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method (unchanged)"""
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """Get text embeddings (unchanged)"""
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    embedding = outputs.last_hidden_state[:, 0, :]
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts with enhanced features"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        # Extract enhanced features
        for text in texts:
            basic_features = self.extract_basic_features(text)
            if NLTK_AVAILABLE:
                enhanced_features = self.extract_advanced_features(text)
                results['enhanced_features'].append(enhanced_features)
            else:
                results['enhanced_features'].append(basic_features)
            
            results['text_features'].append(basic_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\metadata_processor.py
```py
"""
Metadata Processor for Multi-Modal Fusion Network
X·ª≠ l√Ω v√† chu·∫©n b·ªã metadata t·ª´ GitHub commits
"""

import numpy as np
import pandas as pd
import torch
from typing import List, Dict, Tuple, Optional, Any
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime, timezone
import re
from pathlib import Path

class MetadataProcessor:
    """
    L·ªõp x·ª≠ l√Ω metadata cho GitHub commits
    Bao g·ªìm commit stats, file info, author info, timestamp info
    """
    
    def __init__(self, normalize_features: bool = True, 
                 categorical_method: str = "embedding",  # "embedding", "onehot"
                 max_files: int = 50,
                 max_authors: int = 1000):
        """
        Args:
            normalize_features: C√≥ chu·∫©n h√≥a features s·ªë kh√¥ng
            categorical_method: Ph∆∞∆°ng ph√°p encode categorical ("embedding", "onehot")
            max_files: S·ªë file t·ªëi ƒëa ƒë·ªÉ track
            max_authors: S·ªë author t·ªëi ƒëa ƒë·ªÉ track
        """
        self.normalize_features = normalize_features
        self.categorical_method = categorical_method
        self.max_files = max_files
        self.max_authors = max_authors
        
        # Scalers for numerical features
        self.numerical_scaler = StandardScaler()
        self.ratio_scaler = MinMaxScaler()
        
        # Encoders for categorical features
        self.file_type_encoder = LabelEncoder()
        self.author_encoder = LabelEncoder()
        self.file_path_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
        
        # Feature statistics
        self.feature_stats = {}
        self.is_fitted = False
        
    def extract_file_features(self, files_data: List[Dict]) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ th√¥ng tin files
        
        Args:
            files_data: List of file info dicts v·ªõi keys: filename, status, additions, deletions, changes
        """
        if not files_data:
            return self._get_empty_file_features()
        
        features = {}
        
        # Basic file stats
        features['num_files'] = len(files_data)
        features['total_additions'] = sum(f.get('additions', 0) for f in files_data)
        features['total_deletions'] = sum(f.get('deletions', 0) for f in files_data)
        features['total_changes'] = sum(f.get('changes', 0) for f in files_data)
        
        # File types
        file_extensions = []
        file_paths = []
        for file_info in files_data:
            filename = file_info.get('filename', '')
            if filename:
                file_paths.append(filename)
                # Extract extension
                if '.' in filename:
                    ext = filename.split('.')[-1].lower()
                    file_extensions.append(ext)
        
        # File type diversity
        unique_extensions = list(set(file_extensions))
        features['num_file_types'] = len(unique_extensions)
        features['file_types'] = unique_extensions[:10]  # Top 10 types
        
        # File depth analysis
        depths = []
        for path in file_paths:
            depth = len(Path(path).parts) - 1  # Subtract 1 for filename
            depths.append(depth)
        
        if depths:
            features['avg_file_depth'] = np.mean(depths)
            features['max_file_depth'] = np.max(depths)
            features['min_file_depth'] = np.min(depths)
        else:
            features['avg_file_depth'] = 0
            features['max_file_depth'] = 0
            features['min_file_depth'] = 0
        
        # Change distribution
        if features['total_changes'] > 0:
            features['additions_ratio'] = features['total_additions'] / features['total_changes']
            features['deletions_ratio'] = features['total_deletions'] / features['total_changes']
        else:
            features['additions_ratio'] = 0
            features['deletions_ratio'] = 0
        
        # File status analysis
        status_counts = {}
        for file_info in files_data:
            status = file_info.get('status', 'modified')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        features['added_files'] = status_counts.get('added', 0)
        features['modified_files'] = status_counts.get('modified', 0)
        features['deleted_files'] = status_counts.get('removed', 0)
        features['renamed_files'] = status_counts.get('renamed', 0)
        
        # Large file changes indicator
        large_changes = sum(1 for f in files_data if f.get('changes', 0) > 100)
        features['large_change_files'] = large_changes
        features['has_large_changes'] = large_changes > 0
        
        return features
    
    def _get_empty_file_features(self) -> Dict[str, Any]:
        """Tr·∫£ v·ªÅ features m·∫∑c ƒë·ªãnh khi kh√¥ng c√≥ file data"""
        return {
            'num_files': 0,
            'total_additions': 0,
            'total_deletions': 0,
            'total_changes': 0,
            'num_file_types': 0,
            'file_types': [],
            'avg_file_depth': 0,
            'max_file_depth': 0,
            'min_file_depth': 0,
            'additions_ratio': 0,
            'deletions_ratio': 0,            'added_files': 0,
            'modified_files': 0,
            'deleted_files': 0,
            'renamed_files': 0,
            'large_change_files': 0,
            'has_large_changes': False
        }
    
    def extract_author_features(self, author_info, commit_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ th√¥ng tin author
        
        Args:
            author_info: Dict v·ªõi keys: login, name, email, etc. HO·∫∂C string author name
            commit_history: L·ªãch s·ª≠ commit g·∫ßn ƒë√¢y c·ªßa author (optional)
        """
        features = {}
        
        # Handle both dict and string input
        if isinstance(author_info, str):
            # Simple string author name
            features['author_login'] = author_info
            features['author_name'] = author_info
            features['author_email'] = ''
        else:
            # Dict author info
            features['author_login'] = author_info.get('login', 'unknown')
            features['author_name'] = author_info.get('name', '')
            features['author_email'] = author_info.get('email', '')
        
        # Author activity pattern (n·∫øu c√≥ l·ªãch s·ª≠)
        if commit_history:
            features['recent_commits_count'] = len(commit_history)
            
            # T√≠nh average commit size
            recent_changes = [c.get('stats', {}).get('total', 0) for c in commit_history]
            features['avg_recent_commit_size'] = np.mean(recent_changes) if recent_changes else 0
            
            # Frequency pattern
            if len(commit_history) >= 2:
                timestamps = [c.get('timestamp') for c in commit_history if c.get('timestamp')]
                if len(timestamps) >= 2:
                    # Calculate time between commits
                    time_diffs = []
                    for i in range(1, len(timestamps)):
                        try:
                            t1 = datetime.fromisoformat(timestamps[i-1].replace('Z', '+00:00'))
                            t2 = datetime.fromisoformat(timestamps[i].replace('Z', '+00:00'))
                            diff_hours = abs((t2 - t1).total_seconds() / 3600)
                            time_diffs.append(diff_hours)
                        except:
                            continue
                    
                    if time_diffs:
                        features['avg_commit_interval_hours'] = np.mean(time_diffs)
                        features['commit_frequency_score'] = min(24 / np.mean(time_diffs), 10) if np.mean(time_diffs) > 0 else 0
                    else:
                        features['avg_commit_interval_hours'] = 24
                        features['commit_frequency_score'] = 1
                else:
                    features['avg_commit_interval_hours'] = 24
                    features['commit_frequency_score'] = 1
            else:
                features['avg_commit_interval_hours'] = 24
                features['commit_frequency_score'] = 1
        else:
            features['recent_commits_count'] = 0
            features['avg_recent_commit_size'] = 0
            features['avg_commit_interval_hours'] = 24
            features['commit_frequency_score'] = 1
        
        return features
    
    def extract_timestamp_features(self, timestamp: str) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t features t·ª´ timestamp
        """
        features = {}
        
        try:
            # Parse timestamp
            if timestamp.endswith('Z'):
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = datetime.fromisoformat(timestamp)
            
            # Time-based features
            features['hour_of_day'] = dt.hour
            features['day_of_week'] = dt.weekday()  # 0 = Monday
            features['day_of_month'] = dt.day
            features['month'] = dt.month
            features['year'] = dt.year
            
            # Derived features
            features['is_weekend'] = dt.weekday() >= 5
            features['is_business_hours'] = 9 <= dt.hour <= 17
            features['is_late_night'] = dt.hour >= 22 or dt.hour <= 6
            
            # Season (approximate)
            if dt.month in [12, 1, 2]:
                features['season'] = 'winter'
            elif dt.month in [3, 4, 5]:
                features['season'] = 'spring'
            elif dt.month in [6, 7, 8]:
                features['season'] = 'summer'
            else:
                features['season'] = 'fall'
                
        except Exception as e:
            # Default values if parsing fails
            features.update({
                'hour_of_day': 12,
                'day_of_week': 0,
                'day_of_month': 1,
                'month': 1,
                'year': 2024,
                'is_weekend': False,
                'is_business_hours': True,
                'is_late_night': False,
                'season': 'spring'
            })
        
        return features
    
    def create_feature_engineering(self, file_features: Dict, author_features: Dict, timestamp_features: Dict) -> Dict[str, Any]:
        """
        T·∫°o c√°c feature engineering ph·ª©c t·∫°p h∆°n
        """
        engineered = {}
        
        # Commit complexity score
        complexity_score = 0
        complexity_score += min(file_features['num_files'] / 10, 1.0) * 0.3  # File count impact
        complexity_score += min(file_features['total_changes'] / 1000, 1.0) * 0.4  # Change size impact
        complexity_score += min(file_features['num_file_types'] / 5, 1.0) * 0.2  # Diversity impact
        complexity_score += min(file_features['max_file_depth'] / 10, 1.0) * 0.1  # Depth impact
        engineered['complexity_score'] = complexity_score
        
        # Risk assessment
        risk_score = 0
        risk_score += file_features['has_large_changes'] * 0.3
        risk_score += (file_features['deleted_files'] / max(file_features['num_files'], 1)) * 0.2
        risk_score += min(author_features['commit_frequency_score'] / 5, 1.0) * 0.2  # High frequency = higher risk
        risk_score += timestamp_features['is_late_night'] * 0.1
        risk_score += (timestamp_features['is_weekend'] and not timestamp_features['is_business_hours']) * 0.2
        engineered['risk_score'] = min(risk_score, 1.0)
        
        # Urgency indicators
        urgency_score = 0
        urgency_score += timestamp_features['is_late_night'] * 0.4
        urgency_score += timestamp_features['is_weekend'] * 0.3
        urgency_score += (author_features['commit_frequency_score'] > 5) * 0.3
        engineered['urgency_score'] = min(urgency_score, 1.0)
        
        # Code churn metrics
        if file_features['total_changes'] > 0:
            churn_ratio = (file_features['total_additions'] + file_features['total_deletions']) / file_features['total_changes']
            engineered['code_churn'] = min(churn_ratio, 2.0)
        else:
            engineered['code_churn'] = 0
        
        # File type hotspots (common file types that often have issues)
        risky_extensions = ['js', 'ts', 'py', 'java', 'cpp', 'c', 'php']
        config_extensions = ['json', 'xml', 'yml', 'yaml', 'cfg', 'conf']
        doc_extensions = ['md', 'txt', 'rst', 'doc']
        
        engineered['touches_risky_files'] = any(ext in risky_extensions for ext in file_features['file_types'])
        engineered['touches_config_files'] = any(ext in config_extensions for ext in file_features['file_types'])
        engineered['touches_doc_files'] = any(ext in doc_extensions for ext in file_features['file_types'])
        
        return engineered
    
    def fit(self, metadata_samples: List[Dict]) -> None:
        """
        Fit c√°c encoders v√† scalers v·ªõi training data
        """
        print("üîß Fitting metadata processors...")
        
        # Collect all features
        all_numerical_features = []
        all_categorical_features = {
            'authors': [],
            'file_types': [],
            'seasons': []
        }
        
        for sample in metadata_samples:
            # Extract features
            file_features = self.extract_file_features(sample.get('files', []))
            author_features = self.extract_author_features(
                sample.get('author', {}), 
                sample.get('commit_history', [])
            )
            timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
            engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
            
            # Collect numerical features
            numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
            all_numerical_features.append(numerical)
            
            # Collect categorical features
            all_categorical_features['authors'].append(author_features['author_login'])
            all_categorical_features['file_types'].extend(file_features['file_types'])
            all_categorical_features['seasons'].append(timestamp_features['season'])
        
        # Fit scalers
        if self.normalize_features and all_numerical_features:
            numerical_array = np.array(all_numerical_features)
            self.numerical_scaler.fit(numerical_array)
        
        # Fit encoders
        if all_categorical_features['authors']:
            unique_authors = list(set(all_categorical_features['authors']))[:self.max_authors]
            self.author_encoder.fit(unique_authors + ['<UNK>'])
        
        if all_categorical_features['file_types']:
            unique_file_types = list(set(all_categorical_features['file_types']))
            self.file_type_encoder.fit(unique_file_types + ['<UNK>'])
        
        # Fit file path vectorizer
        all_file_paths = []
        for sample in metadata_samples:
            files = sample.get('files', [])
            paths = [f.get('filename', '') for f in files]
            all_file_paths.extend(paths)
        
        if all_file_paths:
            self.file_path_vectorizer.fit(all_file_paths)
        
        self.is_fitted = True
        print("‚úÖ Metadata processors fitted successfully")
    
    def _get_numerical_features(self, file_features: Dict, author_features: Dict, 
                               timestamp_features: Dict, engineered_features: Dict) -> List[float]:
        """
        L·∫•y t·∫•t c·∫£ numerical features th√†nh m·ªôt vector
        """
        features = []
        
        # File features
        features.extend([
            file_features['num_files'],
            file_features['total_additions'],
            file_features['total_deletions'],
            file_features['total_changes'],
            file_features['num_file_types'],
            file_features['avg_file_depth'],
            file_features['max_file_depth'],
            file_features['min_file_depth'],
            file_features['additions_ratio'],
            file_features['deletions_ratio'],
            file_features['added_files'],
            file_features['modified_files'],
            file_features['deleted_files'],
            file_features['renamed_files'],
            file_features['large_change_files'],
            float(file_features['has_large_changes'])
        ])
        
        # Author features
        features.extend([
            author_features['recent_commits_count'],
            author_features['avg_recent_commit_size'],
            author_features['avg_commit_interval_hours'],
            author_features['commit_frequency_score']
        ])
        
        # Timestamp features
        features.extend([
            timestamp_features['hour_of_day'],
            timestamp_features['day_of_week'],
            timestamp_features['day_of_month'],
            timestamp_features['month'],
            float(timestamp_features['is_weekend']),
            float(timestamp_features['is_business_hours']),
            float(timestamp_features['is_late_night'])
        ])
        
        # Engineered features
        features.extend([
            engineered_features['complexity_score'],
            engineered_features['risk_score'],
            engineered_features['urgency_score'],
            engineered_features['code_churn'],
            float(engineered_features['touches_risky_files']),
            float(engineered_features['touches_config_files']),
            float(engineered_features['touches_doc_files'])
        ])
        
        return features
    
    def process_sample(self, sample: Dict) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt sample metadata
        """
        if not self.is_fitted:
            raise ValueError("MetadataProcessor must be fitted before processing samples")
        
        # Extract features
        file_features = self.extract_file_features(sample.get('files', []))
        author_features = self.extract_author_features(
            sample.get('author', {}), 
            sample.get('commit_history', [])
        )
        timestamp_features = self.extract_timestamp_features(sample.get('timestamp', ''))
        engineered_features = self.create_feature_engineering(file_features, author_features, timestamp_features)
        
        result = {}
        
        # Numerical features
        numerical = self._get_numerical_features(file_features, author_features, timestamp_features, engineered_features)
        if self.normalize_features:
            numerical = self.numerical_scaler.transform([numerical])[0]
        result['numerical_features'] = torch.tensor(numerical, dtype=torch.float32)
        
        # Categorical features
        # Author encoding
        author_login = author_features['author_login']
        try:
            author_encoded = self.author_encoder.transform([author_login])[0]
        except ValueError:
            author_encoded = self.author_encoder.transform(['<UNK>'])[0]
        result['author_encoded'] = torch.tensor(author_encoded, dtype=torch.long)
        
        # Season encoding
        season_map = {'spring': 0, 'summer': 1, 'fall': 2, 'winter': 3}
        result['season_encoded'] = torch.tensor(season_map.get(timestamp_features['season'], 0), dtype=torch.long)
          # File types encoding (one-hot or multi-hot)
        try:
            # Get number of classes from fitted encoder
            num_classes = len(self.file_type_encoder.classes_)
        except AttributeError:
            # Fallback if encoder is not fitted or doesn't have classes_ attribute
            num_classes = 10  # Default reasonable size
            
        file_type_vector = np.zeros(num_classes)
        for file_type in file_features['file_types']:
            try:
                idx = self.file_type_encoder.transform([file_type])[0]
                if idx < num_classes:  # Safety check
                    file_type_vector[idx] = 1
            except (ValueError, AttributeError):
                continue
        result['file_types_encoded'] = torch.tensor(file_type_vector, dtype=torch.float32)
        
        return result
    
    def process_batch(self, samples: List[Dict]) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt batch samples
        """
        batch_results = {
            'numerical_features': [],
            'author_encoded': [],
            'season_encoded': [],
            'file_types_encoded': []
        }
        
        for sample in samples:
            processed = self.process_sample(sample)
            for key, value in processed.items():
                batch_results[key].append(value)
        
        # Stack tensors
        for key in batch_results:
            batch_results[key] = torch.stack(batch_results[key])
        
        return batch_results
    
    def get_feature_dimensions(self) -> Dict[str, int]:
        """
        Tr·∫£ v·ªÅ dimensions c·ªßa c√°c feature types
        """
        return {
            'numerical_dim': 33,  # Total numerical features
            'author_vocab_size': len(self.author_encoder.classes_) if hasattr(self.author_encoder, 'classes_') else 1000,
            'season_vocab_size': 4,
            'file_types_dim': len(self.file_type_encoder.classes_) if hasattr(self.file_type_encoder, 'classes_') else 100
        }

```

### backend\ai\multimodal_fusion\data_preprocessing\minimal_enhanced_text_processor.py
```py
"""
Minimal Enhanced Text Processor
Provides basic NLTK functionality without complex dependencies
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import string
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Try minimal NLTK imports
try:
    # Only import basic tokenization without sklearn dependencies
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    import nltk
    
    # Download only essential data
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
    
    NLTK_BASIC = True
    logger.info("Basic NLTK functionality available")
except Exception as e:
    NLTK_BASIC = False
    logger.warning(f"NLTK basic features not available: {e}")

# Try TextBlob for sentiment
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    logger.info("TextBlob available for sentiment analysis")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logger.warning("TextBlob not available")

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("Transformers library available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers not available")


class MinimalEnhancedTextProcessor:
    """
    Minimal Enhanced Text Processor with basic NLTK support
    Focuses on essential improvements without complex dependencies
    """
    
    def __init__(self, 
                 method: str = "lstm",
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased",
                 enable_sentiment: bool = True,
                 enable_advanced_cleaning: bool = True):
        """
        Args:
            method: Processing method ("lstm", "distilbert", "transformer")
            vocab_size: Vocabulary size for LSTM
            max_length: Maximum sequence length
            pretrained_model: Pre-trained model name
            enable_sentiment: Enable sentiment analysis with TextBlob
            enable_advanced_cleaning: Enable advanced text cleaning
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
        self.enable_sentiment = enable_sentiment
        self.enable_advanced_cleaning = enable_advanced_cleaning
        
        # Initialize stopwords
        self._init_stopwords()
        
        # Initialize model components
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
                logger.info(f"Initialized {pretrained_model} with embedding dimension {self.embed_dim}")
            except Exception as e:
                logger.error(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            self._init_lstm_components()
    
    def _init_stopwords(self):
        """Initialize stopwords with fallback"""
        if NLTK_BASIC:
            try:
                self.stop_words = set(stopwords.words('english'))
                logger.info(f"Loaded {len(self.stop_words)} NLTK stopwords")
            except Exception as e:
                logger.warning(f"Failed to load NLTK stopwords: {e}")
                self.stop_words = self._get_basic_stopwords()
        else:
            self.stop_words = self._get_basic_stopwords()
    
    def _get_basic_stopwords(self) -> Set[str]:
        """Basic stopwords list"""
        return set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
            'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours'
        ])
    
    def _init_lstm_components(self):
        """Initialize LSTM components"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128
    
    def enhanced_tokenize(self, text: str) -> List[str]:
        """Enhanced tokenization with NLTK if available"""
        if NLTK_BASIC:
            try:
                tokens = word_tokenize(text.lower())
                return [token for token in tokens if token.isalpha() and len(token) > 1]
            except Exception as e:
                logger.warning(f"NLTK tokenization failed: {e}")
        
        # Fallback to simple tokenization
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        return [word for word in text.split() if len(word) > 1]
    
    def advanced_clean_commit_message(self, text: str) -> str:
        """Advanced commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        original_text = text
        
        # Basic cleaning
        text = self.clean_commit_message(text)
        
        if not self.enable_advanced_cleaning:
            return text
        
        # Advanced cleaning patterns
        advanced_patterns = [
            # Remove version numbers
            (r'\bv?\d+\.\d+(\.\d+)?(-\w+)?\b', ''),
            # Remove file extensions in isolation
            (r'\b\w+\.(js|py|html|css|md|txt|json|xml|yml|yaml)\b', ''),
            # Remove common dev terms that add noise
            (r'\b(eslint|prettier|webpack|babel|npm|yarn|pip)\b', ''),
            # Remove brackets with single words
            (r'\[\w+\]', ''),
            # Remove parentheses with single words
            (r'\(\w+\)', ''),
            # Clean up multiple spaces and special chars
            (r'[^\w\s\.\!\?\,\:\;\-]', ' '),
            (r'\s+', ' '),
        ]
        
        for pattern, replacement in advanced_patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        text = text.strip()
        
        # If cleaning removed too much, return original cleaned version
        if len(text) < len(original_text) * 0.3:
            return self.clean_commit_message(original_text)
        
        return text
    
    def clean_commit_message(self, text: str) -> str:
        """Basic commit message cleaning"""
        if not text or not isinstance(text, str):
            return ""
        
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references
        text = re.sub(r'(closes?|fixes?|resolves?|addresses?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove co-authored-by lines
        text = re.sub(r'co-authored-by:.*', '', text, flags=re.IGNORECASE)
        
        # Remove merge commit patterns
        text = re.sub(r'merge (branch|pull request) .* into .*', '', text, flags=re.IGNORECASE)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_enhanced_features(self, text: str) -> Dict[str, any]:
        """Extract enhanced features with sentiment analysis"""
        features = self.extract_basic_features(text)
        
        if not text or not isinstance(text, str):
            return features
        
        # Add sentiment analysis if available
        if self.enable_sentiment and TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                features['sentiment_polarity'] = blob.sentiment.polarity
                features['sentiment_subjectivity'] = blob.sentiment.subjectivity
                
                # Categorize sentiment
                polarity = blob.sentiment.polarity
                if polarity > 0.1:
                    features['sentiment_category'] = 'positive'
                elif polarity < -0.1:
                    features['sentiment_category'] = 'negative'
                else:
                    features['sentiment_category'] = 'neutral'
                    
            except Exception as e:
                logger.warning(f"Sentiment analysis failed: {e}")
                features['sentiment_polarity'] = 0.0
                features['sentiment_subjectivity'] = 0.0
                features['sentiment_category'] = 'neutral'
        
        # Enhanced text statistics
        words = text.split()
        if words:
            features['avg_word_length'] = np.mean([len(word) for word in words])
            features['max_word_length'] = max(len(word) for word in words)
            features['unique_word_ratio'] = len(set(words)) / len(words)
        else:
            features['avg_word_length'] = 0
            features['max_word_length'] = 0
            features['unique_word_ratio'] = 0
        
        # Enhanced keyword detection
        technical_keywords = ['api', 'database', 'server', 'client', 'config', 'auth', 'security', 'performance']
        ui_keywords = ['ui', 'interface', 'design', 'layout', 'style', 'theme', 'responsive']
        testing_keywords = ['test', 'spec', 'mock', 'coverage', 'unit', 'integration', 'e2e']
        
        text_lower = text.lower()
        features['has_technical_keywords'] = any(kw in text_lower for kw in technical_keywords)
        features['has_ui_keywords'] = any(kw in text_lower for kw in ui_keywords)
        features['has_testing_keywords'] = any(kw in text_lower for kw in testing_keywords)
        
        return features
    
    def extract_basic_features(self, text: str) -> Dict[str, any]:
        """Extract basic text features"""
        features = {}
        
        if not text or not isinstance(text, str):
            return {key: 0 for key in [
                'length', 'word_count', 'char_count', 'digit_count', 'upper_count',
                'punctuation_count', 'has_commit_type', 'commit_type_prefix',
                'has_bug_keywords', 'has_feature_keywords', 'has_doc_keywords'
            ]}
        
        # Basic statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        features['punctuation_count'] = len([c for c in text if c in string.punctuation])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build']
        text_lower = text.lower()
        features['has_commit_type'] = any(text_lower.startswith(ct + ':') or text_lower.startswith(ct + '(') for ct in commit_types)
        
        # Extract commit type
        for ct in commit_types:
            if text_lower.startswith(ct + ':') or text_lower.startswith(ct + '('):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
        
        # Keyword detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'patch', 'hotfix']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support', 'introduce']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation', 'guide']
        
        features['has_bug_keywords'] = any(kw in text_lower for kw in bug_keywords)
        features['has_feature_keywords'] = any(kw in text_lower for kw in feature_keywords)
        features['has_doc_keywords'] = any(kw in text_lower for kw in doc_keywords)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """Build vocabulary for LSTM method"""
        if self.method != "lstm":
            return
        
        logger.info("üî§ Building enhanced vocabulary...")
        
        for text in texts:
            cleaned_text = self.advanced_clean_commit_message(text)
            tokens = self.enhanced_tokenize(cleaned_text)
            tokens = [token for token in tokens if token not in self.stop_words]
            self.word_counts.update(tokens)
        
        # Build vocabulary
        most_common = self.word_counts.most_common(self.vocab_size - 4)
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"‚úÖ Enhanced vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """Enhanced LSTM encoding"""
        cleaned_text = self.advanced_clean_commit_message(text)
        tokens = self.enhanced_tokenize(cleaned_text)
        tokens = [token for token in tokens if token not in self.stop_words]
        
        # Convert to indices
        indices = [self.word_to_idx.get(token, 1) for token in tokens]
        
        # Add start and end tokens
        indices = [2] + indices + [3]
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))
        
        return torch.tensor(indices, dtype=torch.long)
    
    def fit(self, texts: List[str]) -> 'MinimalEnhancedTextProcessor':
        """Fit the processor to training data"""
        logger.info("üöÄ Fitting minimal enhanced text processor...")
        
        if self.method == "lstm":
            self.build_vocabulary(texts)
        
        logger.info("‚úÖ Text processor fitted successfully")
        return self
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """Process a batch of texts"""
        results = {
            'text_features': [],
            'embeddings': None,
            'enhanced_features': []
        }
        
        for text in texts:
            basic_features = self.extract_basic_features(text)
            enhanced_features = self.extract_enhanced_features(text)
            
            results['text_features'].append(basic_features)
            results['enhanced_features'].append(enhanced_features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        
        return results
    
    # Keep essential methods from original processor
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """Encode text for transformer method"""
        cleaned_text = self.advanced_clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_vocab_size(self) -> int:
        """Return vocabulary size"""
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """Return embedding dimension"""
        return self.embed_dim

    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx

```

### backend\ai\multimodal_fusion\data_preprocessing\text_processor.py
```py
"""
Text Processor for Multi-Modal Fusion Network
X·ª≠ l√Ω v√† chu·∫©n b·ªã d·ªØ li·ªáu vƒÉn b·∫£n t·ª´ commit messages
"""

import re
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from collections import Counter

# Optional transformers import
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Transformers not available. Using simple tokenization only.")

# Optional NLTK import
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("NLTK not available. Using simple text processing.")

class TextProcessor:
    """
    L·ªõp x·ª≠ l√Ω vƒÉn b·∫£n cho commit messages
    H·ªó tr·ª£ c·∫£ tokenization ƒë∆°n gi·∫£n v√† pre-trained embeddings
    """
    
    def __init__(self, 
                 method: str = "lstm",  # "lstm", "distilbert", "transformer"
                 vocab_size: int = 10000,
                 max_length: int = 128,
                 pretrained_model: str = "distilbert-base-uncased"):
        """
        Args:
            method: Ph∆∞∆°ng ph√°p x·ª≠ l√Ω ("lstm", "distilbert", "transformer")
            vocab_size: K√≠ch th∆∞·ªõc vocabulary cho LSTM
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
            pretrained_model: T√™n pre-trained model n·∫øu d√πng transformer
        """
        self.method = method
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.pretrained_model = pretrained_model
          # Initialize components based on method
        if method in ["distilbert", "transformer"] and TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)
                self.model = AutoModel.from_pretrained(pretrained_model)
                self.embed_dim = self.model.config.hidden_size
            except Exception as e:
                print(f"Failed to load pretrained model: {e}. Falling back to LSTM method.")
                self.method = "lstm"
                self._init_lstm_components()
        else:
            # LSTM method
            self._init_lstm_components()
    
    def _init_lstm_components(self):
        """Initialize components for LSTM method"""
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        self.word_counts = Counter()
        self.embed_dim = 128  # Default embedding dimension
            
        # Download NLTK data if needed
        if NLTK_AVAILABLE:
            try:
                nltk.data.find('tokenizers/punkt')
                nltk.data.find('corpora/stopwords')
            except LookupError:
                nltk.download('punkt')
                nltk.download('stopwords')
                
            self.stop_words = set(stopwords.words('english'))
        else:
            # Simple fallback stopwords
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'])
        
    def _tokenize_text(self, text: str) -> List[str]:
        """
        Tokenize text with fallback if NLTK not available
        """
        if NLTK_AVAILABLE:
            return word_tokenize(text)
        else:
            # Simple tokenization fallback
            import string            # Remove punctuation and split by whitespace
            text = text.translate(str.maketrans('', '', string.punctuation))
            return text.split()
    
    def build_vocab(self, texts: List[str], vocab_size: int = None):
        """
        Build vocabulary from a list of texts
        """
        if vocab_size:
            self.vocab_size = vocab_size
            
        # Count words
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            tokens = self._tokenize_text(cleaned_text.lower())
            self.word_counts.update(tokens)
        
        # Build vocabulary with most common words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # Reserve space for special tokens
        
        for word, count in most_common:
            if word not in self.word_to_idx:
                idx = len(self.word_to_idx)
                self.word_to_idx[word] = idx
                self.idx_to_word[idx] = word
        
        print(f"Built vocabulary with {len(self.word_to_idx)} words")
        
    @property
    def vocab(self):
        """Return vocabulary dictionary"""
        return self.word_to_idx
    
    def clean_commit_message(self, text: str) -> str:
        """
        L√†m s·∫°ch commit message
        """
        if not text or not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove commit hashes (SHA)
        text = re.sub(r'\b[0-9a-fA-F]{7,40}\b', '', text)
        
        # Remove issue/PR references like #123, Fixes #456
        text = re.sub(r'(closes?|fixes?|resolves?)\s*#\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'#\d+', '', text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s\.\!\?\,\:\;\-\(\)]', ' ', text)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_commit_features(self, text: str) -> Dict[str, any]:
        """
        Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ commit message
        """
        features = {}
        
        # Basic features
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len([c for c in text if c.isalpha()])
        features['digit_count'] = len([c for c in text if c.isdigit()])
        features['upper_count'] = len([c for c in text if c.isupper()])
        
        # Commit type detection
        commit_types = ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf']
        features['has_commit_type'] = any(text.lower().startswith(ct + ':') for ct in commit_types)
        
        # Extract commit type if present
        for ct in commit_types:
            if text.lower().startswith(ct + ':'):
                features['commit_type_prefix'] = ct
                break
        else:
            features['commit_type_prefix'] = 'none'
            
        # Keywords detection
        bug_keywords = ['fix', 'bug', 'error', 'issue', 'problem', 'resolve', 'solve']
        feature_keywords = ['add', 'implement', 'create', 'new', 'feature', 'support']
        doc_keywords = ['doc', 'readme', 'comment', 'documentation']
        
        features['has_bug_keywords'] = any(keyword in text.lower() for keyword in bug_keywords)
        features['has_feature_keywords'] = any(keyword in text.lower() for keyword in feature_keywords)
        features['has_doc_keywords'] = any(keyword in text.lower() for keyword in doc_keywords)
        
        # Sentiment indicators
        positive_words = ['improve', 'enhance', 'optimize', 'better', 'good', 'success']
        negative_words = ['remove', 'delete', 'deprecated', 'broken', 'fail', 'error']
        urgent_words = ['urgent', 'critical', 'hotfix', 'emergency', 'asap']
        
        features['positive_sentiment'] = any(word in text.lower() for word in positive_words)
        features['negative_sentiment'] = any(word in text.lower() for word in negative_words)
        features['urgent_sentiment'] = any(word in text.lower() for word in urgent_words)
        
        return features
    
    def build_vocabulary(self, texts: List[str]) -> None:
        """
        X√¢y d·ª±ng vocabulary cho LSTM method
        """
        if self.method != "lstm":
            return
            
        print("üî§ Building vocabulary for text processing...")
        for text in texts:
            cleaned_text = self.clean_commit_message(text)
            words = self._tokenize_text(cleaned_text.lower())
            # Filter out stop words and very short words
            words = [w for w in words if w not in self.stop_words and len(w) > 1]
            self.word_counts.update(words)
        
        # Keep most frequent words
        most_common = self.word_counts.most_common(self.vocab_size - 4)  # -4 for special tokens
        
        for i, (word, count) in enumerate(most_common):
            idx = i + 4  # Start from 4 (after special tokens)
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"‚úÖ Vocabulary built with {len(self.word_to_idx)} words")
    
    def encode_text_lstm(self, text: str) -> torch.Tensor:
        """
        Encode text cho LSTM method
        """
        cleaned_text = self.clean_commit_message(text)
        words = self._tokenize_text(cleaned_text.lower())
        words = [w for w in words if w not in self.stop_words and len(w) > 1]
        
        # Convert to indices
        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 is UNK
        
        # Add start and end tokens
        indices = [2] + indices + [3]  # 2 is START, 3 is END
        
        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([0] * (self.max_length - len(indices)))  # 0 is PAD
        
        return torch.tensor(indices, dtype=torch.long)
    
    def encode_text_transformer(self, text: str) -> Dict[str, torch.Tensor]:
        """
        Encode text cho transformer method
        """
        cleaned_text = self.clean_commit_message(text)
        
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }
    
    def get_text_embeddings(self, texts: List[str], device: str = 'cpu') -> torch.Tensor:
        """
        L·∫•y embeddings cho list of texts
        """
        if self.method == "lstm":
            # Return token indices for LSTM
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            return torch.stack(embeddings)
        
        elif self.method in ["distilbert", "transformer"]:
            # Get contextual embeddings
            self.model.eval()
            self.model.to(device)
            
            embeddings = []
            with torch.no_grad():
                for text in texts:
                    encoding = self.encode_text_transformer(text)
                    input_ids = encoding['input_ids'].unsqueeze(0).to(device)
                    attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    # Use [CLS] token embedding or mean pooling
                    embedding = outputs.last_hidden_state[:, 0, :]  # CLS token
                    embeddings.append(embedding.cpu())
            
            return torch.cat(embeddings, dim=0)
    
    def process_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """
        X·ª≠ l√Ω m·ªôt batch texts
        """
        results = {
            'text_features': [],
            'embeddings': None,
            'metadata_features': []
        }
        
        # Extract text features
        for text in texts:
            features = self.extract_commit_features(text)
            results['text_features'].append(features)
        
        # Get embeddings
        if self.method == "lstm":
            embeddings = []
            for text in texts:
                embeddings.append(self.encode_text_lstm(text))
            results['embeddings'] = torch.stack(embeddings)
        else:
            results['embeddings'] = self.get_text_embeddings(texts)
        
        return results
    
    def fit(self, texts: List[str]) -> 'TextProcessor':
        """
        Fit the text processor to the training data
        This method builds vocabulary for LSTM method and prepares the processor
        """
        if self.method == "lstm":
            self.build_vocabulary(texts)
        # For transformer methods, no fitting is needed as they use pre-trained models
        return self
    
    def get_vocab_size(self) -> int:
        """
        Tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc vocabulary
        """
        if self.method == "lstm":
            return len(self.word_to_idx)
        else:
            return self.tokenizer.vocab_size
    
    def get_embedding_dim(self) -> int:
        """
        Tr·∫£ v·ªÅ dimension c·ªßa embeddings
        """
        return self.embed_dim

```

### backend\ai\multimodal_fusion\data_preprocessing\__init__.py
```py
"""
Data Preprocessing Module Initialization
"""

from .metadata_processor import MetadataProcessor

# Import text processors with fallback
try:
    from .minimal_enhanced_text_processor import MinimalEnhancedTextProcessor
    ENHANCED_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Enhanced text processor not available: {e}")
    ENHANCED_PROCESSOR_AVAILABLE = False

try:
    from .text_processor import TextProcessor
    BASIC_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"Basic text processor not available: {e}")
    BASIC_PROCESSOR_AVAILABLE = False
    # Use minimal processor as fallback
    if ENHANCED_PROCESSOR_AVAILABLE:
        TextProcessor = MinimalEnhancedTextProcessor

__all__ = ["MetadataProcessor"]

if ENHANCED_PROCESSOR_AVAILABLE:
    __all__.append("MinimalEnhancedTextProcessor")
if BASIC_PROCESSOR_AVAILABLE:
    __all__.append("TextProcessor")

```

### backend\ai\multimodal_fusion\evaluation\interpretability.py
```py

```

### backend\ai\multimodal_fusion\evaluation\metrics_calculator.py
```py

```

### backend\ai\multimodal_fusion\evaluation\visualization.py
```py

```

### backend\ai\multimodal_fusion\evaluation\__init__.py
```py

```

### backend\ai\multimodal_fusion\losses\multi_task_losses.py
```py

```

### backend\ai\multimodal_fusion\losses\__init__.py
```py

```

### backend\ai\multimodal_fusion\models\baselines.py
```py

```

### backend\ai\multimodal_fusion\models\multimodal_fusion.py
```py
"""
Multi-Modal Fusion Network Architecture
M√¥ h√¨nh k·∫øt h·ª£p th√¥ng tin vƒÉn b·∫£n v√† metadata v·ªõi c√°c c∆° ch·∫ø fusion ti√™n ti·∫øn
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class CrossAttentionFusion(nn.Module):
    """
    Cross-Attention mechanism cho fusion gi·ªØa text v√† metadata
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128, num_heads: int = 4):
        super(CrossAttentionFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Feed forward
        self.ff = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Project to same dimension
        text_proj = self.text_proj(text_features)  # (batch_size, hidden_dim)
        metadata_proj = self.metadata_proj(metadata_features)  # (batch_size, hidden_dim)
        
        # Add sequence dimension for attention
        text_seq = text_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        metadata_seq = metadata_proj.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        
        # Cross attention: text attends to metadata
        text_attended, _ = self.attention(text_seq, metadata_seq, metadata_seq)
        text_attended = text_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Cross attention: metadata attends to text
        metadata_attended, _ = self.attention(metadata_seq, text_seq, text_seq)
        metadata_attended = metadata_attended.squeeze(1)  # (batch_size, hidden_dim)
        
        # Combine and normalize
        combined = self.norm1(text_attended + metadata_attended)
        
        # Feed forward
        output = self.ff(combined)
        output = self.norm2(combined + output)
        
        return output

class GatedFusion(nn.Module):
    """
    Gated Multimodal Units (GMU) for fusion
    """
    
    def __init__(self, text_dim: int, metadata_dim: int, hidden_dim: int = 128):
        super(GatedFusion, self).__init__()
        
        self.text_dim = text_dim
        self.metadata_dim = metadata_dim
        self.hidden_dim = hidden_dim
        
        # Project to same dimension
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.metadata_proj = nn.Linear(metadata_dim, hidden_dim)
        
        # Gating mechanism
        self.gate_text = nn.Linear(text_dim + metadata_dim, hidden_dim)
        self.gate_metadata = nn.Linear(text_dim + metadata_dim, hidden_dim)
        
        # Final projection
        self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, text_features: torch.Tensor, metadata_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            text_features: (batch_size, text_dim)
            metadata_features: (batch_size, metadata_dim)
        Returns:
            fused_features: (batch_size, hidden_dim)
        """
        # Concatenate for gating
        combined = torch.cat([text_features, metadata_features], dim=1)
        
        # Compute gates
        text_gate = torch.sigmoid(self.gate_text(combined))
        metadata_gate = torch.sigmoid(self.gate_metadata(combined))
        
        # Project features
        text_proj = self.text_proj(text_features)
        metadata_proj = self.metadata_proj(metadata_features)
        
        # Apply gates
        gated_text = text_gate * text_proj
        gated_metadata = metadata_gate * metadata_proj
        
        # Combine and project
        fused = torch.cat([gated_text, gated_metadata], dim=1)
        output = self.output_proj(fused)
        
        return output

class TextBranch(nn.Module):
    """
    Nh√°nh x·ª≠ l√Ω vƒÉn b·∫£n v·ªõi LSTM/GRU v√† Attention ho·∫∑c Transformer
    """
    
    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, 
                 method: str = "lstm", pretrained_dim: Optional[int] = None):
        super(TextBranch, self).__init__()
        
        self.method = method
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        if method == "lstm":
            # LSTM-based processing
            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
            self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_dim * 2,
                num_heads=4,
                batch_first=True
            )
            self.output_dim = hidden_dim * 2
            
        elif method in ["distilbert", "transformer"]:
            # Transformer-based processing
            if pretrained_dim is None:
                raise ValueError("pretrained_dim must be provided for transformer method")
            
            self.projection = nn.Linear(pretrained_dim, hidden_dim)
            self.transformer_layer = nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=4,
                dim_feedforward=hidden_dim * 2,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)
            self.output_dim = hidden_dim
            
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, text_input: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            text_input: Token IDs (batch_size, seq_len) for LSTM or embeddings (batch_size, embed_dim) for transformer
            attention_mask: Attention mask (batch_size, seq_len) - optional
        Returns:
            text_features: (batch_size, output_dim)
        """
        if self.method == "lstm":
            # LSTM processing
            embedded = self.embedding(text_input)  # (batch_size, seq_len, embed_dim)
            lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)
            
            # Self-attention
            attended, _ = self.attention(lstm_out, lstm_out, lstm_out)
            
            # Global max pooling
            pooled = torch.max(attended, dim=1)[0]  # (batch_size, hidden_dim * 2)
            
        elif self.method in ["distilbert", "transformer"]:
            # Transformer processing
            if text_input.dim() == 2 and text_input.size(1) > 1:
                # Multiple embeddings case
                projected = self.projection(text_input)  # (batch_size, seq_len, hidden_dim)
                output = self.transformer(projected)  # (batch_size, seq_len, hidden_dim)
                pooled = torch.mean(output, dim=1)  # (batch_size, hidden_dim)
            else:
                # Single embedding case
                if text_input.dim() == 2:
                    projected = self.projection(text_input)  # (batch_size, hidden_dim)
                else:
                    projected = self.projection(text_input.unsqueeze(1))  # (batch_size, 1, hidden_dim)
                    projected = projected.squeeze(1)  # (batch_size, hidden_dim)
                pooled = projected
        
        return self.dropout(pooled)

class MetadataBranchV2(nn.Module):
    """
    Flexible metadata branch with configurable categorical and numerical features
    """
    
    def __init__(self, 
                 categorical_dims: Dict[str, int],
                 numerical_features: List[str],
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranchV2, self).__init__()
        
        self.categorical_dims = categorical_dims
        self.numerical_features = numerical_features
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.categorical_embeddings = nn.ModuleDict()
        for feature_name, vocab_size in categorical_dims.items():
            self.categorical_embeddings[feature_name] = nn.Embedding(vocab_size, embed_dim)
          # Projection for numerical features - dynamic sizing
        # We'll set this in the first forward pass when we know the actual dimension
        self.numerical_proj = None
        self.numerical_dim = None
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * len(categorical_dims)
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing features for categorical and numerical data
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        feature_list = []        # Process numerical features
        if 'numerical_features' in metadata_features:
            # Handle case where we have a single tensor with all numerical features
            numerical_tensor = metadata_features['numerical_features']
            if len(numerical_tensor.shape) == 1:
                numerical_tensor = numerical_tensor.unsqueeze(0)  # Add batch dim if missing
            
            # Initialize numerical projection layer if not already done
            if self.numerical_proj is None:
                self.numerical_dim = numerical_tensor.shape[-1]
                self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
            
            numerical_proj = self.numerical_proj(numerical_tensor)
            feature_list.append(numerical_proj)
        else:
            # Handle case where numerical features are split into individual tensors
            numerical_data = []
            for feature_name in self.numerical_features:
                if feature_name in metadata_features:
                    numerical_data.append(metadata_features[feature_name].unsqueeze(-1))
            
            if numerical_data:
                numerical_tensor = torch.cat(numerical_data, dim=1)
                
                # Initialize numerical projection layer if not already done
                if self.numerical_proj is None:
                    self.numerical_dim = numerical_tensor.shape[-1]
                    self.numerical_proj = nn.Linear(self.numerical_dim, self.hidden_dim).to(numerical_tensor.device)
                
                numerical_proj = self.numerical_proj(numerical_tensor)
                feature_list.append(numerical_proj)
        
        # Process categorical embeddings
        for feature_name, embedding_layer in self.categorical_embeddings.items():
            if feature_name in metadata_features:
                embed = embedding_layer(metadata_features[feature_name])
                feature_list.append(embed)
          # Combine all features
        if feature_list:
            combined = torch.cat(feature_list, dim=1)
        else:
            # Fallback if no features found
            batch_size = next(iter(metadata_features.values())).size(0)
            device = next(iter(metadata_features.values())).device
            combined = torch.zeros(batch_size, self.hidden_dim, device=device)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class MetadataBranch(nn.Module):
    """
    Nh√°nh x·ª≠ l√Ω metadata v·ªõi embeddings v√† dense layers
    """
    
    def __init__(self, 
                 numerical_dim: int,
                 author_vocab_size: int,
                 season_vocab_size: int,
                 file_types_dim: int,
                 embed_dim: int = 64,
                 hidden_dim: int = 128):
        super(MetadataBranch, self).__init__()
        
        self.numerical_dim = numerical_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # Embeddings for categorical features
        self.author_embedding = nn.Embedding(author_vocab_size, embed_dim)
        self.season_embedding = nn.Embedding(season_vocab_size, embed_dim)
        
        # Projection for numerical features
        self.numerical_proj = nn.Linear(numerical_dim, hidden_dim)
        
        # Projection for file types (multi-hot encoded)
        self.file_types_proj = nn.Linear(file_types_dim, embed_dim)
        
        # Combine all metadata
        total_embed_dim = hidden_dim + embed_dim * 3  # numerical + author + season + file_types
        self.combine_layers = nn.Sequential(
            nn.Linear(total_embed_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.output_dim = hidden_dim
        
    def forward(self, metadata_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            metadata_features: Dict containing numerical_features, author_encoded, season_encoded, file_types_encoded
        Returns:
            metadata_vector: (batch_size, hidden_dim)
        """
        # Process numerical features
        numerical = self.numerical_proj(metadata_features['numerical_features'])
        
        # Process categorical embeddings
        author_embed = self.author_embedding(metadata_features['author_encoded'])
        season_embed = self.season_embedding(metadata_features['season_encoded'])
        file_types_embed = self.file_types_proj(metadata_features['file_types_encoded'])
        
        # Combine all features
        combined = torch.cat([numerical, author_embed, season_embed, file_types_embed], dim=1)
        
        # Process through dense layers
        output = self.combine_layers(combined)
        
        return output

class TaskSpecificHead(nn.Module):
    """
    Task-specific classification head
    """
    
    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 64):
        super(TaskSpecificHead, self).__init__()
        
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        return self.classifier(features)

class MultiModalFusionNetwork(nn.Module):
    """
    Main Multi-Modal Fusion Network
    """
    
    def __init__(self, config: Dict = None, **kwargs):
        """
        Initialize MultiModalFusionNetwork with flexible configuration
        
        Args:
            config: Configuration dictionary (new format)
            **kwargs: Backward compatibility parameters (old format)
        """
        super(MultiModalFusionNetwork, self).__init__()
        
        # Handle both new config format and old parameter format
        if config is not None:
            self.config = config
            
            # Extract configurations
            text_config = config['text_encoder']
            metadata_config = config['metadata_encoder']
            fusion_config = config['fusion']
            task_configs = config['task_heads']
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=text_config['vocab_size'],
                embed_dim=text_config['embedding_dim'],
                hidden_dim=text_config['hidden_dim'],
                method=text_config.get('method', 'lstm'),
                pretrained_dim=text_config.get('pretrained_dim', None)
            )
            
            # Metadata branch - use flexible version
            self.metadata_branch = MetadataBranchV2(
                categorical_dims=metadata_config['categorical_dims'],
                numerical_features=metadata_config['numerical_features'],
                embed_dim=metadata_config['embedding_dim'],
                hidden_dim=metadata_config['hidden_dim']
            )
            
            # Fusion mechanism
            fusion_method = fusion_config.get('method', 'cross_attention')
            fusion_hidden_dim = fusion_config.get('fusion_dim', 128)
            
        else:
            # Backward compatibility - use old parameter format
            text_method = kwargs.get('text_method', 'lstm')
            vocab_size = kwargs.get('vocab_size', 10000)
            text_embed_dim = kwargs.get('text_embed_dim', 128)
            text_hidden_dim = kwargs.get('text_hidden_dim', 128)
            pretrained_text_dim = kwargs.get('pretrained_text_dim', None)
            
            numerical_dim = kwargs.get('numerical_dim', 34)
            author_vocab_size = kwargs.get('author_vocab_size', 1000)
            season_vocab_size = kwargs.get('season_vocab_size', 4)
            file_types_dim = kwargs.get('file_types_dim', 100)
            metadata_embed_dim = kwargs.get('metadata_embed_dim', 64)
            metadata_hidden_dim = kwargs.get('metadata_hidden_dim', 128)
            
            fusion_method = kwargs.get('fusion_method', 'cross_attention')
            fusion_hidden_dim = kwargs.get('fusion_hidden_dim', 128)
            task_configs = kwargs.get('task_configs', {})
            
            # Text branch
            self.text_branch = TextBranch(
                vocab_size=vocab_size,
                embed_dim=text_embed_dim,
                hidden_dim=text_hidden_dim,
                method=text_method,
                pretrained_dim=pretrained_text_dim
            )
            
            # Metadata branch - use old version for compatibility
            self.metadata_branch = MetadataBranch(
                numerical_dim=numerical_dim,
                author_vocab_size=author_vocab_size,
                season_vocab_size=season_vocab_size,
                file_types_dim=file_types_dim,
                embed_dim=metadata_embed_dim,
                hidden_dim=metadata_hidden_dim
            )
        
        # Common fusion setup
        if fusion_method == "cross_attention":
            self.fusion = CrossAttentionFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        elif fusion_method == "gated":
            self.fusion = GatedFusion(
                text_dim=self.text_branch.output_dim,
                metadata_dim=self.metadata_branch.output_dim,
                hidden_dim=fusion_hidden_dim
            )
            fusion_output_dim = fusion_hidden_dim
        else:  # concat
            self.fusion = None
            fusion_output_dim = self.text_branch.output_dim + self.metadata_branch.output_dim
        
        # Task-specific heads - support both formats
        self.task_heads = nn.ModuleDict()
        for task_name, task_config in task_configs.items():
            if isinstance(task_config, dict):
                if 'num_classes' in task_config:
                    num_classes = task_config['num_classes']
                elif 'classes' in task_config:
                    num_classes = len(task_config['classes'])
                else:
                    num_classes = 2  # default
            else:
                # Old format: direct number
                num_classes = task_config
            
            self.task_heads[task_name] = TaskSpecificHead(
                input_dim=fusion_output_dim,
                num_classes=num_classes
            )
    
    def forward(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor], 
                attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            text_input: Text tokens or embeddings
            metadata_input: Dict of metadata features
            attention_mask: Optional attention mask for text
            
        Returns:
            outputs: Dict mapping task names to logits
        """
        # Process text branch
        text_features = self.text_branch(text_input, attention_mask)
        
        # Process metadata branch
        metadata_features = self.metadata_branch(metadata_input)
        
        # Fusion
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            # Simple concatenation
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        # Task-specific predictions
        outputs = {}
        for task_name, head in self.task_heads.items():
            outputs[task_name] = head(fused_features)
        
        return outputs
    
    def get_fusion_features(self, text_input: torch.Tensor, metadata_input: Dict[str, torch.Tensor],
                           attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Get fused features for analysis/visualization
        """
        text_features = self.text_branch(text_input, attention_mask)
        metadata_features = self.metadata_branch(metadata_input)
        
        if self.fusion is not None:
            fused_features = self.fusion(text_features, metadata_features)
        else:
            fused_features = torch.cat([text_features, metadata_features], dim=1)
        
        return fused_features

```

### backend\ai\multimodal_fusion\models\shared_layers.py
```py

```

### backend\ai\multimodal_fusion\models\__init__.py
```py
"""
Models Module Initialization
"""

from .multimodal_fusion import MultiModalFusionNetwork, CrossAttentionFusion, GatedFusion

__all__ = [
    "MultiModalFusionNetwork",
    "CrossAttentionFusion", 
    "GatedFusion"
]

```

### backend\ai\multimodal_fusion\scripts\train_main.py
```py

```

### backend\ai\multimodal_fusion\scripts\train_multimodal_fusion.py
```py
#!/usr/bin/env python3
"""
Training Script for Multimodal Fusion Model
Complete training pipeline for commit analysis with text + metadata fusion
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, f1_score
from collections import Counter
import logging
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

def custom_collate_fn(batch):
    """Custom collate function to handle metadata dict structure"""
    collated = {}
    
    # Handle text features (simple tensors)
    collated['text_features'] = torch.stack([item['text_features'] for item in batch])
    
    # Handle metadata features (dict of tensors)
    metadata_keys = batch[0]['metadata_features'].keys()
    collated['metadata_features'] = {}
    for key in metadata_keys:
        collated['metadata_features'][key] = torch.stack([item['metadata_features'][key] for item in batch])
    
    # Handle labels (list of dicts)
    collated['labels'] = [item['labels'] for item in batch]
    
    # Handle text (list of strings)
    collated['text'] = [item['text'] for item in batch]
    
    return collated

# Add project paths
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai"))

# Import multimodal components
from ai.multimodal_fusion.models.multimodal_fusion import MultiModalFusionNetwork
from ai.multimodal_fusion.data_preprocessing.text_processor import TextProcessor
from ai.multimodal_fusion.data_preprocessing.metadata_processor import MetadataProcessor
# from ai.multimodal_fusion.training.multitask_trainer import MultiTaskTrainer
# from ai.multimodal_fusion.losses.multi_task_losses import MultiTaskLoss
# from ai.multimodal_fusion.evaluation.metrics_calculator import MetricsCalculator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('multimodal_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MultimodalDataset(Dataset):
    """Dataset for multimodal fusion training"""
    
    def __init__(self, samples, text_processor, metadata_processor, max_length=512):
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.max_length = max_length
        
        # Define label mappings for multimodal tasks
        self.task_labels = {
            'risk_prediction': {'low': 0, 'high': 1},
            'complexity_prediction': {'simple': 0, 'medium': 1, 'complex': 2},
            'hotspot_prediction': {'very_low': 0, 'low': 1, 'medium': 2, 'high': 3, 'very_high': 4},
            'urgency_prediction': {'normal': 0, 'urgent': 1}
        }
        
        logger.info(f"Created dataset with {len(samples)} samples")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
          # Process text
        commit_text = sample.get('text', '') or sample.get('message', '')
        text_features = self.text_processor.encode_text_lstm(commit_text)
          # Process metadata
        metadata = self._extract_metadata(sample)
        metadata_features = self.metadata_processor.process_sample(sample)
        
        # Generate multimodal task labels
        labels = self._generate_multimodal_labels(sample, commit_text, metadata)
        
        return {
            'text_features': text_features,
            'metadata_features': metadata_features,
            'labels': labels,
            'text': commit_text
        }
    
    def _extract_metadata(self, sample):
        """Extract metadata features from sample"""
        return {
            'author': sample.get('author', 'unknown'),
            'files_changed': len(sample.get('files_changed', [])),
            'additions': sample.get('additions', 0),
            'deletions': sample.get('deletions', 0),
            'time_of_day': sample.get('time_of_day', 12),  # hour
            'day_of_week': sample.get('day_of_week', 1),   # 1-7
            'commit_size': sample.get('additions', 0) + sample.get('deletions', 0),
            'is_merge': 'merge' in sample.get('text', '').lower()
        }
    
    def _generate_multimodal_labels(self, sample, text, metadata):
        """Generate labels for multimodal tasks based on commit analysis"""
        labels = {}
        
        # Risk prediction (high/low) - based on commit patterns
        risk_score = self._calculate_risk_score(text, metadata)
        labels['risk_prediction'] = 1 if risk_score > 0.5 else 0
        
        # Complexity prediction (simple/medium/complex) - based on changes
        complexity = self._calculate_complexity(text, metadata)
        labels['complexity_prediction'] = complexity
        
        # Hotspot prediction (very_low to very_high) - based on file patterns
        hotspot = self._calculate_hotspot_score(text, metadata)
        labels['hotspot_prediction'] = hotspot
        
        # Urgency prediction (normal/urgent) - based on keywords
        urgency = self._calculate_urgency(text, metadata)
        labels['urgency_prediction'] = urgency
        
        return labels
    
    def _calculate_risk_score(self, text, metadata):
        """Calculate risk score from commit text and metadata"""
        risk_keywords = ['fix', 'bug', 'error', 'crash', 'security', 'vulnerability', 'critical']
        risk_score = 0.0
        
        text_lower = text.lower()
        for keyword in risk_keywords:
            if keyword in text_lower:
                risk_score += 0.2
        
        # Add metadata-based risk
        if metadata['commit_size'] > 1000:  # Large commits are risky
            risk_score += 0.2
        if metadata['files_changed'] > 10:  # Many files changed
            risk_score += 0.1
            
        return min(risk_score, 1.0)
    
    def _calculate_complexity(self, text, metadata):
        """Calculate complexity level (0=simple, 1=medium, 2=complex)"""
        commit_size = metadata['commit_size']
        files_changed = metadata['files_changed']
        
        if commit_size < 50 and files_changed <= 2:
            return 0  # simple
        elif commit_size < 500 and files_changed <= 10:
            return 1  # medium
        else:
            return 2  # complex
    
    def _calculate_hotspot_score(self, text, metadata):
        """Calculate hotspot prediction (0-4 scale)"""
        # Based on files changed and commit frequency patterns
        files_changed = metadata['files_changed']
        
        if files_changed <= 1:
            return 0  # very_low
        elif files_changed <= 3:
            return 1  # low
        elif files_changed <= 7:
            return 2  # medium
        elif files_changed <= 15:
            return 3  # high
        else:
            return 4  # very_high
    
    def _calculate_urgency(self, text, metadata):
        """Calculate urgency (0=normal, 1=urgent)"""
        urgent_keywords = ['urgent', 'critical', 'hotfix', 'emergency', 'asap', 'immediately']
        text_lower = text.lower()
        
        for keyword in urgent_keywords:
            if keyword in text_lower:
                return 1
        
        # Large commits on weekends might be urgent
        if metadata['day_of_week'] in [6, 7] and metadata['commit_size'] > 500:
            return 1
            
        return 0

def load_training_data(data_file):
    """Load and prepare training data"""
    logger.info(f"Loading training data from {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Handle different data formats
    if 'data' in data:
        samples = data['data']
    elif isinstance(data, list):
        samples = data
    else:
        samples = [data]
    
    logger.info(f"Loaded {len(samples)} samples")
    return samples

def setup_model_and_training(device, vocab_size=10000):
    """Setup model, optimizer, and training components"""
      # Model configuration
    config = {
        'text_encoder': {
            'vocab_size': vocab_size,
            'embedding_dim': 768,
            'hidden_dim': 256,
            'num_layers': 2,
            'dropout': 0.1,
            'max_length': 512,
            'method': 'lstm'
        },        'metadata_encoder': {
            'categorical_dims': {'author_encoded': 1000, 'season_encoded': 4},  # vocab sizes to match processor output
            'numerical_features': ['numerical_features'],  # single tensor from processor
            'embedding_dim': 128,
            'hidden_dim': 128,
            'dropout': 0.1
        },
        'fusion': {
            'method': 'cross_attention',
            'fusion_dim': 256,
            'dropout': 0.1
        },
        'task_heads': {
            'risk_prediction': {'num_classes': 2},
            'complexity_prediction': {'num_classes': 3},
            'hotspot_prediction': {'num_classes': 5},
            'urgency_prediction': {'num_classes': 2}
        }
    }
    
    # Initialize model
    model = MultiModalFusionNetwork(config).to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"Model created with {total_params:,} total parameters")
    logger.info(f"Trainable parameters: {trainable_params:,}")
      # Optimizer and scheduler
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
      # Loss functions
    loss_fns = {
        'risk_prediction': nn.CrossEntropyLoss().to(device),
        'complexity_prediction': nn.CrossEntropyLoss().to(device),
        'hotspot_prediction': nn.CrossEntropyLoss().to(device),
        'urgency_prediction': nn.CrossEntropyLoss().to(device)
    }
    
    return model, optimizer, scheduler, loss_fns, config

def train_epoch(model, train_loader, optimizer, loss_fns, device, scaler=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    progress_bar = tqdm(train_loader, desc="Training")
    for batch in progress_bar:
        optimizer.zero_grad()
        
        # Move data to device
        text_features = batch['text_features'].to(device)
        
        # Handle metadata features (dict of tensors from custom collate)
        metadata_features = {}
        for key, value in batch['metadata_features'].items():
            metadata_features[key] = value.to(device)
        
        # Forward pass
        if scaler:
            with torch.cuda.amp.autocast():
                outputs = model(text_features, metadata_features)
                
                # Calculate losses
                batch_loss = 0
                for task, loss_fn in loss_fns.items():
                    labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                    task_loss = loss_fn(outputs[task], labels)
                    batch_loss += task_loss
                    task_losses[task] += task_loss.item()
                    
                    # Calculate accuracy
                    _, predicted = torch.max(outputs[task], 1)
                    task_correct[task] += (predicted == labels).sum().item()
                    task_total[task] += labels.size(0)
            
            scaler.scale(batch_loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(text_features, metadata_features)
            
            # Calculate losses
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            batch_loss.backward()
            optimizer.step()
        
        total_loss += batch_loss.item()
        
        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{batch_loss.item():.4f}",
            'avg_loss': f"{total_loss/len(progress_bar):.4f}"
        })
    
    # Calculate average metrics
    avg_loss = total_loss / len(train_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def validate_epoch(model, val_loader, loss_fns, device):
    """Validate for one epoch"""
    model.eval()
    total_loss = 0
    task_losses = {task: 0 for task in loss_fns.keys()}
    task_correct = {task: 0 for task in loss_fns.keys()}
    task_total = {task: 0 for task in loss_fns.keys()}
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            # Move data to device
            text_features = batch['text_features'].to(device)
            
            # Handle metadata features (dict of tensors from custom collate)
            metadata_features = {}
            for key, value in batch['metadata_features'].items():
                metadata_features[key] = value.to(device)
            
            # Forward pass
            outputs = model(text_features, metadata_features)
            
            # Calculate losses and metrics
            batch_loss = 0
            for task, loss_fn in loss_fns.items():
                labels = torch.tensor([sample[task] for sample in batch['labels']], device=device)
                task_loss = loss_fn(outputs[task], labels)
                batch_loss += task_loss
                task_losses[task] += task_loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs[task], 1)
                task_correct[task] += (predicted == labels).sum().item()
                task_total[task] += labels.size(0)
            
            total_loss += batch_loss.item()
    
    # Calculate average metrics
    avg_loss = total_loss / len(val_loader)
    task_accuracies = {task: task_correct[task] / task_total[task] for task in loss_fns.keys()}
    avg_accuracy = sum(task_accuracies.values()) / len(task_accuracies)
    
    return avg_loss, task_accuracies, avg_accuracy

def main():
    """Main training function"""
    logger.info("üöÄ MULTIMODAL FUSION MODEL TRAINING")
    logger.info("=" * 60)
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üîß Device: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        torch.cuda.empty_cache()
    
    # Paths
    data_file = Path(__file__).parent.parent.parent / "training_data" / "sample_preview.json"
    output_dir = Path(__file__).parent.parent.parent / "trained_models" / "multimodal_fusion"
    log_dir = Path(__file__).parent.parent.parent / "training_logs" / "multimodal_fusion"
    
    # Create directories
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load data
    if not data_file.exists():
        logger.error(f"‚ùå Dataset not found: {data_file}")
        return
    
    samples = load_training_data(data_file)
    
    # Take a subset for training (adjust as needed)
    if len(samples) > 10000:
        samples = samples[:10000]
        logger.info(f"Using subset of {len(samples)} samples for training")
      # Initialize processors
    text_processor = TextProcessor()
    metadata_processor = MetadataProcessor()
    
    # Build vocabulary from sample texts
    texts = [sample.get('text', '') or sample.get('message', '') for sample in samples]
    text_processor.build_vocab(texts, vocab_size=10000)
    
    # Fit metadata processor with samples
    logger.info("üîß Fitting metadata processor...")
    metadata_processor.fit(samples)
    
    # Create dataset
    dataset = MultimodalDataset(samples, text_processor, metadata_processor)
    
    # Split dataset
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    logger.info(f"üìä Train samples: {len(train_dataset)}")
    logger.info(f"üìä Val samples: {len(val_dataset)}")
      # Data loaders
    batch_size = 32 if device.type == 'cuda' else 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)
    
    # Setup model and training
    model, optimizer, scheduler, loss_fns, config = setup_model_and_training(
        device, vocab_size=len(text_processor.vocab)
    )
    
    # Mixed precision setup
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    # Training loop
    num_epochs = 20
    best_val_accuracy = 0
    patience = 5
    patience_counter = 0
    
    logger.info(f"üèÉ Starting training for {num_epochs} epochs")
    
    for epoch in range(num_epochs):
        logger.info(f"\nüìÖ Epoch {epoch+1}/{num_epochs}")
        
        # Train
        train_loss, train_accuracies, train_avg_acc = train_epoch(
            model, train_loader, optimizer, loss_fns, device, scaler
        )
        
        # Validate
        val_loss, val_accuracies, val_avg_acc = validate_epoch(
            model, val_loader, loss_fns, device
        )
        
        # Scheduler step
        scheduler.step(val_loss)
        
        # Log metrics
        logger.info(f"Train Loss: {train_loss:.4f}, Train Acc: {train_avg_acc:.4f}")
        logger.info(f"Val Loss: {val_loss:.4f}, Val Acc: {val_avg_acc:.4f}")
        
        for task in loss_fns.keys():
            logger.info(f"  {task}: Train {train_accuracies[task]:.4f}, Val {val_accuracies[task]:.4f}")
        
        # Save best model
        if val_avg_acc > best_val_accuracy:
            best_val_accuracy = val_avg_acc
            patience_counter = 0
            
            # Save model
            save_dict = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': config,
                'epoch': epoch + 1,
                'best_val_accuracy': best_val_accuracy,
                'val_accuracies': val_accuracies,
                'text_processor': text_processor,
                'metadata_processor': metadata_processor
            }
            
            torch.save(save_dict, output_dir / 'best_multimodal_fusion_model.pth')
            logger.info(f"üíæ Saved best model (Val Acc: {best_val_accuracy:.4f})")
        else:
            patience_counter += 1
            
        # Early stopping
        if patience_counter >= patience:
            logger.info(f"‚èπÔ∏è Early stopping after {patience} epochs without improvement")
            break
    
    logger.info(f"\nüéâ Training completed!")
    logger.info(f"Best validation accuracy: {best_val_accuracy:.4f}")
    
    # Save final model
    final_save_dict = {
        'model_state_dict': model.state_dict(),
        'config': config,
        'final_epoch': epoch + 1,
        'final_val_accuracy': val_avg_acc,
        'best_val_accuracy': best_val_accuracy,
        'text_processor': text_processor,
        'metadata_processor': metadata_processor
    }
    
    torch.save(final_save_dict, output_dir / 'final_multimodal_fusion_model.pth')
    logger.info(f"üíæ Saved final model")

if __name__ == "__main__":
    main()

```

### backend\ai\multimodal_fusion\training\multitask_trainer.py
```py
"""
Multi-Task Trainer for Multi-Modal Fusion Network
Tri·ªÉn khai Joint Multi-Task Learning v·ªõi Dynamic Loss Weighting
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
from pathlib import Path
import json
import logging
from collections import defaultdict
from sklearn.metrics import classification_report, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

class MultiModalDataset(Dataset):
    """
    Dataset cho Multi-Modal Fusion Network
    """
    
    def __init__(self, samples: List[Dict], text_processor, metadata_processor, 
                 label_encoders: Dict[str, Any]):
        """
        Args:
            samples: List of samples with text, metadata, and labels
            text_processor: TextProcessor instance
            metadata_processor: MetadataProcessor instance
            label_encoders: Dict mapping task names to label encoders
        """
        self.samples = samples
        self.text_processor = text_processor
        self.metadata_processor = metadata_processor
        self.label_encoders = label_encoders
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # Process text
        text = sample.get('text', '')
        if self.text_processor.method == "lstm":
            text_input = self.text_processor.encode_text_lstm(text)
            attention_mask = None
        else:
            text_encoding = self.text_processor.encode_text_transformer(text)
            text_input = text_encoding['input_ids']
            attention_mask = text_encoding['attention_mask']
        
        # Process metadata
        metadata_input = self.metadata_processor.process_sample(sample)
        
        # Process labels
        labels = {}
        for task_name, label_value in sample.get('labels', {}).items():
            if task_name in self.label_encoders:
                try:
                    encoded_label = self.label_encoders[task_name].transform([label_value])[0]
                    labels[task_name] = torch.tensor(encoded_label, dtype=torch.long)
                except ValueError:
                    # Handle unknown labels
                    labels[task_name] = torch.tensor(0, dtype=torch.long)
        
        result = {
            'text_input': text_input,
            'metadata_input': metadata_input,
            'labels': labels
        }
        
        if attention_mask is not None:
            result['attention_mask'] = attention_mask
        
        return result

def collate_fn(batch):
    """
    Custom collate function for DataLoader
    """
    # Stack text inputs
    text_inputs = torch.stack([item['text_input'] for item in batch])
    
    # Stack attention masks if present
    attention_masks = None
    if 'attention_mask' in batch[0]:
        attention_masks = torch.stack([item['attention_mask'] for item in batch])
    
    # Stack metadata inputs
    metadata_keys = batch[0]['metadata_input'].keys()
    metadata_inputs = {}
    for key in metadata_keys:
        metadata_inputs[key] = torch.stack([item['metadata_input'][key] for item in batch])
    
    # Collect labels
    task_names = batch[0]['labels'].keys()
    labels = {}
    for task_name in task_names:
        labels[task_name] = torch.stack([item['labels'][task_name] for item in batch])
    
    result = {
        'text_input': text_inputs,
        'metadata_input': metadata_inputs,
        'labels': labels
    }
    
    if attention_masks is not None:
        result['attention_mask'] = attention_masks
    
    return result

class DynamicLossWeighting:
    """
    Dynamic Loss Weighting cho Multi-Task Learning
    """
    
    def __init__(self, task_names: List[str], method: str = "uncertainty", alpha: float = 0.5):
        """
        Args:
            task_names: List of task names
            method: "uncertainty", "gradnorm", "equal"
            alpha: Learning rate for weight updates
        """
        self.task_names = task_names
        self.method = method
        self.alpha = alpha
        
        # Initialize weights
        self.weights = {task: 1.0 for task in task_names}
        self.loss_history = {task: [] for task in task_names}
        self.prev_losses = {task: 0.0 for task in task_names}
        
        if method == "uncertainty":
            # Learnable uncertainty parameters
            self.log_vars = nn.Parameter(torch.zeros(len(task_names)))
    
    def compute_weighted_loss(self, losses: Dict[str, torch.Tensor], 
                            model: nn.Module = None) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute weighted loss
        """
        if self.method == "equal":
            # Equal weighting
            total_loss = sum(losses.values())
            return total_loss, self.weights
        
        elif self.method == "uncertainty":
            # Uncertainty weighting (Kendall et al.)
            total_loss = 0
            for i, (task, loss) in enumerate(losses.items()):
                precision = torch.exp(-self.log_vars[i])
                total_loss += precision * loss + self.log_vars[i]
            
            return total_loss, self.weights
        
        elif self.method == "gradnorm":
            # GradNorm (Chen et al.)
            if model is None:
                # Fallback to equal weighting
                total_loss = sum(losses.values())
                return total_loss, self.weights
            
            # Compute weighted loss
            weighted_losses = []
            for task in self.task_names:
                weighted_losses.append(self.weights[task] * losses[task])
            
            total_loss = sum(weighted_losses)
            
            # Update weights based on gradient norms (simplified version)
            self._update_gradnorm_weights(losses, model)
            
            return total_loss, self.weights
    
    def _update_gradnorm_weights(self, losses: Dict[str, torch.Tensor], model: nn.Module):
        """
        Update weights using GradNorm algorithm (simplified)
        """
        # This is a simplified version - full GradNorm requires more complex implementation
        for task in self.task_names:
            current_loss = losses[task].item()
            self.loss_history[task].append(current_loss)
            
            if len(self.loss_history[task]) > 1:
                # Simple heuristic: increase weight if loss is increasing
                loss_change = current_loss - self.prev_losses[task]
                if loss_change > 0:
                    self.weights[task] = min(self.weights[task] * 1.1, 5.0)
                else:
                    self.weights[task] = max(self.weights[task] * 0.95, 0.1)
            
            self.prev_losses[task] = current_loss

class MultiTaskTrainer:
    """
    Multi-Task Trainer cho Multi-Modal Fusion Network
    """
    
    def __init__(self, 
                 model: nn.Module,
                 task_configs: Dict[str, int],
                 loss_weighting_method: str = "uncertainty",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 save_dir: str = "./models/multimodal_fusion"):
        """
        Args:
            model: MultiModalFusionNetwork instance
            task_configs: Dict mapping task names to number of classes
            loss_weighting_method: "uncertainty", "gradnorm", "equal"
            device: Training device
            save_dir: Directory to save models and logs
        """
        self.model = model.to(device)
        self.task_configs = task_configs
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Loss weighting
        self.loss_weighting = DynamicLossWeighting(
            task_names=list(task_configs.keys()),
            method=loss_weighting_method
        )
          # Loss functions
        self.criterion = nn.CrossEntropyLoss()
        
        # Training history
        self.train_history = defaultdict(list)
        self.val_history = defaultdict(list)
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup logging"""
        log_file = self.save_dir / "training.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def train_epoch(self, train_loader: DataLoader, optimizer: optim.Optimizer) -> Dict[str, float]:
        """
        Train one epoch
        """
        self.model.train()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        total_batches = len(train_loader)
        
        for batch_idx, batch in enumerate(train_loader):
            # Move to device
            text_input = batch['text_input'].to(self.device)
            metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
            labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
            attention_mask = batch.get('attention_mask')
            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = self.model(text_input, metadata_input, attention_mask)
            
            # Compute losses for each task
            task_losses = {}
            task_accuracies = {}
            
            for task_name, logits in outputs.items():
                if task_name in labels:
                    task_loss = self.criterion(logits, labels[task_name])
                    task_losses[task_name] = task_loss
                    
                    # Compute accuracy
                    predictions = torch.argmax(logits, dim=1)
                    accuracy = (predictions == labels[task_name]).float().mean()
                    task_accuracies[task_name] = accuracy
                    
                    epoch_losses[task_name].append(task_loss.item())
                    epoch_accuracies[task_name].append(accuracy.item())
            
            # Compute weighted loss
            total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
            
            # Backward pass
            total_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # Log progress
            if batch_idx % 100 == 0:
                self.logger.info(f"Batch {batch_idx}/{total_batches} - Total Loss: {total_loss.item():.4f}")
                for task_name, loss in task_losses.items():
                    self.logger.info(f"  {task_name}: Loss={loss.item():.4f}, Acc={task_accuracies[task_name].item():.4f}")
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """
        Validate one epoch
        """
        self.model.eval()
        epoch_losses = defaultdict(list)
        epoch_accuracies = defaultdict(list)
        
        all_predictions = defaultdict(list)
        all_labels = defaultdict(list)
        
        with torch.no_grad():
            for batch in val_loader:
                # Move to device
                text_input = batch['text_input'].to(self.device)
                metadata_input = {k: v.to(self.device) for k, v in batch['metadata_input'].items()}
                labels = {k: v.to(self.device) for k, v in batch['labels'].items()}
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                # Forward pass
                outputs = self.model(text_input, metadata_input, attention_mask)
                
                # Compute losses and metrics
                for task_name, logits in outputs.items():
                    if task_name in labels:
                        task_loss = self.criterion(logits, labels[task_name])
                        epoch_losses[task_name].append(task_loss.item())
                        
                        # Predictions and accuracy
                        predictions = torch.argmax(logits, dim=1)
                        accuracy = (predictions == labels[task_name]).float().mean()
                        epoch_accuracies[task_name].append(accuracy.item())
                        
                        # Store for detailed metrics
                        all_predictions[task_name].extend(predictions.cpu().numpy())
                        all_labels[task_name].extend(labels[task_name].cpu().numpy())
        
        # Compute epoch averages
        epoch_results = {}
        for task_name in self.task_configs.keys():
            if task_name in epoch_losses:
                epoch_results[f"{task_name}_loss"] = np.mean(epoch_losses[task_name])
                epoch_results[f"{task_name}_accuracy"] = np.mean(epoch_accuracies[task_name])
                
                # Compute F1 score
                if task_name in all_predictions:
                    f1 = f1_score(all_labels[task_name], all_predictions[task_name], average='weighted')
                    epoch_results[f"{task_name}_f1"] = f1
        
        epoch_results["total_loss"] = sum(epoch_results[f"{task}_loss"] for task in self.task_configs.keys() if f"{task}_loss" in epoch_results)
        
        return epoch_results, all_predictions, all_labels
    
    def train(self, 
              train_loader: DataLoader, 
              val_loader: DataLoader,
              num_epochs: int = 50,
              learning_rate: float = 1e-3,
              weight_decay: float = 1e-5,
              patience: int = 10,
              save_best: bool = True) -> Dict[str, List[float]]:
        """
        Main training loop
        """
        # Optimizer and scheduler
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience//2, factor=0.5)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        self.logger.info(f"Starting training for {num_epochs} epochs")
        self.logger.info(f"Tasks: {list(self.task_configs.keys())}")
        self.logger.info(f"Device: {self.device}")
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            # Training
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Training...")
            train_results = self.train_epoch(train_loader, optimizer)
            
            # Validation
            self.logger.info(f"Epoch {epoch+1}/{num_epochs} - Validation...")
            val_results, val_predictions, val_labels = self.validate_epoch(val_loader)
            
            # Update learning rate
            scheduler.step(val_results['total_loss'])
            
            # Log results
            epoch_time = time.time() - start_time
            self.logger.info(f"Epoch {epoch+1} completed in {epoch_time:.2f}s")
            self.logger.info(f"Train Loss: {train_results['total_loss']:.4f}, Val Loss: {val_results['total_loss']:.4f}")
            
            for task_name in self.task_configs.keys():
                if f"{task_name}_accuracy" in train_results and f"{task_name}_accuracy" in val_results:
                    self.logger.info(f"  {task_name}: Train Acc={train_results[f'{task_name}_accuracy']:.4f}, "
                                   f"Val Acc={val_results[f'{task_name}_accuracy']:.4f}")
            
            # Save history
            for key, value in train_results.items():
                self.train_history[key].append(value)
            for key, value in val_results.items():
                self.val_history[key].append(value)
            
            # Early stopping and model saving
            if val_results['total_loss'] < best_val_loss:
                best_val_loss = val_results['total_loss']
                patience_counter = 0
                
                if save_best:
                    self.save_model(epoch, val_results, "best_model.pth")
                    self.logger.info(f"New best model saved with validation loss: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {patience} epochs without improvement")
                break
            
            # Save checkpoint
            if (epoch + 1) % 10 == 0:
                self.save_model(epoch, val_results, f"checkpoint_epoch_{epoch+1}.pth")
        
        # Save final model
        self.save_model(epoch, val_results, "final_model.pth")
        
        # Save training history
        self._save_training_history()
        
        # Generate training plots
        self._plot_training_history()
        
        return {
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
    
    def save_model(self, epoch: int, metrics: Dict[str, float], filename: str):
        """
        Save model checkpoint
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'task_configs': self.task_configs,
            'metrics': metrics,
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }
        
        # Save loss weighting parameters if using uncertainty method
        if hasattr(self.loss_weighting, 'log_vars'):
            checkpoint['loss_weighting_log_vars'] = self.loss_weighting.log_vars.data
        
        torch.save(checkpoint, self.save_dir / filename)
    
    def _save_training_history(self):
        """
        Save training history to JSON
        """
        history = {
            'train_history': {k: [float(x) for x in v] for k, v in self.train_history.items()},
            'val_history': {k: [float(x) for x in v] for k, v in self.val_history.items()}
        }
        
        with open(self.save_dir / "training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
    
    def _plot_training_history(self):
        """
        Plot training history
        """
        plt.style.use('seaborn-v0_8')
        
        # Plot losses
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training History', fontsize=16)
        
        # Total loss
        axes[0, 0].plot(self.train_history['total_loss'], label='Train')
        axes[0, 0].plot(self.val_history['total_loss'], label='Validation')
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Task-specific losses
        task_names = list(self.task_configs.keys())
        if len(task_names) > 0:
            for i, task_name in enumerate(task_names[:3]):  # Show first 3 tasks
                row = (i + 1) // 2
                col = (i + 1) % 2
                if row < 2 and col < 2:
                    train_key = f"{task_name}_loss"
                    val_key = f"{task_name}_loss"
                    if train_key in self.train_history and val_key in self.val_history:
                        axes[row, col].plot(self.train_history[train_key], label='Train')
                        axes[row, col].plot(self.val_history[val_key], label='Validation')
                        axes[row, col].set_title(f'{task_name} Loss')
                        axes[row, col].set_xlabel('Epoch')
                        axes[row, col].set_ylabel('Loss')
                        axes[row, col].legend()
                        axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_losses.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot accuracies
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Accuracies', fontsize=16)
        
        for i, task_name in enumerate(task_names[:4]):  # Show first 4 tasks
            row = i // 2
            col = i % 2
            train_key = f"{task_name}_accuracy"
            val_key = f"{task_name}_accuracy"
            
            if train_key in self.train_history and val_key in self.val_history:
                axes[row, col].plot(self.train_history[train_key], label='Train')
                axes[row, col].plot(self.val_history[val_key], label='Validation')
                axes[row, col].set_title(f'{task_name} Accuracy')
                axes[row, col].set_xlabel('Epoch')
                axes[row, col].set_ylabel('Accuracy')
                axes[row, col].legend()
                axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_accuracies.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("Training plots saved successfully")
    
    def train_step(self, text_input, metadata_input, targets, optimizer=None):
        """
        Perform a single training step
        Args:
            text_input: Text input tensor
            metadata_input: Metadata input dict
            targets: Target labels dict
            optimizer: Optional optimizer (creates AdamW if None)
        Returns:
            float: Total loss value
        """
        # Create optimizer if not provided
        if optimizer is None:
            optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        
        self.model.train()
        
        # Forward pass
        optimizer.zero_grad()
        outputs = self.model(text_input, metadata_input)
        
        # Compute losses for each task
        task_losses = {}
        for task_name, logits in outputs.items():
            if task_name in targets:
                task_loss = self.criterion(logits, targets[task_name])
                task_losses[task_name] = task_loss
        
        # Compute weighted loss
        total_loss, loss_weights = self.loss_weighting.compute_weighted_loss(task_losses, self.model)
        
        # Backward pass
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()

```

### backend\ai\testmodelAi\han_model_demo.py
```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST M√î H√åNH HAN - MINH H·ªåA KH·∫¢ NƒÇNG PH√ÇN LO·∫†I
(Kh√¥ng load model th·ª±c, ch·ªâ demo flow ho·∫°t ƒë·ªông)
"""

import os
from datetime import datetime

def simulate_han_model_prediction(commit_message):
    """
    M√¥ ph·ªèng k·∫øt qu·∫£ t·ª´ model HAN th·ª±c
    (Th·ª±c t·∫ø s·∫Ω load t·ª´ best_model.pth)
    """
    text = commit_message.lower()
    
    # M√¥ ph·ªèng logic ph√¢n lo·∫°i c·ªßa HAN model
    
    # 1. Commit Type Classification
    if any(word in text for word in ['feat:', 'feature:', 'add', 'implement', 'create']):
        commit_type = 'feat'
        type_confidence = 0.95
    elif any(word in text for word in ['fix:', 'bug:', 'resolve', 'patch']):
        commit_type = 'fix'  
        type_confidence = 0.92
    elif any(word in text for word in ['docs:', 'documentation', 'readme']):
        commit_type = 'docs'
        type_confidence = 0.89
    elif any(word in text for word in ['test:', 'testing', 'spec']):
        commit_type = 'test'
        type_confidence = 0.87
    elif any(word in text for word in ['refactor:', 'restructure', 'cleanup']):
        commit_type = 'refactor'
        type_confidence = 0.91
    elif any(word in text for word in ['chore:', 'update', 'dependency']):
        commit_type = 'chore'
        type_confidence = 0.88
    elif any(word in text for word in ['style:', 'format', 'lint']):
        commit_type = 'style'
        type_confidence = 0.86
    elif any(word in text for word in ['perf:', 'performance', 'optimize']):
        commit_type = 'perf'
        type_confidence = 0.93
    else:
        commit_type = 'other'
        type_confidence = 0.75
    
    # 2. Purpose Classification
    purpose_map = {
        'feat': 'Feature Implementation',
        'fix': 'Bug Fix',
        'docs': 'Documentation Update', 
        'test': 'Test Update',
        'refactor': 'Code Refactoring',
        'chore': 'Maintenance',
        'style': 'Code Style',
        'perf': 'Performance Improvement',
        'other': 'Other'
    }
    purpose = purpose_map.get(commit_type, 'Other')
    purpose_confidence = type_confidence - 0.03
    
    # 3. Sentiment Analysis
    if any(word in text for word in ['critical', 'urgent', 'emergency', 'severe']):
        sentiment = 'urgent'
        sentiment_confidence = 0.94
    elif any(word in text for word in ['error', 'bug', 'fail', 'problem']):
        sentiment = 'negative'
        sentiment_confidence = 0.88
    elif any(word in text for word in ['improve', 'enhance', 'optimize', 'add', 'new']):
        sentiment = 'positive'
        sentiment_confidence = 0.90
    else:
        sentiment = 'neutral'
        sentiment_confidence = 0.85
    
    # 4. Tech Tag Classification (m·ªü r·ªông)
    if any(word in text for word in ['auth', 'authentication', 'login', 'oauth']):
        tech_tag = 'authentication'
        tech_confidence = 0.92
    elif any(word in text for word in ['database', 'db', 'sql', 'query']):
        tech_tag = 'database'
        tech_confidence = 0.89
    elif any(word in text for word in ['api', 'endpoint', 'rest']):
        tech_tag = 'api'
        tech_confidence = 0.91
    elif any(word in text for word in ['ui', 'frontend', 'component']):
        tech_tag = 'frontend'
        tech_confidence = 0.87
    elif any(word in text for word in ['security', 'vulnerability', 'encryption']):
        tech_tag = 'security'
        tech_confidence = 0.95
    else:
        tech_tag = 'general'
        tech_confidence = 0.80
    
    return {
        'commit_type': {'label': commit_type, 'confidence': type_confidence},
        'purpose': {'label': purpose, 'confidence': purpose_confidence},
        'sentiment': {'label': sentiment, 'confidence': sentiment_confidence},
        'tech_tag': {'label': tech_tag, 'confidence': tech_confidence}
    }

def run_han_model_demo():
    """Demo kh·∫£ nƒÉng ph√¢n lo·∫°i c·ªßa model HAN v·ªõi ph√¢n t√≠ch chi ti·∫øt"""
    
    print("=" * 80)
    print("ü§ñ DEMO M√î H√åNH HAN - PH√ÇN T√çCH COMMIT CHI TI·∫æT")
    print("=" * 80)
    print(f"‚è∞ Th·ªùi gian demo: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("üìù L∆ØU √ù: Demo n√†y m√¥ ph·ªèng k·∫øt qu·∫£ t·ª´ model HAN th·ª±c")
    print("üîß Model th·ª±c ƒë∆∞·ª£c l∆∞u t·∫°i: models/han_github_model/best_model.pth")
    print()
    
    # Test cases ƒëa d·∫°ng v·ªõi 30 commits v√† t√°c gi·∫£
    test_commits = [
        # Developer 1: John Smith (Frontend specialist)
        ("john.smith@company.com", "feat: implement responsive navigation component"),
        ("john.smith@company.com", "feat: add dark mode toggle functionality"),
        ("john.smith@company.com", "fix: resolve mobile layout issues in header"),
        ("john.smith@company.com", "style: update CSS variables for consistent theming"),
        ("john.smith@company.com", "feat: create user profile management page"),
        
        # Developer 2: Sarah Johnson (Backend specialist)  
        ("sarah.johnson@company.com", "feat: implement user authentication API"),
        ("sarah.johnson@company.com", "fix: resolve database connection timeout issues"),
        ("sarah.johnson@company.com", "feat: add JWT token refresh mechanism"),
        ("sarah.johnson@company.com", "perf: optimize database queries for user search"),
        ("sarah.johnson@company.com", "fix: handle edge case in password validation"),
        ("sarah.johnson@company.com", "feat: implement role-based access control"),
        
        # Developer 3: Mike Chen (DevOps/Infrastructure)
        ("mike.chen@company.com", "chore: update Docker configuration for production"),
        ("mike.chen@company.com", "fix: resolve CI/CD pipeline deployment issues"),
        ("mike.chen@company.com", "chore: upgrade Node.js to version 18 LTS"),
        ("mike.chen@company.com", "perf: optimize build process with caching"),
        
        # Developer 4: Emily Davis (QA/Testing)
        ("emily.davis@company.com", "test: add unit tests for authentication service"),
        ("emily.davis@company.com", "test: implement integration tests for API endpoints"),
        ("emily.davis@company.com", "fix: correct test data setup for user scenarios"),
        ("emily.davis@company.com", "test: add end-to-end tests for login flow"),
        
        # Developer 5: Alex Rodriguez (Security specialist)
        ("alex.rodriguez@company.com", "fix(security): patch XSS vulnerability in user input"),
        ("alex.rodriguez@company.com", "feat(security): implement rate limiting for API"),
        ("alex.rodriguez@company.com", "fix(security): resolve CSRF token validation issue"),
        
        # Developer 6: Lisa Wang (Documentation)
        ("lisa.wang@company.com", "docs: update API documentation with new endpoints"),
        ("lisa.wang@company.com", "docs: add installation guide for development setup"),
        ("lisa.wang@company.com", "docs: create user manual for admin features"),
        
        # Developer 7: Tom Brown (Performance specialist)
        ("tom.brown@company.com", "perf: implement lazy loading for large datasets"),
        ("tom.brown@company.com", "perf: optimize image compression and caching"),
        ("tom.brown@company.com", "refactor: simplify complex rendering logic"),
        
        # Developer 8: Anna Kim (Junior developer - fewer commits)
        ("anna.kim@company.com", "fix: correct typo in error messages"),
        ("anna.kim@company.com", "style: fix indentation in configuration files")
    ]    
    print("üß™ B·∫ÆT ƒê·∫¶U DEMO V·ªöI 30 COMMIT MESSAGES")
    print("=" * 80)
    
    total_tests = len(test_commits)
    author_stats = {}
    commit_type_stats = {}
    purpose_stats = {}
    sentiment_stats = {}
    tech_tag_stats = {}
    
    for i, (author, commit_message) in enumerate(test_commits, 1):
        print(f"\nüîç DEMO #{i}")
        print("-" * 60)
        
        # Input
        print(f"üìù ƒê·∫¶U V√ÄO:")
        print(f"   Author: {author}")
        print(f"   Commit Message: '{commit_message}'")
        
        # Model prediction (simulated)
        predictions = simulate_han_model_prediction(commit_message)
        
        print(f"\nü§ñ K·∫æT QU·∫¢ T·ª™ MODEL HAN:")
        print(f"   üìã Commit Type: {predictions['commit_type']['label']} "
              f"(tin c·∫≠y: {predictions['commit_type']['confidence']:.0%})")
        print(f"   üéØ Purpose: {predictions['purpose']['label']} "
              f"(tin c·∫≠y: {predictions['purpose']['confidence']:.0%})")
        print(f"   üòä Sentiment: {predictions['sentiment']['label']} "
              f"(tin c·∫≠y: {predictions['sentiment']['confidence']:.0%})")
        print(f"   üè∑Ô∏è Tech Tag: {predictions['tech_tag']['label']} "
              f"(tin c·∫≠y: {predictions['tech_tag']['confidence']:.0%})")
        
        # Ph√¢n t√≠ch
        expected_type = commit_message.split(':')[0].split('(')[0]
        predicted_type = predictions['commit_type']['label']
        is_correct = expected_type.lower() == predicted_type.lower()
        
        print(f"\n‚úÖ PH√ÇN T√çCH:")
        print(f"   Expected: {expected_type}")
        print(f"   Predicted: {predicted_type}")
        print(f"   K·∫øt qu·∫£: {'‚úì CH√çNH X√ÅC' if is_correct else '‚úó SAI S√ìT'}")
        
        # Thu th·∫≠p th·ªëng k√™
        if author not in author_stats:
            author_stats[author] = {
                'total_commits': 0,
                'commit_types': {},
                'purposes': {},
                'sentiments': {},
                'tech_tags': {}
            }
        
        author_stats[author]['total_commits'] += 1
        
        # Th·ªëng k√™ theo lo·∫°i commit
        commit_type = predictions['commit_type']['label']
        author_stats[author]['commit_types'][commit_type] = author_stats[author]['commit_types'].get(commit_type, 0) + 1
        commit_type_stats[commit_type] = commit_type_stats.get(commit_type, 0) + 1
        
        # Th·ªëng k√™ theo purpose
        purpose = predictions['purpose']['label']
        author_stats[author]['purposes'][purpose] = author_stats[author]['purposes'].get(purpose, 0) + 1
        purpose_stats[purpose] = purpose_stats.get(purpose, 0) + 1
        
        # Th·ªëng k√™ theo sentiment
        sentiment = predictions['sentiment']['label']
        author_stats[author]['sentiments'][sentiment] = author_stats[author]['sentiments'].get(sentiment, 0) + 1
        sentiment_stats[sentiment] = sentiment_stats.get(sentiment, 0) + 1
        
        # Th·ªëng k√™ theo tech tag
        tech_tag = predictions['tech_tag']['label']
        author_stats[author]['tech_tags'][tech_tag] = author_stats[author]['tech_tags'].get(tech_tag, 0) + 1
        tech_tag_stats[tech_tag] = tech_tag_stats.get(tech_tag, 0) + 1
        
        print("-" * 60)
    
    # T·ªïng k·∫øt v√† ph√¢n t√≠ch chi ti·∫øt
    print(f"\nüìä T·ªîNG K·∫æT DEMO & PH√ÇN T√çCH CHI TI·∫æT")
    print("=" * 80)
    print(f"üî¢ T·ªïng s·ªë commits demo: {total_tests}")
    print(f"üë• T·ªïng s·ªë developers: {len(author_stats)}")
    print()
    
    # Ph√¢n t√≠ch theo t√°c gi·∫£
    print("üë§ PH√ÇN T√çCH THEO T√ÅC GI·∫¢:")
    print("=" * 60)
    
    # S·∫Øp x·∫øp theo s·ªë commit (t·ª´ nhi·ªÅu ƒë·∫øn √≠t)
    sorted_authors = sorted(author_stats.items(), key=lambda x: x[1]['total_commits'], reverse=True)
    
    for author, stats in sorted_authors:
        name = author.split('@')[0].replace('.', ' ').title()
        print(f"\nüßë‚Äçüíª {name} ({author})")
        print(f"   üìä T·ªïng commits: {stats['total_commits']}")
        
        # Top commit types
        top_commit_types = sorted(stats['commit_types'].items(), key=lambda x: x[1], reverse=True)
        print(f"   üè∑Ô∏è Commit types:")
        for commit_type, count in top_commit_types:
            percentage = (count / stats['total_commits']) * 100
            print(f"      ‚Ä¢ {commit_type}: {count} l·∫ßn ({percentage:.1f}%)")
        
        # Top purposes
        top_purposes = sorted(stats['purposes'].items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"   üéØ Top purposes:")
        for purpose, count in top_purposes:
            print(f"      ‚Ä¢ {purpose}: {count} l·∫ßn")
        
        # Dominant tech tags
        top_tech_tags = sorted(stats['tech_tags'].items(), key=lambda x: x[1], reverse=True)[:2]
        print(f"   üîß Tech focus:")
        for tech_tag, count in top_tech_tags:
            print(f"      ‚Ä¢ {tech_tag}: {count} l·∫ßn")
    
    print("\n" + "=" * 60)
    
    # Ph√¢n t√≠ch t·ªïng quan
    print("\nüìà TH·ªêNG K√ä T·ªîNG QUAN:")
    print("=" * 60)
    
    # Top commit types
    print("\nüè∑Ô∏è PH√ÇN B·ªê COMMIT TYPES:")
    sorted_commit_types = sorted(commit_type_stats.items(), key=lambda x: x[1], reverse=True)
    for commit_type, count in sorted_commit_types:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {commit_type}: {count} commits ({percentage:.1f}%)")
    
    # Top purposes
    print("\nüéØ PH√ÇN B·ªê PURPOSES:")
    sorted_purposes = sorted(purpose_stats.items(), key=lambda x: x[1], reverse=True)
    for purpose, count in sorted_purposes[:5]:  # Top 5
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {purpose}: {count} commits ({percentage:.1f}%)")
    
    # Sentiment analysis
    print("\nüòä PH√ÇN B·ªê SENTIMENT:")
    sorted_sentiments = sorted(sentiment_stats.items(), key=lambda x: x[1], reverse=True)
    for sentiment, count in sorted_sentiments:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {sentiment}: {count} commits ({percentage:.1f}%)")
    
    # Tech tags
    print("\nüîß PH√ÇN B·ªê TECH TAGS:")
    sorted_tech_tags = sorted(tech_tag_stats.items(), key=lambda x: x[1], reverse=True)
    for tech_tag, count in sorted_tech_tags:
        percentage = (count / total_tests) * 100
        print(f"   ‚Ä¢ {tech_tag}: {count} commits ({percentage:.1f}%)")
    
    # Insights v√† recommendations
    print("\nüí° INSIGHTS & NH·∫¨N X√âT:")
    print("=" * 60)
    
    # Developer v·ªõi nhi·ªÅu commits nh·∫•t
    most_active = sorted_authors[0]
    least_active = sorted_authors[-1]
    
    print(f"üèÜ Developer ho·∫°t ƒë·ªông nh·∫•t: {most_active[0].split('@')[0].replace('.', ' ').title()}")
    print(f"   ‚Ä¢ {most_active[1]['total_commits']} commits ({(most_active[1]['total_commits']/total_tests)*100:.1f}% t·ªïng commits)")
    
    print(f"\nüìâ Developer √≠t commits nh·∫•t: {least_active[0].split('@')[0].replace('.', ' ').title()}")
    print(f"   ‚Ä¢ {least_active[1]['total_commits']} commits ({(least_active[1]['total_commits']/total_tests)*100:.1f}% t·ªïng commits)")
    
    # Ph√¢n t√≠ch xu h∆∞·ªõng
    feat_count = commit_type_stats.get('feat', 0)
    fix_count = commit_type_stats.get('fix', 0)
    
    print(f"\nüîç Ph√¢n t√≠ch xu h∆∞·ªõng:")
    print(f"   ‚Ä¢ T·ª∑ l·ªá feat/fix: {feat_count}:{fix_count}")
    if feat_count > fix_count:
        print("   ‚Ä¢ Team ƒëang focus v√†o ph√°t tri·ªÉn t√≠nh nƒÉng m·ªõi")
    elif fix_count > feat_count:
        print("   ‚Ä¢ Team ƒëang focus v√†o s·ª≠a l·ªói v√† ·ªïn ƒë·ªãnh h·ªá th·ªëng")
    else:
        print("   ‚Ä¢ Team c√≥ s·ª± c√¢n b·∫±ng gi·ªØa ph√°t tri·ªÉn v√† maintenance")
    
    print(f"\nüìà Model HAN c√≥ th·ªÉ ph√¢n lo·∫°i: 4 tasks ƒë·ªìng th·ªùi")
    print(f"üéØ C√°c tasks:")
    print(f"   ‚Ä¢ Commit Type (feat, fix, docs, test, refactor, etc.)")
    print(f"   ‚Ä¢ Purpose (Feature Implementation, Bug Fix, etc.)")
    print(f"   ‚Ä¢ Sentiment (positive, negative, neutral, urgent)")
    print(f"   ‚Ä¢ Tech Tag (authentication, database, api, etc.)")
    print()
    print(f"‚ö° ∆Øu ƒëi·ªÉm c·ªßa Model HAN:")
    print(f"   ‚úì Multi-task learning (4 tasks c√πng l√∫c)")
    print(f"   ‚úì Hierarchical attention (word-level + sentence-level)")
    print(f"   ‚úì High accuracy tr√™n training data (~99%)")
    print(f"   ‚úì H·ªó tr·ª£ conventional commit format")
    print(f"   ‚úì Ph√¢n t√≠ch ƒë∆∞·ª£c patterns c·ªßa t·ª´ng developer")
    print()
    print(f"üîß S·ª≠ d·ª•ng Model th·ª±c:")
    print(f"   1. Load t·ª´: models/han_github_model/best_model.pth")
    print(f"   2. Thay th·∫ø simulate_han_model_prediction() b·∫±ng model th·ª±c")
    print(f"   3. S·ª≠ d·ª•ng tokenizer v√† label_encoders t·ª´ checkpoint")
    print(f"\nüéâ DEMO HO√ÄN TH√ÄNH!")
    print("=" * 80)
    
    # T·∫°o v√† l∆∞u b√°o c√°o chi ti·∫øt
    print(f"\nüìÑ T·∫†O B√ÅO C√ÅO CHI TI·∫æT...")
    detailed_report = generate_detailed_report(
        author_stats, commit_type_stats, purpose_stats, 
        sentiment_stats, tech_tag_stats, total_tests
    )
    
    # L∆∞u b√°o c√°o
    report_saved = save_analysis_report(detailed_report)
    
    if report_saved:
        print(f"‚úÖ B√°o c√°o ph√¢n t√≠ch ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
        print(f"üìä C√≥ th·ªÉ s·ª≠ d·ª•ng b√°o c√°o n√†y ƒë·ªÉ:")
        print(f"   ‚Ä¢ ƒê√°nh gi√° hi·ªáu su·∫•t team")
        print(f"   ‚Ä¢ Ph√¢n t√≠ch xu h∆∞·ªõng ph√°t tri·ªÉn")
        print(f"   ‚Ä¢ L·∫≠p k·∫ø ho·∫°ch ph√¢n c√¥ng c√¥ng vi·ªác")
        print(f"   ‚Ä¢ Training v√† mentoring developers")
    
    return detailed_report

def generate_detailed_report(author_stats, commit_type_stats, purpose_stats, sentiment_stats, tech_tag_stats, total_commits):
    """T·∫°o b√°o c√°o chi ti·∫øt v·ªÅ ph√¢n t√≠ch commits"""
    
    report = {
        'summary': {
            'total_commits': total_commits,
            'total_developers': len(author_stats),
            'analysis_date': datetime.now().isoformat()
        },
        'developer_analysis': {},
        'overall_statistics': {
            'commit_types': commit_type_stats,
            'purposes': purpose_stats,
            'sentiments': sentiment_stats,
            'tech_tags': tech_tag_stats
        },
        'insights': {}
    }
    
    # Ph√¢n t√≠ch chi ti·∫øt t·ª´ng developer
    sorted_authors = sorted(author_stats.items(), key=lambda x: x[1]['total_commits'], reverse=True)
    
    for author, stats in sorted_authors:
        name = author.split('@')[0].replace('.', ' ').title()
        
        # T√¨m commit type ch·ªß ƒë·∫°o
        main_commit_type = max(stats['commit_types'].items(), key=lambda x: x[1])
        
        # T√≠nh productivity score (commits per category diversity)
        diversity_score = len(stats['commit_types']) / len(commit_type_stats) * 100
        
        report['developer_analysis'][author] = {
            'name': name,
            'total_commits': stats['total_commits'],
            'commit_percentage': (stats['total_commits'] / total_commits) * 100,
            'main_commit_type': main_commit_type[0],
            'main_commit_type_count': main_commit_type[1],
            'diversity_score': diversity_score,
            'specialization': 'Specialist' if diversity_score < 40 else 'Generalist',
            'detailed_stats': stats
        }
    
    # Insights t·ªïng quan
    most_active = sorted_authors[0]
    least_active = sorted_authors[-1]
    feat_count = commit_type_stats.get('feat', 0)
    fix_count = commit_type_stats.get('fix', 0)
    
    report['insights'] = {
        'most_active_developer': {
            'email': most_active[0],
            'name': most_active[0].split('@')[0].replace('.', ' ').title(),
            'commits': most_active[1]['total_commits']
        },
        'least_active_developer': {
            'email': least_active[0],
            'name': least_active[0].split('@')[0].replace('.', ' ').title(),
            'commits': least_active[1]['total_commits']
        },
        'team_focus': 'Feature Development' if feat_count > fix_count else 'Bug Fixing' if fix_count > feat_count else 'Balanced',
        'feat_fix_ratio': f"{feat_count}:{fix_count}",
        'productivity_distribution': 'Balanced' if max(author_stats.values(), key=lambda x: x['total_commits'])['total_commits'] <= total_commits * 0.4 else 'Concentrated'
    }
    
    return report

def save_analysis_report(report, filename="commit_analysis_detailed_report.json"):
    """L∆∞u b√°o c√°o ph√¢n t√≠ch ra file JSON"""
    import json
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"üìÑ B√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u: {filename}")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u b√°o c√°o: {e}")
        return False

def main():
    """H√†m ch√≠nh"""
    try:
        detailed_report = run_han_model_demo()
        
        # Hi·ªÉn th·ªã m·ªôt s·ªë insights quan tr·ªçng
        print(f"\nüéØ INSIGHTS QUAN TR·ªåNG:")
        print(f"=" * 50)
        
        insights = detailed_report['insights']
        print(f"üëë Developer t√≠ch c·ª±c nh·∫•t: {insights['most_active_developer']['name']}")
        print(f"   ({insights['most_active_developer']['commits']} commits)")
        
        print(f"üìâ Developer √≠t commit nh·∫•t: {insights['least_active_developer']['name']}")
        print(f"   ({insights['least_active_developer']['commits']} commits)")
        
        print(f"üéØ Focus c·ªßa team: {insights['team_focus']}")
        print(f"‚öñÔ∏è T·ª∑ l·ªá feat/fix: {insights['feat_fix_ratio']}")
        print(f"üìä Ph√¢n b·ªë productivity: {insights['productivity_distribution']}")
        
        print(f"\nüíº G·ª¢I √ù QU·∫¢N L√ù TEAM:")
        print(f"=" * 50)
        
        # Ph√¢n t√≠ch v√† ƒë∆∞a ra g·ª£i √Ω
        dev_analysis = detailed_report['developer_analysis']
        specialists = [dev for dev in dev_analysis.values() if dev['specialization'] == 'Specialist']
        generalists = [dev for dev in dev_analysis.values() if dev['specialization'] == 'Generalist']
        
        print(f"üîß Specialists ({len(specialists)} ng∆∞·ªùi): Focus s√¢u v√†o 1-2 lƒ©nh v·ª±c")
        for dev in specialists[:3]:  # Top 3
            print(f"   ‚Ä¢ {dev['name']}: chuy√™n {dev['main_commit_type']} ({dev['main_commit_type_count']} commits)")
        
        print(f"üåê Generalists ({len(generalists)} ng∆∞·ªùi): ƒêa d·∫°ng nhi·ªÅu lƒ©nh v·ª±c")
        for dev in generalists[:3]:  # Top 3
            print(f"   ‚Ä¢ {dev['name']}: diversity score {dev['diversity_score']:.1f}%")
        
        return detailed_report
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y demo: {e}")
        return None

if __name__ == "__main__":
    main()

```

### backend\ai\testmodelAi\han_model_real_test_fixed.py
```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST M√î H√åNH HAN TH·ª∞C - S·ª¨ D·ª§NG MODEL ƒê√É TRAIN
"""

import os
import sys
import torch
import json
import numpy as np
from datetime import datetime
from pathlib import Path

# Add backend directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

# Import c√°c class c·∫ßn thi·∫øt t·ª´ train script
sys.path.insert(0, os.path.join(current_dir, '..', '..'))
from ai import train_han_github
from ai.train_han_github import SimpleHANModel, SimpleTokenizer

def load_han_model():
    """Load model HAN th·ª±c ƒë√£ train v·ªõi 100k+ commits"""
    
    model_path = Path(current_dir).parent / "models" / "han_github_model" / "best_model.pth"
    
    if not model_path.exists():
        print(f"‚ùå Model kh√¥ng t·ªìn t·∫°i: {model_path}")
        print("   C·∫ßn ch·∫°y script train tr∆∞·ªõc: python train_han_github.py")
        return None, None, None
    
    print(f"üì• Loading model t·ª´: {model_path}")
    
    try:
        # Load checkpoint v·ªõi weights_only=False ƒë·ªÉ c√≥ th·ªÉ load tokenizer
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # Extract model components
        tokenizer = checkpoint['tokenizer']
        label_encoders = checkpoint['label_encoders']
        model_state = checkpoint['model_state_dict']
        num_classes = checkpoint['num_classes']
        metadata = checkpoint['metadata']
        
        print(f"‚úÖ Model metadata:")
        print(f"   üìä Validation Accuracy: {checkpoint.get('val_accuracy', 'N/A'):.4f}")
        print(f"   üìà Training Loss: {checkpoint.get('train_loss', 'N/A'):.4f}")
        print(f"   üè∑Ô∏è Tasks: {list(num_classes.keys())}")
        print(f"   üìè Vocab Size: {len(tokenizer.word_to_idx)}")
        print(f"   üî¢ Model Parameters: {checkpoint.get('model_params', 'N/A'):,}")
        
        # Load model architecture
        model = SimpleHANModel(
            vocab_size=len(tokenizer.word_to_idx),
            embed_dim=100,
            hidden_dim=128,
            num_classes=num_classes
        )
        
        # Load trained weights
        model.load_state_dict(model_state)
        model.eval()
        
        print(f"üéØ Model loaded th√†nh c√¥ng!")
        return model, tokenizer, label_encoders
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def preprocess_commit_message(message, tokenizer, max_sentences=10, max_words=50):
    """Preprocess commit message theo format HAN"""
    # S·ª≠ d·ª•ng encode_text c·ªßa SimpleTokenizer ƒë·ªÉ l·∫•y token ids
    tokenized_sentences = tokenizer.encode_text(message, max_sentences, max_words)
    return torch.tensor([tokenized_sentences], dtype=torch.long)

def predict_with_real_model(model, tokenizer, label_encoders, commit_message):
    """D·ª± ƒëo√°n v·ªõi model HAN th·ª±c"""
    
    try:
        # Preprocess
        input_tensor = preprocess_commit_message(commit_message, tokenizer)
        
        # Predict
        with torch.no_grad():
            outputs = model(input_tensor)
        
        # Decode predictions
        predictions = {}
        
        for task, output in outputs.items():
            # Get prediction probabilities
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_idx = torch.max(probabilities, 1)
            
            # Decode label
            encoder_keys = list(label_encoders[task].keys())
            predicted_label = encoder_keys[predicted_idx.item()]
            confidence_score = confidence.item()
            
            predictions[task] = {
                'label': predicted_label,
                'confidence': confidence_score
            }
        
        return predictions
        
    except Exception as e:
        print(f"‚ùå L·ªói prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_real_han_test():
    """Test model HAN th·ª±c v·ªõi commits ƒëa d·∫°ng"""
    
    print("=" * 80)
    print("ü§ñ TEST M√î H√åNH HAN TH·ª∞C - MODEL ƒê√É TRAIN V·ªöI 100K+ COMMITS")
    print("=" * 80)
    print(f"‚è∞ Th·ªùi gian test: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Load model
    model, tokenizer, label_encoders = load_han_model()
    
    if model is None:
        return None
    
    print(f"üîç Available tasks: {list(label_encoders.keys())}")
    print(f"üìã Available labels per task:")
    for task, encoder in label_encoders.items():
        print(f"   {task}: {list(encoder.keys())}")
    print()
    
    # Test cases th·ª±c t·∫ø t·ª´ GitHub - gi·∫£m xu·ªëng 10 ƒë·ªÉ test nhanh
    test_commits = [
        ("real_user1@gmail.com", "feat: add user authentication with JWT tokens"),
        ("real_user2@github.com", "fix: resolve memory leak in image processing module"),
        ("dev.team@company.com", "docs: update API documentation for v2.0 endpoints"),
        ("qa.engineer@startup.io", "test: add unit tests for payment processing service"),
        ("senior.dev@bigtech.com", "refactor: simplify database connection pooling logic"),
        ("intern@company.com", "chore: update dependencies to latest versions"),
        ("designer@agency.com", "style: improve CSS styling for mobile responsiveness"),
        ("performance.eng@scale.com", "perf: optimize query performance for large datasets"),
        ("vn.dev@company.vn", "feat: th√™m t√≠nh nƒÉng ƒëƒÉng nh·∫≠p b·∫±ng Google OAuth"),
        ("security.expert@bank.com", "fix: patch critical XSS vulnerability in user input"),
    ]
    
    print("üß™ B·∫ÆT ƒê·∫¶U TEST V·ªöI MODEL TH·ª∞C")
    print("=" * 80)
    
    total_tests = len(test_commits)
    successful_predictions = 0
    
    # Statistics tracking
    prediction_stats = {task: {} for task in label_encoders.keys()}
    confidence_stats = {task: [] for task in label_encoders.keys()}
    
    for i, (author, commit_message) in enumerate(test_commits, 1):
        print(f"\nüîç TEST #{i}/{total_tests}")
        print("-" * 60)
        
        # Input
        print(f"üìù ƒê·∫¶U V√ÄO:")
        print(f"   Author: {author}")
        print(f"   Commit Message: '{commit_message}'")
        
        # Real model prediction
        predictions = predict_with_real_model(model, tokenizer, label_encoders, commit_message)
        
        if predictions:
            successful_predictions += 1
            
            print(f"\nü§ñ K·∫æT QU·∫¢ T·ª™ MODEL HAN TH·ª∞C:")
            
            for task, result in predictions.items():
                print(f"   üè∑Ô∏è {task.upper()}: {result['label']} "
                      f"(confidence: {result['confidence']:.3f})")
                
                # Collect statistics
                label = result['label']
                confidence = result['confidence']
                
                if label not in prediction_stats[task]:
                    prediction_stats[task][label] = 0
                prediction_stats[task][label] += 1
                confidence_stats[task].append(confidence)
            
            print(f"   ‚úÖ Prediction th√†nh c√¥ng")
        else:
            print(f"   ‚ùå Prediction th·∫•t b·∫°i")
        
        print("-" * 60)
    
    # Summary statistics
    print(f"\nüìä T·ªîNG K·∫æT TEST MODEL TH·ª∞C")
    print("=" * 80)
    print(f"üî¢ T·ªïng s·ªë test: {total_tests}")
    print(f"‚úÖ Predictions th√†nh c√¥ng: {successful_predictions}")
    print(f"üìà Success rate: {successful_predictions/total_tests*100:.1f}%")
    print()
    
    # Task-wise statistics
    print("üìã TH·ªêNG K√ä THEO TASK:")
    print("=" * 60)
    
    for task in label_encoders.keys():
        print(f"\nüè∑Ô∏è {task.upper()}:")
        
        # Label distribution
        if prediction_stats[task]:
            sorted_labels = sorted(prediction_stats[task].items(), 
                                 key=lambda x: x[1], reverse=True)
            print(f"   üìä Label distribution:")
            for label, count in sorted_labels:
                percentage = (count / successful_predictions) * 100
                print(f"      ‚Ä¢ {label}: {count} ({percentage:.1f}%)")
        
        # Confidence statistics
        if confidence_stats[task]:
            confidences = confidence_stats[task]
            avg_confidence = np.mean(confidences)
            min_confidence = np.min(confidences)
            max_confidence = np.max(confidences)
            
            print(f"   üéØ Confidence statistics:")
            print(f"      ‚Ä¢ Average: {avg_confidence:.3f}")
            print(f"      ‚Ä¢ Range: {min_confidence:.3f} - {max_confidence:.3f}")
            print(f"      ‚Ä¢ High confidence (>0.9): {len([c for c in confidences if c > 0.9])}")
            print(f"      ‚Ä¢ Low confidence (<0.7): {len([c for c in confidences if c < 0.7])}")
    
    # Model insights
    print(f"\nüí° INSIGHTS V·ªÄ MODEL:")
    print("=" * 60)
    
    # Task performance analysis
    for task in label_encoders.keys():
        if confidence_stats[task]:
            avg_conf = np.mean(confidence_stats[task])
            if avg_conf > 0.85:
                print(f"üéØ {task}: Hi·ªáu su·∫•t t·ªët (avg confidence: {avg_conf:.3f})")
            elif avg_conf > 0.7:
                print(f"‚ö†Ô∏è {task}: Hi·ªáu su·∫•t trung b√¨nh (avg confidence: {avg_conf:.3f})")
            else:
                print(f"‚ùå {task}: C·∫ßn c·∫£i thi·ªán (avg confidence: {avg_conf:.3f})")
    
    print(f"\n‚úÖ Model evaluation:")
    print(f"   ‚Ä¢ Model ƒë√£ ƒë∆∞·ª£c train v·ªõi dataset th·ª±c t·ª´ GitHub")
    print(f"   ‚Ä¢ C√≥ th·ªÉ ph√¢n lo·∫°i ƒë∆∞·ª£c {len(label_encoders)} tasks ƒë·ªìng th·ªùi")
    print(f"   ‚Ä¢ Ho·∫°t ƒë·ªông t·ªët v·ªõi conventional commit format")
    print(f"   ‚Ä¢ H·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát")
    
    # Save detailed results
    results = {
        'test_summary': {
            'total_tests': total_tests,
            'successful_predictions': successful_predictions,
            'success_rate': successful_predictions/total_tests,
            'test_date': datetime.now().isoformat()
        },
        'prediction_statistics': prediction_stats,
        'confidence_statistics': {
            task: {
                'average': float(np.mean(confidences)) if confidences else 0,
                'min': float(np.min(confidences)) if confidences else 0,
                'max': float(np.max(confidences)) if confidences else 0,
                'count': len(confidences)
            } for task, confidences in confidence_stats.items()
        },
        'model_info': {
            'vocab_size': len(tokenizer.word_to_idx) if tokenizer else 0,
            'tasks': list(label_encoders.keys()) if label_encoders else [],
            'available_labels': {
                task: list(encoder.keys()) 
                for task, encoder in label_encoders.items()
            } if label_encoders else {}
        }
    }
    
    print(f"\nüéâ TEST MODEL TH·ª∞C HO√ÄN TH√ÄNH!")
    print("=" * 80)
    
    return results

def main():
    """H√†m ch√≠nh"""
    try:
        results = run_real_han_test()
        
        if results:
            print(f"\nüéØ K·∫æT LU·∫¨N:")
            print(f"=" * 50)
            
            success_rate = results['test_summary']['success_rate']
            
            if success_rate >= 0.9:
                print(f"üåü Model ho·∫°t ƒë·ªông xu·∫•t s·∫Øc ({success_rate:.1%} success rate)")
            elif success_rate >= 0.7:
                print(f"‚úÖ Model ho·∫°t ƒë·ªông t·ªët ({success_rate:.1%} success rate)")
            else:
                print(f"‚ö†Ô∏è Model c·∫ßn c·∫£i thi·ªán ({success_rate:.1%} success rate)")
            
            print(f"\nüíº KHUY·∫æN NGH·ªä:")
            print(f"   ‚Ä¢ C√≥ th·ªÉ s·ª≠ d·ª•ng model n√†y cho production")
            print(f"   ‚Ä¢ Model support t·ªët conventional commits")
            print(f"   ‚Ä¢ Ph√π h·ª£p cho automated commit analysis")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

```

### backend\api\deps.py
```py
# KLTN04\backend\api\deps.py
# File ch·ª©a c√°c dependencies (ph·ª• thu·ªôc) chung c·ªßa ·ª©ng d·ª•ng

# Import AsyncSession t·ª´ SQLAlchemy ƒë·ªÉ l√†m vi·ªác v·ªõi database async
from sqlalchemy.ext.asyncio import AsyncSession

# Import k·∫øt n·ªëi database t·ª´ module database
from db.database import database

# Dependency (ph·ª• thu·ªôc) ƒë·ªÉ l·∫•y database session
async def get_db() -> AsyncSession:
    """
    Dependency t·∫°o v√† qu·∫£n l√Ω database session
    
    C√°ch ho·∫°t ƒë·ªông:
    - T·∫°o m·ªôt async session m·ªõi t·ª´ connection pool
    - Yield session ƒë·ªÉ s·ª≠ d·ª•ng trong request
    - ƒê·∫£m b·∫£o session ƒë∆∞·ª£c ƒë√≥ng sau khi request ho√†n th√†nh
    
    Returns:
        AsyncSession: Session database async ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi DB
    """
    # T·∫°o v√† qu·∫£n l√Ω session th√¥ng qua context manager
    async with database.session() as session:
        # Yield session ƒë·ªÉ s·ª≠ d·ª•ng trong route
        yield session
        # Session s·∫Ω t·ª± ƒë·ªông ƒë√≥ng khi ra kh·ªèi block with
```

### backend\api\__init__.py
```py

```

### backend\api\auth\middleware.py
```py

```

### backend\api\routes\ai.py
```py
# backend/api/routes/ai.py
"""
AI Routes - API endpoints for HAN-based AI features
Provides endpoints for commit analysis, task assignment, and project insights
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
import logging

from services.han_ai_service import get_han_ai_service
from services.commit_service import get_commits_by_repo
from services.repo_service import get_repo_by_owner_and_name

logger = logging.getLogger(__name__)

ai_router = APIRouter(prefix="/ai", tags=["AI & Machine Learning"])

# Pydantic models for request/response
class CommitAnalysisRequest(BaseModel):
    message: str = Field(..., description="Commit message to analyze")

class BatchCommitAnalysisRequest(BaseModel):
    messages: List[str] = Field(..., description="List of commit messages to analyze")

class CommitAnalysisResponse(BaseModel):
    success: bool
    message: str
    analysis: Dict[str, Any]
    model_version: Optional[str] = None
    confidence: Optional[Dict[str, float]] = None

class TaskAssignmentRequest(BaseModel):
    tasks: List[Dict[str, Any]] = Field(..., description="List of tasks to assign")
    developers: List[Dict[str, Any]] = Field(..., description="List of available developers")

class ProjectInsightsRequest(BaseModel):
    project_data: Dict[str, Any] = Field(..., description="Project data for analysis")

class DeveloperPattern(BaseModel):
    developer: str
    commits: List[str]

class DeveloperPatternsRequest(BaseModel):
    developer_commits: List[DeveloperPattern] = Field(..., description="Developer commit patterns")

# AI Analysis Endpoints
@ai_router.post("/analyze-commit", response_model=CommitAnalysisResponse)
async def analyze_commit(request: CommitAnalysisRequest):
    """
    Analyze a single commit message using HAN model
    
    Provides:
    - Commit category classification (bug, feature, docs, etc.)
    - Impact assessment (low, medium, high)
    - Urgency evaluation (low, medium, high)
    - Confidence scores for predictions
    """
    try:
        ai_service = get_han_ai_service()
        result = await ai_service.analyze_commit_message(request.message)
        
        return CommitAnalysisResponse(**result)
        
    except Exception as e:
        logger.error(f"Error analyzing commit: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@ai_router.post("/analyze-commits-batch")
async def analyze_commits_batch(request: BatchCommitAnalysisRequest):
    """
    Analyze multiple commit messages in batch
    
    Provides:
    - Batch analysis of all commits
    - Statistical summary of commit patterns
    - Distribution of categories, impacts, and urgencies
    """
    try:
        ai_service = get_han_ai_service()
        result = await ai_service.analyze_commits_batch(request.messages)
        
        return {
            "success": True,
            "data": result,
            "message": f"Analyzed {len(request.messages)} commits successfully"
        }
        
    except Exception as e:
        logger.error(f"Error in batch analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Batch analysis failed: {str(e)}")

@ai_router.get("/analyze-repo/{owner}/{repo}")
async def analyze_repository_commits(
    owner: str, 
    repo: str,
    limit: int = 100
):
    """
    Analyze all commits in a repository
    
    Provides:
    - Complete commit analysis for the repository
    - Repository-specific insights and patterns
    - Quality metrics and trends
    """
    try:
        # Get commits from database
        commits = await get_commits_by_repo(owner, repo, limit)
        
        if not commits:
            raise HTTPException(status_code=404, detail="No commits found for this repository")
        
        # Extract commit messages
        commit_messages = [commit.message for commit in commits if commit.message]
        
        ai_service = get_han_ai_service()
        result = await ai_service.analyze_commits_batch(commit_messages)
        
        # Add repository context
        result['repository'] = f"{owner}/{repo}"
        result['analyzed_commits'] = len(commit_messages)
        
        return {
            "success": True,
            "data": result,
            "message": f"Repository {owner}/{repo} analysis completed"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing repository: {e}")
        raise HTTPException(status_code=500, detail=f"Repository analysis failed: {str(e)}")

# Developer Analysis Endpoints
@ai_router.post("/analyze-developer-patterns")
async def analyze_developer_patterns(request: DeveloperPatternsRequest):
    """
    Analyze commit patterns for developers
    
    Provides:
    - Individual developer profiles based on commit history
    - Specialization identification (bug-fixer, feature-developer, etc.)
    - Activity and expertise scores
    """
    try:
        # Convert request to expected format
        developer_commits = {
            pattern.developer: pattern.commits 
            for pattern in request.developer_commits
        }
        
        ai_service = get_han_ai_service()
        result = await ai_service.analyze_developer_patterns(developer_commits)
        
        return {
            "success": True,
            "data": result,
            "message": f"Analyzed patterns for {len(developer_commits)} developers"
        }
        
    except Exception as e:
        logger.error(f"Error analyzing developer patterns: {e}")
        raise HTTPException(status_code=500, detail=f"Developer analysis failed: {str(e)}")

@ai_router.get("/developer-profile/{owner}/{repo}/{developer}")
async def get_developer_profile(owner: str, repo: str, developer: str):
    """
    Get AI-generated profile for a specific developer in a repository
    
    Provides:
    - Developer specialization analysis
    - Commit pattern insights
    - Productivity and focus area recommendations
    """
    try:
        # Get commits by this developer
        commits = await get_commits_by_repo(owner, repo, limit=200)
        developer_commits = [
            commit.message for commit in commits 
            if commit.author == developer and commit.message
        ]
        
        if not developer_commits:
            raise HTTPException(status_code=404, detail=f"No commits found for developer {developer}")
        
        ai_service = get_han_ai_service()
        result = await ai_service.analyze_developer_patterns({developer: developer_commits})
        
        return {
            "success": True,
            "developer": developer,
            "repository": f"{owner}/{repo}",
            "profile": result['developer_profiles'].get(developer, {}),
            "commit_count": len(developer_commits)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting developer profile: {e}")
        raise HTTPException(status_code=500, detail=f"Profile generation failed: {str(e)}")

# Task Management Endpoints
@ai_router.post("/suggest-task-assignment")
async def suggest_task_assignment(request: TaskAssignmentRequest):
    """
    Suggest optimal task assignments based on AI analysis
    
    Provides:
    - AI-powered task-to-developer matching
    - Confidence scores for assignments
    - Reasoning behind each recommendation
    """
    try:
        ai_service = get_han_ai_service()
        result = await ai_service.suggest_task_assignment(request.tasks, request.developers)
        
        return {
            "success": True,
            "data": result,
            "message": f"Generated assignments for {len(request.tasks)} tasks"
        }
        
    except Exception as e:
        logger.error(f"Error in task assignment: {e}")
        raise HTTPException(status_code=500, detail=f"Task assignment failed: {str(e)}")

@ai_router.get("/recommend-tasks/{owner}/{repo}")
async def recommend_tasks_for_repo(owner: str, repo: str):
    """
    Recommend task priorities and assignments for a repository
    
    Provides:
    - Repository-specific task recommendations
    - Priority suggestions based on commit analysis
    - Developer assignment recommendations
    """
    try:
        # Get repository data
        repo_data = await get_repo_by_owner_and_name(owner, repo)
        if not repo_data:
            raise HTTPException(status_code=404, detail="Repository not found")
        
        # Get recent commits for analysis
        commits = await get_commits_by_repo(owner, repo, limit=100)
        
        # Create project data structure
        project_data = {
            'name': f"{owner}/{repo}",
            'commits': [{'message': c.message, 'author': c.author} for c in commits],
            'contributors': list(set(c.author for c in commits if c.author))
        }
        
        ai_service = get_han_ai_service()
        insights = await ai_service.generate_project_insights(project_data)
        
        return {
            "success": True,
            "repository": f"{owner}/{repo}",
            "recommendations": insights,
            "message": "Task recommendations generated successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating task recommendations: {e}")
        raise HTTPException(status_code=500, detail=f"Recommendation generation failed: {str(e)}")

# Project Insights Endpoints
@ai_router.post("/project-insights")
async def generate_project_insights(request: ProjectInsightsRequest):
    """
    Generate comprehensive AI-powered project insights
    
    Provides:
    - Overall project health assessment
    - Code quality trends analysis
    - Team collaboration patterns
    - Actionable recommendations
    """
    try:
        ai_service = get_han_ai_service()
        result = await ai_service.generate_project_insights(request.project_data)
        
        return {
            "success": True,
            "data": result,
            "message": "Project insights generated successfully"
        }
        
    except Exception as e:
        logger.error(f"Error generating project insights: {e}")
        raise HTTPException(status_code=500, detail=f"Insight generation failed: {str(e)}")

@ai_router.get("/insights/{owner}/{repo}")
async def get_repository_insights(owner: str, repo: str):
    """
    Get comprehensive insights for a specific repository
    
    Provides:
    - Repository health metrics
    - Development pattern analysis
    - Quality assessment and recommendations
    - Team productivity insights
    """
    try:
        # Get repository and commit data
        commits = await get_commits_by_repo(owner, repo, limit=200)
        
        if not commits:
            raise HTTPException(status_code=404, detail="No data found for analysis")
        
        # Prepare project data
        commit_data = []
        contributors = set()
        
        for commit in commits:
            if commit.message:
                commit_data.append({
                    'message': commit.message,
                    'author': commit.author,
                    'sha': commit.sha,
                    'date': commit.date.isoformat() if commit.date else None
                })
                if commit.author:
                    contributors.add(commit.author)
        
        project_data = {
            'name': f"{owner}/{repo}",
            'commits': commit_data,
            'contributors': list(contributors)
        }
        
        ai_service = get_han_ai_service()
        insights = await ai_service.generate_project_insights(project_data)
        
        return {
            "success": True,
            "repository": f"{owner}/{repo}",
            "total_commits_analyzed": len(commit_data),
            "total_contributors": len(contributors),
            "insights": insights,
            "message": "Repository insights generated successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting repository insights: {e}")
        raise HTTPException(status_code=500, detail=f"Insight generation failed: {str(e)}")

# Model Management Endpoints
@ai_router.get("/model/status")
async def get_model_status():
    """
    Get current status of the HAN AI model
    
    Provides:
    - Model loading status
    - Version information
    - Performance metrics
    """
    try:
        ai_service = get_han_ai_service()
        
        return {
            "success": True,
            "model_loaded": ai_service.is_model_loaded,
            "model_type": "HAN (Hierarchical Attention Network)",
            "version": "1.0",
            "capabilities": [
                "Commit message classification",
                "Impact assessment", 
                "Urgency evaluation",
                "Developer pattern analysis",
                "Task assignment suggestions",
                "Project insights generation"
            ]
        }
        
    except Exception as e:
        logger.error(f"Error getting model status: {e}")
        raise HTTPException(status_code=500, detail=f"Status check failed: {str(e)}")

@ai_router.post("/model/warm-up")
async def warm_up_model():
    """
    Warm up the HAN model for faster subsequent predictions
    """
    try:
        ai_service = get_han_ai_service()
        
        # Run a test prediction to warm up the model
        test_result = await ai_service.analyze_commit_message("feat: add new feature for testing")
        
        return {
            "success": True,
            "message": "Model warmed up successfully",
            "test_prediction": test_result.get('analysis', {})
        }
        
    except Exception as e:
        logger.error(f"Error warming up model: {e}")
        raise HTTPException(status_code=500, detail=f"Model warm-up failed: {str(e)}")

# Health check endpoint
@ai_router.get("/health")
async def ai_health_check():
    """
    Health check for AI services
    """
    try:
        ai_service = get_han_ai_service()
        
        return {
            "status": "healthy",
            "model_available": ai_service.is_model_loaded,
            "service": "HAN AI Service",
            "endpoints_available": 12
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "service": "HAN AI Service"
        }

```

### backend\api\routes\auth.py
```py
# KLTN04\backend\api\routes\auth.py
from fastapi import APIRouter, Request, HTTPException
from core.oauth import oauth
from fastapi.responses import RedirectResponse
from services.user_service import save_user  # Import h√†m l∆∞u ng∆∞·ªùi d√πng
import os

auth_router = APIRouter()

# Endpoint /login ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh x√°c th·ª±c v·ªõi GitHub
@auth_router.get("/login")
async def login(request: Request):
    # L·∫•y callback URL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    redirect_uri = os.getenv("GITHUB_CALLBACK_URL")
    
    # Chuy·ªÉn h∆∞·ªõng ng∆∞·ªùi d√πng ƒë·∫øn trang x√°c th·ª±c GitHub
    return await oauth.github.authorize_redirect(request, redirect_uri)

# Endpoint /auth/callback - GitHub s·∫Ω g·ªçi l·∫°i endpoint n√†y sau khi x√°c th·ª±c th√†nh c√¥ng
@auth_router.get("/auth/callback")
async def auth_callback(request: Request):
    code = request.query_params.get("code")
    if not code:
        raise HTTPException(status_code=400, detail="Missing code")

    # L·∫•y access token t·ª´ GitHub
    token = await oauth.github.authorize_access_token(request)
    
    # G·ªçi API GitHub ƒë·ªÉ l·∫•y th√¥ng tin user c∆° b·∫£n
    resp = await oauth.github.get("user", token=token)
    profile = resp.json()  # Chuy·ªÉn response th√†nh dictionary

    # L·∫•y email n·∫øu kh√¥ng c√≥ trong profile
    if not profile.get("email"):
        # G·ªçi API ri√™ng ƒë·ªÉ l·∫•y danh s√°ch email
        emails_resp = await oauth.github.get("user/emails", token=token)
        emails = emails_resp.json()
        
        # T√¨m email ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† primary (ch√≠nh)
        primary_email = next((e["email"] for e in emails if e["primary"]), None)
        
        # G√°n email ch√≠nh v√†o profile
        profile["email"] = primary_email

    # Ki·ªÉm tra th√¥ng tin b·∫Øt bu·ªôc
    if not profile.get("email") or not profile.get("login"):
        raise HTTPException(status_code=400, detail="Missing required user information")

    # L∆∞u th√¥ng tin ng∆∞·ªùi d√πng v√†o c∆° s·ªü d·ªØ li·ªáu
    user_data = {
        "github_id": profile["id"],
        "github_username": profile["login"],
        "email": profile["email"],
        "avatar_url": profile["avatar_url"],
    }
    await save_user(user_data)

    # Redirect v·ªÅ frontend v·ªõi token v√† th√¥ng tin ng∆∞·ªùi d√πng
    redirect_url = (
        f"http://localhost:5173/auth-success"
        f"?token={token['access_token']}"
        f"&username={profile['login']}"
        f"&email={profile['email']}"
        f"&avatar_url={profile['avatar_url']}"
    )

    # Th·ª±c hi·ªán chuy·ªÉn h∆∞·ªõng v·ªÅ frontend
    return RedirectResponse(redirect_url)
```

### backend\api\routes\branch.py
```py
# backend/api/routes/branch.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.branch_service import save_branch
from services.repo_service import get_repo_id_by_owner_and_name

branch_router = APIRouter()

@branch_router.get("/github/{owner}/{repo}/branches")
async def get_branches(owner: str, repo: str, request: Request):
    # L·∫•y token t·ª´ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # G·ªçi GitHub API l·∫•y danh s√°ch branch
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

# L∆∞u branch v√†o database
@branch_router.post("/github/{owner}/{repo}/save-branches")
async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # L·∫•y danh s√°ch branch t·ª´ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()

    # L∆∞u branch v√†o database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    saved_count = 0
    for branch in branches:
        try:
            branch_data = {
                "name": branch["name"],
                "repo_id": repo_id,
            }
            await save_branch(branch_data)
            saved_count += 1
        except Exception as e:
            print(f"L·ªói khi l∆∞u branch {branch['name']}: {e}")
            continue

    return {"message": f"ƒê√£ l∆∞u {saved_count}/{len(branches)} branches!"}

```

### backend\api\routes\commit.py
```py
# backend/api/routes/commit.py
from fastapi import APIRouter, Request, HTTPException, Depends
import httpx
from services.commit_service import save_commit
from services.repo_service import get_repo_id_by_owner_and_name
from services.github_service import fetch_commits
from sqlalchemy.future import select
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
from db.models.commits import commits
from db.models.repositories import repositories
from db.database import database
from schemas.commit import CommitCreate, CommitOut

commit_router = APIRouter()

def get_db():
    return database

# Endpoint l·∫•y danh s√°ch commit c·ªßa m·ªôt repository
@commit_router.get("/github/{owner}/{repo}/commits")
async def get_commits(owner: str, repo: str, request: Request, branch: str = "main"):
    # L·∫•y token t·ª´ header
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # G·ªçi GitHub API ƒë·ªÉ l·∫•y commit
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}"
        headers = {"Authorization": token}

        resp = await client.get(url, headers=headers)
        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p repository tr·ªëng (409)
        if resp.status_code == 409:
            return []
        # X·ª≠ l√Ω l·ªói kh√°c
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        return resp.json()

# L·∫•y danh s√°ch commit t·ª´ database
@commit_router.get("/github/{owner}/{repo}/commits/db")
async def get_commits_from_db(owner: str, repo: str, db: AsyncSession = Depends(get_db)):
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    query = select(commits).where(commits.c.repo_id == repo_id)
    result = await db.fetch_all(query)
    return result

# Endpoint l∆∞u commit v√†o database
@commit_router.post("/github/{owner}/{repo}/save-commits")
async def save_repo_commits(owner: str, repo: str, request: Request, branch: str = None, limit: int = 50):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # N·∫øu kh√¥ng truy·ªÅn branch, l·∫•y branch m·∫∑c ƒë·ªãnh t·ª´ GitHub
    if not branch:
        async with httpx.AsyncClient() as client:
            repo_url = f"https://api.github.com/repos/{owner}/{repo}"
            headers = {"Authorization": token}
            repo_resp = await client.get(repo_url, headers=headers)
            if repo_resp.status_code != 200:
                raise HTTPException(status_code=repo_resp.status_code, detail=repo_resp.text)
            repo_data = repo_resp.json()
            branch = repo_data.get("default_branch", "main")

    # L·∫•y danh s√°ch commit t·ª´ GitHub API (gi·ªõi h·∫°n s·ªë l∆∞·ª£ng)
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}&per_page={limit}"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        commit_list = resp.json()

    # L·∫•y repo_id t·ª´ c∆° s·ªü d·ªØ li·ªáu
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    # L∆∞u t·ª´ng commit v√†o c∆° s·ªü d·ªØ li·ªáu (song song ƒë·ªÉ tƒÉng t·ªëc)
    saved_commits = 0
    async with httpx.AsyncClient() as client:
        for commit in commit_list[:limit]:  # Gi·ªõi h·∫°n th√™m m·ªôt l·∫ßn n·ªØa
            try:
                # L·∫•y th√¥ng tin chi ti·∫øt c·ªßa commit
                commit_url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit['sha']}"
                commit_resp = await client.get(commit_url, headers={"Authorization": token})
                if commit_resp.status_code != 200:
                    continue  # B·ªè qua commit n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin chi ti·∫øt

                commit_details = commit_resp.json()
                stats = commit_details.get("stats", {})
                commit_data = {
                    "sha": commit["sha"],
                    "message": commit["commit"]["message"],
                    "author_name": commit["commit"]["author"]["name"],
                    "author_email": commit["commit"]["author"]["email"],
                    "date": datetime.strptime(commit["commit"]["author"]["date"], "%Y-%m-%dT%H:%M:%SZ"),
                    "insertions": stats.get("additions", 0),
                    "deletions": stats.get("deletions", 0),
                    "files_changed": stats.get("total", 0),
                    "repo_id": repo_id,
                }
                await save_commit(commit_data)
                saved_commits += 1
            except Exception as e:
                print(f"L·ªói khi l∆∞u commit {commit['sha']}: {e}")
                continue

    return {"message": f"ƒê√£ l∆∞u {saved_commits}/{len(commit_list)} commits!"}

# Endpoint l·∫•y t·∫•t c·∫£ commit t·ª´ database
@commit_router.get("/commits")
async def get_all_commits(db = Depends(get_db)):
    query = commits.select()  # L·∫•y t·∫•t c·∫£ commit
    result = await db.fetch_all(query)
    return result

# Endpoint ƒë·ªìng b·ªô commit t·ª´ GitHub v·ªÅ database
@commit_router.get("/sync-commits")
async def sync_commits(
    repo_id: int,
    branch: str = "main",
    since: str = None,
    until: str = None,
    db: AsyncSession = Depends(get_db)
):
    # 1. L·∫•y th√¥ng tin repository t·ª´ database
    repo = await db.scalar(select(repositories).where(repositories.c.id == repo_id))
    if not repo:
        raise HTTPException(status_code=404, detail="Repository kh√¥ng t·ªìn t·∫°i")

    # 2. G·ªçi GitHub API l·∫•y commit v·ªõi c√°c tham s·ªë l·ªçc
    commits_data = await fetch_commits(
        token=repo.token,  # Access token
        owner=repo.owner,  # Ch·ªß repository
        name=repo.name,  # T√™n repository
        branch=branch,  # Branch c·∫ßn l·∫•y
        since=since,  # L·ªçc t·ª´ th·ªùi gian
        until=until  # L·ªçc ƒë·∫øn th·ªùi gian
    )

    # 3. L∆∞u commit m·ªõi v√†o database
    new_commits = []
    for item in commits_data:
        sha = item["sha"]
        # Ki·ªÉm tra commit ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing = await db.scalar(select(commits).where(commits.c.sha == sha))
        if existing:
            continue  # B·ªè qua n·∫øu ƒë√£ t·ªìn t·∫°i

        # T·∫°o commit m·ªõi
        new_commit = CommitCreate(
            sha=sha,
            message=item["commit"]["message"],
            author=item["commit"]["author"]["name"],
            date=item["commit"]["author"]["date"],
            repository_id=repo.id
        )
        commit_obj = commits.insert().values(**new_commit.dict())
        await db.execute(commit_obj)
        new_commits.append(new_commit)

    await db.commit()

    return {
        "message": f"ƒê·ªìng b·ªô th√†nh c√¥ng {len(new_commits)} commit.",
        "data": [c.sha for c in new_commits]
    }

```

### backend\api\routes\commit_routes.py
```py
# File: backend/api/routes/commit_routes.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Header
from fastapi.responses import JSONResponse
from typing import List, Optional
import pandas as pd
from services.model_loader import predict_commit
from pathlib import Path
import tempfile
import httpx

router = APIRouter(prefix="/api/commits", tags=["Commit Analysis"])

@router.get("/analyze-github/{owner}/{repo}")
async def analyze_github_commits(
    owner: str,
    repo: str,
    authorization: str = Header(..., alias="Authorization"),
    per_page: int = 30,
    since: Optional[str] = None,
    until: Optional[str] = None
):
    """
    Ph√¢n t√≠ch commit t·ª´ repository GitHub
    
    Args:
        owner: T√™n ch·ªß repo
        repo: T√™n repository
        authorization: Token GitHub (Format: Bearer <token>)
        per_page: S·ªë commit t·ªëi ƒëa c·∫ßn ph√¢n t√≠ch (1-100)
        since: L·ªçc commit t·ª´ ng√†y (YYYY-MM-DDTHH:MM:SSZ)
        until: L·ªçc commit ƒë·∫øn ng√†y (YYYY-MM-DDTHH:MM:SSZ)
    
    Returns:
        {
            "repo": f"{owner}/{repo}",
            "total": int,
            "critical": int,
            "critical_percentage": float,
            "details": List[dict],
            "analysis_date": str
        }
    """
    try:
        # Validate input
        if per_page < 1 or per_page > 100:
            raise HTTPException(
                status_code=400,
                detail="per_page must be between 1 and 100"
            )

        # Configure GitHub API request
        headers = {
            "Authorization": authorization,
            "Accept": "application/vnd.github.v3+json"
        }
        params = {
            "per_page": per_page,
            "since": since,
            "until": until
        }
        
        # Fetch commits from GitHub
        async with httpx.AsyncClient() as client:
            # Get first page to check repo accessibility
            initial_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(initial_url, headers=headers, params={**params, "per_page": 1})
            
            if response.status_code == 404:
                raise HTTPException(
                    status_code=404,
                    detail="Repository not found or access denied"
                )
            response.raise_for_status()

            # Get all requested commits
            full_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
            response = await client.get(full_url, headers=headers, params=params)
            response.raise_for_status()
            commits_data = response.json()

        # Prepare analysis data
        commits_for_analysis = [
            {
                "id": commit["sha"],
                "message": commit["commit"]["message"],
                "date": commit["commit"]["committer"]["date"] if commit["commit"]["committer"] else None
            }
            for commit in commits_data
            if commit.get("sha") and commit.get("commit", {}).get("message")
        ]

        # Analyze commits
        results = {
            "repo": f"{owner}/{repo}",
            "total": len(commits_for_analysis),
            "critical": 0,
            "critical_percentage": 0.0,
            "details": [],
            "analysis_date": datetime.utcnow().isoformat()
        }

        for commit in commits_for_analysis:
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
            
            results["details"].append({
                "id": commit["id"],
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message'],
                "date": commit["date"]
            })

        # Calculate percentage
        if results["total"] > 0:
            results["critical_percentage"] = round(
                (results["critical"] / results["total"]) * 100, 2
            )

        return results

    except httpx.HTTPStatusError as e:
        error_detail = "GitHub API error"
        if e.response.status_code == 403:
            error_detail = "API rate limit exceeded" if "rate limit" in str(e.response.content) else "Forbidden"
        elif e.response.status_code == 401:
            error_detail = "Invalid GitHub token"
        
        raise HTTPException(
            status_code=e.response.status_code,
            detail=f"{error_detail}: {e.response.text}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing GitHub commits: {str(e)}"
        )
@router.post("/analyze-text")
async def analyze_commit_text(message: str):
    """
    Ph√¢n t√≠ch m·ªôt commit message d·∫°ng text
    
    Args:
        message: N·ªôi dung commit message
    
    Returns:
        {"is_critical": 0|1, "message": string}
    """
    try:
        is_critical = predict_commit(message)
        return {
            "is_critical": is_critical,
            "message": "Ph√¢n t√≠ch th√†nh c√¥ng",
            "input_sample": message[:100] + "..." if len(message) > 100 else message
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch: {str(e)}")

@router.post("/analyze-json")
async def analyze_commits_json(commits: List[dict]):
    """
    Ph√¢n t√≠ch nhi·ªÅu commit t·ª´ JSON
    
    Args:
        commits: List[{"id": string, "message": string}]
    
    Returns:
        {"total": int, "critical": int, "details": List[dict]}
    """
    try:
        results = {
            "total": len(commits),
            "critical": 0,
            "details": []
        }
        
        for commit in commits:
            if not isinstance(commit, dict) or 'message' not in commit:
                continue
                
            is_critical = predict_commit(commit['message'])
            if is_critical:
                results["critical"] += 1
                
            results["details"].append({
                "id": commit.get("id", ""),
                "is_critical": is_critical,
                "message_preview": commit['message'][:100] + "..." if len(commit['message']) > 100 else commit['message']
            })
            
        return JSONResponse(content=results)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch h√†ng lo·∫°t: {str(e)}")

@router.post("/analyze-csv", response_model=dict)
async def analyze_commits_csv(file: UploadFile = File(...)):
    """
    Ph√¢n t√≠ch commit t·ª´ file CSV
    
    Args:
        file: File CSV c√≥ c·ªôt 'message' ho·∫∑c 'commit_message'
    
    Returns:
        {"filename": string, "total": int, "critical": int}
    """
    try:
        # L∆∞u file t·∫°m
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(await file.read())
            tmp_path = Path(tmp.name)
        
        # ƒê·ªçc file CSV
        df = pd.read_csv(tmp_path)
        tmp_path.unlink()  # X√≥a file t·∫°m
        
        # Ki·ªÉm tra c·ªôt message
        message_col = 'message' if 'message' in df.columns else 'commit_message'
        if message_col not in df.columns:
            raise HTTPException(status_code=400, detail="File thi·∫øu c·ªôt 'message' ho·∫∑c 'commit_message'")
        
        # Ph√¢n t√≠ch
        results = {
            "filename": file.filename,
            "total": len(df),
            "critical": 0,
            "sample_results": []
        }
        
        df['is_critical'] = df[message_col].apply(predict_commit)
        results["critical"] = int(df['is_critical'].sum())
        
        # L·∫•y 5 k·∫øt qu·∫£ m·∫´u
        sample = df.head(5).to_dict('records')
        results["sample_results"] = [{
            "message": row[message_col][:100] + "..." if len(row[message_col]) > 100 else row[message_col],
            "is_critical": bool(row['is_critical'])
        } for row in sample]
        
        return results
        
    except Exception as e:
        if tmp_path.exists():
            tmp_path.unlink()
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}")
```

### backend\api\routes\github.py
```py
# backend/api/routes/github.py
# File t·ªïng h·ª£p c√°c router GitHub APIs

from fastapi import APIRouter
from .repo import repo_router
from .commit import commit_router
from .branch import branch_router
from .issue import issue_router
from .sync import sync_router

# Router ch√≠nh cho GitHub APIs
github_router = APIRouter()

# Include c√°c sub-routers
github_router.include_router(repo_router, tags=["repositories"])
github_router.include_router(commit_router, tags=["commits"])
github_router.include_router(branch_router, tags=["branches"])
github_router.include_router(issue_router, tags=["issues"])
github_router.include_router(sync_router, tags=["synchronization"])
```

### backend\api\routes\issue.py
```py
# backend/api/routes/issue.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.issue_service import save_issue
from services.repo_service import get_repo_id_by_owner_and_name

issue_router = APIRouter()

# L∆∞u issues v√†o database
@issue_router.post("/github/{owner}/{repo}/save-issues")
async def save_issues(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    # L·∫•y danh s√°ch issue t·ª´ GitHub API
    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        issues = resp.json()

    # L∆∞u issue v√†o database
    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    saved_count = 0
    for issue in issues:
        try:
            issue_data = {
                "title": issue["title"],
                "body": issue["body"],
                "state": issue["state"],
                "created_at": issue["created_at"],
                "updated_at": issue["updated_at"],
                "repo_id": repo_id,
            }
            await save_issue(issue_data)
            saved_count += 1
        except Exception as e:
            print(f"L·ªói khi l∆∞u issue {issue['title']}: {e}")
            continue

    return {"message": f"ƒê√£ l∆∞u {saved_count}/{len(issues)} issues!"}

```

### backend\api\routes\projects.py
```py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import select, insert, update, delete, and_
from typing import List, Optional
from datetime import datetime
from pydantic import BaseModel

# from core.security import get_current_user  # Temporarily disabled
from db.database import get_db, engine
from db.models.project_tasks import project_tasks, TaskStatus, TaskPriority

router = APIRouter()

# Temporary mock user dependency
async def get_current_user():
    return {"username": "test_user", "id": 1}

# Pydantic models cho Task
class TaskBase(BaseModel):
    title: str
    description: Optional[str] = None
    assignee: str
    priority: str = "medium"  # low, medium, high
    status: str = "todo"  # todo, in_progress, done
    due_date: Optional[str] = None

class TaskCreate(TaskBase):
    repo_owner: str
    repo_name: str

class TaskUpdate(TaskBase):
    pass

class TaskResponse(TaskBase):
    id: int
    repo_owner: str
    repo_name: str
    created_at: datetime
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True

@router.get("/projects/{owner}/{repo}/tasks", response_model=List[TaskResponse])
async def get_project_tasks(
    owner: str,
    repo: str,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch tasks c·ªßa repository"""
    try:
        # Query tasks from database
        with engine.connect() as conn:
            query = select(project_tasks).where(
                and_(
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )
            ).order_by(project_tasks.c.created_at.desc())
            
            result = conn.execute(query)
            tasks = []
            
            for row in result:
                task_dict = {
                    "id": row.id,
                    "title": row.title,
                    "description": row.description,
                    "assignee": row.assignee,
                    "priority": row.priority.value if row.priority else "medium",
                    "status": row.status.value if row.status else "todo",
                    "due_date": row.due_date,
                    "repo_owner": row.repo_owner,
                    "repo_name": row.repo_name,
                    "created_at": row.created_at,
                    "updated_at": row.updated_at
                }
                tasks.append(task_dict)
            
            return tasks
    except Exception as e:
        print(f"Database error: {e}")
        # Fallback to empty list if database error
        return []

@router.post("/projects/{owner}/{repo}/tasks", response_model=TaskResponse)
async def create_project_task(
    owner: str,
    repo: str,
    task: TaskCreate,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """T·∫°o task m·ªõi cho repository"""
    try:
        # Insert task into database
        with engine.connect() as conn:
            # Validate priority and status
            priority_enum = TaskPriority.MEDIUM
            if task.priority == "low":
                priority_enum = TaskPriority.LOW
            elif task.priority == "high":
                priority_enum = TaskPriority.HIGH
                
            status_enum = TaskStatus.TODO
            if task.status == "in_progress":
                status_enum = TaskStatus.IN_PROGRESS
            elif task.status == "done":
                status_enum = TaskStatus.DONE
            
            insert_stmt = insert(project_tasks).values(
                title=task.title,
                description=task.description,
                assignee=task.assignee,
                priority=priority_enum,
                status=status_enum,
                due_date=task.due_date,
                repo_owner=owner,
                repo_name=repo,
                created_by=current_user["username"]
            )
            
            result = conn.execute(insert_stmt)
            conn.commit()
            
            # Get the created task
            task_id = result.inserted_primary_key[0]
            query = select(project_tasks).where(project_tasks.c.id == task_id)
            created_task = conn.execute(query).fetchone()
            
            return {
                "id": created_task.id,
                "title": created_task.title,
                "description": created_task.description,
                "assignee": created_task.assignee,
                "priority": created_task.priority.value,
                "status": created_task.status.value,
                "due_date": created_task.due_date,
                "repo_owner": created_task.repo_owner,
                "repo_name": created_task.repo_name,
                "created_at": created_task.created_at,
                "updated_at": created_task.updated_at
            }
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.put("/projects/{owner}/{repo}/tasks/{task_id}", response_model=TaskResponse)
async def update_project_task(
    owner: str,
    repo: str,
    task_id: int,
    task_update: TaskUpdate,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """C·∫≠p nh·∫≠t task"""
    try:
        with engine.connect() as conn:
            # Check if task exists
            check_query = select(project_tasks).where(
                and_(
                    project_tasks.c.id == task_id,
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )
            )
            existing_task = conn.execute(check_query).fetchone()
            
            if not existing_task:
                raise HTTPException(status_code=404, detail="Task not found")
            
            # Validate priority and status
            priority_enum = TaskPriority.MEDIUM
            if task_update.priority == "low":
                priority_enum = TaskPriority.LOW
            elif task_update.priority == "high":
                priority_enum = TaskPriority.HIGH
                
            status_enum = TaskStatus.TODO
            if task_update.status == "in_progress":
                status_enum = TaskStatus.IN_PROGRESS
            elif task_update.status == "done":
                status_enum = TaskStatus.DONE
            
            # Update task
            update_stmt = update(project_tasks).where(
                project_tasks.c.id == task_id
            ).values(
                title=task_update.title,
                description=task_update.description,
                assignee=task_update.assignee,
                priority=priority_enum,
                status=status_enum,
                due_date=task_update.due_date
            )
            
            conn.execute(update_stmt)
            conn.commit()
            
            # Get updated task
            updated_task = conn.execute(check_query).fetchone()
            
            return {
                "id": updated_task.id,
                "title": updated_task.title,
                "description": updated_task.description,
                "assignee": updated_task.assignee,
                "priority": updated_task.priority.value,
                "status": updated_task.status.value,
                "due_date": updated_task.due_date,
                "repo_owner": updated_task.repo_owner,
                "repo_name": updated_task.repo_name,
                "created_at": updated_task.created_at,
                "updated_at": updated_task.updated_task
            }
    except HTTPException:
        raise
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.delete("/projects/{owner}/{repo}/tasks/{task_id}")
async def delete_project_task(
    owner: str,
    repo: str,
    task_id: int,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """X√≥a task"""
    try:
        with engine.connect() as conn:
            # Check if task exists
            check_query = select(project_tasks).where(
                and_(
                    project_tasks.c.id == task_id,
                    project_tasks.c.repo_owner == owner,
                    project_tasks.c.repo_name == repo
                )
            )
            existing_task = conn.execute(check_query).fetchone()
            
            if not existing_task:
                raise HTTPException(status_code=404, detail="Task not found")
            
            # Delete task
            delete_stmt = delete(project_tasks).where(
                project_tasks.c.id == task_id
            )
            
            conn.execute(delete_stmt)
            conn.commit()
            
            return {"message": "Task deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Database error: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@router.get("/projects/{owner}/{repo}/collaborators")
async def get_project_collaborators(
    owner: str,
    repo: str,
    current_user=Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch collaborators c·ªßa repository"""
    try:
        # Mock data - trong th·ª±c t·∫ø s·∫Ω g·ªçi GitHub API
        collaborators = [
            {
                "login": "john_doe",
                "avatar_url": "https://via.placeholder.com/32",
                "type": "User"
            },
            {
                "login": "jane_smith", 
                "avatar_url": "https://via.placeholder.com/32",
                "type": "User"
            },
            {
                "login": owner,
                "avatar_url": "https://via.placeholder.com/32",
                "type": "Owner"
            }
        ]
        return collaborators
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### backend\api\routes\repo.py
```py
# backend/api/routes/repo.py
from fastapi import APIRouter, Request, HTTPException, Query, Path
from pydantic import BaseModel, Field
from typing import List, Optional
import httpx
import logging
from services.repo_service import get_repo_data, save_repository

# Setup logging
logger = logging.getLogger(__name__)

repo_router = APIRouter()

# Response Models
class RepoInfo(BaseModel):
    name: str
    full_name: str
    description: Optional[str] = None
    owner: str
    stars: int = Field(alias="stargazers_count")
    forks: int = Field(alias="forks_count")
    watchers: int = Field(alias="watchers_count")
    language: Optional[str] = None
    open_issues: int = Field(alias="open_issues_count")
    url: str = Field(alias="html_url")
    created_at: str
    updated_at: str

class GitHubRepo(BaseModel):
    id: int
    name: str
    full_name: str
    private: bool
    owner: dict
    html_url: str
    description: Optional[str] = None
    language: Optional[str] = None
    stargazers_count: int
    forks_count: int

# Endpoint l·∫•y th√¥ng tin repository c·ª• th·ªÉ
@repo_router.get("/github/{owner}/{repo}", response_model=RepoInfo)
async def fetch_repo(
    owner: str = Path(..., min_length=1, max_length=100, description="GitHub username ho·∫∑c organization"),
    repo: str = Path(..., min_length=1, max_length=100, description="T√™n repository")
):
    """
    L·∫•y th√¥ng tin chi ti·∫øt c·ªßa m·ªôt repository t·ª´ GitHub
    
    Args:
        owner: T√™n ch·ªß s·ªü h·ªØu repository
        repo: T√™n repository
        
    Returns:
        RepoInfo: Th√¥ng tin chi ti·∫øt c·ªßa repository
        
    Raises:
        HTTPException: 404 n·∫øu repository kh√¥ng t·ªìn t·∫°i, 500 n·∫øu l·ªói server
    """
    try:
        logger.info(f"Fetching repository info for {owner}/{repo}")
        data = await get_repo_data(owner, repo)
        return data
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            raise HTTPException(
                status_code=404, 
                detail=f"Repository {owner}/{repo} kh√¥ng t·ªìn t·∫°i ho·∫∑c b·∫°n kh√¥ng c√≥ quy·ªÅn truy c·∫≠p"
            )
        else:
            logger.error(f"GitHub API error: {e}")
            raise HTTPException(status_code=500, detail="L·ªói khi g·ªçi GitHub API")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise HTTPException(status_code=500, detail="L·ªói server kh√¥ng x√°c ƒë·ªãnh")

@repo_router.get("/github/repos", response_model=List[GitHubRepo])
async def get_user_repos(
    request: Request,
    per_page: int = Query(30, ge=1, le=100, description="S·ªë repository tr√™n m·ªói trang"),
    page: int = Query(1, ge=1, description="S·ªë trang"),
    sort: str = Query("updated", regex="^(created|updated|pushed|full_name)$", description="S·∫Øp x·∫øp theo"),
    direction: str = Query("desc", regex="^(asc|desc)$", description="H∆∞·ªõng s·∫Øp x·∫øp")
):
    """
    L·∫•y danh s√°ch repositories c·ªßa user hi·ªán t·∫°i
    
    Args:
        request: FastAPI request object ch·ª©a headers
        per_page: S·ªë repository tr√™n m·ªói trang (1-100)
        page: S·ªë trang (b·∫Øt ƒë·∫ßu t·ª´ 1)
        sort: S·∫Øp x·∫øp theo (created, updated, pushed, full_name)
        direction: H∆∞·ªõng s·∫Øp x·∫øp (asc, desc)
        
    Returns:
        List[GitHubRepo]: Danh s√°ch repositories
        
    Raises:
        HTTPException: 401 n·∫øu token kh√¥ng h·ª£p l·ªá, 500 n·∫øu l·ªói server
    """
    # L·∫•y token t·ª´ header Authorization
    token = request.headers.get("Authorization")
    
    # Validation token chi ti·∫øt h∆°n
    if not token:
        raise HTTPException(
            status_code=401, 
            detail="Missing Authorization header. Vui l√≤ng cung c·∫•p token GitHub."
        )
    
    if not token.startswith("token ") and not token.startswith("Bearer "):
        raise HTTPException(
            status_code=401, 
            detail="Invalid token format. Token ph·∫£i c√≥ format 'token <your_token>' ho·∫∑c 'Bearer <your_token>'"
        )
    
    try:
        # G·ªçi GitHub API ƒë·ªÉ l·∫•y danh s√°ch repo v·ªõi parameters
        async with httpx.AsyncClient(timeout=30.0) as client:
            logger.info(f"Fetching user repositories with params: page={page}, per_page={per_page}, sort={sort}")
            
            resp = await client.get(
                "https://api.github.com/user/repos",
                headers={"Authorization": token},
                params={
                    "per_page": per_page,
                    "page": page,
                    "sort": sort,
                    "direction": direction,
                    "type": "all"  # L·∫•y t·∫•t c·∫£ lo·∫°i repo (owner, collaborator, organization_member)
                }
            )
            
            # X·ª≠ l√Ω response d·ª±a theo status code
            if resp.status_code == 401:
                raise HTTPException(
                    status_code=401, 
                    detail="Token kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ki·ªÉm tra l·∫°i token GitHub."
                )
            elif resp.status_code == 403:
                raise HTTPException(
                    status_code=403, 
                    detail="Token kh√¥ng c√≥ quy·ªÅn truy c·∫≠p repositories. Vui l√≤ng ki·ªÉm tra scope c·ªßa token."
                )
            elif resp.status_code != 200:
                logger.error(f"GitHub API error: {resp.status_code} - {resp.text}")
                raise HTTPException(
                    status_code=resp.status_code, 
                    detail=f"GitHub API error: {resp.text}"
                )
        
        repos_data = resp.json()
        logger.info(f"Successfully fetched {len(repos_data)} repositories")
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c validate qua Pydantic model
        return repos_data
        
    except httpx.TimeoutException:
        logger.error("Timeout when calling GitHub API")
        raise HTTPException(status_code=504, detail="Timeout khi g·ªçi GitHub API")
    except httpx.RequestError as e:
        logger.error(f"Request error: {e}")
        raise HTTPException(status_code=500, detail="L·ªói k·∫øt n·ªëi v·ªõi GitHub API")
    except Exception as e:
        logger.error(f"Unexpected error in get_user_repos: {e}")
        raise HTTPException(status_code=500, detail="L·ªói server kh√¥ng x√°c ƒë·ªãnh")

# Endpoint ƒë·ªÉ l∆∞u repository v√†o database
@repo_router.post("/github/{owner}/{repo}/save")
async def save_repo_to_db(
    owner: str = Path(..., min_length=1, max_length=100),
    repo: str = Path(..., min_length=1, max_length=100)
):
    """
    L∆∞u th√¥ng tin repository v√†o database
    
    Args:
        owner: T√™n ch·ªß s·ªü h·ªØu repository
        repo: T√™n repository
        
    Returns:
        dict: Th√¥ng b√°o k·∫øt qu·∫£
    """
    try:
        logger.info(f"Saving repository {owner}/{repo} to database")
        
        # L·∫•y th√¥ng tin repo t·ª´ GitHub
        repo_data = await get_repo_data(owner, repo)
        
        # Chuy·ªÉn ƒë·ªïi format ƒë·ªÉ l∆∞u v√†o database
        repo_entry = {
            "github_id": repo_data.get("id"),
            "owner": owner,
            "name": repo,
            "full_name": repo_data.get("full_name"),
            "description": repo_data.get("description"),
            "stars": repo_data.get("stars"),
            "forks": repo_data.get("forks"),
            "language": repo_data.get("language"),
            "open_issues": repo_data.get("open_issues"),
            "url": repo_data.get("url"),
            "is_private": repo_data.get("private", False),
            "is_fork": repo_data.get("fork", False),
            "default_branch": repo_data.get("default_branch", "main")
        }
        
        # L∆∞u v√†o database
        result = await save_repository(repo_entry)
        
        return {
            "message": f"Repository {owner}/{repo} ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng",
            "repository_id": result
        }
        
    except Exception as e:
        logger.error(f"Error saving repository: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi l∆∞u repository: {str(e)}")

# Endpoint health check cho repos API
@repo_router.get("/health")
async def health_check():
    """
    Health check endpoint cho repos API
    """
    return {
        "status": "healthy",
        "service": "repositories API",
        "timestamp": "2025-06-13"
    }
```

### backend\api\routes\sync.py
```py
# backend/api/routes/sync.py
from fastapi import APIRouter, Request, HTTPException
import httpx
from services.repo_service import save_repository, get_repo_id_by_owner_and_name
from services.branch_service import save_branch
from services.commit_service import save_commit
from services.issue_service import save_issue

sync_router = APIRouter()

# ƒê·ªìng b·ªô to√†n b·ªô d·ªØ li·ªáu
@sync_router.post("/github/{owner}/{repo}/sync-all")
async def sync_all(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    try:
        # ƒê·ªìng b·ªô repository
        async with httpx.AsyncClient() as client:
            url = f"https://api.github.com/repos/{owner}/{repo}"
            headers = {"Authorization": token}
            resp = await client.get(url, headers=headers)
            if resp.status_code != 200:
                raise HTTPException(status_code=resp.status_code, detail=resp.text)
            repo_data = resp.json()
        repo_entry = {
            "github_id": repo_data["id"],
            "name": repo_data["name"],
            "owner": repo_data["owner"]["login"],
            "description": repo_data["description"],
            "stars": repo_data["stargazers_count"],
            "forks": repo_data["forks_count"],
            "language": repo_data["language"],
            "open_issues": repo_data["open_issues_count"],
            "url": repo_data["html_url"],
        }
        await save_repository(repo_entry)        # ƒê·ªìng b·ªô branches
        async with httpx.AsyncClient() as client:
            url = f"https://api.github.com/repos/{owner}/{repo}/branches"
            headers = {"Authorization": token}
            resp = await client.get(url, headers=headers)
            if resp.status_code != 200:
                raise HTTPException(status_code=resp.status_code, detail=resp.text)
            branches_data = resp.json()
        # L·∫•y repo_id ƒë·ªÉ save branches
        repo_id = await get_repo_id_by_owner_and_name(owner, repo)
        if not repo_id:
            raise HTTPException(status_code=404, detail="Repository not found")
        for branch_data in branches_data:
            branch_entry = {
                "name": branch_data["name"],
                "repo_id": repo_id,
            }
            await save_branch(branch_entry)
        return {"message": f"ƒê·ªìng b·ªô repository {owner}/{repo} th√†nh c√¥ng!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªìng b·ªô {owner}/{repo}: {str(e)}")

# Endpoint ƒë·ªìng b·ªô nhanh - ch·ªâ th√¥ng tin c∆° b·∫£n
@sync_router.post("/github/{owner}/{repo}/sync-basic")
async def sync_basic(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    try:
        # Ch·ªâ ƒë·ªìng b·ªô repository
        async with httpx.AsyncClient() as client:
            url = f"https://api.github.com/repos/{owner}/{repo}"
            headers = {"Authorization": token}
            resp = await client.get(url, headers=headers)
            if resp.status_code != 200:
                raise HTTPException(status_code=resp.status_code, detail=resp.text)

            repo_data = resp.json()

        repo_entry = {
            "github_id": repo_data["id"],
            "name": repo_data["name"],
            "owner": repo_data["owner"]["login"],
            "description": repo_data["description"],
            "stars": repo_data["stargazers_count"],
            "forks": repo_data["forks_count"],
            "language": repo_data["language"],
            "open_issues": repo_data["open_issues_count"],
            "url": repo_data["html_url"],
        }
        await save_repository(repo_entry)
        
        return {"message": f"ƒê·ªìng b·ªô c∆° b·∫£n {owner}/{repo} th√†nh c√¥ng!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói ƒë·ªìng b·ªô c∆° b·∫£n {owner}/{repo}: {str(e)}")

```

### backend\api\routes\__init__.py
```py

```

### backend\core\config.py
```py
# backend/core/config.py
# File c·∫•u h√¨nh ch√≠nh cho ·ª©ng d·ª•ng FastAPI

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os  # L√†m vi·ªác v·ªõi bi·∫øn m√¥i tr∆∞·ªùng
from fastapi.middleware.cors import CORSMiddleware  # Middleware CORS
from starlette.middleware.sessions import SessionMiddleware  # Middleware qu·∫£n l√Ω session
from fastapi import FastAPI  # Framework ch√≠nh
from api.routes.github import github_router  # Router cho GitHub API
from api.routes.auth import auth_router  # Router cho x√°c th·ª±c
from dotenv import load_dotenv  # ƒê·ªçc file .env

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# H√†m c·∫•u h√¨nh c√°c middleware cho ·ª©ng d·ª•ng
def setup_middlewares(app: FastAPI):
    """
    Thi·∫øt l·∫≠p c√°c middleware c·∫ßn thi·∫øt cho ·ª©ng d·ª•ng
    
    Args:
        app (FastAPI): Instance c·ªßa FastAPI app
    """
    
    # Th√™m middleware CORS (Cross-Origin Resource Sharing)
    app.add_middleware(
        CORSMiddleware,
        # Danh s√°ch domain ƒë∆∞·ª£c ph√©p truy c·∫≠p
        allow_origins=[
            "http://localhost:5173",  # Frontend dev (Vite th∆∞·ªùng ch·∫°y ·ªü port 5173)
            "http://localhost:3000"   # Frontend dev (React c√≥ th·ªÉ ch·∫°y ·ªü port 3000)
        ],
        allow_credentials=True,  # Cho ph√©p g·ª≠i credential (cookies, auth headers)
        allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ HTTP methods
        allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers (bao g·ªìm Authorization)
    )

    # Th√™m middleware qu·∫£n l√Ω session
    app.add_middleware(
        SessionMiddleware,
        secret_key=os.getenv('SECRET_KEY')  # Kh√≥a b√≠ m·∫≠t t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    )


# H√†m c·∫•u h√¨nh c√°c router cho ·ª©ng d·ª•ng
def setup_routers(app: FastAPI):
    """
    ƒêƒÉng k√Ω c√°c router ch√≠nh c·ªßa ·ª©ng d·ª•ng
    
    Args:
        app (FastAPI): Instance c·ªßa FastAPI app
    """
    
    # ƒêƒÉng k√Ω auth router v·ªõi prefix /auth
    app.include_router(auth_router, prefix="/auth")
    
    # ƒêƒÉng k√Ω github router v·ªõi prefix /api
    app.include_router(github_router, prefix="/api")  # G·ªôp chung kh√¥ng b·ªã ƒë√® l·∫´n nhau
```

### backend\core\lifespan.py
```py
from fastapi import FastAPI
from contextlib import asynccontextmanager
from db.database import database
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        await database.connect()
        logger.info("‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi database th√†nh c√¥ng.")
        yield  # Ch·ªâ yield n·∫øu connect th√†nh c√¥ng
    except Exception as e:
        logger.error(f"‚ùå K·∫øt n·ªëi database th·∫•t b·∫°i: {e}")
        raise e  # D·ª´ng app n·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c DB
    finally:
        try:
            await database.disconnect()
            logger.info("üõë ƒê√£ ng·∫Øt k·∫øt n·ªëi database.")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ng·∫Øt k·∫øt n·ªëi database: {e}")

```

### backend\core\logger.py
```py
# core/logger.py

import logging

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,  # Hi·ªán log t·ª´ c·∫•p INFO tr·ªü l√™n
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

```

### backend\core\oauth.py
```py
# backend/core/oauth.py
# File c·∫•u h√¨nh OAuth cho ·ª©ng d·ª•ng, ch·ªß y·∫øu d√πng cho GitHub OAuth

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os  # ƒê·ªÉ l√†m vi·ªác v·ªõi bi·∫øn m√¥i tr∆∞·ªùng
from dotenv import load_dotenv  # ƒê·ªÉ ƒë·ªçc file .env
from authlib.integrations.starlette_client import OAuth  # Th∆∞ vi·ªán OAuth cho Starlette/FastAPI

# Load c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# Kh·ªüi t·∫°o instance OAuth
oauth = OAuth()

# ƒêƒÉng k√Ω provider GitHub cho OAuth
oauth.register(
    name='github',  # T√™n provider
    
    # Client ID t·ª´ ·ª©ng d·ª•ng GitHub OAuth App
    client_id=os.getenv('GITHUB_CLIENT_ID'),
    
    # Client Secret t·ª´ ·ª©ng d·ª•ng GitHub OAuth App
    client_secret=os.getenv('GITHUB_CLIENT_SECRET'),
    
    # URL ƒë·ªÉ l·∫•y access token
    access_token_url='https://github.com/login/oauth/access_token',
    
    # C√°c params th√™m khi l·∫•y access token (None n·∫øu kh√¥ng c√≥)
    access_token_params=None,
    
    # URL ƒë·ªÉ x√°c th·ª±c
    authorize_url='https://github.com/login/oauth/authorize',
    
    # C√°c params th√™m khi x√°c th·ª±c (None n·∫øu kh√¥ng c√≥)
    authorize_params=None,
    
    # Base URL cho API GitHub
    api_base_url='https://api.github.com/',
    
    # C√°c tham s·ªë b·ªï sung cho client
    client_kwargs={
        'scope': 'read:user user:email repo'  # C√°c quy·ªÅn y√™u c·∫ßu
        # read:user - ƒê·ªçc th√¥ng tin user
        # user:email - ƒê·ªçc email user
        # repo - Truy c·∫≠p repository
    }
)
```

### backend\core\security.py
```py

```

### backend\migrations\env.py
```py
import os
from dotenv import load_dotenv
from sqlalchemy import engine_from_config, pool
from alembic import context
from db.metadata import metadata  # Import metadata t·ª´ metadata.py

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# L·∫•y DATABASE_URL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
config = context.config
database_url = os.getenv("DATABASE_URL").replace("asyncpg", "psycopg2")
config.set_main_option("sqlalchemy.url", database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    from logging.config import fileConfig
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```

### backend\migrations\versions\5b8fe79e0fa5_enhance_commits_branches_add_.py
```py

```

### backend\migrations\versions\add_user_repositories_mapping.py
```py

```

### backend\migrations\versions\d2fd206aedc4_create_clean_database_schema.py
```py

```

### backend\migrations\versions\fb48677d0aa5_add_author_avatar_url_to_commits_table.py
```py

```

### backend\models\commit_model.py
```py
# KLTN04\backend\models\commit_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
import joblib
import os

class CommitClassifier:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.model = RandomForestClassifier()
        self.labels = ['normal', 'critical']  # 0: normal, 1: critical/bugfix

    def train(self, df: pd.DataFrame):
        """Hu·∫•n luy·ªán model t·ª´ dataframe"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical']  # C·ªôt nh√£n (0/1)
        self.model.fit(X, y)
        
    def predict(self, new_messages: list):
        """D·ª± ƒëo√°n commit quan tr·ªçng c·∫ßn review"""
        X_new = self.vectorizer.transform(new_messages)
        return self.model.predict(X_new)
    
    def save(self, path=None):
        """L∆∞u model"""
        if path is None:
            # L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa file model
            current_dir = os.path.dirname(os.path.abspath(__file__))
            path = os.path.join(current_dir, 'commit_classifier.joblib')
        
        joblib.dump({
            'vectorizer': self.vectorizer,
            'model': self.model
        }, path)
    
    @classmethod
    def load(cls, path=None):
        """Load model ƒë√£ l∆∞u"""
        if path is None:
            # L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa file model
            current_dir = os.path.dirname(os.path.abspath(__file__))
            path = os.path.join(current_dir, 'commit_classifier.joblib')
        
        try:
            data = joblib.load(path)
            classifier = cls()
            classifier.vectorizer = data['vectorizer']
            classifier.model = data['model']
            return classifier
        except FileNotFoundError:
            print(f"Warning: Model file not found at {path}. Creating new classifier.")
            return cls()
```

### backend\schemas\commit.py
```py
from pydantic import BaseModel
from datetime import datetime


class CommitCreate(BaseModel):
    commit_id: str
    message: str
    author_name: str
    author_email: str
    committed_date: datetime
    repository_id: int


class CommitOut(CommitCreate):
    id: int

    class Config:
        from_attributes = True  # D√†nh cho Pydantic V2 thay cho orm_mode

```

### backend\scripts\commit_analysis_system.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import warnings
warnings.filterwarnings('ignore')

class CommitAnalysisSystem:
    def __init__(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
        self.vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            ngram_range=(1, 1)
        )
        self.model = RandomForestClassifier(
            n_estimators=30,
            max_depth=8,
            n_jobs=1,
            class_weight='balanced'
        )
        self.client = None

    def init_dask_client(self):
        """Kh·ªüi t·∫°o Dask client"""
        self.client = Client(n_workers=2, threads_per_worker=1, memory_limit='2GB')

    @staticmethod
    def lightweight_heuristic(msg):
        """H√†m heuristic tƒ©nh ƒë·ªÉ x·ª≠ l√Ω song song"""
        if not isinstance(msg, str) or not msg.strip():
            return 0
        msg = msg.lower()[:150]
        return int(any(kw in msg for kw in ['fix', 'bug', 'error', 'fail']))

    def process_large_file(self, input_path, output_dir):
        """X·ª≠ l√Ω file l·ªõn v·ªõi Dask """
        try:
            if self.client:
                self.client.close()
            self.init_dask_client()

            # ƒê·ªçc file v·ªõi Dask
            ddf = dd.read_csv(
                str(input_path),
                blocksize="20MB",
                dtype={'message': 'string'},
                usecols=['commit', 'message'],
                na_values=['', 'NA', 'N/A', 'nan']
            )
            
            # S·ª≠a l·ªói: Thay .notna() b·∫±ng .notnull() cho Dask
            ddf = ddf[ddf['message'].notnull()]
            
            # G√°n nh√£n
            ddf['is_critical'] = ddf['message'].map(
                self.lightweight_heuristic,
                meta=('is_critical', 'int8')
            )
            
           # L∆∞u k·∫øt qu·∫£ (ƒë√£ s·ª≠a ph·∫ßn compute)
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # S·ª≠a l·ªói: G·ªçi compute() tr·ª±c ti·∫øp tr√™n to_csv()
            ddf.to_csv(
                str(output_dir / "part_*.csv"),
                index=False
            )
            
            return True
        except Exception as e:
            print(f"üö® L·ªói x·ª≠ l√Ω file: {str(e)}")
            return False
        finally:
            if self.client:
                self.client.close()

    def clean_data(self, df):
        """L√†m s·∫°ch d·ªØ li·ªáu"""
        if 'message' not in df.columns:
            raise ValueError("Thi·∫øu c·ªôt 'message' trong d·ªØ li·ªáu")
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df):
        """G√°n nh√£n t·ª± ƒë·ªông"""
        df = self.clean_data(df)
        df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
        return df

    def train_model(self, df):
        """Hu·∫•n luy·ªán m√¥ h√¨nh"""
        X = self.vectorizer.fit_transform(df['message'])
        y = df['is_critical'].values
        self.model.fit(X, y)

    def evaluate(self, test_df):
        """ƒê√°nh gi√° m√¥ h√¨nh"""
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        print(classification_report(y_test, self.model.predict(X_test)))

    def save_model(self, path):
        """L∆∞u m√¥ h√¨nh"""
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer
        }, str(path))

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch commit...")
    system = CommitAnalysisSystem()
    
    input_path = Path("D:/Project/KLTN04/data/oneline.csv")
    output_dir = Path("D:/Project/KLTN04/data/processed")
    
    if system.process_large_file(input_path, output_dir):
        print("‚úÖ ƒê√£ x·ª≠ l√Ω file th√†nh c√¥ng")
        
        # N·∫°p v√† x·ª≠ l√Ω d·ªØ li·ªáu
        df = pd.concat([pd.read_csv(f) for f in output_dir.glob("part_*.csv")])
        df = system.auto_label(df)
        
        # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
        system.train_model(df)
        test_df = df.sample(frac=0.2, random_state=42)
        system.evaluate(test_df)
        
        # L∆∞u m√¥ h√¨nh
        model_path = "backend/models/commit_classifier.joblib"
        system.save_model(model_path)
        print(f"üíæ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {model_path}")

if __name__ == "__main__":
    main()
```

### backend\scripts\commit_analysis_system_v1.py
```py
# File: backend/scripts/commit_analysis_system.py
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import dask.dataframe as dd
import joblib
from dask.distributed import Client
import logging
from typing import Optional, Union
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CommitAnalysisSystem:
    """H·ªá th·ªëng ph√¢n t√≠ch commit t·ª± ƒë·ªông v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn"""
    
    VERSION = "1.0.0"
    
    def __init__(self, model_params: Optional[dict] = None, 
                 vectorizer_params: Optional[dict] = None):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng ph√¢n t√≠ch commit
        
        Args:
            model_params: Tham s·ªë cho RandomForestClassifier
            vectorizer_params: Tham s·ªë cho TfidfVectorizer
        """
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        default_vectorizer_params = {
            'max_features': 1000,
            'stop_words': 'english',
            'ngram_range': (1, 2),  # Th√™m bigram
            'min_df': 5,
            'max_df': 0.8
        }
        
        default_model_params = {
            'n_estimators': 100,
            'max_depth': 15,
            'class_weight': 'balanced',
            'random_state': 42
        }
        
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or default_vectorizer_params))
        self.model = RandomForestClassifier(**(model_params or default_model_params))
        self.client = None
        self._is_trained = False

    def init_dask_client(self, **kwargs):
        """Kh·ªüi t·∫°o Dask client v·ªõi c·∫•u h√¨nh t√πy ch·ªçn"""
        default_config = {
            'n_workers': 2,
            'threads_per_worker': 1,
            'memory_limit': '2GB',
            'silence_logs': logging.ERROR
        }
        config = {**default_config, **kwargs}
        
        try:
            self.client = Client(**config)
            logger.info(f"Dask client initialized with config: {config}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Dask client: {str(e)}")
            return False

    @staticmethod
    def lightweight_heuristic(msg: str) -> int:
        """Ph√¢n lo·∫°i commit s·ª≠ d·ª•ng heuristic ƒë∆°n gi·∫£n
        
        Args:
            msg: N·ªôi dung commit message
            
        Returns:
            1 n·∫øu l√† commit quan tr·ªçng (bugfix), 0 n·∫øu kh√¥ng
        """
        if not isinstance(msg, str) or not msg.strip():
            return 0
            
        msg = msg.lower()[:200]  # Gi·ªõi h·∫°n ƒë·ªô d√†i x·ª≠ l√Ω
        keywords = {
            'fix', 'bug', 'error', 'fail', 'patch', 
            'resolve', 'crash', 'defect', 'issue'
        }
        return int(any(kw in msg for kw in keywords))

    def process_large_file(self, input_path: Union[str, Path], output_dir: Union[str, Path]) -> bool:
        """X·ª≠ l√Ω file d·ªØ li·ªáu l·ªõn b·∫±ng Dask"""
        try:
            input_path = Path(input_path)
            output_dir = Path(output_dir)

            if not input_path.exists():
                logger.error(f"Input file not found: {input_path}")
                return False

            logger.info(f"Starting processing large file: {input_path}")
            start_time = datetime.now()

            # Kh·ªüi t·∫°o Dask client
            if not self.init_dask_client():
                return False

            try:
                # ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu
                ddf = dd.read_csv(
                    str(input_path),
                    blocksize="10MB",  # Gi·∫£m k√≠ch th∆∞·ªõc block ƒë·ªÉ an to√†n
                    dtype={'message': 'string'},
                    usecols=['commit', 'message'],
                    na_values=['', 'NA', 'N/A', 'nan']
                )

                # L·ªçc v√† g√°n nh√£n
                ddf = ddf[ddf['message'].notnull()]
                ddf['is_critical'] = ddf['message'].map(
                    self.lightweight_heuristic,
                    meta=('is_critical', 'int8')
                )

                # L∆∞u k·∫øt qu·∫£
                output_dir.mkdir(exist_ok=True, parents=True)
                output_path = str(output_dir / f"processed_{input_path.stem}.csv")

                # S·ª≠ d·ª•ng dask.dataframe.to_csv v·ªõi single_file=True
                ddf.to_csv(
                    output_path,
                    index=False,
                    single_file=True
                )

                logger.info(f"Processing completed in {datetime.now() - start_time}")
                logger.info(f"Results saved to: {output_path}")
                return True

            except Exception as e:
                logger.exception(f"Error during processing: {str(e)}")
                return False

        except Exception as e:
            logger.exception(f"System error: {str(e)}")
            return False

        finally:
            if self.client:
                self.client.close()
                self.client = None

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """L√†m s·∫°ch d·ªØ li·ªáu ƒë·∫ßu v√†o
        
        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu commit
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        if 'message' not in df.columns:
            raise ValueError("Input data must contain 'message' column")
            
        df = df.copy()
        df['message'] = df['message'].astype('string').fillna('')
        return df[df['message'].str.strip() != '']

    def auto_label(self, df: pd.DataFrame) -> pd.DataFrame:
        """T·ª± ƒë·ªông g√°n nh√£n cho d·ªØ li·ªáu commit
        
        Args:
            df: DataFrame ch·ª©a c√°c commit message
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
        """
        try:
            df = self.clean_data(df)
            df['is_critical'] = df['message'].apply(self.lightweight_heuristic)
            logger.info(f"Label distribution:\n{df['is_critical'].value_counts()}")
            return df
        except Exception as e:
            logger.error(f"Auto-labeling failed: {str(e)}")
            raise

    def train_model(self, df: pd.DataFrame) -> bool:
        """Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i commit
        
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
            
        Returns:
            True n·∫øu hu·∫•n luy·ªán th√†nh c√¥ng
        """
        try:
            logger.info("Starting model training...")
            
            X = self.vectorizer.fit_transform(df['message'])
            y = df['is_critical'].values
            
            self.model.fit(X, y)
            self._is_trained = True
            
            logger.info("Model training completed successfully")
            return True
        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            return False

    def evaluate(self, test_df: pd.DataFrame) -> None:
        """ƒê√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh
        
        Args:
            test_df: DataFrame ch·ª©a d·ªØ li·ªáu test
        """
        if not self._is_trained:
            logger.warning("Model has not been trained yet")
            return
            
        X_test = self.vectorizer.transform(test_df['message'])
        y_test = test_df['is_critical'].values
        y_pred = self.model.predict(X_test)
        
        report = classification_report(
            y_test, 
            y_pred, 
            target_names=['normal', 'critical']
        )
        logger.info(f"\nModel evaluation:\n{report}")

    def save_model(self, path: Union[str, Path]) -> bool:
        """L∆∞u m√¥ h√¨nh v√† vectorizer
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n l∆∞u model
            
        Returns:
            True n·∫øu l∆∞u th√†nh c√¥ng
        """
        try:
            path = Path(path)
            path.parent.mkdir(exist_ok=True, parents=True)
            
            model_data = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'version': self.VERSION,
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, str(path))
            logger.info(f"Model saved to {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            return False

    @classmethod
    def load_model(cls, path: Union[str, Path]):
        """T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model
            
        Returns:
            Instance c·ªßa CommitAnalysisSystem v·ªõi model ƒë√£ t·∫£i
        """
        try:
            path = Path(path)
            model_data = joblib.load(str(path))
            
            system = cls()
            system.model = model_data['model']
            system.vectorizer = model_data['vectorizer']
            system._is_trained = True
            
            logger.info(f"Loaded model (v{model_data.get('version', 'unknown')} "
                       f"created at {model_data.get('timestamp', 'unknown')}")
            return system
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

def main():
    """Entry point cho ·ª©ng d·ª•ng"""
    try:
        logger.info("üöÄ Starting commit analysis system")
        
        # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
        input_path = Path("D:/Project/KLTN04/data/oneline.csv")
        output_dir = Path("D:/Project/KLTN04/data/processed")
        model_path = Path("backend/models/commit_classifier_v1.joblib")
        
        # Kh·ªüi t·∫°o h·ªá th·ªëng
        system = CommitAnalysisSystem()
        
        # X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn
        if system.process_large_file(input_path, output_dir):
            # T·ªïng h·ª£p k·∫øt qu·∫£
            df = pd.concat([
                pd.read_csv(f) 
                for f in output_dir.glob("processed_*.csv")
            ])
            
            # G√°n nh√£n v√† hu·∫•n luy·ªán
            labeled_data = system.auto_label(df)
            system.train_model(labeled_data)
            
            # ƒê√°nh gi√° tr√™n t·∫≠p test
            test_df = labeled_data.sample(frac=0.2, random_state=42)
            system.evaluate(test_df)
            
            # L∆∞u model
            if system.save_model(model_path):
                logger.info(f"‚úÖ Pipeline completed successfully. Model saved to {model_path}")
        
    except Exception as e:
        logger.exception("‚ùå Critical error in main pipeline")
    finally:
        logger.info("üèÅ System shutdown")

if __name__ == "__main__":
    main()
```

### backend\services\ai_model.py
```py

```

### backend\services\ai_service.py
```py
from fastapi import APIRouter, HTTPException
from typing import List

router = APIRouter()

# TODO: Implement HAN model integration for commit analysis and task assignment
# Currently disabled to avoid import errors

@router.post("/analyze-commits")
async def analyze_commits(messages: List[str]):
    """Analyze commit messages using HAN model (To be implemented)"""
    # Mock response until HAN model is integrated
    predictions = [
        {
            "message": msg,
            "category": "feature",
            "confidence": 0.85,
            "analysis": "Mock analysis - HAN model integration pending"
        }
        for msg in messages
    ]
    return {"predictions": predictions}

@router.post("/assign-tasks")
async def assign_tasks(developers: List[dict], tasks: List[dict]):
    """Assign tasks to developers based on commit analysis (To be implemented)"""
    # Mock response until HAN model is integrated
    assignments = [
        {
            "task": task.get("title", "Unknown task"),
            "assigned_to": developers[0].get("login", "Unknown") if developers else "No developer",
            "confidence": 0.75,
            "reasoning": "Mock assignment - HAN model integration pending"
        }
        for task in tasks
    ]
    return {"assignments": assignments}
```

### backend\services\branch_service.py
```py
from db.models.branches import branches
from db.database import database
from fastapi import HTTPException, Request
import httpx
import logging

logger = logging.getLogger(__name__)

async def get_repo_id_by_owner_and_name(owner: str, repo: str):
    # Placeholder function for getting repository ID by owner and name
    pass

async def save_branch(branch_data):
    query = branches.insert().values(
        name=branch_data["name"],
        repo_id=branch_data["repo_id"],
    )
    await database.execute(query)

async def save_branches(owner: str, repo: str, request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("token "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")

    async with httpx.AsyncClient() as client:
        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        headers = {"Authorization": token}
        resp = await client.get(url, headers=headers)
        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        branches = resp.json()
        logger.info(f"Branches data: {branches}")

    repo_id = await get_repo_id_by_owner_and_name(owner, repo)
    if not repo_id:
        raise HTTPException(status_code=404, detail="Repository not found")

    for branch in branches:
        branch_data = {
            "name": branch["name"],
            "repo_id": repo_id,
        }
        await save_branch(branch_data)
```

### backend\services\collaborator_service.py
```py

```

### backend\services\collaborator_service_new.py
```py

```

### backend\services\commit_service.py
```py
from db.models.commits import commits
from db.database import database
from sqlalchemy import select, insert
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def save_commit(commit_data):
    """Save commit with proper datetime conversion"""
    try:
        # Ki·ªÉm tra commit ƒë√£ t·ªìn t·∫°i ch∆∞a
        query = select(commits).where(commits.c.sha == commit_data["sha"])
        existing_commit = await database.fetch_one(query)

        if existing_commit:
            logger.info(f"Commit {commit_data['sha']} already exists, skipping")
            return existing_commit.id

        # Convert datetime strings
        commit_entry = {
            **commit_data,
            "date": parse_github_datetime(commit_data.get("date")),
            "committer_date": parse_github_datetime(commit_data.get("committer_date"))
        }

        # Ch√®n commit m·ªõi
        query = insert(commits).values(commit_entry)
        result = await database.execute(query)
        logger.info(f"Created new commit: {commit_data['sha']}")
        return result
        
    except Exception as e:
        logger.error(f"Error saving commit {commit_data.get('sha')}: {e}")
        raise e

async def get_commits_by_repo(owner: str, repo: str, limit: int = 100):
    """Get commits by repository owner and name"""
    query = select(commits).where(
        commits.c.repo_owner == owner,
        commits.c.repo_name == repo
    ).limit(limit)
    return await database.fetch_all(query)

async def get_repo_by_owner_and_name(owner: str, repo: str):
    """Get repository information by owner and name"""
    # Dummy implementation - replace with actual repo service
    return {"owner": owner, "name": repo}
```

### backend\services\github_service.py
```py
# backend/services/github_service.py
# Service x·ª≠ l√Ω c√°c t∆∞∆°ng t√°c v·ªõi GitHub API

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import httpx  # Th∆∞ vi·ªán HTTP client async
import os  # L√†m vi·ªác v·ªõi bi·∫øn m√¥i tr∆∞·ªùng
from dotenv import load_dotenv  # ƒê·ªçc file .env
from typing import Optional  # ƒê·ªÉ khai b√°o ki·ªÉu d·ªØ li·ªáu optional
load_dotenv()  # N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env

# L·∫•y GitHub token t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

# Base URL cho GitHub API
BASE_URL = "https://api.github.com"

# Headers m·∫∑c ƒë·ªãnh cho c√°c request GitHub API
headers = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",  # Token x√°c th·ª±c
    "Accept": "application/vnd.github+json",  # Lo·∫°i response mong mu·ªën
}

async def fetch_from_github(url: str):
    """
    H√†m t·ªïng qu√°t ƒë·ªÉ fetch d·ªØ li·ªáu t·ª´ GitHub API
    
    Args:
        url (str): Ph·∫ßn cu·ªëi c·ªßa URL (sau BASE_URL)
    
    Returns:
        dict: D·ªØ li·ªáu JSON tr·∫£ v·ªÅ t·ª´ GitHub API
    
    Raises:
        HTTPError: N·∫øu request l·ªói
    """
    async with httpx.AsyncClient() as client:
        # G·ªçi GET request t·ªõi GitHub API
        response = await client.get(f"{BASE_URL}{url}", headers=headers)
        # T·ª± ƒë·ªông raise exception n·∫øu c√≥ l·ªói HTTP
        response.raise_for_status()
        # Tr·∫£ v·ªÅ d·ªØ li·ªáu d·∫°ng JSON
        return response.json()

async def fetch_commits(
    token: str, 
    owner: str, 
    name: str, 
    branch: str, 
    since: Optional[str], 
    until: Optional[str]
):
    """
    L·∫•y danh s√°ch commit t·ª´ repository GitHub
    
    Args:
        token (str): GitHub access token
        owner (str): Ch·ªß repository
        name (str): T√™n repository
        branch (str): T√™n branch
        since (Optional[str]): L·ªçc commit t·ª´ th·ªùi gian n√†y (ISO format)
        until (Optional[str]): L·ªçc commit ƒë·∫øn th·ªùi gian n√†y (ISO format)
    
    Returns:
        list: Danh s√°ch commit
    
    Raises:
        HTTPError: N·∫øu request l·ªói
    """
    # X√¢y d·ª±ng URL API ƒë·ªÉ l·∫•y commit
    url = f"https://api.github.com/repos/{owner}/{name}/commits"
    
    # Headers cho request
    headers = {
        "Authorization": f"token {token}",  # S·ª≠ d·ª•ng token t·ª´ tham s·ªë
        "Accept": "application/vnd.github+json"  # Lo·∫°i response mong mu·ªën
    }
    
    # Parameters cho request
    params = {
        "sha": branch  # L·ªçc theo branch
    }
    
    # Th√™m tham s·ªë l·ªçc th·ªùi gian n·∫øu c√≥
    if since:
        params["since"] = since
    if until:
        params["until"] = until

    # G·ªçi API GitHub
    async with httpx.AsyncClient() as client:
        res = await client.get(url, headers=headers, params=params)
        # Ki·ªÉm tra l·ªói HTTP
        res.raise_for_status()
        # Tr·∫£ v·ªÅ d·ªØ li·ªáu d·∫°ng JSON
        return res.json()
```

### backend\services\gitlab_service.py
```py

```

### backend\services\han_ai_service.py
```py
# backend/services/han_ai_service.py
"""
HAN AI Service - Service layer for HAN model integration
Provides high-level API for commit analysis and project management AI features
"""

import os
import sys
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
import asyncio
from functools import lru_cache

# Add AI directory to path
ai_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'ai')
sys.path.insert(0, ai_dir)

try:
    from ai.han_commit_analyzer import HANCommitAnalyzer
except ImportError as e:
    logging.warning(f"HAN model not available: {e}")
    HANCommitAnalyzer = None

logger = logging.getLogger(__name__)

class HANAIService:
    """
    Service class for HAN-based AI analysis in project management
    """
    
    def __init__(self):
        self.analyzer = None
        self.is_model_loaded = False
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize HAN model with error handling"""
        try:
            if HANCommitAnalyzer is None:
                logger.warning("HAN analyzer not available - using mock responses")
                return
                
            self.analyzer = HANCommitAnalyzer()
            self.analyzer.load_model()
            self.is_model_loaded = True
            logger.info("HAN model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load HAN model: {e}")
            self.is_model_loaded = False
    
    async def analyze_commit_message(self, message: str) -> Dict[str, Any]:
        """
        Analyze a single commit message
        
        Args:
            message: Commit message text
            
        Returns:
            Analysis results including category, impact, urgency
        """
        try:
            if not self.is_model_loaded:
                return self._mock_commit_analysis(message)
            
            # Run prediction in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.analyzer.predict_commit_analysis, 
                message
            )
            
            return {
                'success': True,
                'message': message,
                'analysis': result,
                'model_version': 'HAN-v1',
                'confidence': result.get('confidence', {})
            }
            
        except Exception as e:
            logger.error(f"Error analyzing commit: {e}")
            return {
                'success': False,
                'message': message,
                'error': str(e),
                'analysis': self._mock_commit_analysis(message)['analysis']
            }
    
    async def analyze_commits_batch(self, messages: List[str]) -> Dict[str, Any]:
        """
        Analyze multiple commit messages in batch
        
        Args:
            messages: List of commit message texts
            
        Returns:
            Batch analysis results
        """
        results = []
        
        for message in messages:
            analysis = await self.analyze_commit_message(message)
            results.append(analysis)
        
        # Generate batch statistics
        stats = self._calculate_batch_statistics(results)
        
        return {
            'success': True,
            'total_commits': len(messages),
            'results': results,
            'statistics': stats
        }
    
    async def analyze_developer_patterns(self, developer_commits: Dict[str, List[str]]) -> Dict[str, Any]:
        """
        Analyze commit patterns for each developer
        
        Args:
            developer_commits: Dict mapping developer name to list of commits
            
        Returns:
            Developer pattern analysis
        """
        developer_profiles = {}
        
        for developer, commits in developer_commits.items():
            if not commits:
                continue
                
            # Analyze all commits for this developer
            batch_result = await self.analyze_commits_batch(commits)
            
            # Create developer profile
            profile = self._create_developer_profile(commits, batch_result)
            developer_profiles[developer] = profile
        
        return {
            'success': True,
            'developer_profiles': developer_profiles,
            'total_developers': len(developer_profiles)
        }
    
    async def suggest_task_assignment(self, tasks: List[Dict], developers: List[Dict]) -> Dict[str, Any]:
        """
        Suggest task assignments based on commit analysis and developer profiles
        
        Args:
            tasks: List of task dictionaries
            developers: List of developer dictionaries with commit history
            
        Returns:
            Task assignment suggestions
        """
        try:
            # Analyze developer patterns first
            developer_commits = {}
            for dev in developers:
                developer_commits[dev['login']] = dev.get('recent_commits', [])
            
            developer_analysis = await self.analyze_developer_patterns(developer_commits)
            
            # Generate task assignments
            assignments = []
            for task in tasks:
                assignment = self._match_task_to_developer(
                    task, 
                    developer_analysis['developer_profiles']
                )
                assignments.append(assignment)
            
            return {
                'success': True,
                'assignments': assignments,
                'developer_analysis': developer_analysis
            }
            
        except Exception as e:
            logger.error(f"Error in task assignment: {e}")
            return {
                'success': False,
                'error': str(e),
                'assignments': self._mock_task_assignments(tasks, developers)
            }
    
    async def generate_project_insights(self, project_data: Dict) -> Dict[str, Any]:
        """
        Generate comprehensive project insights based on commit analysis
        
        Args:
            project_data: Project data including commits, contributors, etc.
            
        Returns:
            Project insights and recommendations
        """
        try:
            all_commits = project_data.get('commits', [])
            contributors = project_data.get('contributors', [])
            
            # Analyze all commits
            commit_messages = [commit.get('message', '') for commit in all_commits]
            batch_analysis = await self.analyze_commits_batch(commit_messages)
            
            # Generate insights
            insights = {
                'commit_analysis': batch_analysis,
                'code_quality_trends': self._analyze_quality_trends(batch_analysis),
                'team_collaboration': self._analyze_team_collaboration(all_commits, contributors),
                'project_health': self._assess_project_health(batch_analysis),
                'recommendations': self._generate_recommendations(batch_analysis)
            }
            
            return {
                'success': True,
                'project_name': project_data.get('name', 'Unknown'),
                'insights': insights
            }
            
        except Exception as e:
            logger.error(f"Error generating project insights: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _mock_commit_analysis(self, message: str) -> Dict[str, Any]:
        """Mock analysis when model is not available"""
        # Simple keyword-based mock analysis
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['fix', 'bug', 'error', 'issue']):
            category = 'bug'
            impact = 'medium'
            urgency = 'high'
        elif any(word in message_lower for word in ['feat', 'feature', 'add', 'new']):
            category = 'feature'
            impact = 'high'
            urgency = 'medium'
        elif any(word in message_lower for word in ['docs', 'doc', 'readme']):
            category = 'docs'
            impact = 'low'
            urgency = 'low'
        elif any(word in message_lower for word in ['test', 'spec']):
            category = 'test'
            impact = 'medium'
            urgency = 'low'
        else:
            category = 'chore'
            impact = 'low'
            urgency = 'medium'
        
        return {
            'success': True,
            'analysis': {
                'category': category,
                'impact': impact,
                'urgency': urgency,
                'is_mock': True
            }
        }
    
    def _calculate_batch_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """Calculate statistics from batch analysis results"""
        if not results:
            return {}
        
        categories = {}
        impacts = {}
        urgencies = {}
        successful = 0
        
        for result in results:
            if result.get('success', False):
                successful += 1
                analysis = result.get('analysis', {})
                
                # Count categories
                category = analysis.get('category', 'unknown')
                categories[category] = categories.get(category, 0) + 1
                
                # Count impacts
                impact = analysis.get('impact', 'unknown')
                impacts[impact] = impacts.get(impact, 0) + 1
                
                # Count urgencies
                urgency = analysis.get('urgency', 'unknown')
                urgencies[urgency] = urgencies.get(urgency, 0) + 1
        
        return {
            'total_analyzed': len(results),
            'successful_analyses': successful,
            'success_rate': successful / len(results) if results else 0,
            'category_distribution': categories,
            'impact_distribution': impacts,
            'urgency_distribution': urgencies
        }
    
    def _create_developer_profile(self, commits: List[str], analysis_result: Dict) -> Dict[str, Any]:
        """Create developer profile from commit analysis"""
        stats = analysis_result.get('statistics', {})
        
        return {
            'total_commits': len(commits),
            'preferred_categories': self._get_top_categories(stats.get('category_distribution', {})),
            'impact_pattern': self._get_top_categories(stats.get('impact_distribution', {})),
            'urgency_pattern': self._get_top_categories(stats.get('urgency_distribution', {})),
            'activity_score': min(len(commits) / 10, 10),  # Scale 0-10
            'specialization': self._determine_specialization(stats)
        }
    
    def _get_top_categories(self, distribution: Dict[str, int], top_n: int = 3) -> List[str]:
        """Get top N categories from distribution"""
        if not distribution:
            return []
        
        sorted_items = sorted(distribution.items(), key=lambda x: x[1], reverse=True)
        return [item[0] for item in sorted_items[:top_n]]
    
    def _determine_specialization(self, stats: Dict) -> str:
        """Determine developer specialization based on commit patterns"""
        categories = stats.get('category_distribution', {})
        
        if not categories:
            return 'generalist'
        
        top_category = max(categories.items(), key=lambda x: x[1])
        total_commits = sum(categories.values())
        
        if top_category[1] / total_commits > 0.5:
            return f"{top_category[0]}_specialist"
        else:
            return 'generalist'
    
    def _match_task_to_developer(self, task: Dict, developer_profiles: Dict) -> Dict[str, Any]:
        """Match a task to the best developer based on profiles"""
        task_type = task.get('type', 'feature').lower()
        task_priority = task.get('priority', 'medium').lower()
        
        best_match = None
        best_score = 0
        
        for dev_name, profile in developer_profiles.items():
            score = self._calculate_match_score(task_type, task_priority, profile)
            if score > best_score:
                best_score = score
                best_match = dev_name
        
        return {
            'task_title': task.get('title', 'Untitled'),
            'task_type': task_type,
            'recommended_developer': best_match or 'No suitable match',
            'confidence_score': best_score,
            'reasoning': self._generate_assignment_reasoning(task, best_match, developer_profiles.get(best_match, {}))
        }
    
    def _calculate_match_score(self, task_type: str, task_priority: str, profile: Dict) -> float:
        """Calculate match score between task and developer"""
        score = 0.0
        
        # Check category preference
        preferred_categories = profile.get('preferred_categories', [])
        if task_type in preferred_categories:
            score += 0.4
        
        # Check specialization
        specialization = profile.get('specialization', '')
        if task_type in specialization:
            score += 0.3
        
        # Activity score
        activity_score = profile.get('activity_score', 0)
        score += (activity_score / 10) * 0.3
        
        return score
    
    def _generate_assignment_reasoning(self, task: Dict, developer: str, profile: Dict) -> str:
        """Generate reasoning for task assignment"""
        if not developer or not profile:
            return "No suitable developer found based on commit analysis"
        
        specialization = profile.get('specialization', 'generalist')
        activity_score = profile.get('activity_score', 0)
        
        return f"Recommended {developer} based on {specialization} specialization and activity score of {activity_score:.1f}/10"
    
    def _mock_task_assignments(self, tasks: List[Dict], developers: List[Dict]) -> List[Dict]:
        """Generate mock task assignments when model is unavailable"""
        assignments = []
        
        for i, task in enumerate(tasks):
            dev_index = i % len(developers) if developers else 0
            developer = developers[dev_index]['login'] if developers else 'Unknown'
            
            assignments.append({
                'task_title': task.get('title', 'Untitled'),
                'recommended_developer': developer,
                'confidence_score': 0.5,
                'reasoning': 'Mock assignment - HAN model not available'
            })
        
        return assignments
    
    def _analyze_quality_trends(self, analysis: Dict) -> Dict[str, Any]:
        """Analyze code quality trends from commit analysis"""
        stats = analysis.get('statistics', {})
        categories = stats.get('category_distribution', {})
        
        bug_ratio = categories.get('bug', 0) / max(stats.get('total_analyzed', 1), 1)
        test_ratio = categories.get('test', 0) / max(stats.get('total_analyzed', 1), 1)
        
        quality_score = max(0, 1 - bug_ratio + test_ratio * 0.5)
        
        return {
            'quality_score': round(quality_score, 2),
            'bug_fix_ratio': round(bug_ratio, 2),
            'test_coverage_indicator': round(test_ratio, 2),
            'trend': 'improving' if quality_score > 0.7 else 'needs_attention'
        }
    
    def _analyze_team_collaboration(self, commits: List[Dict], contributors: List[Dict]) -> Dict[str, Any]:
        """Analyze team collaboration patterns"""
        return {
            'total_contributors': len(contributors),
            'commit_distribution': 'balanced',  # Simplified
            'collaboration_score': 0.8  # Mock score
        }
    
    def _assess_project_health(self, analysis: Dict) -> Dict[str, Any]:
        """Assess overall project health"""
        stats = analysis.get('statistics', {})
        success_rate = stats.get('success_rate', 0)
        
        health_score = success_rate * 0.8 + 0.2  # Base score
        
        return {
            'health_score': round(health_score, 2),
            'status': 'healthy' if health_score > 0.7 else 'needs_attention',
            'analysis_coverage': f"{stats.get('successful_analyses', 0)}/{stats.get('total_analyzed', 0)}"
        }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Generate project recommendations based on analysis"""
        recommendations = []
        stats = analysis.get('statistics', {})
        categories = stats.get('category_distribution', {})
        
        total = sum(categories.values()) if categories else 1
        
        if categories.get('bug', 0) / total > 0.3:
            recommendations.append("Consider increasing code review practices to reduce bug fixes")
        
        if categories.get('test', 0) / total < 0.1:
            recommendations.append("Increase test coverage to improve code quality")
        
        if categories.get('docs', 0) / total < 0.05:
            recommendations.append("Improve documentation practices")
        
        if not recommendations:
            recommendations.append("Project shows good development practices")
        
        return recommendations

# Singleton instance
_han_ai_service = None

def get_han_ai_service() -> HANAIService:
    """Get singleton instance of HAN AI Service"""
    global _han_ai_service
    if _han_ai_service is None:
        _han_ai_service = HANAIService()
    return _han_ai_service

```

### backend\services\issue_service.py
```py
from db.database import database
from db.models.issues import issues
from sqlalchemy import select, insert
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_issue_by_github_id(github_id: int):
    """Get issue by GitHub ID"""
    query = select(issues).where(issues.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

# L∆∞u m·ªôt issue duy nh·∫•t
async def save_issue(issue_data):
    """Save issue with proper datetime conversion"""
    try:
        # Ki·ªÉm tra issue ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_issue = await get_issue_by_github_id(issue_data["github_id"])
        
        if existing_issue:
            logger.info(f"Issue {issue_data['github_id']} already exists, skipping")
            return existing_issue.id

        # Convert datetime strings
        issue_entry = {
            **issue_data,
            "created_at": parse_github_datetime(issue_data.get("created_at")),
            "updated_at": parse_github_datetime(issue_data.get("updated_at"))
        }

        query = insert(issues).values(issue_entry)
        result = await database.execute(query)
        logger.info(f"Created new issue: {issue_data['title']}")
        return result
        
    except Exception as e:
        logger.error(f"Error saving issue {issue_data.get('title')}: {e}")
        raise e

# L∆∞u danh s√°ch nhi·ªÅu issue
async def save_issues(issue_list):
    """Save multiple issues"""
    for issue in issue_list:
        await save_issue(issue)

```

### backend\services\model_loader.py
```py
# KLTN04\backend\services\model_loader.py
import joblib
from pathlib import Path
from typing import Optional, Union
import logging
from functools import lru_cache
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoader:
    _instance = None
    
    def __init__(self):
        try:
            model_path = self._get_model_path()
            logger.info(f"Loading model from {model_path}")
            
            self.model_data = joblib.load(model_path)
            self.model = self.model_data['model']
            self.vectorizer = self.model_data['vectorizer']
            
            # Warm-up predict
            self._warm_up()
            logger.info("Model loaded successfully")
            
        except Exception as e:
            logger.exception("Failed to load model")
            raise

    @staticmethod
    def _get_model_path() -> Path:
        """Validate and return model path"""
        model_path = Path(__file__).parent.parent / "models" / "commit_classifier_v1.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        return model_path

    def _warm_up(self):
        """Warm-up model with sample input"""
        sample = "fix: critical security vulnerability"
        self.predict(sample)
        
    @lru_cache(maxsize=1000)
    def vectorize(self, message: str) -> np.ndarray:
        """Cache vectorized results for frequent messages"""
        return self.vectorizer.transform([message])

    def predict(self, message: str) -> int:
        """Predict if commit is critical (with input validation)"""
        if not message or not isinstance(message, str):
            raise ValueError("Input must be non-empty string")
            
        X = self.vectorize(message.strip())
        return int(self.model.predict(X)[0])

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

def predict_commit(message: str) -> dict:
    """Public API for commit prediction
    
    Returns:
        {
            "prediction": 0|1,
            "confidence": float,
            "error": str|None
        }
    """
    try:
        loader = ModelLoader.get_instance()
        proba = loader.model.predict_proba(loader.vectorize(message))[0]
        return {
            "prediction": loader.predict(message),
            "confidence": float(np.max(proba)),
            "error": None
        }
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        return {
            "prediction": -1,
            "confidence": 0.0,
            "error": str(e)
        }
```

### backend\services\pull_request_service.py
```py
from db.models.pull_requests import pull_requests
from sqlalchemy import select, insert, update, func
from db.database import database
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str):
    """Convert GitHub API datetime string to Python datetime object"""
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_pull_request_by_github_id(github_id: int):
    """L·∫•y pull request t·ª´ github_id"""
    query = select(pull_requests).where(pull_requests.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

async def save_pull_request(pr_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin pull request
    
    Args:
        pr_data (dict): Th√¥ng tin pull request t·ª´ GitHub API
    
    Returns:
        int: pull_request_id
    """
    try:
        # Ki·ªÉm tra xem pull request ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_pr = await get_pull_request_by_github_id(pr_data["github_id"])

        if existing_pr:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(pull_requests)
                .where(pull_requests.c.github_id == pr_data["github_id"])
                .values(
                    title=pr_data.get("title"),
                    description=pr_data.get("description"),
                    state=pr_data.get("state"),
                    updated_at=parse_github_datetime(pr_data.get("updated_at"))
                )
            )
            await database.execute(query)
            logger.info(f"Updated pull request: {pr_data.get('title')}")
            return existing_pr.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(pull_requests).values(
                github_id=pr_data["github_id"],
                title=pr_data.get("title"),
                description=pr_data.get("description"),
                state=pr_data.get("state"),
                repo_id=pr_data["repo_id"],
                created_at=parse_github_datetime(pr_data.get("created_at")),
                updated_at=parse_github_datetime(pr_data.get("updated_at"))
            )
            
            result = await database.execute(query)
            logger.info(f"Created new pull request: {pr_data.get('title')}")
            return result
            
    except Exception as e:
        logger.error(f"Error saving pull request {pr_data.get('title')}: {e}")
        raise e

async def get_pull_requests_by_repo_id(repo_id: int):
    """L·∫•y danh s√°ch pull requests c·ªßa repository"""
    query = select(pull_requests).where(pull_requests.c.repo_id == repo_id)
    results = await database.fetch_all(query)
    return results

```

### backend\services\report_generator.py
```py

```

### backend\services\repository_collaborator_service.py
```py
from db.models.repository_collaborators import repository_collaborators
from sqlalchemy import select, insert, update, func
from db.database import database
import logging

logger = logging.getLogger(__name__)

async def get_repository_collaborator(repository_id: int, user_id: int):
    """L·∫•y th√¥ng tin collaborator c·ªßa repository"""
    query = select(repository_collaborators).where(
        repository_collaborators.c.repository_id == repository_id,
        repository_collaborators.c.user_id == user_id
    )
    result = await database.fetch_one(query)
    return result

async def save_repository_collaborator(collaborator_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin repository collaborator
    
    Args:
        collaborator_data (dict): Th√¥ng tin collaborator
            - repository_id: ID c·ªßa repository
            - user_id: ID c·ªßa user
            - role: vai tr√≤ (admin, write, read, etc.)
            - permissions: quy·ªÅn h·∫°n
            - is_owner: c√≥ ph·∫£i owner kh√¥ng
    
    Returns:
        int: collaborator_id
    """
    try:
        # Ki·ªÉm tra xem collaborator ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_collaborator = await get_repository_collaborator(
            collaborator_data["repository_id"], 
            collaborator_data["user_id"]
        )

        if existing_collaborator:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(repository_collaborators)
                .where(
                    repository_collaborators.c.repository_id == collaborator_data["repository_id"],
                    repository_collaborators.c.user_id == collaborator_data["user_id"]
                )
                .values(
                    role=collaborator_data.get("role", "read"),
                    permissions=collaborator_data.get("permissions"),
                    is_owner=collaborator_data.get("is_owner", False),
                    invitation_status=collaborator_data.get("invitation_status", "accepted"),
                    last_synced=func.now()
                )
            )
            await database.execute(query)
            logger.info(f"Updated repository collaborator: repo_id={collaborator_data['repository_id']}, user_id={collaborator_data['user_id']}")
            return existing_collaborator.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(repository_collaborators).values(
                repository_id=collaborator_data["repository_id"],
                user_id=collaborator_data["user_id"],
                role=collaborator_data.get("role", "read"),
                permissions=collaborator_data.get("permissions"),
                is_owner=collaborator_data.get("is_owner", False),
                joined_at=collaborator_data.get("joined_at"),
                invited_by=collaborator_data.get("invited_by"),
                invitation_status=collaborator_data.get("invitation_status", "accepted"),
                commits_count=collaborator_data.get("commits_count", 0),
                issues_count=collaborator_data.get("issues_count", 0),
                prs_count=collaborator_data.get("prs_count", 0),
                last_activity=collaborator_data.get("last_activity"),
                last_synced=func.now()
            )
            
            result = await database.execute(query)
            logger.info(f"Created new repository collaborator: repo_id={collaborator_data['repository_id']}, user_id={collaborator_data['user_id']}")
            return result
            
    except Exception as e:
        logger.error(f"Error saving repository collaborator: {e}")
        raise e

async def get_collaborators_by_repository_id(repository_id: int):
    """L·∫•y danh s√°ch collaborators c·ªßa repository"""
    query = select(repository_collaborators).where(
        repository_collaborators.c.repository_id == repository_id
    )
    results = await database.fetch_all(query)
    return results

```

### backend\services\repository_service.py
```py

```

### backend\services\repo_service.py
```py
# backend/services/repo_service.py
from .github_service import fetch_from_github
from db.models.repositories import repositories
from sqlalchemy import select, update
from sqlalchemy.sql import func
from db.database import database

async def get_repo_data(owner: str, repo: str):
    url = f"/repos/{owner}/{repo}"
    data = await fetch_from_github(url)

    # Optionally: l·ªçc data b·∫°n mu·ªën tr·∫£ v·ªÅ
    return {
        "name": data.get("name"),
        "full_name": data.get("full_name"),
        "description": data.get("description"),
        "owner": data.get("owner", {}).get("login"),
        "stars": data.get("stargazers_count"),
        "forks": data.get("forks_count"),
        "watchers": data.get("watchers_count"),
        "language": data.get("language"),
        "open_issues": data.get("open_issues_count"),
        "url": data.get("html_url"),
        "created_at": data.get("created_at"),
        "updated_at": data.get("updated_at"),
    }


async def get_repo_id_by_owner_and_name(owner: str, repo_name: str):
    query = select(repositories).where(
        repositories.c.owner == owner,
        repositories.c.name == repo_name
    )
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None


async def save_repository(repo_entry):
    # Ki·ªÉm tra xem repository ƒë√£ t·ªìn t·∫°i ch∆∞a
    query = select(repositories).where(repositories.c.github_id == repo_entry["github_id"])
    existing_repo = await database.fetch_one(query)

    if existing_repo:
        # N·∫øu repository ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin (n·∫øu c·∫ßn)
        update_query = (
            update(repositories)
            .where(repositories.c.github_id == repo_entry["github_id"])
            .values(
                name=repo_entry["name"],
                owner=repo_entry["owner"],
                description=repo_entry["description"],
                stars=repo_entry["stars"],
                forks=repo_entry["forks"],
                language=repo_entry["language"],
                open_issues=repo_entry["open_issues"],
                url=repo_entry["url"],
                updated_at=func.now(),
            )
        )
        await database.execute(update_query)
    else:
        # N·∫øu repository ch∆∞a t·ªìn t·∫°i, ch√®n m·ªõi
        query = repositories.insert().values(repo_entry)
        await database.execute(query)


async def get_repo_by_owner_and_name(owner: str, repo: str):
    """Get repository information by owner and name"""
    query = select(repositories).where(
        repositories.c.owner == owner,
        repositories.c.name == repo
    )
    result = await database.fetch_one(query)
    if result:
        return dict(result)
    return None


async def get_github_repo_id(owner: str, repo: str):
    """Get GitHub repository ID from GitHub API"""
    try:
        url = f"/repos/{owner}/{repo}"
        data = await fetch_from_github(url)
        return data.get("id")  # GitHub repo ID
    except Exception as e:
        print(f"Error getting GitHub repo ID: {e}")
        return None
```

### backend\services\user_resolution_service.py
```py

```

### backend\services\user_service.py
```py
from db.models.users import users
from sqlalchemy import select, insert, update, func
from db.database import database
import logging
from datetime import datetime
from typing import Optional

logger = logging.getLogger(__name__)

def parse_github_datetime(date_str: Optional[str]) -> Optional[datetime]:
    """
    Convert GitHub API datetime string to Python datetime object
    
    Args:
        date_str: GitHub datetime string in ISO format (e.g., '2021-03-06T14:28:54Z')
    
    Returns:
        datetime object or None if parsing fails
    """
    if not date_str:
        return None
    
    try:
        # GitHub datetime format: 2021-03-06T14:28:54Z
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'  # Replace Z with +00:00 for proper parsing
        
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, AttributeError) as e:
        logger.warning(f"Failed to parse datetime '{date_str}': {e}")
        return None

async def get_user_id_by_github_username(username: str):
    """L·∫•y user_id t·ª´ github_username"""
    query = select(users).where(users.c.github_username == username)
    result = await database.fetch_one(query)
    if result:
        return result.id
    return None

async def get_user_by_github_id(github_id: int):
    """L·∫•y user t·ª´ github_id"""
    query = select(users).where(users.c.github_id == github_id)
    result = await database.fetch_one(query)
    return result

async def save_user(user_data):
    """
    L∆∞u ho·∫∑c c·∫≠p nh·∫≠t th√¥ng tin user
    
    Args:
        user_data (dict): Th√¥ng tin user t·ª´ GitHub API
    
    Returns:
        int: user_id
    """
    try:
        # Convert datetime strings to datetime objects
        github_created_at = parse_github_datetime(user_data.get("github_created_at"))
        
        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_user = await get_user_by_github_id(user_data["github_id"])

        if existing_user:
            # N·∫øu ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
            query = (
                update(users)
                .where(users.c.github_id == user_data["github_id"])
                .values(
                    github_username=user_data.get("github_username"),
                    email=user_data.get("email"),
                    display_name=user_data.get("display_name"),
                    full_name=user_data.get("full_name"),
                    avatar_url=user_data.get("avatar_url"),
                    bio=user_data.get("bio"),
                    location=user_data.get("location"),
                    company=user_data.get("company"),
                    blog=user_data.get("blog"),
                    twitter_username=user_data.get("twitter_username"),
                    github_profile_url=user_data.get("github_profile_url"),
                    repos_url=user_data.get("repos_url"),
                    github_created_at=github_created_at,
                    last_synced=func.now(),
                    updated_at=func.now()
                )
            )
            await database.execute(query)
            logger.info(f"Updated user: {user_data.get('github_username')}")
            return existing_user.id
        else:
            # N·∫øu ch∆∞a t·ªìn t·∫°i, th√™m m·ªõi
            query = insert(users).values(
                github_id=user_data["github_id"],
                github_username=user_data.get("github_username"),
                email=user_data.get("email"),
                display_name=user_data.get("display_name"),
                full_name=user_data.get("full_name"),
                avatar_url=user_data.get("avatar_url"),
                bio=user_data.get("bio"),
                location=user_data.get("location"),
                company=user_data.get("company"),
                blog=user_data.get("blog"),
                twitter_username=user_data.get("twitter_username"),
                github_profile_url=user_data.get("github_profile_url"),
                repos_url=user_data.get("repos_url"),
                is_active=True,
                is_verified=False,
                github_created_at=github_created_at,
                last_synced=func.now(),
                created_at=func.now(),
                updated_at=func.now()
            )
            
            result = await database.execute(query)
            user_id = result
            logger.info(f"Created new user: {user_data.get('github_username')}")
            return user_id
            
    except Exception as e:
        logger.error(f"Error saving user {user_data.get('github_username')}: {e}")
        raise e

```

### backend\services\__init__.py
```py

```

### backend\utils\formatter.py
```py

```

### backend\utils\scheduler.py
```py

```

### backend\utils\__init__.py
```py

```

### frontend\eslint.config.js
```js
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]

```

### frontend\vite.config.js
```js
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import { fileURLToPath, URL } from 'node:url'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url)),
      '@components': fileURLToPath(new URL('./src/components', import.meta.url)),
      '@dashboard': fileURLToPath(new URL('./src/components/Dashboard', import.meta.url)),
      '@taskmanager': fileURLToPath(new URL('./src/components/Dashboard/ProjectTaskManager', import.meta.url)),
    }
  }
})

```

### frontend\src\App.jsx
```jsx
import { BrowserRouter as Router, Routes, Route, Navigate } from "react-router-dom";
import Login from "./pages/Login";
import AuthSuccess from "./pages/AuthSuccess";
import Dashboard from "./pages/Dashboard"; 
import RepoDetails from "./pages/RepoDetails";
import CommitTable from './components/commits/CommitTable';
import TestPage from './pages/TestPage';

function App() {
  return (
    <Router>
      <Routes>
        {/* ‚úÖ Test route */}
        <Route path="/test" element={<TestPage />} />
        
        {/* ‚úÖ Trang m·∫∑c ƒë·ªãnh l√† Login */}
        <Route path="/" element={<Navigate to="/login" />} />

        {/* C√°c route ch√≠nh */}
        <Route path="/login" element={<Login />} />
        <Route path="/auth-success" element={<AuthSuccess />} />
        <Route path="/dashboard" element={<Dashboard />} />
        <Route path="/repo/:owner/:repo" element={<RepoDetails />} />
        <Route path="/commits" element={<CommitTable />} />

      </Routes>
    </Router>
  );
}

export default App;

```

### frontend\src\config.js
```js

```

### frontend\src\main.jsx
```jsx
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css';
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)

```

### frontend\src\api\github.js
```js

```

### frontend\src\components\AliasTest.jsx
```jsx
// Test component ƒë·ªÉ ki·ªÉm tra alias
import React from 'react';
// Test named imports from index
import { 
  RepoSelector, 
  StatisticsPanel, 
  FiltersPanel 
} from '@taskmanager/index';

const AliasTest = () => {
  console.log('‚úÖ Alias @taskmanager ho·∫°t ƒë·ªông!');
  console.log('Components imported:', { RepoSelector, StatisticsPanel, FiltersPanel });
  
  return (
    <div style={{ padding: '20px', border: '2px solid green', borderRadius: '8px' }}>
      <h3>‚úÖ Alias Test th√†nh c√¥ng!</h3>
      <p>ƒê√£ import th√†nh c√¥ng t·ª´ @taskmanager:</p>
      <ul>
        <li>RepoSelector: {RepoSelector ? '‚úÖ' : '‚ùå'}</li>
        <li>StatisticsPanel: {StatisticsPanel ? '‚úÖ' : '‚ùå'}</li>
        <li>FiltersPanel: {FiltersPanel ? '‚úÖ' : '‚ùå'}</li>
      </ul>
    </div>
  );
};

export default AliasTest;

```

### frontend\src\components\SimpleAliasTest.jsx
```jsx
// Simple test for alias
import React from 'react';

// Test individual imports
import RepoSelector from '@taskmanager/RepoSelector';
import StatisticsPanel from '@taskmanager/StatisticsPanel';

const SimpleAliasTest = () => {
  console.log('Testing individual imports:', { RepoSelector, StatisticsPanel });
  
  return (
    <div style={{ padding: '10px', background: '#f0f0f0', margin: '10px' }}>
      <h4>Simple Alias Test</h4>
      <p>RepoSelector: {RepoSelector ? '‚úÖ Loaded' : '‚ùå Failed'}</p>
      <p>StatisticsPanel: {StatisticsPanel ? '‚úÖ Loaded' : '‚ùå Failed'}</p>
    </div>
  );
};

export default SimpleAliasTest;

```

### frontend\src\components\AI\AICommitAnalyzer.jsx
```jsx

```

### frontend\src\components\AI\AIDashboard.jsx
```jsx

```

### frontend\src\components\AI\AIRepositoryInsights.jsx
```jsx

```

### frontend\src\components\AI\index.js
```js

```

### frontend\src\components\Branchs\BranchSelector.jsx
```jsx
import { useEffect, useState } from "react";
import { Select, Spin, message, Tag, Typography, Divider } from "antd";
import { GithubOutlined, BranchesOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Option } = Select;
const { Text } = Typography;

const SelectContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 8px 12px;
  background: #f8f9fa;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
`;

const StyledSelect = styled(Select)`
  min-width: 240px;
  
  .ant-select-selector {
    border-radius: 6px !important;
    border: 1px solid #d9d9d9 !important;
    transition: all 0.3s !important;
    
    &:hover {
      border-color: #1890ff !important;
    }
  }
  
  .ant-select-selection-item {
    font-weight: 500;
  }
`;

const BranchTag = styled(Tag)`
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  border-radius: 6px;
  background: #f0f5ff;
  color: #1890ff;
  border: 1px solid #d6e4ff;
`;

const BranchSelector = ({ owner, repo, onBranchChange }) => {
  const [branches, setBranches] = useState([]);
  const [loading, setLoading] = useState(true);
  const [selectedBranch, setSelectedBranch] = useState(null);

  useEffect(() => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchBranches = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/branches`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setBranches(response.data);
        if (response.data.length > 0) {
          setSelectedBranch(response.data[0].name);
          onBranchChange(response.data[0].name);
        }
      } catch (err) {
        console.error(err);
        message.error("Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch branch");
      } finally {
        setLoading(false);
      }
    };

    fetchBranches();
  }, [owner, repo]);

  const handleChange = (value) => {
    setSelectedBranch(value);
    onBranchChange(value);
  };

  if (loading) return <Spin size="small" />;

  return (
    <div style={{ marginBottom: 16 }}>
      {/* <Divider orientation="left" style={{ fontSize: 32, color: '#666' }}>
        Ch·ªçn branch
      </Divider> */}
      
      <SelectContainer>
        <BranchTag>
          <BranchesOutlined />
          <Text strong>Branch:</Text>
        </BranchTag>
        
        <StyledSelect
          value={selectedBranch}
          onChange={handleChange}
          suffixIcon={<GithubOutlined style={{ color: '#1890ff' }} />}
          dropdownMatchSelectWidth={false}
        >
          {branches.map((branch) => (
            <Option key={branch.name} value={branch.name}>
              <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                <BranchesOutlined style={{ color: '#52c41a' }} />
                <Text strong>{branch.name}</Text>
              </div>
            </Option>
          ))}
        </StyledSelect>
      </SelectContainer>
    </div>
  );
};

export default BranchSelector;
```

### frontend\src\components\commits\AnalyzeGitHubCommits.jsx
```jsx
import { useState } from 'react';
import { Button, Badge, Popover, List, Typography, Divider, Spin, Tag, Alert, Tooltip } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled, InfoCircleOutlined } from '@ant-design/icons';
import axios from 'axios';

const { Text, Title } = Typography;

const AnalyzeGitHubCommits = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [popoverVisible, setPopoverVisible] = useState(false);

  const analyzeCommits = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      
      if (!token) {
        throw new Error('Authentication required');
      }

      const response = await axios.get(
        `http://localhost:8000/api/commits/analyze-github/${repo.owner.login}/${repo.name}`,
        {
          headers: { 
            Authorization: `Bearer ${token}`,
            Accept: "application/json"
          },
          params: { 
            per_page: 10,
            // Add cache busting to avoid stale data
            timestamp: Date.now()
          },
          timeout: 10000 // 10 second timeout
        }
      );
      
      if (!response.data) {
        throw new Error('Invalid response data');
      }

      setAnalysis(response.data);
    } catch (err) {
      let errorMessage = 'Failed to analyze commits';
      
      if (err.response) {
        if (err.response.status === 401) {
          errorMessage = 'Please login to analyze commits';
        } else if (err.response.status === 403) {
          errorMessage = 'Access to this repository is denied';
        } else if (err.response.data?.detail) {
          errorMessage = err.response.data.detail;
        }
      } else if (err.message) {
        errorMessage = err.message;
      }

      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  const handlePopoverOpen = (visible) => {
    setPopoverVisible(visible);
    if (visible && !analysis && !error) {
      analyzeCommits();
    }
  };

  const getStatusColor = () => {
    if (error) return 'warning';
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (error) return 'Error';
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical` 
      : 'No Issues';
  };

  const getStatusIcon = () => {
    if (error) return <InfoCircleOutlined />;
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };
  const renderContent = () => {
    if (loading) {
      return (
        <div style={{ textAlign: 'center', padding: '20px' }}>
          <Spin size="small" />
          <div style={{ marginTop: 8 }}>
            <Text type="secondary">Analyzing commits...</Text>
          </div>
        </div>
      );
    }

    if (error) {
      return (
        <Alert
          message="Analysis Failed"
          description={error}
          type="error"
          showIcon
        />
      );
    }

    if (!analysis) {
      return <Text type="secondary">Click to analyze commits</Text>;
    }

    return (
      <>
        <div style={{ marginBottom: 16 }}>
          <Title level={5} style={{ marginBottom: 4 }}>
            Commit Analysis Summary
          </Title>
          <Text>
            <Tag color={analysis.critical > 0 ? 'error' : 'success'}>
              {analysis.critical > 0 ? 'Needs Review' : 'All Clear'}
            </Tag>
            {analysis.critical} of {analysis.total} commits are critical
          </Text>
        </div>

        <Divider style={{ margin: '12px 0' }} />

        <List
          size="small"
          dataSource={analysis.details.slice(0, 5)}
          renderItem={item => (
            <List.Item>
              <div style={{ width: '100%' }}>
                <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                  <Tag color={item.is_critical ? 'error' : 'success'}>
                    {item.is_critical ? 'CRITICAL' : 'Normal'}
                  </Tag>
                  <Tooltip title="Commit ID">
                    <Text code style={{ fontSize: 12 }}>
                      {item.id.substring(0, 7)}
                    </Text>
                  </Tooltip>
                </div>
                <Text
                  ellipsis={{ tooltip: item.message_preview }}
                  style={{ 
                    color: item.is_critical ? '#f5222d' : 'inherit',
                    marginTop: 4,
                    display: 'block'
                  }}
                >
                  {item.message_preview}
                </Text>
              </div>
            </List.Item>
          )}
        />

        {analysis.total > 5 && (
          <Text type="secondary" style={{ display: 'block', marginTop: 8 }}>
            Showing 5 of {analysis.total} commits
          </Text>
        )}
      </>
    );
  };

  return (
    <Popover 
      content={renderContent()}
      title={
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
          <span>Commit Analysis</span>
          {analysis && (
            <Badge 
              count={`${analysis.critical_percentage}%`} 
              style={{ 
                backgroundColor: analysis.critical > 0 ? '#f5222d' : '#52c41a'
              }} 
            />
          )}
        </div>
      }
      trigger="click"
      open={popoverVisible}
      onOpenChange={handlePopoverOpen}
      overlayStyle={{ width: 350 }}
      placement="bottomRight"
    >
      <Badge 
        count={analysis?.critical || 0} 
        color={getStatusColor()}
        offset={[-10, 10]}
      >
        <Button 
          type={error ? 'default' : analysis ? (analysis.critical ? 'danger' : 'success') : 'default'}
          icon={getStatusIcon()}
          loading={loading}
          onClick={(e) => e.stopPropagation()}
          style={{ 
            marginLeft: 'auto',
            fontWeight: 500,
            borderRadius: 20,
            padding: '0 16px',
            border: error ? '1px solid #faad14' : undefined
          }}
        >
          {getStatusText()}
        </Button>
      </Badge>
    </Popover>
  );
};

export default AnalyzeGitHubCommits;
```

### frontend\src\components\commits\CommitAnalysisBadge.jsx
```jsx
// components/CommitAnalysisBadge.jsx
import { Tag, Tooltip, Popover, List, Typography, Divider, Badge, Spin } from 'antd';
import { ExclamationCircleFilled, CheckCircleFilled } from '@ant-design/icons';
import { useState } from 'react';
import axios from 'axios';

const { Text } = Typography;

const CommitAnalysisBadge = ({ repo }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchCommitAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 5 } // Get last 5 commits for analysis
        }
      );
      
      // Analyze the commits
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  const getStatusColor = () => {
    if (!analysis) return 'default';
    return analysis.critical > 0 ? 'error' : 'success';
  };

  const getStatusText = () => {
    if (!analysis) return 'Analyze Commits';
    return analysis.critical > 0 
      ? `${analysis.critical} Critical Commits` 
      : 'No Critical Commits';
  };

  const getStatusIcon = () => {
    if (!analysis) return null;
    return analysis.critical > 0 
      ? <ExclamationCircleFilled /> 
      : <CheckCircleFilled />;
  };

  const content = (
    <div style={{ maxWidth: 300 }}>
      {loading && <Spin size="small" />}
      {error && <Text type="danger">{error}</Text>}
      {analysis && (
        <>
          <Text strong>Recent Commits Analysis</Text>
          <Divider style={{ margin: '8px 0' }} />
          <List
            size="small"
            dataSource={analysis.details.slice(0, 5)}
            renderItem={item => (
              <List.Item>
                <div style={{ width: '100%' }}>
                  <div style={{ 
                    display: 'flex', 
                    justifyContent: 'space-between',
                    marginBottom: 4
                  }}>
                    <Text 
                      ellipsis 
                      style={{ 
                        maxWidth: 180,
                        color: item.is_critical ? '#f5222d' : 'inherit'
                      }}
                    >
                      {item.message_preview}
                    </Text>
                    <Tag color={item.is_critical ? 'error' : 'success'}>
                      {item.is_critical ? 'Critical' : 'Normal'}
                    </Tag>
                  </div>
                  <Text type="secondary" style={{ fontSize: 12 }}>
                    {item.id.substring(0, 7)}
                  </Text>
                </div>
              </List.Item>
            )}
          />
          <Divider style={{ margin: '8px 0' }} />
          <Text type="secondary">
            {analysis.critical} of {analysis.total} recent commits are critical
          </Text>
        </>
      )}
    </div>
  );

  return (
    <Popover 
      content={content}
      title="Commit Analysis"
      trigger="click"
      onVisibleChange={visible => visible && !analysis && fetchCommitAnalysis()}
    >
      <Badge 
        count={analysis?.critical || 0} 
        style={{ backgroundColor: getStatusColor() }}
      >
        <Tag 
          icon={getStatusIcon()}
          color={getStatusColor()}
          style={{ cursor: 'pointer' }}
        >
          {getStatusText()}
        </Tag>
      </Badge>
    </Popover>
  );
};

export default CommitAnalysisBadge;
```

### frontend\src\components\commits\CommitAnalysisModal.jsx
```jsx
// components/CommitAnalysisModal.jsx
import { Modal, List, Typography, Tag, Divider, Spin, Tabs, Progress, Alert } from 'antd';
import { 
  ExclamationCircleOutlined, 
  CheckCircleOutlined,
  BarChartOutlined,
  FileTextOutlined 
} from '@ant-design/icons';
import axios from 'axios';
import { useState, useEffect } from 'react';

const { Title, Text } = Typography;

const CommitAnalysisModal = ({ repo, visible, onCancel }) => {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const fetchFullAnalysis = async () => {
    try {
      setLoading(true);
      setError(null);
      const token = localStorage.getItem("access_token");
      const response = await axios.get(
        `http://localhost:8000/api/github/repos/${repo.owner.login}/${repo.name}/commits`,
        {
          headers: { Authorization: `token ${token}` },
          params: { per_page: 100 } // Get more commits for detailed analysis
        }
      );
      
      const analysisRes = await axios.post(
        'http://localhost:8000/api/commits/analyze-json',
        {
          commits: response.data.map(commit => ({
            id: commit.sha,
            message: commit.commit.message
          }))
        },
        {
          headers: { Authorization: `token ${token}` }
        }
      );
      
      setAnalysis(analysisRes.data);
    } catch (err) {
      setError(err.response?.data?.detail || 'Failed to analyze commits');
    } finally {
      setLoading(false);
    }
  };

  useEffect(() => {
    if (visible) {
      fetchFullAnalysis();
    }
  }, [visible]);

  const criticalPercentage = analysis 
    ? Math.round((analysis.critical / analysis.total) * 100) 
    : 0;

  return (
    <Modal
      title={<><BarChartOutlined /> Commit Analysis for {repo.name}</>}
      visible={visible}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {loading && <Spin size="large" style={{ display: 'block', margin: '40px auto' }} />}
      
      {error && (
        <Alert 
          message="Error" 
          description={error} 
          type="error" 
          showIcon 
          style={{ marginBottom: 20 }}
        />
      )}
        {analysis && (
        <Tabs 
          defaultActiveKey="1"
          items={[
            {
              key: '1',
              label: (
                <span>
                  <FileTextOutlined /> Commits
                </span>
              ),
              children: (
                <div style={{ marginBottom: 20 }}>
                  <div style={{ display: 'flex', alignItems: 'center', marginBottom: 16 }}>
                    <Progress
                      type="circle"
                      percent={criticalPercentage}
                      width={80}
                      format={percent => (
                        <Text strong style={{ fontSize: 24, color: percent > 0 ? '#f5222d' : '#52c41a' }}>
                          {percent}%
                        </Text>
                      )}
                      status={criticalPercentage > 0 ? 'exception' : 'success'}
                    />
                    <div style={{ marginLeft: 20 }}>
                      <Title level={4} style={{ marginBottom: 0 }}>
                        {analysis.critical} of {analysis.total} commits are critical
                      </Title>
                      <Text type="secondary">
                        {criticalPercentage > 0 
                          ? 'This repository contains potentially critical changes'
                          : 'No critical commits detected'}
                      </Text>
                    </div>
                  </div>
                  
                  <List
                    size="large"
                    dataSource={analysis.details}
                    renderItem={item => (
                      <List.Item>
                        <div style={{ width: '100%' }}>
                          <div style={{ display: 'flex', justifyContent: 'space-between' }}>
                            <Tag color={item.is_critical ? 'error' : 'success'}>
                              {item.is_critical ? 'CRITICAL' : 'Normal'}
                            </Tag>
                            <Text type="secondary" copyable>
                              {item.id.substring(0, 7)}
                            </Text>
                          </div>
                          <Divider style={{ margin: '8px 0' }} />
                          <Text style={{ color: item.is_critical ? '#f5222d' : 'inherit' }}>
                            {item.message_preview}
                          </Text>
                        </div>
                      </List.Item>
                    )}
                  />
                </div>
              )
            },
            {
              key: '2', 
              label: (
                <span>
                  <ExclamationCircleOutlined /> Critical Commits
                </span>
              ),
              children: analysis.critical > 0 ? (
                <List
                  dataSource={analysis.details.filter(c => c.is_critical)}
                  renderItem={item => (
                    <List.Item>
                      <Alert
                        message="Critical Commit"
                        description={
                          <>
                            <Text strong style={{ display: 'block', marginBottom: 4 }}>
                              {item.message_preview}
                            </Text>
                            <Text type="secondary">Commit ID: {item.id.substring(0, 7)}</Text>
                          </>
                        }
                        type="error"
                        showIcon
                      />
                    </List.Item>
                  )}
                />
              ) : (
                <div style={{ textAlign: 'center', padding: '40px 0' }}>
                  <CheckCircleOutlined style={{ fontSize: 48, color: '#52c41a', marginBottom: 20 }} />
                  <Title level={4} style={{ color: '#52c41a' }}>
                    No Critical Commits Found
                  </Title>
                  <Text type="secondary">
                    All analyzed commits appear to be normal changes
                  </Text>
                </div>
              )
            }
          ]}
        />
      )}
    </Modal>
  );
};

export default CommitAnalysisModal;
```

### frontend\src\components\commits\CommitList.jsx
```jsx
import { useEffect, useState } from "react";
import { List, Avatar, Typography, Spin, message, Tooltip, Card, Tag, Pagination } from "antd";
import { GithubOutlined, BranchesOutlined, ClockCircleOutlined, UserOutlined } from '@ant-design/icons';
import axios from "axios";
import styled from "styled-components";

const { Title, Text } = Typography;

const CommitCard = styled(Card)`
  margin-bottom: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
  }
`;

const CommitHeader = styled.div`
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
`;

const CommitMessage = styled.div`
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  font-weight: 500;
  
  &:hover {
    white-space: normal;
    overflow: visible;
  }
`;

const CommitMeta = styled.div`
  display: flex;
  align-items: center;
  gap: 12px;
  margin-top: 8px;
  color: #666;
  font-size: 13px;
`;

const PaginationContainer = styled.div`
  display: flex;
  justify-content: center;
  margin-top: 20px;
`;

const CommitList = ({ owner, repo, branch }) => {
  const [commits, setCommits] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const pageSize = 5;

  useEffect(() => {
    if (!branch) return;

    const token = localStorage.getItem("access_token");
    if (!token) return;

    const fetchCommits = async () => {
      try {
        const response = await axios.get(
          `http://localhost:8000/api/github/${owner}/${repo}/commits?branch=${branch}`,
          {
            headers: {
              Authorization: `token ${token}`,
            },
          }
        );
        setCommits(response.data);
      } catch (err) {
        console.error(err);
        message.error("L·ªói khi l·∫•y danh s√°ch commit");
      } finally {
        setLoading(false);
      }
    };

    setLoading(true);
    fetchCommits();
  }, [owner, repo, branch]);

  const formatDate = (dateString) => {
    const options = { year: 'numeric', month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' };
    return new Date(dateString).toLocaleDateString('vi-VN', options);
  };

  // T√≠nh to√°n d·ªØ li·ªáu hi·ªÉn th·ªã theo trang hi·ªán t·∫°i
  const paginatedCommits = commits.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );

  if (loading) {
    return (
      <div style={{ textAlign: 'center', padding: '40px' }}>
        <Spin size="large" />
        <div style={{ marginTop: 16 }}>
          <Text>ƒêang t·∫£i commit...</Text>
        </div>
      </div>
    );
  }

  return (
    <div style={{ padding: '16px' }}>
      <div style={{ display: 'flex', alignItems: 'center', marginBottom: '20px' }}>
        <Title level={4} style={{ margin: 0 }}>
          <BranchesOutlined style={{ marginRight: '8px', color: '#1890ff' }} />
          Commit tr√™n branch: <Tag color="blue">{branch}</Tag>
          <Tag style={{ marginLeft: '8px' }}>{commits.length} commits</Tag>
        </Title>
      </div>
      
      <List
        itemLayout="vertical"
        dataSource={paginatedCommits}
        renderItem={(item) => (
          <List.Item>
            <CommitCard>
              <CommitHeader>
                <Tooltip title={item.sha} placement="topLeft">
                  <Tag icon={<GithubOutlined />} color="default">
                    {item.sha.substring(0, 7)}
                  </Tag>
                </Tooltip>
              </CommitHeader>
              
              <CommitMessage>
                <Tooltip title={item.commit.message} placement="topLeft">
                  {item.commit.message.split('\n')[0]}
                </Tooltip>
              </CommitMessage>
              
              <CommitMeta>
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <Avatar 
                    src={item.author?.avatar_url} 
                    size="small" 
                    icon={<UserOutlined />}
                    style={{ marginRight: '8px' }}
                  />
                  <Text>{item.commit.author.name}</Text>
                </div>
                
                <div style={{ display: 'flex', alignItems: 'center' }}>
                  <ClockCircleOutlined style={{ marginRight: '4px' }} />
                  <Text>{formatDate(item.commit.author.date)}</Text>
                </div>
              </CommitMeta>
            </CommitCard>
          </List.Item>
        )}
      />

      <PaginationContainer>
        <Pagination
          current={currentPage}
          pageSize={pageSize}
          total={commits.length}
          onChange={(page) => setCurrentPage(page)}
          showSizeChanger={false}
          showQuickJumper
          style={{ marginTop: '20px' }}
        />
      </PaginationContainer>
    </div>
  );
};

export default CommitList;
```

### frontend\src\components\commits\CommitTable.jsx
```jsx
//frontend\src\components\commits\CommitTable.jsxCommitTable.jsx

import { useEffect, useState } from 'react';
import { Table } from 'antd';
import axios from 'axios';

const CommitTable = () => {
  const [commits, setCommits] = useState([]);

  useEffect(() => {
    const fetchCommits = async () => {
      try {
        const response = await axios.get('http://localhost:8000/commits');
        setCommits(response.data);
      } catch (error) {
        console.error('Failed to fetch commits:', error);
      }
    };
    fetchCommits();
  }, []);

  const columns = [
    {
      title: 'ID',
      dataIndex: 'id',
    },
    {
      title: 'Repo ID',
      dataIndex: 'repo_id',
    },
    {
      title: 'User ID',
      dataIndex: 'user_id',
    },
    {
      title: 'Message',
      dataIndex: 'message',
    },
    {
      title: 'Hash',
      dataIndex: 'commit_hash',
    },
    {
      title: 'Date',
      dataIndex: 'commit_date',
    },
  ];

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-4">L·ªãch s·ª≠ Commit</h2>
      <Table columns={columns} dataSource={commits} rowKey="id" />
    </div>
  );
};

export default CommitTable;
```

### frontend\src\components\common\SyncProgressNotification.jsx
```jsx
import React, { useState, useEffect, useLayoutEffect } from 'react';
import { Progress, Card, Typography, Button, Space } from 'antd';
import { CloseOutlined, CheckCircleOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Text } = Typography;

const ProgressContainer = styled(Card)`
  position: fixed;
  top: 80px;
  right: 20px;
  width: 320px;
  z-index: 1000;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  border-radius: 8px;
  opacity: ${props => props.visible ? 1 : 0};
  transform: ${props => props.visible ? 'translateX(0)' : 'translateX(100%)'};
  transition: opacity 0.1s ease-out, transform 0.1s ease-out;
  pointer-events: ${props => props.visible ? 'auto' : 'none'};

  .ant-card-body {
    padding: 16px;
  }

  /* Force immediate display */
  &.instant-show {
    opacity: 1 !important;
    transform: translateX(0) !important;
    transition: none !important;
  }
`;

const ProgressHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 12px;
`;

const ProgressTitle = styled(Text)`
  font-weight: 600;
  color: #1e293b;
`;

const RepoProgress = styled.div`
  margin-bottom: 8px;
  padding: 8px;
  background: #f8fafc;
  border-radius: 6px;
  border-left: 3px solid ${props => 
    props.status === 'completed' ? '#10b981' : 
    props.status === 'error' ? '#ef4444' : '#3b82f6'
  };
`;

const SyncProgressNotification = ({ 
  visible, 
  onClose, 
  totalRepos = 0, 
  completedRepos = 0, 
  currentRepo = '', 
  repoProgresses = [], 
  overallProgress = 0 
}) => {
  const [autoClose, setAutoClose] = useState(false);
  const [showInstantly, setShowInstantly] = useState(false);
  const [forceInstantShow, setForceInstantShow] = useState(false);
  // Show immediately when visible becomes true - using useLayoutEffect for immediate DOM update
  useLayoutEffect(() => {
    if (visible) {
      setShowInstantly(true);
      setForceInstantShow(true);
    }
  }, [visible]);

  useEffect(() => {
    if (visible) {
      // Reset force instant after a tiny delay to allow normal transitions
      const timer = setTimeout(() => setForceInstantShow(false), 50);
      return () => clearTimeout(timer);
    } else {
      setForceInstantShow(false);
      // Delay hiding for animation
      const timer = setTimeout(() => setShowInstantly(false), 200);
      return () => clearTimeout(timer);
    }
  }, [visible]);

  useEffect(() => {
    if (completedRepos === totalRepos && totalRepos > 0) {
      setAutoClose(true);
      const timer = setTimeout(() => {
        onClose();
      }, 3000); // T·ª± ƒë·ªông ƒë√≥ng sau 3 gi√¢y
      return () => clearTimeout(timer);
    }
  }, [completedRepos, totalRepos, onClose]);
  
  // Render even if not visible for smooth transitions
  if (!showInstantly && !visible) return null;

  const isCompleted = completedRepos === totalRepos && totalRepos > 0;
  const hasErrors = repoProgresses.some(repo => repo.status === 'error');
  return (
    <ProgressContainer 
      visible={visible} 
      className={forceInstantShow ? 'instant-show' : (visible ? 'show' : '')}
    >
      <ProgressHeader>
        <ProgressTitle>
          {isCompleted ? '‚úÖ ƒê·ªìng b·ªô ho√†n th√†nh' : 'üîÑ ƒêang ƒë·ªìng b·ªô repository'}
        </ProgressTitle>
        <Button 
          type="text" 
          size="small" 
          icon={<CloseOutlined />} 
          onClick={onClose}
        />
      </ProgressHeader>

      {/* Overall Progress */}
      <div style={{ marginBottom: 16 }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', marginBottom: 4 }}>
          <Text type="secondary">T·ªïng ti·∫øn tr√¨nh</Text>
          <Text strong>{Math.round(overallProgress)}%</Text>
        </div>
        <Progress 
          percent={overallProgress} 
          strokeColor={isCompleted ? '#10b981' : '#3b82f6'}
          showInfo={false}
          size="small"
        />
        <Text type="secondary" style={{ fontSize: '12px' }}>
          {completedRepos}/{totalRepos} repository
        </Text>
      </div>

      {/* Current Repository */}
      {currentRepo && !isCompleted && (
        <div style={{ marginBottom: 12 }}>
          <Text type="secondary" style={{ fontSize: '12px' }}>ƒêang x·ª≠ l√Ω:</Text>
          <div style={{ 
            background: '#e0f2fe', 
            padding: '4px 8px', 
            borderRadius: '4px',
            marginTop: '4px'
          }}>
            <Text style={{ fontSize: '12px', color: '#0369a1' }}>{currentRepo}</Text>
          </div>
        </div>
      )}

      {/* Repository List (hi·ªÉn th·ªã khi c√≥ nhi·ªÅu repo) */}
      {repoProgresses.length > 0 && (
        <div style={{ maxHeight: '200px', overflowY: 'auto' }}>
          {repoProgresses.slice(-5).map((repo, index) => (
            <RepoProgress key={index} status={repo.status}>
              <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                <Text style={{ fontSize: '11px', fontWeight: 500 }}>
                  {repo.name}
                </Text>
                <Space size={4}>
                  {repo.status === 'completed' && <CheckCircleOutlined style={{ color: '#10b981' }} />}
                  {repo.status === 'error' && <ExclamationCircleOutlined style={{ color: '#ef4444' }} />}
                  <Text style={{ fontSize: '10px' }}>
                    {repo.status === 'completed' ? '‚úì' : 
                     repo.status === 'error' ? '‚úó' : '...'}
                  </Text>
                </Space>
              </div>
              {repo.progress !== undefined && repo.status === 'syncing' && (
                <Progress 
                  percent={repo.progress} 
                  size="small" 
                  showInfo={false}
                  strokeColor="#3b82f6"
                  style={{ marginTop: 4 }}
                />
              )}
            </RepoProgress>
          ))}
        </div>
      )}

      {/* Summary */}
      {isCompleted && (
        <div style={{ 
          background: hasErrors ? '#fef3c7' : '#d1fae5', 
          padding: '8px', 
          borderRadius: '6px',
          marginTop: '12px'
        }}>
          <Text style={{ 
            fontSize: '12px', 
            color: hasErrors ? '#92400e' : '#047857'
          }}>
            {hasErrors 
              ? `Ho√†n th√†nh v·ªõi ${repoProgresses.filter(r => r.status === 'error').length} l·ªói`
              : 'T·∫•t c·∫£ repository ƒë√£ ƒë∆∞·ª£c ƒë·ªìng b·ªô th√†nh c√¥ng!'
            }
          </Text>
          {autoClose && (
            <div style={{ marginTop: 4 }}>
              <Text style={{ fontSize: '10px', color: '#6b7280' }}>
                T·ª± ƒë·ªông ƒë√≥ng sau 3 gi√¢y...
              </Text>
            </div>
          )}
        </div>
      )}
    </ProgressContainer>
  );
};

export default SyncProgressNotification;

```

### frontend\src\components\Dashboard\AIInsightWidget.jsx
```jsx
import React from 'react';
import { Card, Space, Typography, Button, Tag } from 'antd';
import { BulbOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Title, Text } = Typography;

// Styled components
const InsightContainer = styled(Card)`
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  background: #ffffff;
  transition: all 0.3s ease;

  &:hover {
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.12);
    transform: translateY(-2px);
  }
`;

const InsightCard = styled(Card).withConfig({
  shouldForwardProp: (prop) => !['borderColor'].includes(prop),
})`
  border-radius: 8px;
  border: 1px solid ${(props) => props.borderColor || '#f0f0f0'};
  background: #fff;
  transition: all 0.3s ease;
  padding: 12px;

  &:hover {
    border-color: ${(props) => props.borderColor || '#d9d9d9'};
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  }

  @media (max-width: 576px) {
    padding: 8px;
  }
`;

const IconWrapper = styled.div.withConfig({
  shouldForwardProp: (prop) => !['bgColor'].includes(prop),
})`
  display: flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background: ${(props) => props.bgColor || '#f0f0f0'};
`;

const ActionWrapper = styled.div`
  display: flex;
  justify-content: flex-end;
  gap: 8px;

  @media (max-width: 576px) {
    justify-content: flex-start;
    margin-top: 8px;
  }
`;

const AIInsightWidget = () => {
  const insights = [
    {
      id: 1,
      type: 'suggestion',
      title: 'Ph√¢n c√¥ng ƒë·ªÅ xu·∫•t',
      description: 'Th√™m 2 developer v√†o repo "frontend" ƒë·ªÉ ƒë·∫£m b·∫£o deadline 25/04/2025.',
    },
    {
      id: 2,
      type: 'warning',
      title: 'D·ª± ƒëo√°n ti·∫øn ƒë·ªô',
      description: 'Repo "backend" c√≥ nguy c∆° tr·ªÖ h·∫°n 3 ng√†y. Xem x√©t tƒÉng t√†i nguy√™n.',
    },
  ];

  const getInsightStyle = (type) => {
    switch (type) {
      case 'suggestion':
        return {
          icon: <BulbOutlined style={{ fontSize: 20, color: '#1890ff' }} />,
          tag: <Tag color="blue">ƒê·ªÅ xu·∫•t</Tag>,
          borderColor: '#e6f7ff',
          iconBg: '#e6f7ff',
        };
      case 'warning':
        return {
          icon: <WarningOutlined style={{ fontSize: 20, color: '#fa8c16' }} />,
          tag: <Tag color="orange">C·∫£nh b√°o</Tag>,
          borderColor: '#fff7e6',
          iconBg: '#fff7e6',
        };
      default:
        return {
          icon: null,
          tag: null,
          borderColor: '#f0f0f0',
          iconBg: '#f0f0f0',
        };
    }
  };
  return (
    <InsightContainer
      title={<Title level={4} style={{ margin: 0 }}>G·ª£i √Ω AI</Title>}
      variant="outlined"
    >
      <Space direction="vertical" size="middle" style={{ width: '100%' }}>
        {insights.map((item) => {
          const { icon, tag, borderColor, iconBg } = getInsightStyle(item.type);
          return (
            <InsightCard key={item.id} borderColor={borderColor}>
              <Space direction="horizontal" size="middle" style={{ width: '100%', alignItems: 'center' }}>
                <IconWrapper bgColor={iconBg}>{icon}</IconWrapper>
                <Space direction="vertical" size={4} style={{ flex: 1 }}>
                  <Space>
                    <Title level={5} style={{ margin: 0 }}>{item.title}</Title>
                    {tag}
                  </Space>
                  <Text type="secondary">{item.description}</Text>
                </Space>
                <ActionWrapper>
                  <Button type="primary" size="small">Th·ª±c hi·ªán</Button>
                  <Button size="small">B·ªè qua</Button>
                </ActionWrapper>
              </Space>
            </InsightCard>
          );
        })}
      </Space>
    </InsightContainer>
  );
};

export default AIInsightWidget;
```

### frontend\src\components\Dashboard\OverviewCard.jsx
```jsx
import React from 'react';
import { Card, Statistic, Space, Row, Col } from 'antd';
import { ProjectOutlined, CheckCircleOutlined, WarningOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const SidebarOverviewCard = styled(Card)`
  .ant-card-body {
    padding: 16px;
  }
`;

const StatisticItem = styled.div`
  padding: 12px;
  border-radius: 8px;
  background: #f8fafc;
  border: 1px solid #e2e8f0;
  margin-bottom: 12px;
  transition: all 0.2s ease;

  &:hover {
    border-color: #3b82f6;
    transform: translateY(-1px);
  }

  &:last-child {
    margin-bottom: 0;
  }
`;

const OverviewCard = ({ projects = 10, completedTasks = 50, overdueTasks = 5, sidebar = false }) => {
  if (sidebar) {
    return (
      <SidebarOverviewCard 
        title="T·ªïng quan d·ª± √°n" 
        variant="outlined"
        size="small"
      >
        <Space direction="vertical" style={{ width: '100%' }} size={0}>
          <StatisticItem>
            <Statistic
              title="S·ªë d·ª± √°n"
              value={projects}
              prefix={<ProjectOutlined />}
              valueStyle={{ color: '#1890ff', fontSize: '18px' }}
            />
          </StatisticItem>
          <StatisticItem>
            <Statistic
              title="C√¥ng vi·ªác ho√†n th√†nh"
              value={completedTasks}
              prefix={<CheckCircleOutlined />}
              valueStyle={{ color: '#52c41a', fontSize: '18px' }}
            />
          </StatisticItem>
          <StatisticItem>
            <Statistic
              title="C√¥ng vi·ªác tr·ªÖ h·∫°n"
              value={overdueTasks}
              prefix={<WarningOutlined />}
              valueStyle={{ color: '#ff4d4f', fontSize: '18px' }}
            />
          </StatisticItem>
        </Space>
      </SidebarOverviewCard>
    );
  }  // Layout ngang cho desktop th∆∞·ªùng
  return (
    <Card title="T·ªïng quan d·ª± √°n" variant="outlined">
      <Row gutter={16}>
        <Col span={8}>
          <Statistic
            title="S·ªë d·ª± √°n"
            value={projects}
            prefix={<ProjectOutlined />}
            valueStyle={{ color: '#1890ff' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="C√¥ng vi·ªác ho√†n th√†nh"
            value={completedTasks}
            prefix={<CheckCircleOutlined />}
            valueStyle={{ color: '#52c41a' }}
          />
        </Col>
        <Col span={8}>
          <Statistic
            title="C√¥ng vi·ªác tr·ªÖ h·∫°n"
            value={overdueTasks}
            prefix={<WarningOutlined />}
            valueStyle={{ color: '#ff4d4f' }}
          />
        </Col>
      </Row>
    </Card>
  );
};

export default OverviewCard;
```

### frontend\src\components\Dashboard\ProjectTaskManager.jsx
```jsx
import React, { useState, useEffect, useCallback } from 'react';
import { 
  Card, 
  Select, 
  List, 
  Button, 
  Modal, 
  Input, 
  DatePicker, 
  Tag, 
  Avatar, 
  Space,
  Tooltip,
  message,
  Empty,
  Spin,
  Row,
  Col,
  Statistic,
  Progress,
  Badge,
  Dropdown,
  Menu,
  Form
} from 'antd';
import { 
  PlusOutlined, 
  UserOutlined, 
  CalendarOutlined, 
  EditOutlined,
  DeleteOutlined,
  CheckCircleOutlined,
  ClockCircleOutlined,
  ExclamationCircleOutlined,
  SearchOutlined,
  FilterOutlined,
  BarChartOutlined,
  ReloadOutlined,
  UnorderedListOutlined,
  AppstoreOutlined
} from '@ant-design/icons';
import styled from 'styled-components';
import axios from 'axios';
import RepoSelector from './ProjectTaskManager/RepoSelector';
import StatisticsPanel from './ProjectTaskManager/StatisticsPanel';
import FiltersPanel from './ProjectTaskManager/FiltersPanel';
import TaskList from './ProjectTaskManager/TaskList';
import TaskModal from './ProjectTaskManager/TaskModal';
import KanbanBoard from './ProjectTaskManager/KanbanBoard';

const TaskCard = styled(Card)`
  margin-bottom: 12px;
  border-radius: 8px;
  transition: all 0.3s ease;
  
  &:hover {
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    transform: translateY(-2px);
  }
`;

const TaskHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 8px;
`;

const TaskActions = styled.div`
  display: flex;
  gap: 8px;
`;

const ProjectTaskManager = () => {
  const [repositories, setRepositories] = useState([]);
  const [selectedRepo, setSelectedRepo] = useState(null);
  const [tasks, setTasks] = useState([]);
  const [filteredTasks, setFilteredTasks] = useState([]);
  const [collaborators, setCollaborators] = useState([]);
  const [loading, setLoading] = useState(false);
  const [tasksLoading, setTasksLoading] = useState(false);
  const [isModalVisible, setIsModalVisible] = useState(false);
  const [editingTask, setEditingTask] = useState(null);
  const [form] = Form.useForm();
    // Filter states
  const [searchText, setSearchText] = useState('');
  const [statusFilter, setStatusFilter] = useState('all');
  const [priorityFilter, setPriorityFilter] = useState('all');
  const [assigneeFilter, setAssigneeFilter] = useState('all');

  // View state - true for Kanban, false for List
  const [viewMode, setViewMode] = useState(true); // Default to Kanban

  const fetchRepositories = useCallback(async () => {
    const token = localStorage.getItem('access_token');
    if (!token) return;

    try {
      setLoading(true);
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: { Authorization: `token ${token}` },
      });
      setRepositories(response.data || []);
    } catch (error) {
      console.error('L·ªói khi t·∫£i repositories:', error);
      message.error('Kh√¥ng th·ªÉ t·∫£i danh s√°ch repository!');
    } finally {
      setLoading(false);
    }
  }, []);

  const fetchTasks = useCallback(async () => {
    if (!selectedRepo) return;

    try {
      setTasksLoading(true);
      const response = await axios.get(
        `http://localhost:8000/api/projects/${selectedRepo.owner.login}/${selectedRepo.name}/tasks`,
        {
          headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
        }
      );
      setTasks(response.data || []);
      console.log('Tasks fetched from API:', response.data);
    } catch {
      console.log('API ch∆∞a c√≥ d·ªØ li·ªáu, s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫´u cho tasks');
      // D·ªØ li·ªáu m·∫´u khi API ch∆∞a c√≥ - s·ª≠ d·ª•ng owner th·∫≠t c·ªßa repo
      const ownerLogin = selectedRepo.owner.login;
      setTasks([
        {
          id: 1,
          title: 'Fix authentication bug',
          description: 'S·ª≠a l·ªói ƒëƒÉng nh·∫≠p kh√¥ng th√†nh c√¥ng trong h·ªá th·ªëng',
          assignee: ownerLogin,
          status: 'todo',
          priority: 'high',
          due_date: '2025-06-20',
          created_at: '2025-06-10'
        },
        {
          id: 2,
          title: 'Update documentation',
          description: 'C·∫≠p nh·∫≠t t√†i li·ªáu h∆∞·ªõng d·∫´n API v√† README',
          assignee: ownerLogin,
          status: 'in_progress',
          priority: 'medium',
          due_date: '2025-06-25',
          created_at: '2025-06-11'
        },
        {
          id: 3,
          title: 'Optimize database queries',
          description: 'T·ªëi ∆∞u h√≥a c√°c truy v·∫•n database ƒë·ªÉ c·∫£i thi·ªán performance',
          assignee: ownerLogin,
          status: 'todo',
          priority: 'low',
          due_date: '2025-06-30',
          created_at: '2025-06-12'
        }
      ]);
    } finally {
      setTasksLoading(false);
    }
  }, [selectedRepo]);
  const fetchCollaborators = useCallback(async () => {
    if (!selectedRepo) return;

    try {
      // Th·ª≠ l·∫•y contributors t·ª´ GitHub API (√≠t b·ªã h·∫°n ch·∫ø h∆°n collaborators)
      const token = localStorage.getItem('access_token');
      const response = await axios.get(
        `https://api.github.com/repos/${selectedRepo.owner.login}/${selectedRepo.name}/contributors`,
        {
          headers: { 
            Authorization: `token ${token}`,
            Accept: 'application/vnd.github.v3+json'
          },
        }
      );
      
      // Th√™m owner v√†o ƒë·∫ßu danh s√°ch
      const ownerData = {
        login: selectedRepo.owner.login,
        avatar_url: selectedRepo.owner.avatar_url,
        type: 'Owner',
        contributions: 0
      };
      
      // L·ªçc v√† format contributors
      const contributors = response.data.slice(0, 10).map(contributor => ({
        login: contributor.login,
        avatar_url: contributor.avatar_url,
        type: contributor.login === selectedRepo.owner.login ? 'Owner' : 'Contributor',
        contributions: contributor.contributions
      }));
      
      // ƒê·∫£m b·∫£o owner lu√¥n ·ªü ƒë·∫ßu danh s√°ch
      const uniqueCollaborators = [
        ownerData,
        ...contributors.filter(c => c.login !== selectedRepo.owner.login)
      ];
      
      setCollaborators(uniqueCollaborators);
      console.log(`‚úÖ Loaded ${uniqueCollaborators.length} contributors for ${selectedRepo.name}`);
      
    } catch (error) {
      console.log('Kh√¥ng th·ªÉ l·∫•y contributors t·ª´ GitHub API, s·ª≠ d·ª•ng fallback:', error.message);
      
      // Fallback: hi·ªÉn th·ªã owner v√† m·ªôt s·ªë th√†nh vi√™n t·ª´ backend n·∫øu c√≥
      try {
        const backupResponse = await axios.get(
          `http://localhost:8000/api/github/${selectedRepo.owner.login}/${selectedRepo.name}/collaborators`,
          {
            headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
          }
        );
        setCollaborators(backupResponse.data || []);
      } catch {
        // Last fallback: ch·ªâ hi·ªÉn th·ªã owner
        setCollaborators([
          {
            login: selectedRepo.owner.login,
            avatar_url: selectedRepo.owner.avatar_url,
            type: 'Owner',
            contributions: 0
          }
        ]);
      }
    }
  }, [selectedRepo]);

  // Load repositories khi component mount
  useEffect(() => {
    fetchRepositories();
  }, [fetchRepositories]);

  // Load tasks v√† collaborators khi ch·ªçn repo
  useEffect(() => {
    if (selectedRepo) {
      fetchTasks();
      fetchCollaborators();
    }
  }, [selectedRepo, fetchTasks, fetchCollaborators]);

  const handleRepoChange = (repoId) => {
    const repo = repositories.find(r => r.id === repoId);
    setSelectedRepo(repo);
    setTasks([]);
  };

  const showTaskModal = (task = null) => {
    setEditingTask(task);
    setIsModalVisible(true);
    
    if (task) {
      form.setFieldsValue({
        title: task.title,
        description: task.description,
        assignee: task.assignee,
        priority: task.priority,
        dueDate: task.due_date ? new Date(task.due_date) : null
      });
    } else {
      form.resetFields();
    }
  };

  const handleTaskSubmit = async (values) => {
    try {
      const taskData = {
        ...values,
        due_date: values.dueDate ? values.dueDate.format('YYYY-MM-DD') : null,
        status: editingTask ? editingTask.status : 'todo',
        repo_owner: selectedRepo.owner.login,
        repo_name: selectedRepo.name
      };

      if (editingTask) {
        // Update task via API
        try {
          await axios.put(
            `http://localhost:8000/api/projects/${selectedRepo.owner.login}/${selectedRepo.name}/tasks/${editingTask.id}`,
            taskData,
            {
              headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
            }
          );
          // Refresh tasks from server
          await fetchTasks();
          message.success('C·∫≠p nh·∫≠t task th√†nh c√¥ng!');
        } catch (apiError) {
          console.log('API call failed, using local update:', apiError);
          // Fallback to local update
          const updatedTasks = tasks.map(task => 
            task.id === editingTask.id ? { ...task, ...taskData } : task
          );
          setTasks(updatedTasks);
          message.success('C·∫≠p nh·∫≠t task th√†nh c√¥ng (local)!');
        }
      } else {
        // Create new task via API
        try {
          await axios.post(
            `http://localhost:8000/api/projects/${selectedRepo.owner.login}/${selectedRepo.name}/tasks`,
            taskData,
            {
              headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
            }
          );
          // Refresh tasks from server
          await fetchTasks();
          message.success('T·∫°o task m·ªõi th√†nh c√¥ng!');
        } catch (apiError) {
          console.log('API call failed, using local creation:', apiError);
          // Fallback to local creation
          const newTask = {
            id: Date.now(),
            ...taskData,
            created_at: new Date().toISOString().split('T')[0]
          };
          setTasks([...tasks, newTask]);
          message.success('T·∫°o task m·ªõi th√†nh c√¥ng (local)!');
        }
      }

      setIsModalVisible(false);
      form.resetFields();
    } catch (formError) {
      console.error('Form submission error:', formError);
      message.error('L·ªói khi l∆∞u task!');
    }
  };  const updateTaskStatus = async (taskId, newStatus) => {
    try {
      console.log(`Updating task ${taskId} to status ${newStatus}`);
      const taskToUpdate = tasks.find(t => t.id === taskId);
      if (!taskToUpdate) {
        console.error(`Task with ID ${taskId} not found`);
        return;
      }

      const updatedTaskData = { ...taskToUpdate, status: newStatus };
      
      try {
        await axios.put(
          `http://localhost:8000/api/projects/${selectedRepo.owner.login}/${selectedRepo.name}/tasks/${taskId}`,
          updatedTaskData,
          {
            headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
          }
        );
        console.log('Task updated successfully via API');
        // Refresh tasks from server
        await fetchTasks();
        message.success('C·∫≠p nh·∫≠t tr·∫°ng th√°i th√†nh c√¥ng!');
      } catch (apiError) {
        console.log('API call failed, using local update:', apiError);
        // Fallback to local update
        const updatedTasks = tasks.map(task => 
          task.id === taskId ? { ...task, status: newStatus } : task
        );
        console.log('Updated tasks locally:', updatedTasks);
        setTasks(updatedTasks);
        message.success('C·∫≠p nh·∫≠t tr·∫°ng th√°i th√†nh c√¥ng (local)!');
      }
    } catch (error) {
      console.error('Error updating task status:', error);
      message.error('L·ªói khi c·∫≠p nh·∫≠t tr·∫°ng th√°i!');
    }
  };

  const deleteTask = async (taskId) => {
    try {
      try {
        await axios.delete(
          `http://localhost:8000/api/projects/${selectedRepo.owner.login}/${selectedRepo.name}/tasks/${taskId}`,
          {
            headers: { Authorization: `token ${localStorage.getItem('access_token')}` },
          }
        );
        // Refresh tasks from server
        await fetchTasks();
        message.success('X√≥a task th√†nh c√¥ng!');
      } catch (apiError) {
        console.log('API call failed, using local delete:', apiError);
        // Fallback to local delete
        const updatedTasks = tasks.filter(task => task.id !== taskId);
        setTasks(updatedTasks);
        message.success('X√≥a task th√†nh c√¥ng (local)!');
      }
    } catch (error) {
      console.error('Error deleting task:', error);
      message.error('L·ªói khi x√≥a task!');
    }
  };

  const getStatusIcon = (status) => {
    switch (status) {
      case 'todo': return <ClockCircleOutlined style={{ color: '#faad14' }} />;
      case 'in_progress': return <ExclamationCircleOutlined style={{ color: '#1890ff' }} />;
      case 'done': return <CheckCircleOutlined style={{ color: '#52c41a' }} />;
      default: return <ClockCircleOutlined />;
    }
  };

  const getPriorityColor = (priority) => {
    switch (priority) {
      case 'high': return '#f5222d';
      case 'medium': return '#fa8c16';
      case 'low': return '#52c41a';
      default: return '#d9d9d9';
    }
  };

  const getAssigneeInfo = (assigneeLogin) => {
    return collaborators.find(c => c.login === assigneeLogin) || 
           { login: assigneeLogin, avatar_url: null };
  };

  // Filter and search functions
  const applyFilters = useCallback(() => {
    let filtered = [...tasks];
    
    // Search filter
    if (searchText) {
      filtered = filtered.filter(task => 
        task.title.toLowerCase().includes(searchText.toLowerCase()) ||
        task.description?.toLowerCase().includes(searchText.toLowerCase()) ||
        task.assignee.toLowerCase().includes(searchText.toLowerCase())
      );
    }
    
    // Status filter
    if (statusFilter !== 'all') {
      filtered = filtered.filter(task => task.status === statusFilter);
    }
    
    // Priority filter
    if (priorityFilter !== 'all') {
      filtered = filtered.filter(task => task.priority === priorityFilter);
    }
    
    // Assignee filter
    if (assigneeFilter !== 'all') {
      filtered = filtered.filter(task => task.assignee === assigneeFilter);
    }
    
    setFilteredTasks(filtered);
  }, [tasks, searchText, statusFilter, priorityFilter, assigneeFilter]);

  // Statistics calculation
  const getTaskStats = useCallback(() => {
    const total = tasks.length;
    const completed = tasks.filter(t => t.status === 'done').length;
    const inProgress = tasks.filter(t => t.status === 'in_progress').length;
    const todo = tasks.filter(t => t.status === 'todo').length;
    const highPriority = tasks.filter(t => t.priority === 'high').length;
    
    return {
      total,
      completed,
      inProgress,
      todo,
      highPriority,
      completionRate: total > 0 ? Math.round((completed / total) * 100) : 0
    };
  }, [tasks]);

  // Apply filters whenever dependencies change
  useEffect(() => {
    applyFilters();
  }, [applyFilters]);  return (
    <Card 
      title="üéØ Qu·∫£n l√Ω Task D·ª± √°n" 
      variant="outlined"
      extra={selectedRepo && (
          <Space>
            <Space.Compact>
              <Button 
                type={viewMode ? "primary" : "default"}
                icon={<AppstoreOutlined />}
                onClick={() => setViewMode(true)}
              >
                Kanban
              </Button>
              <Button 
                type={!viewMode ? "primary" : "default"}
                icon={<UnorderedListOutlined />}
                onClick={() => setViewMode(false)}
              >
                List
              </Button>
            </Space.Compact>
            <Button 
              type="primary" 
              icon={<PlusOutlined />}
              onClick={() => showTaskModal()}
            >
              Th√™m Task
            </Button>
          </Space>
        )
      }
    >
      <div style={{ marginBottom: 16 }}>
        <RepoSelector 
          repositories={repositories}
          selectedRepo={selectedRepo}
          loading={loading}
          handleRepoChange={handleRepoChange}
        />
      </div>
      {selectedRepo && (
        <>
          <div style={{ marginBottom: 16, padding: 12, background: '#f5f5f5', borderRadius: 8 }}>
            <Space>
              <Avatar src={selectedRepo.owner.avatar_url} />
              <div>
                <strong>{selectedRepo.owner.login}/{selectedRepo.name}</strong>
                <div style={{ fontSize: 12, color: '#666' }}>
                  {selectedRepo.description || 'Kh√¥ng c√≥ m√¥ t·∫£'}
                </div>
              </div>
            </Space>
          </div>
          <StatisticsPanel stats={getTaskStats()} />
          <Card size="small" style={{ marginBottom: 16 }}>
            <FiltersPanel
              searchText={searchText}
              setSearchText={setSearchText}
              statusFilter={statusFilter}
              setStatusFilter={setStatusFilter}
              priorityFilter={priorityFilter}
              setPriorityFilter={setPriorityFilter}
              assigneeFilter={assigneeFilter}
              setAssigneeFilter={setAssigneeFilter}
              collaborators={collaborators}
              fetchTasks={fetchTasks}
              tasksLoading={tasksLoading}
              filteredTasks={filteredTasks}
            />          </Card>
          
          {/* Task View - Kanban or List */}
          {viewMode ? (
            <KanbanBoard
              tasks={filteredTasks}
              getAssigneeInfo={getAssigneeInfo}
              getPriorityColor={getPriorityColor}
              showTaskModal={showTaskModal}
              deleteTask={deleteTask}
              updateTaskStatus={updateTaskStatus}
            />
          ) : (
            <TaskList
              filteredTasks={filteredTasks}
              tasksLoading={tasksLoading}
              getAssigneeInfo={getAssigneeInfo}
              getStatusIcon={getStatusIcon}
              getPriorityColor={getPriorityColor}
              updateTaskStatus={updateTaskStatus}
              showTaskModal={showTaskModal}
              deleteTask={deleteTask}
            />
          )}
        </>
      )}
      <TaskModal
        isModalVisible={isModalVisible}
        editingTask={editingTask}
        form={form}
        handleTaskSubmit={handleTaskSubmit}
        setIsModalVisible={setIsModalVisible}
        collaborators={collaborators}
      />
    </Card>
  );
};

export default ProjectTaskManager;
```

### frontend\src\components\Dashboard\RepoListFilter.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col, Input, Select, Button } from 'antd';
import { SearchOutlined } from '@ant-design/icons';

const { Option } = Select;

const RepoListFilter = ({ onFilterChange }) => {
  const [searchText, setSearchText] = useState('');
  const [status, setStatus] = useState('all');
  const [assignee, setAssignee] = useState('all');

  const handleApplyFilter = () => {
    onFilterChange({ searchText, status, assignee });
  };

  return (
    <Card title="B·ªô l·ªçc Repository" bordered={false}>
      <Row gutter={16}>
        <Col span={8}>
          <Input
            placeholder="T√¨m ki·∫øm repo"
            prefix={<SearchOutlined />}
            value={searchText}
            onChange={(e) => setSearchText(e.target.value)}
          />
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={status}
            onChange={(value) => setStatus(value)}
            placeholder="Tr·∫°ng th√°i"
          >
            <Option value="all">T·∫•t c·∫£</Option>
            <Option value="active">ƒêang ho·∫°t ƒë·ªông</Option>
            <Option value="archived">ƒê√£ l∆∞u tr·ªØ</Option>
          </Select>
        </Col>
        <Col span={6}>
          <Select
            style={{ width: '100%' }}
            value={assignee}
            onChange={(value) => setAssignee(value)}
            placeholder="Ng∆∞·ªùi ph·ª• tr√°ch"
          >
            <Option value="all">T·∫•t c·∫£</Option>
            <Option value="user1">User 1</Option>
            <Option value="user2">User 2</Option>
          </Select>
        </Col>
        <Col span={4}>
          <Button type="primary" onClick={handleApplyFilter}>
            √Åp d·ª•ng
          </Button>
        </Col>
      </Row>
    </Card>
  );
};

export default RepoListFilter;
```

### frontend\src\components\Dashboard\TaskBoard.jsx
```jsx
import React, { useState } from 'react';
import { Card, Row, Col } from 'antd';
import { DndContext, closestCenter } from '@dnd-kit/core';
import { SortableContext, useSortable, arrayMove } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { Task } from '../../utils/types';

const SortableTask = ({ task }) => {
  const { attributes, listeners, setNodeRef, transform, transition } = useSortable({ id: task.id });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition,
    marginBottom: 8,
  };

  return (
    <Card ref={setNodeRef} style={style} {...attributes} {...listeners}>
      <p>{task.title}</p>
      <p>Ng∆∞·ªùi ph·ª• tr√°ch: {task.assignee}</p>
    </Card>
  );
};

const TaskBoard = ({ initialTasks = [] }) => {
  const [tasks, setTasks] = useState(initialTasks);

  const onDragEnd = (event) => {
    const { active, over } = event;
    if (active.id !== over.id) {
      setTasks((items) => {
        const oldIndex = items.findIndex((item) => item.id === active.id);
        const newIndex = items.findIndex((item) => item.id === over.id);
        return arrayMove(items, oldIndex, newIndex);
      });
    }
  };

  const columns = {
    todo: { title: 'Ch·ªù x·ª≠ l√Ω', tasks: tasks.filter((task) => task.status === 'todo') },
    inProgress: { title: 'ƒêang th·ª±c hi·ªán', tasks: tasks.filter((task) => task.status === 'inProgress') },
    done: { title: 'Ho√†n th√†nh', tasks: tasks.filter((task) => task.status === 'done') },
  };

  return (
    <Card title="B·∫£ng c√¥ng vi·ªác" bordered={false}>
      <DndContext collisionDetection={closestCenter} onDragEnd={onDragEnd}>
        <Row gutter={16}>
          {Object.keys(columns).map((columnId) => (            <Col span={8} key={columnId}>
              <Card title={columns[columnId].title} variant="outlined">
                <SortableContext items={columns[columnId].tasks.map((task) => task.id)}>
                  {columns[columnId].tasks.map((task) => (
                    <SortableTask key={task.id} task={task} />
                  ))}
                </SortableContext>
              </Card>
            </Col>
          ))}
        </Row>
      </DndContext>
    </Card>
  );
};

export default TaskBoard;
```

### frontend\src\components\Dashboard\ProjectTaskManager\DragOverlayContent.jsx
```jsx
// DragOverlayContent.jsx
import React from 'react';
import { Tag } from 'antd';
import { truncateDescription } from './kanbanUtils';
import styles from './KanbanBoard.module.css';

const DragOverlayContent = ({ activeTask, getPriorityColor }) => {
  if (!activeTask) return null;

  return (
    <div className={styles.dragOverlay}>
      <div className={styles.dragOverlayContent}>
        <div className={styles.dragOverlayTitle}>
          {activeTask.title}
        </div>
        {activeTask.description && (
          <div className={styles.dragOverlayDescription}>
            {truncateDescription(activeTask.description)}
          </div>
        )}
        <div className={styles.dragOverlayFooter}>
          <Tag 
            color={getPriorityColor(activeTask.priority)} 
            className={styles.dragOverlayTag}
          >
            {activeTask.priority?.toUpperCase()}
          </Tag>
          <span className={styles.dragOverlayId}>
            #{activeTask.id}
          </span>
        </div>
      </div>
    </div>
  );
};

export default DragOverlayContent;

```

### frontend\src\components\Dashboard\ProjectTaskManager\DroppableColumn.jsx
```jsx
// DroppableColumn.jsx
import React from 'react';
import { useDroppable } from '@dnd-kit/core';
import styles from './KanbanBoard.module.css';

const DroppableColumn = ({ columnId, children }) => {
  const { setNodeRef, isOver } = useDroppable({
    id: columnId,
  });

  return (
    <div 
      ref={setNodeRef} 
      className={`${styles.tasksContainer} ${isOver ? styles.dragOver : ''}`}
    >
      {children}
    </div>
  );
};

export default DroppableColumn;

```

### frontend\src\components\Dashboard\ProjectTaskManager\FiltersPanel.jsx
```jsx
import React from 'react';
import { Row, Col, Select, Button, Space, Badge, Input, Avatar } from 'antd';
import { ReloadOutlined, FilterOutlined, SearchOutlined } from '@ant-design/icons';

const { Option } = Select;
const { Search } = Input;

const FiltersPanel = ({
  searchText,
  setSearchText,
  statusFilter,
  setStatusFilter,
  priorityFilter,
  setPriorityFilter,
  assigneeFilter,
  setAssigneeFilter,
  collaborators,
  fetchTasks,
  tasksLoading,
  filteredTasks
}) => (
  <Row gutter={16} align="middle">
    <Col span={6}>
      <Search
        placeholder="T√¨m ki·∫øm tasks..."
        value={searchText}
        onChange={(e) => setSearchText(e.target.value)}
        prefix={<SearchOutlined />}
        allowClear
      />
    </Col>
    <Col span={4}>
      <Select 
        placeholder="Tr·∫°ng th√°i"
        value={statusFilter}
        onChange={setStatusFilter}
        style={{ width: '100%' }}
      >
        <Option value="all">T·∫•t c·∫£</Option>
        <Option value="todo">Ch∆∞a b·∫Øt ƒë·∫ßu</Option>
        <Option value="in_progress">ƒêang l√†m</Option>
        <Option value="done">Ho√†n th√†nh</Option>
      </Select>
    </Col>
    <Col span={4}>
      <Select 
        placeholder="ƒê·ªô ∆∞u ti√™n"
        value={priorityFilter}
        onChange={setPriorityFilter}
        style={{ width: '100%' }}
      >
        <Option value="all">T·∫•t c·∫£</Option>
        <Option value="high">Cao</Option>
        <Option value="medium">Trung b√¨nh</Option>
        <Option value="low">Th·∫•p</Option>
      </Select>
    </Col>
    <Col span={4}>
      <Select 
        placeholder="Ng∆∞·ªùi th·ª±c hi·ªán"
        value={assigneeFilter}
        onChange={setAssigneeFilter}
        style={{ width: '100%' }}
      >
        <Option value="all">T·∫•t c·∫£</Option>
        {collaborators.map(collab => (
          <Option key={collab.login} value={collab.login}>
            <Space>
              <Avatar src={collab.avatar_url} size="small" />
              {collab.login}
            </Space>
          </Option>
        ))}
      </Select>
    </Col>
    <Col span={6}>
      <Space>
        <Button 
          icon={<ReloadOutlined />}
          onClick={fetchTasks}
          disabled={tasksLoading}
        >
          L√†m m·ªõi
        </Button>
        <Badge count={filteredTasks.length} showZero>
          <Button icon={<FilterOutlined />}>
            K·∫øt qu·∫£ l·ªçc
          </Button>
        </Badge>
      </Space>
    </Col>
  </Row>
);

export default FiltersPanel;

```

### frontend\src\components\Dashboard\ProjectTaskManager\index.js
```js
// Export all ProjectTaskManager components
export { default as RepoSelector } from './RepoSelector';
export { default as StatisticsPanel } from './StatisticsPanel';
export { default as FiltersPanel } from './FiltersPanel';
export { default as TaskList } from './TaskList';
export { default as TaskModal } from './TaskModal';
export { default as TaskCard } from './TaskCard';

```

### frontend\src\components\Dashboard\ProjectTaskManager\KanbanBoard.jsx
```jsx
// KanbanBoard.jsx
import React from 'react';
import { Typography } from 'antd';
import { DndContext, closestCenter, DragOverlay } from '@dnd-kit/core';
import { SortableContext, verticalListSortingStrategy } from '@dnd-kit/sortable';

// Import custom modules
import { COLUMN_CONFIG } from './kanbanConstants';
import { useKanbanDragDrop } from './useKanbanDragDrop';
import { getTasksByStatus } from './kanbanUtils';
import DroppableColumn from './DroppableColumn';
import SortableTaskCard from './SortableTaskCard';
import DragOverlayContent from './DragOverlayContent';

// Import CSS Module
import styles from './KanbanBoard.module.css';

const { Title } = Typography;

const KanbanBoard = ({ 
  tasks, 
  getAssigneeInfo, 
  getPriorityColor, 
  showTaskModal, 
  deleteTask, 
  updateTaskStatus 
}) => {
  // Use custom hook for drag & drop logic
  const {
    sensors,
    activeTask,
    handleDragStart,
    handleDragEnd,
    dropAnimation
  } = useKanbanDragDrop({ 
    tasks, 
    updateTaskStatus, 
    columns: COLUMN_CONFIG 
  });

  return (
    <DndContext
      sensors={sensors}
      collisionDetection={closestCenter}
      onDragStart={handleDragStart}
      onDragEnd={handleDragEnd}
    >
      <div className={styles.kanbanContainer}>
        {COLUMN_CONFIG.map(column => {
          const columnTasks = getTasksByStatus(tasks, column.id);
          const IconComponent = column.icon;
          
          return (
            <div key={column.id} className={styles.kanbanColumn}>
              <div className={`${styles.columnHeader} ${styles[column.cssClass + 'Border']}`}>
                <Title 
                  level={5}
                  className={`${styles.columnTitle} ${styles[column.cssClass + 'Color']}`}
                >
                  <IconComponent />
                  {column.title}
                </Title>
                <div className={`${styles.taskCount} ${styles[column.cssClass + 'Bg']}`}>
                  {columnTasks.length}
                </div>
              </div>

              <DroppableColumn columnId={column.id}>
                <SortableContext 
                  items={columnTasks.map(task => task.id)}
                  strategy={verticalListSortingStrategy}
                >
                  {columnTasks.map(task => (
                    <SortableTaskCard
                      key={task.id}
                      task={task}
                      getAssigneeInfo={getAssigneeInfo}
                      getPriorityColor={getPriorityColor}
                      showTaskModal={showTaskModal}
                      deleteTask={deleteTask}
                    />
                  ))}
                </SortableContext>
                
                {/* Empty state when no tasks */}
                {columnTasks.length === 0 && (
                  <div className={styles.emptyState}>
                    <IconComponent />
                    <span>K√©o task v√†o ƒë√¢y</span>
                  </div>
                )}
              </DroppableColumn>
            </div>
          );
        })}
      </div>      {/* Drag Overlay */}
      <DragOverlay 
        adjustScale={false}
        dropAnimation={dropAnimation}
        modifiers={[]}
        style={{
          cursor: 'grabbing',
          zIndex: 1000,
          transformOrigin: '0 0',
        }}
      >
        <div style={{ 
          transform: 'translate(-10px, -10px)', // ƒêi·ªÅu ch·ªânh v·ªã tr√≠ g·∫ßn con tr·ªè h∆°n
        }}>
          <DragOverlayContent 
            activeTask={activeTask}
            getPriorityColor={getPriorityColor}
          />
        </div>
      </DragOverlay>
    </DndContext>
  );
};

export default KanbanBoard;
```

### frontend\src\components\Dashboard\ProjectTaskManager\KanbanBoard_new.jsx
```jsx

```

### frontend\src\components\Dashboard\ProjectTaskManager\kanbanConstants.js
```js
// kanbanConstants.js
import { ClockCircleOutlined, ExclamationCircleOutlined, CheckCircleOutlined } from '@ant-design/icons';

export const COLUMN_CONFIG = [
  {
    id: 'todo',
    title: 'To Do',
    icon: ClockCircleOutlined,
    color: '#faad14',
    bgColor: '#faad14',
    borderColor: '#faad14',
    cssClass: 'todo'
  },
  {
    id: 'in_progress',
    title: 'In Progress',
    icon: ExclamationCircleOutlined,
    color: '#1890ff',
    bgColor: '#1890ff',
    borderColor: '#1890ff',
    cssClass: 'inProgress'
  },
  {
    id: 'done',
    title: 'Done',
    icon: CheckCircleOutlined,
    color: '#52c41a',
    bgColor: '#52c41a',
    borderColor: '#52c41a',
    cssClass: 'done'
  }
];

export const DRAG_CONFIG = {
  ACTIVATION_DISTANCE: 0,
  DROP_ANIMATION: {
    duration: 200,
    easing: 'cubic-bezier(0.18, 0.67, 0.6, 1.22)',
  }
};

export const TASK_CARD_CONFIG = {
  DESCRIPTION_MAX_LENGTH: 35,
  AVATAR_SIZE: 24
};

```

### frontend\src\components\Dashboard\ProjectTaskManager\kanbanUtils.js
```js
// kanbanUtils.js
import { TASK_CARD_CONFIG } from './kanbanConstants';

/**
 * L·ªçc tasks theo status
 */
export const getTasksByStatus = (tasks, status) => {
  return tasks.filter(task => task.status === status);
};

/**
 * Truncate description 
 */
export const truncateDescription = (description) => {
  if (!description) return '';
  
  return description.length > TASK_CARD_CONFIG.DESCRIPTION_MAX_LENGTH
    ? description.substring(0, TASK_CARD_CONFIG.DESCRIPTION_MAX_LENGTH) + '...'
    : description;
};

/**
 * Format date cho display
 */
export const formatDate = (dateString) => {
  if (!dateString) return null;
  
  return new Date(dateString).toLocaleDateString('vi-VN', { 
    month: 'short', 
    day: 'numeric' 
  });
};

/**
 * Format full date cho tooltip
 */
export const formatFullDate = (dateString) => {
  if (!dateString) return '';
  
  return new Date(dateString).toLocaleDateString('vi-VN');
};

```

### frontend\src\components\Dashboard\ProjectTaskManager\RepoSelector.jsx
```jsx
import React from 'react';
import { Select, Avatar, Space, Tag } from 'antd';

const { Option } = Select;

const RepoSelector = ({ repositories, selectedRepo, loading, handleRepoChange }) => (
  <Select
    style={{ width: '100%' }}
    placeholder="Ch·ªçn repository ƒë·ªÉ qu·∫£n l√Ω tasks"
    loading={loading}
    value={selectedRepo?.id}
    onChange={handleRepoChange}
    showSearch
    optionFilterProp="children"
  >
    {repositories.map(repo => (
      <Option key={repo.id} value={repo.id}>
        <Space>
          <Avatar src={repo.owner.avatar_url} size="small" />
          {repo.owner.login}/{repo.name}
          <Tag color={repo.private ? 'red' : 'green'}>
            {repo.private ? 'Private' : 'Public'}
          </Tag>
        </Space>
      </Option>
    ))}
  </Select>
);

export default RepoSelector;

```

### frontend\src\components\Dashboard\ProjectTaskManager\SortableTaskCard.jsx
```jsx
// SortableTaskCard.jsx
import React from 'react';
import { Card, Avatar, Tag, Space, Typography, Button, Tooltip } from 'antd';
import { EditOutlined, DeleteOutlined, UserOutlined, CalendarOutlined } from '@ant-design/icons';
import { useSortable } from '@dnd-kit/sortable';
import { CSS } from '@dnd-kit/utilities';
import { formatDate, formatFullDate } from './kanbanUtils';
import { TASK_CARD_CONFIG } from './kanbanConstants';
import styles from './KanbanBoard.module.css';

const { Text } = Typography;

const SortableTaskCard = ({ 
  task, 
  getAssigneeInfo, 
  getPriorityColor, 
  showTaskModal, 
  deleteTask 
}) => {  const {
    attributes,
    listeners,
    setNodeRef,
    transform,
    transition,
    isDragging,
  } = useSortable({ 
    id: task.id,
  });

  const style = {
    transform: CSS.Transform.toString(transform),
    transition: transition || 'transform 200ms cubic-bezier(0.18, 0.67, 0.6, 1.22)',
    opacity: isDragging ? 0.5 : 1,
    zIndex: isDragging ? 999 : 'auto',
    cursor: isDragging ? 'grabbing' : 'grab',
    touchAction: 'none',
  };

  const assigneeInfo = getAssigneeInfo(task.assignee);

  return (    <Card
      ref={setNodeRef}
      style={style}
      {...attributes}
      {...listeners}
      className={`${styles.taskCard} ${isDragging ? styles.isDragging : ''}`}
      styles={{ body: { padding: 12 } }}
    >
      <div className={styles.taskCardHeader}>
        <div className={styles.taskTitle}>{task.title}</div>
        <div className={styles.taskActions}>
          <Tooltip title="Ch·ªânh s·ª≠a">
            <Button
              type="text"
              size="small"
              icon={<EditOutlined />}
              onClick={(e) => {
                e.stopPropagation();
                showTaskModal(task);
              }}
            />
          </Tooltip>
          <Tooltip title="X√≥a">
            <Button
              type="text"
              size="small"
              danger
              icon={<DeleteOutlined />}
              onClick={(e) => {
                e.stopPropagation();
                deleteTask(task.id);
              }}
            />
          </Tooltip>
        </div>
      </div>

      {task.description && (
        <Text className={styles.taskDescription}>
          {task.description}
        </Text>
      )}

      <div className={styles.taskMeta}>
        <Space size="small">
          <Tag color={getPriorityColor(task.priority)} style={{ margin: 0 }}>
            {task.priority?.toUpperCase()}
          </Tag>
          {task.due_date && (
            <Tooltip title={`H·∫°n: ${formatFullDate(task.due_date)}`}>
              <div style={{ display: 'flex', alignItems: 'center', gap: 4, color: '#666' }}>
                <CalendarOutlined style={{ fontSize: 12 }} />
                <Text style={{ fontSize: 11, color: '#666' }}>
                  {formatDate(task.due_date)}
                </Text>
              </div>
            </Tooltip>
          )}
        </Space>
      </div>

      <div className={styles.taskFooter}>
        <Space>
          <Avatar 
            size={TASK_CARD_CONFIG.AVATAR_SIZE} 
            src={assigneeInfo.avatar_url} 
            icon={<UserOutlined />}
          />
          <Text style={{ fontSize: 12, color: '#666' }}>
            {assigneeInfo.login}
          </Text>
        </Space>
        <Text style={{ fontSize: 11, color: '#999' }}>
          #{task.id}
        </Text>
      </div>
    </Card>
  );
};

export default SortableTaskCard;

```

### frontend\src\components\Dashboard\ProjectTaskManager\StatisticsPanel.jsx
```jsx
import React from 'react';
import { Card, Row, Col, Statistic, Progress } from 'antd';
import { BarChartOutlined, CheckCircleOutlined, ExclamationCircleOutlined } from '@ant-design/icons';

const StatisticsPanel = ({ stats }) => (
  <div style={{ marginBottom: 16 }}>
    <Row gutter={16}>
      <Col span={6}>
        <Card size="small">
          <Statistic 
            title="T·ªïng tasks" 
            value={stats.total}
            prefix={<BarChartOutlined />}
          />
        </Card>
      </Col>
      <Col span={6}>
        <Card size="small">
          <Statistic 
            title="Ho√†n th√†nh" 
            value={stats.completed}
            valueStyle={{ color: '#52c41a' }}
            prefix={<CheckCircleOutlined />}
          />
        </Card>
      </Col>
      <Col span={6}>
        <Card size="small">
          <Statistic 
            title="ƒêang l√†m" 
            value={stats.inProgress}
            valueStyle={{ color: '#1890ff' }}
            prefix={<ExclamationCircleOutlined />}
          />
        </Card>
      </Col>
      <Col span={6}>
        <Card size="small">
          <Statistic 
            title="T·ª∑ l·ªá ho√†n th√†nh" 
            value={stats.completionRate}
            suffix="%"
            valueStyle={{ color: stats.completionRate > 70 ? '#52c41a' : '#fa8c16' }}
          />
          <Progress 
            percent={stats.completionRate} 
            showInfo={false}
            size="small"
            strokeColor={stats.completionRate > 70 ? '#52c41a' : '#fa8c16'}
          />
        </Card>
      </Col>
    </Row>
  </div>
);

export default StatisticsPanel;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskCard.jsx
```jsx
import React from 'react';
import { Card, Space, Button, Tooltip, Tag, Avatar, Select } from 'antd';
import { EditOutlined, DeleteOutlined, UserOutlined, CalendarOutlined } from '@ant-design/icons';
import styled from 'styled-components';

const { Option } = Select;

const TaskHeader = styled.div`
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 8px;
`;

const TaskActions = styled.div`
  display: flex;
  gap: 8px;
`;

const PriorityTag = styled(Tag)`
  font-weight: 500;
`;

const TaskCard = ({
  task,
  getAssigneeInfo,
  getStatusIcon,
  getPriorityColor,
  updateTaskStatus,
  showTaskModal,
  deleteTask
}) => {
  const assigneeInfo = getAssigneeInfo(task.assignee);
  return (
    <Card size="small">
      <TaskHeader>
        <Space>
          {getStatusIcon(task.status)}
          <strong>{task.title}</strong>
          <PriorityTag color={getPriorityColor(task.priority)}>
            {task.priority?.toUpperCase()}
          </PriorityTag>
        </Space>
        <TaskActions>
          <Tooltip title="Ch·ªânh s·ª≠a">
            <Button 
              size="small" 
              icon={<EditOutlined />}
              onClick={() => showTaskModal(task)}
            />
          </Tooltip>
          <Tooltip title="X√≥a">
            <Button 
              size="small" 
              danger
              icon={<DeleteOutlined />}
              onClick={() => deleteTask(task.id)}
            />
          </Tooltip>
        </TaskActions>
      </TaskHeader>
      <div style={{ marginBottom: 8 }}>
        {task.description}
      </div>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
        <Space>
          <Avatar 
            src={assigneeInfo.avatar_url} 
            icon={<UserOutlined />}
            size="small"
          />
          <div style={{ display: 'flex', flexDirection: 'column' }}>
            <span style={{ fontSize: 12, fontWeight: 500 }}>{assigneeInfo.login}</span>
            {assigneeInfo.type && (
              <Tag 
                size="small" 
                color={assigneeInfo.type === 'Owner' ? 'gold' : 'blue'}
                style={{ fontSize: '9px', marginTop: 2 }}
              >
                {assigneeInfo.type}
              </Tag>
            )}
          </div>
        </Space>
        <Space>
          {task.due_date && (
            <Space style={{ fontSize: 12, color: '#666' }}>
              <CalendarOutlined />
              {task.due_date}
            </Space>
          )}
          <Select 
            size="small"
            value={task.status}
            onChange={(newStatus) => updateTaskStatus(task.id, newStatus)}
            style={{ width: 100 }}
          >
            <Option value="todo">To Do</Option>
            <Option value="in_progress">ƒêang l√†m</Option>
            <Option value="done">Ho√†n th√†nh</Option>
          </Select>
        </Space>
      </div>
    </Card>
  );
};

export default TaskCard;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskList.jsx
```jsx
import React from 'react';
import { List, Empty, Spin } from 'antd';
import TaskCard from './TaskCard';

const TaskList = ({ filteredTasks, tasksLoading, getAssigneeInfo, getStatusIcon, getPriorityColor, updateTaskStatus, showTaskModal, deleteTask }) => (
  <Spin spinning={tasksLoading}>
    {filteredTasks.length === 0 ? (
      <Empty 
        description="Ch∆∞a c√≥ task n√†o cho repository n√†y"
        image={Empty.PRESENTED_IMAGE_SIMPLE}
      />
    ) : (
      <List
        dataSource={filteredTasks}
        renderItem={task => (
          <TaskCard
            task={task}
            getAssigneeInfo={getAssigneeInfo}
            getStatusIcon={getStatusIcon}
            getPriorityColor={getPriorityColor}
            updateTaskStatus={updateTaskStatus}
            showTaskModal={showTaskModal}
            deleteTask={deleteTask}
          />
        )}
      />
    )}
  </Spin>
);

export default TaskList;

```

### frontend\src\components\Dashboard\ProjectTaskManager\TaskModal.jsx
```jsx
import React from 'react';
import { Modal, Form, Input, Select, DatePicker, Button, Space, Avatar, Tag } from 'antd';

const { Option } = Select;
const { TextArea } = Input;

const TaskModal = ({
  isModalVisible,
  editingTask,
  form,
  handleTaskSubmit,
  setIsModalVisible,
  collaborators
}) => (
  <Modal
    title={editingTask ? "Ch·ªânh s·ª≠a Task" : "T·∫°o Task M·ªõi"}
    open={isModalVisible}
    onCancel={() => setIsModalVisible(false)}
    footer={null}
  >
    <Form
      form={form}
      layout="vertical"
      onFinish={handleTaskSubmit}
    >
      <Form.Item
        name="title"
        label="Ti√™u ƒë·ªÅ"
        rules={[{ required: true, message: 'Vui l√≤ng nh·∫≠p ti√™u ƒë·ªÅ!' }]}
      >
        <Input placeholder="Nh·∫≠p ti√™u ƒë·ªÅ task" />
      </Form.Item>
      <Form.Item
        name="description"
        label="M√¥ t·∫£"
        rules={[{ required: true, message: 'Vui l√≤ng nh·∫≠p m√¥ t·∫£!' }]}
      >
        <TextArea rows={3} placeholder="M√¥ t·∫£ chi ti·∫øt task" />
      </Form.Item>
      <Form.Item
        name="assignee"
        label="Giao cho"
        rules={[{ required: true, message: 'Vui l√≤ng ch·ªçn ng∆∞·ªùi th·ª±c hi·ªán!' }]}
      >
        <Select 
          placeholder="Ch·ªçn th√†nh vi√™n"
          showSearch
          optionFilterProp="children"
          filterOption={(input, option) =>
            option.children.props.children[1].toLowerCase().indexOf(input.toLowerCase()) >= 0
          }
        >
          {collaborators.map(collab => (
            <Option key={collab.login} value={collab.login}>
              <Space>
                <Avatar src={collab.avatar_url} size="small" />
                <span>{collab.login}</span>
                {collab.type === 'Owner' && <Tag color="gold">Owner</Tag>}
                {collab.type === 'Contributor' && <Tag color="blue">Contributor</Tag>}
                {collab.contributions > 0 && (
                  <Tag color="green" style={{ fontSize: '10px' }}>
                    {collab.contributions} commits
                  </Tag>
                )}
              </Space>
            </Option>
          ))}
        </Select>
      </Form.Item>
      <Form.Item
        name="priority"
        label="ƒê·ªô ∆∞u ti√™n"
        rules={[{ required: true, message: 'Vui l√≤ng ch·ªçn ƒë·ªô ∆∞u ti√™n!' }]}
      >
        <Select placeholder="Ch·ªçn ƒë·ªô ∆∞u ti√™n">
          <Option value="low">
            <Tag color="#52c41a">Th·∫•p</Tag>
          </Option>
          <Option value="medium">
            <Tag color="#fa8c16">Trung b√¨nh</Tag>
          </Option>
          <Option value="high">
            <Tag color="#f5222d">Cao</Tag>
          </Option>
        </Select>
      </Form.Item>
      <Form.Item
        name="dueDate"
        label="H·∫°n ho√†n th√†nh"
      >
        <DatePicker style={{ width: '100%' }} />
      </Form.Item>
      <Form.Item>
        <Space style={{ width: '100%', justifyContent: 'flex-end' }}>
          <Button onClick={() => setIsModalVisible(false)}>
            H·ªßy
          </Button>
          <Button type="primary" htmlType="submit">
            {editingTask ? 'C·∫≠p nh·∫≠t' : 'T·∫°o m·ªõi'}
          </Button>
        </Space>
      </Form.Item>
    </Form>
  </Modal>
);

export default TaskModal;

```

### frontend\src\components\Dashboard\ProjectTaskManager\useKanbanDragDrop.js
```js
// useKanbanDragDrop.js
import { useState } from 'react';
import { useSensor, useSensors, PointerSensor, KeyboardSensor } from '@dnd-kit/core';
import { DRAG_CONFIG } from './kanbanConstants';

export const useKanbanDragDrop = ({ tasks, updateTaskStatus, columns }) => {
  const [activeId, setActiveId] = useState(null);
  // C·∫•u h√¨nh sensors ƒë·ªÉ cursor g·∫ßn h∆°n v·ªõi task
  const sensors = useSensors(
    useSensor(PointerSensor, {
      activationConstraint: {
        distance: DRAG_CONFIG.ACTIVATION_DISTANCE,
      },
      // ƒêi·ªÅu ch·ªânh ƒë·ªÉ cursor g·∫ßn task h∆°n
      coordinateGetter: (event) => ({
        x: event.clientX,
        y: event.clientY,
      }),
    }),
    useSensor(KeyboardSensor)
  );

  const handleDragStart = (event) => {
    setActiveId(event.active.id);
  };
  const handleDragEnd = (event) => {
    const { active, over } = event;
    setActiveId(null);
    
    if (!over) return;

    const draggedTaskId = active.id;
    const targetId = over.id;

    try {
      // T√¨m task ƒëang ƒë∆∞·ª£c k√©o
      const draggedTask = tasks.find(t => t.id === draggedTaskId);
      if (!draggedTask) {
        console.error(`Dragged task with ID ${draggedTaskId} not found`);
        return;
      }

      // Ki·ªÉm tra xem target c√≥ ph·∫£i l√† column kh√¥ng
      const targetColumn = columns.find(col => col.id === targetId);
      
      if (targetColumn) {
        // K√©o v√†o column
        if (draggedTask.status !== targetColumn.id) {
          console.log(`Moving task ${draggedTaskId} to column ${targetColumn.id}`);
          updateTaskStatus(draggedTaskId, targetColumn.id);
        }
      } else {
        // K√©o v√†o task kh√°c - t√¨m column ch·ª©a task ƒë√≥
        const targetTask = tasks.find(t => t.id === targetId);
        if (targetTask && draggedTask.status !== targetTask.status) {
          console.log(`Moving task ${draggedTaskId} to column ${targetTask.status} (via task)`);
          updateTaskStatus(draggedTaskId, targetTask.status);
        }
      }
    } catch (error) {
      console.error('Error in handleDragEnd:', error);
      // Kh√¥ng show message error ƒë·ªÉ tr√°nh l√†m crash UI
    }
  };

  const activeTask = activeId ? tasks.find(t => t.id === activeId) : null;

  return {
    sensors,
    activeId,
    activeTask,
    handleDragStart,
    handleDragEnd,
    dropAnimation: DRAG_CONFIG.DROP_ANIMATION
  };
};

```

### frontend\src\components\repo\RepoList.jsx
```jsx
import { useEffect, useState } from "react";
import { Avatar, Typography, Spin, message, Card, Tag, Pagination } from "antd";
import { useNavigate } from "react-router-dom";
import { GithubOutlined, StarFilled, EyeFilled, ForkOutlined, CalendarOutlined } from "@ant-design/icons";
import styled from "styled-components";
import axios from "axios";

const { Title, Text } = Typography;

const RepoContainer = styled.div`
  max-width: 900px;
  margin: 0 auto;
  padding: 24px;
`;

const RepoCard = styled(Card)`
  margin-bottom: 20px;
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
  transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
  cursor: pointer;
  border: none;
  
  &:hover {
    box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    transform: translateY(-5px);
  }
`;

const RepoHeader = styled.div`
  display: flex;
  align-items: flex-start;
  margin-bottom: 12px;
`;

const RepoTitle = styled.div`
  flex: 1;
  min-width: 0;
`;

const RepoName = styled(Text)`
  display: block;
  font-size: 18px;
  font-weight: 600;
  color: #24292e;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
`;

const RepoDescription = styled(Text)`
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
  color: #586069;
  margin: 8px 0;
`;

const RepoMeta = styled.div`
  display: flex;
  flex-wrap: wrap;
  gap: 16px;
  margin-top: 16px;
  align-items: center;
`;

const MetaItem = styled.div`
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 14px;
  color: #586069;
`;

const StyledPagination = styled(Pagination)`
  margin-top: 32px;
  text-align: center;
  
  .ant-pagination-item-active {
    border-color: #1890ff;
    background: #1890ff;
    
    a {
      color: white;
    }
  }
`;

const HighlightTag = styled(Tag)`
  font-weight: 500;
  border-radius: 12px;
  padding: 0 10px;
`;

const RepoList = () => {
  const [repos, setRepos] = useState([]);
  const [loading, setLoading] = useState(true);
  const [currentPage, setCurrentPage] = useState(1);
  const [totalRepos, setTotalRepos] = useState(0);
  const navigate = useNavigate();
  const pageSize = 8;

  useEffect(() => {
    const fetchRepos = async () => {
      const token = localStorage.getItem("access_token");
      if (!token) return message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");

      try {
        setLoading(true);
        const response = await axios.get("http://localhost:8000/api/github/repos", {
          headers: { Authorization: `token ${token}` },
          params: { sort: 'updated', direction: 'desc' } // S·∫Øp x·∫øp theo m·ªõi nh·∫•t
        });
        
        // S·∫Øp x·∫øp l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o m·ªõi nh·∫•t l√™n ƒë·∫ßu
        const sortedRepos = response.data.sort((a, b) => 
          new Date(b.updated_at) - new Date(a.updated_at)
        );
        
        setRepos(sortedRepos);
        setTotalRepos(sortedRepos.length);
      } catch (error) {
        message.error("Kh√¥ng th·ªÉ t·∫£i danh s√°ch repository!");
        console.error(error);
      } finally {
        setLoading(false);
      }
    };

    fetchRepos();
  }, []);

  const formatDate = (dateString) => {
    return new Date(dateString).toLocaleDateString('vi-VN', {
      day: '2-digit',
      month: '2-digit',
      year: 'numeric'
    });
  };

  const paginatedRepos = repos.slice(
    (currentPage - 1) * pageSize,
    currentPage * pageSize
  );
  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', marginTop: '100px' }}>
        <Spin size="large" />
        <div style={{ marginLeft: 16 }}>
          <Text>ƒêang t·∫£i d·ªØ li·ªáu...</Text>
        </div>
      </div>
    );
  }

  return (
    <RepoContainer>
      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '24px' }}>
        <Title level={2} style={{ margin: 0, color: '#24292e' }}>
          <GithubOutlined style={{ marginRight: '12px', color: '#1890ff' }} />
          GitHub Repositories
        </Title>
        <Text strong style={{ fontSize: '16px' }}>
          T·ªïng c·ªông: {totalRepos} repositories
        </Text>
      </div>

      {paginatedRepos.map((repo) => (
        <RepoCard 
          key={repo.id} 
          onClick={() => navigate(`/repo/${repo.owner.login}/${repo.name}`)}
        >
          <RepoHeader>
            <Avatar 
              src={repo.owner.avatar_url} 
              size={48}
              style={{ marginRight: '16px', flexShrink: 0 }}
            />
            <RepoTitle>
              <div style={{ display: 'flex', alignItems: 'center' }}>
                <RepoName>{repo.name}</RepoName>
                {repo.private ? (
                  <HighlightTag color="error" style={{ marginLeft: '12px' }}>
                    Private
                  </HighlightTag>
                ) : (
                  <HighlightTag color="success" style={{ marginLeft: '12px' }}>
                    Public
                  </HighlightTag>
                )}
              </div>
              
              <RepoDescription type="secondary">
                {repo.description || "Kh√¥ng c√≥ m√¥ t·∫£"}
              </RepoDescription>
            </RepoTitle>
          </RepoHeader>

          <RepoMeta>
            <MetaItem>
              <StarFilled style={{ color: '#ffc53d' }} />
              <Text strong>{repo.stargazers_count}</Text>
              <Text>stars</Text>
            </MetaItem>
            
            <MetaItem>
              <EyeFilled style={{ color: '#1890ff' }} />
              <Text strong>{repo.watchers_count}</Text>
              <Text>watchers</Text>
            </MetaItem>
            
            <MetaItem>
              <ForkOutlined style={{ color: '#73d13d' }} />
              <Text strong>{repo.forks_count}</Text>
              <Text>forks</Text>
            </MetaItem>
            
            {repo.language && (
              <MetaItem>
                <div style={{
                  width: 12,
                  height: 12,
                  borderRadius: '50%',
                  backgroundColor: '#1890ff',
                  marginRight: 6
                }} />
                <Text>{repo.language}</Text>
              </MetaItem>
            )}
            
            <MetaItem style={{ marginLeft: 'auto' }}>
              <CalendarOutlined />
              <Text>C·∫≠p nh·∫≠t: {formatDate(repo.updated_at)}</Text>
            </MetaItem>
          </RepoMeta>
        </RepoCard>
      ))}

      <StyledPagination
        current={currentPage}
        pageSize={pageSize}
        total={totalRepos}
        onChange={(page) => setCurrentPage(page)}
        showSizeChanger={false}
        showQuickJumper
      />
    </RepoContainer>
  );
};

export default RepoList;
```

### frontend\src\contexts\SyncContext.jsx
```jsx

```

### frontend\src\pages\AuthSuccess.jsx
```jsx
// src/pages/AuthSuccess.jsx
import React, { useEffect } from "react";
import { useNavigate, useLocation } from "react-router-dom";
import { message } from "antd";

const AuthSuccess = () => {
  const navigate = useNavigate();
  const location = useLocation();

  useEffect(() => {
    const params = new URLSearchParams(location.search);
    const token = params.get("token");
    const username = params.get("username");
    const email = params.get("email");

    if (token) {
      const profile = {
        token,
        username,
        email,
        avatar_url: params.get("avatar_url"),
      };      localStorage.setItem("github_profile", JSON.stringify(profile));
      localStorage.setItem("access_token", token);

      // Chuy·ªÉn h∆∞·ªõng ngay l·∫≠p t·ª©c, ƒë·ªÉ Dashboard x·ª≠ l√Ω ƒë·ªìng b·ªô
      message.success("ƒêƒÉng nh·∫≠p th√†nh c√¥ng!");
      navigate("/dashboard");
    } else {
      navigate("/login");
    }
  }, [location, navigate]);

  return (
    <div className="h-screen flex items-center justify-center">
      <p className="text-xl">ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu...</p>
    </div>
  );
};

export default AuthSuccess;
```

### frontend\src\pages\Dashboard.jsx
```jsx
import React, { useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button, Typography, Avatar, Card, Grid, Space, Divider, Badge, message, Spin } from 'antd';
import { LogoutOutlined, GithubOutlined, NotificationOutlined } from '@ant-design/icons';
import styled from 'styled-components';
import RepoList from '../components/repo/RepoList';
import OverviewCard from '../components/Dashboard/OverviewCard';
import AIInsightWidget from '../components/Dashboard/AIInsightWidget';
import ProjectTaskManager from '../components/Dashboard/ProjectTaskManager';
import RepoListFilter from '../components/Dashboard/RepoListFilter';
import TaskBoard from '../components/Dashboard/TaskBoard';
import SyncProgressNotification from '../components/common/SyncProgressNotification';
import axios from 'axios';

const { Title, Text } = Typography;
const { useBreakpoint } = Grid;

// Styled components v·ªõi theme hi·ªán ƒë·∫°i
const DashboardContainer = styled.div`
  padding: 24px;
  max-width: 1440px;
  margin: 0 auto;
  background: #f8fafc;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  gap: 24px;

  @media (max-width: 768px) {
    padding: 16px;
    gap: 16px;
  }
`;

const MainLayout = styled.div`
  display: grid;
  grid-template-columns: 280px 1fr;
  gap: 24px;
  min-height: calc(100vh - 200px);

  @media (max-width: 1200px) {
    grid-template-columns: 250px 1fr;
    gap: 16px;
  }

  @media (max-width: 768px) {
    grid-template-columns: 1fr;
    gap: 16px;
  }
`;

const Sidebar = styled.div`
  position: sticky;
  top: 24px;
  height: fit-content;
  display: flex;
  flex-direction: column;
  gap: 16px;

  @media (max-width: 768px) {
    position: static;
    order: 2;
  }
`;

const MainContent = styled.div`
  display: flex;
  flex-direction: column;
  gap: 24px;
  min-width: 0; /* ƒê·ªÉ tr√°nh overflow */
`;

const SidebarCard = styled(Card)`
  border-radius: 12px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);

  .ant-card-body {
    padding: 16px;
  }

  .ant-card-head {
    padding: 12px 16px;
    border-bottom: 1px solid #f1f5f9;
  }
`;

const HeaderCard = styled(Card)`
  border-radius: 16px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  
  .ant-card-body {
    padding: 24px;
  }
`;

const DashboardCard = styled(Card)`
  border-radius: 16px;
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
  transition: all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);
  
  &:hover {
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
    transform: translateY(-2px);
  }

  .ant-card-head {
    border-bottom: 1px solid #f1f5f9;
    padding: 16px 24px;
  }

  .ant-card-body {
    padding: 24px;
  }

  @media (max-width: 768px) {
    .ant-card-body {
      padding: 16px;
    }
  }
`;

const PrimaryButton = styled(Button)`
  border-radius: 8px;
  font-weight: 500;
  height: 40px;
  padding: 0 20px;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const UserInfoContainer = styled.div`
  display: flex;
  align-items: center;
  gap: 16px;
`;

const UserAvatar = styled(Avatar)`
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
  border: 2px solid #ffffff;
`;

const ContentSection = styled.section`
  display: flex;
  flex-direction: column;
  gap: 24px;
`;

const SectionTitle = styled(Title)`
  margin-bottom: 0 !important;
  font-weight: 600 !important;
  color: #1e293b !important;
  display: flex;
  align-items: center;
  gap: 8px;
`;

const NotificationBadge = styled(Badge)`
  .ant-badge-count {
    background: #3b82f6;
    box-shadow: 0 0 0 1px #fff;
  }
`;

const Dashboard = () => {
  const [user, setUser] = useState(null);
  const [loading] = useState(false);
  const navigate = useNavigate();
  const screens = useBreakpoint();
  // Progress notification states
  const [syncProgress, setSyncProgress] = useState({
    visible: false,
    totalRepos: 0,
    completedRepos: 0,
    currentRepo: '',
    repoProgresses: [],
    overallProgress: 0
  });

  const [isSyncing, setIsSyncing] = useState(false);  const syncAllRepositories = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) {
      message.error('Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!');
      return;
    }

    // Hi·ªÉn th·ªã progress ngay l·∫≠p t·ª©c TR∆Ø·ªöC khi set loading
    setSyncProgress({
      visible: true,
      totalRepos: 0,
      completedRepos: 0,
      currentRepo: 'ƒêang l·∫•y danh s√°ch repository...',
      repoProgresses: [],
      overallProgress: 0
    });

    setIsSyncing(true);

    try {
      // Th√™m timeout nh·ªè ƒë·ªÉ ƒë·∫£m b·∫£o UI render progress tr∆∞·ªõc
      await new Promise(resolve => setTimeout(resolve, 50));
      
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: {
          Authorization: `token ${token}`,
        },
      });

      const repositories = response.data;
      
      // C·∫≠p nh·∫≠t v·ªõi danh s√°ch repository th·ª±c t·∫ø
      setSyncProgress(prev => ({
        ...prev,
        totalRepos: repositories.length,
        currentRepo: 'Chu·∫©n b·ªã ƒë·ªìng b·ªô...',
        repoProgresses: repositories.map(repo => ({
          name: `${repo.owner.login}/${repo.name}`,
          status: 'pending',
          progress: 0
        }))
      }));

      let completedCount = 0;
      
      // ƒê·ªìng b·ªô t·ª´ng repository m·ªôt c√°ch tu·∫ßn t·ª± ƒë·ªÉ tracking d·ªÖ h∆°n
      for (const repo of repositories) {
        const repoName = `${repo.owner.login}/${repo.name}`;
        
        // C·∫≠p nh·∫≠t repository hi·ªán t·∫°i
        setSyncProgress(prev => ({
          ...prev,
          currentRepo: repoName,
          repoProgresses: prev.repoProgresses.map(r => 
            r.name === repoName ? { ...r, status: 'syncing', progress: 0 } : r
          )
        }));

        try {
          // ƒê·ªìng b·ªô repository
          await axios.post(
            `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-all`,
            {},
            {
              headers: {
                Authorization: `token ${token}`,
              },
            }
          );

          completedCount++;
          
          // C·∫≠p nh·∫≠t tr·∫°ng th√°i ho√†n th√†nh
          setSyncProgress(prev => ({
            ...prev,
            completedRepos: completedCount,
            overallProgress: (completedCount / repositories.length) * 100,
            repoProgresses: prev.repoProgresses.map(r => 
              r.name === repoName ? { ...r, status: 'completed', progress: 100 } : r
            )
          }));

        } catch (error) {
          console.error(`L·ªói ƒë·ªìng b·ªô ${repoName}:`, error);
          
          // C·∫≠p nh·∫≠t tr·∫°ng th√°i l·ªói
          setSyncProgress(prev => ({
            ...prev,
            repoProgresses: prev.repoProgresses.map(r => 
              r.name === repoName ? { ...r, status: 'error', progress: 0 } : r
            )
          }));
          
          completedCount++; // V·∫´n t√≠nh l√† completed ƒë·ªÉ ti·∫øp t·ª•c
        }
      }

      message.success('ƒê·ªìng b·ªô t·∫•t c·∫£ repository ho√†n th√†nh!');

    } catch (error) {
      console.error('L·ªói khi l·∫•y danh s√°ch repository:', error);
      message.error('Kh√¥ng th·ªÉ l·∫•y danh s√°ch repository!');
      setSyncProgress(prev => ({ ...prev, visible: false }));
    } finally {
      setIsSyncing(false);
    }
  };
  useEffect(() => {
    const storedProfile = localStorage.getItem('github_profile');
    if (!storedProfile) {
      navigate('/login');
    } else {
      setUser(JSON.parse(storedProfile));
      
      // ƒê·ªìng b·ªô c∆° b·∫£n nhanh ƒë·ªÉ hi·ªÉn th·ªã danh s√°ch repo ngay l·∫≠p t·ª©c
      syncBasicRepositories();
    }
  }, [navigate]);

  // ƒê·ªìng b·ªô c∆° b·∫£n (nhanh) - ch·ªâ th√¥ng tin repo v√† branches
  const syncBasicRepositories = async () => {
    const token = localStorage.getItem('access_token');
    if (!token) return;

    try {
      const response = await axios.get('http://localhost:8000/api/github/repos', {
        headers: { Authorization: `token ${token}` },
      });

      const repositories = response.data;
      message.info(`ƒê·ªìng b·ªô c∆° b·∫£n ${repositories.length} repository...`);
      
      // ƒê·ªìng b·ªô c∆° b·∫£n song song (nhanh h∆°n)
      Promise.all(
        repositories.slice(0, 10).map(repo => // Ch·ªâ ƒë·ªìng b·ªô 10 repo ƒë·∫ßu ti√™n
          axios.post(
            `http://localhost:8000/api/github/${repo.owner.login}/${repo.name}/sync-basic`,
            {},
            { headers: { Authorization: `token ${token}` } }
          ).catch(() => null)
        )
      ).then(() => {
        message.success('ƒê·ªìng b·ªô c∆° b·∫£n ho√†n th√†nh!');
      });

    } catch (error) {
      console.error('L·ªói ƒë·ªìng b·ªô c∆° b·∫£n:', error);
    }
  };

  const handleLogout = () => {
    localStorage.removeItem('github_profile');
    localStorage.removeItem('access_token');
    navigate('/login');
  };

  const handleFilterChange = (filters) => {
    console.log('Applied filters:', filters);
  };

  const handleStatusChange = (taskId, newStatus) => {
    console.log(`Updated task ${taskId} status to ${newStatus}`);
  };

  if (loading) {
    return <Spin tip="ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu..." size="large" />;
  }

  return (
    <DashboardContainer>
      {/* Header Section */}
      <HeaderCard bordered={false}>
        <Space 
          direction={screens.md ? 'horizontal' : 'vertical'} 
          align={screens.md ? 'center' : 'start'}
          style={{ width: '100%', justifyContent: 'space-between' }}
        >
          <UserInfoContainer>
            <UserAvatar src={user?.avatar_url} size={screens.md ? 72 : 56} />
            <div>
              <Title level={4} style={{ margin: 0, color: '#1e293b' }}>
                Welcome back, {user?.username || 'User'}!
              </Title>
              <Text type="secondary" style={{ color: '#64748b' }}>
                {user?.email || 'No email provided'}
              </Text>
            </div>
          </UserInfoContainer>          <Space size={screens.md ? 16 : 8}>
            
            <Button 
              type="default" 
              onClick={syncAllRepositories}
              loading={isSyncing}
              disabled={isSyncing}
              style={{ backgroundColor: '#f8fafc', borderColor: '#e2e8f0' }}
            >
              {isSyncing ? 'ƒêang ƒë·ªìng b·ªô...' : 'ƒê·ªìng b·ªô ƒë·∫ßy ƒë·ªß'}
            </Button>
            
            {/* Test button for instant progress */}
            <Button 
              onClick={() => {
                setSyncProgress({
                  visible: true,
                  totalRepos: 5,
                  completedRepos: 0,
                  currentRepo: 'Test repository...',
                  repoProgresses: [],
                  overallProgress: 0
                });
              }}
              style={{ background: '#10b981', borderColor: '#10b981', color: 'white' }}
            >
              Test Progress
            </Button>
            
            <NotificationBadge count={3} size="small">
              <Button 
                icon={<NotificationOutlined />} 
                shape="circle" 
                style={{ border: 'none' }}
              />
            </NotificationBadge>
            <PrimaryButton 
              type="primary" 
              danger 
              onClick={handleLogout}
              icon={<LogoutOutlined />}
            >
              {screens.md ? 'Log Out' : ''}
            </PrimaryButton>
          </Space>        </Space>
      </HeaderCard>

      {/* Main Layout v·ªõi Sidebar v√† Content */}
      <MainLayout>        {/* Sidebar b√™n tr√°i */}
        <Sidebar>
          {/* Overview Metrics trong Sidebar */}
          <OverviewCard sidebar={true} />
          
          {/* Quick Actions */}
          <SidebarCard 
            title={<SectionTitle level={5} style={{ fontSize: '14px' }}>Thao t√°c nhanh</SectionTitle>}
            size="small"
          >
            <Space direction="vertical" style={{ width: '100%' }} size="small">              <Button 
                type="default" 
                onClick={syncAllRepositories}
                loading={isSyncing}
                disabled={isSyncing}
                block
                size="small"
                style={{ backgroundColor: '#f8fafc', borderColor: '#e2e8f0' }}
              >
                {isSyncing ? 'ƒêang ƒë·ªìng b·ªô...' : 'ƒê·ªìng b·ªô ƒë·∫ßy ƒë·ªß'}
              </Button>
            </Space>
          </SidebarCard>

          {/* Activity Summary */}
          <SidebarCard 
            title={<SectionTitle level={5} style={{ fontSize: '14px' }}>Ho·∫°t ƒë·ªông g·∫ßn ƒë√¢y</SectionTitle>}
            size="small"
          >
            <Space direction="vertical" style={{ width: '100%' }} size="small">
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ Task "Tr√≤ game tƒÉng ƒë·ªô kh√≥" ƒë√£ ho√†n th√†nh
              </div>
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ 2 repositories m·ªõi ƒë∆∞·ª£c ƒë·ªìng b·ªô
              </div>
              <div style={{ fontSize: '12px', color: '#666' }}>
                ‚Ä¢ AI ph√¢n t√≠ch 15 commits m·ªõi
              </div>
            </Space>
          </SidebarCard>
        </Sidebar>

        {/* Main Content b√™n ph·∫£i */}
        <MainContent>
          {/* Project Task Manager - Full Width */}
          <DashboardCard>
            <ProjectTaskManager />
          </DashboardCard>

          {/* Repository Analysis */}
          <DashboardCard 
            title={
              <SectionTitle level={5}>
                <GithubOutlined />
                Repository Analysis
              </SectionTitle>
            }
          >
            <AIInsightWidget />
          </DashboardCard>

          {/* Filters Section */}
          <DashboardCard 
            title={<SectionTitle level={5}>Filters & Settings</SectionTitle>}
          >
            <RepoListFilter onFilterChange={handleFilterChange} />
          </DashboardCard>

          {/* Main Content Sections */}
          <ContentSection>
            <DashboardCard 
              title={
                <SectionTitle level={5}>
                  My Repositories
                  <Text type="secondary" style={{ fontSize: 14, marginLeft: 8 }}>
                    (24 repositories)
                  </Text>
                </SectionTitle>
              }
            >
              <RepoList />
            </DashboardCard>

            <DashboardCard 
              title={<SectionTitle level={5}>Project Tasks</SectionTitle>}
            >
              <TaskBoard onStatusChange={handleStatusChange} />
            </DashboardCard>
          </ContentSection>
        </MainContent>
      </MainLayout>

      {/* Progress Notification */}
      <SyncProgressNotification
        visible={syncProgress.visible}
        onClose={() => setSyncProgress(prev => ({ ...prev, visible: false }))}
        totalRepos={syncProgress.totalRepos}
        completedRepos={syncProgress.completedRepos}
        currentRepo={syncProgress.currentRepo}
        repoProgresses={syncProgress.repoProgresses}
        overallProgress={syncProgress.overallProgress}
      />
    </DashboardContainer>
  );
};

export default Dashboard;
```

### frontend\src\pages\Login.jsx
```jsx
// src/pages/Login.jsx
import React from "react";
import { Button, Card, Typography } from "antd";
import { GithubOutlined } from "@ant-design/icons";

const { Title } = Typography;

const Login = () => {
  const handleGitHubLogin = () => {
    window.location.href = "http://localhost:8000/api/login"; // backend redirect to GitHub OAuth
  };

  return (
    <div className="h-screen flex items-center justify-center bg-gradient-to-br from-gray-100 to-white">
      <Card
        className="shadow-xl rounded-2xl w-full max-w-md"
        style={{ textAlign: "center", padding: "3rem 2rem" }}
      >
        <Title level={2} style={{ marginBottom: "2rem" }}>
          ƒêƒÉng nh·∫≠p v√†o <span style={{ color: "#1890ff" }}>TaskFlowAI</span>
        </Title>
        <Button
          type="primary"
          icon={<GithubOutlined />}
          size="large"
          onClick={handleGitHubLogin}
          style={{
            backgroundColor: "#000",
            borderColor: "#000",
            width: "100%",
          }}
        >
          ƒêƒÉng nh·∫≠p v·ªõi GitHub
        </Button>
      </Card>
    </div>
  );
};

export default Login;
```

### frontend\src\pages\RepoDetails.jsx
```jsx
import { useEffect, useState, useCallback } from "react";
import { useParams } from "react-router-dom";
import { message, Button, Card, Typography, Alert, Progress } from "antd";
import { SyncOutlined, SaveOutlined, GithubOutlined } from "@ant-design/icons";
import BranchSelector from "../components/Branchs/BranchSelector";
import CommitList from "../components/commits/CommitList";
import axios from "axios";

const { Title, Text } = Typography;

const RepoDetails = () => {
  const { owner, repo } = useParams();
  const [branch, setBranch] = useState("");
  const [loading, setLoading] = useState(false);
  const [syncing, setSyncing] = useState(false);
  const [syncProgress, setSyncProgress] = useState(0);

  // Sync repository trong background kh√¥ng block UI
  const syncRepositoryInBackground = useCallback(async () => {
    const token = localStorage.getItem("access_token");
    if (!token || syncing) return;

    try {
      setSyncing(true);
      setSyncProgress(0);
      
      // Hi·ªÉn th·ªã th√¥ng b√°o b·∫Øt ƒë·∫ßu sync
      message.info(`ƒêang ƒë·ªìng b·ªô repository ${repo} trong background...`, 2);
      
      // Sync c∆° b·∫£n tr∆∞·ªõc (nhanh)
      setSyncProgress(30);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-basic`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      
      // Sync ƒë·∫ßy ƒë·ªß
      setSyncProgress(70);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      
      setSyncProgress(100);
      message.success(`ƒê·ªìng b·ªô repository ${repo} th√†nh c√¥ng!`);
      
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô repository:", error);
      message.error("ƒê·ªìng b·ªô repository th·∫•t b·∫°i!");
    } finally {
      setSyncing(false);
      setTimeout(() => setSyncProgress(0), 2000);
    }
  }, [owner, repo, syncing]);

  // Ki·ªÉm tra v√† sync repository trong background
  const checkAndSyncRepository = useCallback(async () => {
    const token = localStorage.getItem("access_token");
    if (!token) return;

    try {
      // Ki·ªÉm tra xem repo ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a
      const checkResponse = await axios.get(
        `http://localhost:8000/api/github/${owner}/${repo}/branches`,
        {
          headers: { Authorization: `token ${token}` },
        }
      );

      // N·∫øu c√≥ d·ªØ li·ªáu r·ªìi th√¨ kh√¥ng c·∫ßn sync
      if (checkResponse.data && checkResponse.data.length > 0) {
        console.log('Repository ƒë√£ c√≥ d·ªØ li·ªáu, kh√¥ng c·∫ßn sync');
        return;
      }
    } catch {
      console.log('Repository ch∆∞a c√≥ d·ªØ li·ªáu, b·∫Øt ƒë·∫ßu sync...');
    }

    // Sync repository trong background
    syncRepositoryInBackground();
  }, [owner, repo, syncRepositoryInBackground]);

  // Load trang ngay l·∫≠p t·ª©c v·ªõi d·ªØ li·ªáu c√≥ s·∫µn
  useEffect(() => {
    // Sync trong background n·∫øu c·∫ßn
    checkAndSyncRepository();
  }, [owner, repo, checkAndSyncRepository]);

  // Sync th·ªß c√¥ng
  const manualSync = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    try {
      setLoading(true);
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/sync-all`,
        {},
        {
          headers: { Authorization: `token ${token}` },
        }
      );
      message.success("ƒê·ªìng b·ªô d·ªØ li·ªáu th√†nh c√¥ng!");
    } catch (error) {
      console.error("L·ªói khi ƒë·ªìng b·ªô d·ªØ li·ªáu:", error);
      message.error("Kh√¥ng th·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu!");
    } finally {
      setLoading(false);
    }
  };

  const saveCommits = async () => {
    const token = localStorage.getItem("access_token");
    if (!token) {
      message.error("Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!");
      return;
    }

    try {
      await axios.post(
        `http://localhost:8000/api/github/${owner}/${repo}/save-commits`,
        { branch },
        {
          headers: {
            Authorization: `token ${token}`,
          },
        }
      );
      message.success("L∆∞u commit th√†nh c√¥ng!");
    } catch (error) {
      console.error("L·ªói khi l∆∞u commit:", error);
      message.error("Kh√¥ng th·ªÉ l∆∞u commit!");
    }
  };

  // Hi·ªÉn th·ªã trang ngay l·∫≠p t·ª©c, kh√¥ng ƒë·ª£i sync
  return (
    <div style={{ padding: 24 }}>
      <Card>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: 16 }}>
          <Title level={2} style={{ margin: 0 }}>
            <GithubOutlined /> {owner}/{repo}
          </Title>
          
          <div style={{ display: 'flex', gap: 8, alignItems: 'center' }}>
            {syncing && (
              <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                <Progress 
                  type="circle" 
                  size={24} 
                  percent={syncProgress}
                  showInfo={false}
                />
                <Text type="secondary">ƒêang ƒë·ªìng b·ªô...</Text>
              </div>
            )}
            
            <Button 
              icon={<SyncOutlined />} 
              onClick={manualSync}
              loading={loading}
              disabled={loading || syncing}
            >
              ƒê·ªìng b·ªô th·ªß c√¥ng
            </Button>
            
            <Button 
              type="primary" 
              icon={<SaveOutlined />} 
              onClick={saveCommits}
              disabled={!branch}
            >
              L∆∞u Commit
            </Button>
          </div>
        </div>

        {syncing && (
          <Alert
            message="ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu trong background"
            description="B·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c s·ª≠ d·ª•ng trang n√†y, vi·ªác ƒë·ªìng b·ªô s·∫Ω ho√†n th√†nh trong gi√¢y l√°t."
            type="info"
            showIcon
            style={{ marginBottom: 16 }}
          />
        )}

        <BranchSelector owner={owner} repo={repo} onBranchChange={setBranch} />
      </Card>

      <div style={{ marginTop: 16 }}>
        <CommitList owner={owner} repo={repo} branch={branch} />
      </div>
    </div>
  );
};

export default RepoDetails;

```

### frontend\src\pages\TestPage.jsx
```jsx
import React from 'react';

const TestPage = () => {
  return (
    <div style={{ padding: '20px', background: 'lightblue', minHeight: '100vh' }}>
      <h1>üöÄ Test Page - App ƒëang ho·∫°t ƒë·ªông!</h1>
      <p>Th·ªùi gian: {new Date().toLocaleString()}</p>
      <div>
        <button style={{ padding: '10px', background: 'green', color: 'white', border: 'none', borderRadius: '5px' }}>
          Click me!
        </button>
      </div>
    </div>
  );
};

export default TestPage;

```

### frontend\src\utils\types.js
```js
export const Task = {
  id: '',
  title: '',
  assignee: '',
  status: '', // 'todo', 'inProgress', 'done'
};
```
